

## Two Coin Example

Suppose your brother has a coin which you know to be loaded so that it comes up heads 70% of the time. He then comes to you with some coin, you're not sure which one and he wants to make a bet with you. Betting money that it's going to come up heads.

You're not sure if it's the loaded coin or if it's just a fair one. So he gives you a chance to flip it 5 times to check it out.

You flip it five times and get 2 heads and 3 tails. Which coin do you think it is and how sure are you about that?

We'll start by defining the unknown parameter $\theta$, this is either that the coin is fair or it's a loaded coin.

$$
\theta = \{\text{fair} ,\text{loaded}\}
$$

$$
X \sim Bin(5, ?)
$$

$$
f(x\mid\theta) = \begin{cases} 
      {5 \choose x}(\frac{1}{2})^5            & \theta = \text{fair} \\
      {5 \choose x} (.7)^x (.3)^{5 - x}       & \theta = \text{loaded}\\
   \end{cases}
$$

We can also rewrite $f(x \mid \theta)$ with indicator functions 

$$
f(x\mid\theta) = {5\choose x}(.5)^5I_{\{\theta= \text{fair}\}} + {5 \choose x}(.7)^x(.3)^{5 - x}I_{\{\theta = \text{loaded}\}}
$$

In this case, we observed that $x = 2$ 

$$
f(\theta \mid x = 2) = \begin{cases} 
    0.3125 & \theta = \text{fair} \\
    0.1323 & \theta = \text{loaded}
\end{cases}
$$

 MLE $\hat{\theta} = \text{fair}$

That's a good point estimate, but then how do we answer the question, how sure are you?

This is not a question that's easily answered in the frequentest paradigm. Another question is that we might like to know what is the probability that theta equals fair, give, we observe two heads.

$$
\mathbb{P}r(\theta = \text{fair} \mid x = 2) = ?
$$ In the frequentest paradigm, the coin is a physical quantity. It's a fixed coin, and therefore it has a fixed probability of coining up heads. It is either the fair coin, or it's the loaded coin.

$$
\mathbb{P}r(\theta =  \text{fair}) = \{0,1\}
$$

### Bayesian Approach to the Problem

An advantage of the Bayesian approach is that it allows you to easily incorporate prior information, when you know something in advance of the looking at the data. This is difficult to do under the Frequentest paradigm.

In this case, we're talking about your brother. You probably know him pretty well. So suppose you think that before you've looked at the coin, there's a 60% probability that this is the loaded coin.

This case, we put this into our prior. Our prior is that the probability the coin is loaded is 0.6. We can update our prior with the data to get our posterior beliefs, and we can do this using Bayes theorem.

Prior: $\mathbb{P}r(loaded) = 0.6$

$$
f(\theta\mid x) = \frac{f(x\mid\theta)f(\theta)}{\sum_\theta{f(x\mid\theta)f(\theta)}}
$$

$$
f(\theta \mid x) = \frac{{5\choose x} [(\frac{1}{2})^5(.4)I_{\{\theta = \text{fair}\}} + (.7)^x (.3)^{5-x}(.6)I_{\{\theta = \text{loaded}\}}  ] }
{{5\choose x} [(\frac{1}{2})^5(.4) + (.7)^x (.3)^{5-x}(0.6)  ] }
$$

$$
f(\theta \mid x=2)= \frac{0.0125I_{\{\theta=\text{fair}\}}  + 0.0079I_{\{\theta=\text{loaded}\}} }{0.0125+0.0079}
$$

$$
f(\theta\mid x=2) = 0.612I_{\{\theta=\text{fair}\}} + 0.388I_{\{\theta = \text{loaded}\}}
$$

As you can see in the calculation here, we have the likelihood times the prior in the numerator, and in the denominator, we have a normalizing constant, so that when we divide by this, we'll get answer that add up to one. These numbers match exactly in this case, because it's a very simple problem. But this is a concept that goes on, what's in the denominator here is always a normalizing constant.

$$
\mathbb{P}r(\theta = \text{loaded} \mid x = 2) = 0.388
$$

This here updates our beliefs after seeing some data about what the probability might be.

We can also examine what would happen under different choices of prior.

$$
\mathbb{P}r(\theta = \text{loaded}) = \frac{1}{2} \implies \mathbb{P}r(\theta = \text{loaded} \mid x = 2) = 0.297
$$

$$
\mathbb{P}r(\theta = \text{loaded}) = 0.9 \implies \mathbb{P}r(\theta = \text{loaded} \mid x = 2) = 0.792
$$

In this case, the Bayesian approach is inherently subjective. It represents your own personal perspective, and this is an important part of the paradigm. If you have a different perspective, you will get different answers, and that's okay. It's all done in a mathematically vigorous framework, and it's all mathematically consistent and coherent.

And in the end, we get results that are interpretable

## Continuous Bayes

![Continuous Bayes](images/c1l05-ss-03-continuous-bayes-theorem.png){.column-margin width="200px"}


$$
\begin{aligned}
f(\theta \mid y) &= \frac{f(y \mid \theta)f(\theta)}{f(y)} 
\\ &= \frac{f(y\mid\theta) f(\theta)} {\int{f(y\mid\theta) f(\theta) d\theta}} 
\\ &= \frac{\text{likelihood} \times \text{prior}}{\text{normalization}}
\\ & \propto \text{likelihood} \times text{prior}
\end{aligned}
$$ #{eq-continuous-bayes-theorem-derivation}

In practice, sometimes the integral $\int{f(y\mid\theta) f(\theta) d\theta}$ can be a pain to compute. So, we may prefer to use the proportionality of the likelihood times the prior. And if we can figure out its form we may be able to reintroduce the appropriate normalizing constant on at the end, we don't necessarily have to compute this integral.

::: {#exm-bayesian-coin-toss}
#### Bayesian Coin Toss

So for example, suppose we're looking at a coin and it has unknown probability $\theta$ of coming up heads. Suppose we express ignorance about the value of $\theta$ by assigning it a uniform distribution.

$$
\theta \sim U[0, 1]
$$ {#eq-bayesian-coin-toss-rv}

$$
f(\theta) = I_{\{0 \le \theta\le 1\}}
$$ {#eq-bayesian-coin-toss-uniform-prior}

$$
\begin{aligned}
  f(\theta \mid Y = 1) &= \frac{\theta^1(1-\theta)^0\ \mathbb{I}_{\{0 \le \theta\le1\}}}{\int_{-\infty}^\infty{\theta^1(1-\theta)^0\ \mathbb{I}_{\{0\le \theta \le 1\}}}}
\\ &= \frac{\theta\ \mathbb{I}_{\{0\le\theta\le 1\}}}{\int_0^1{\theta d\theta}}
\\ &= 2\theta\ \mathbb{I}_{\{0\le\theta\le1\}}
\end{aligned}
$$ {#eq-bayesian-coin-toss-integral-approach}

Now if we didn't want to take the integral we could have used this approach:

$$
\begin{aligned}
  f(\theta \mid Y=1) &\propto f(y\mid\theta)\ f(\theta)
  \\ &\propto \theta\ \mathbb{I}_{\{0\le\theta\le1\}} 
  \\ &= \mathbb{I}_{\{0\le\theta\le1\}} 
\end{aligned}
$$ {#eq-bayesian-coin-toss-non-integral-approach}

Which then we need to find the constant such that it's a proper PMF. In this case, it's $2$.
:::

So this is the same type of approach, we get to the same answer. We stick the normalizing constant on at the end, if we can recognize what this is at the end.

In some cases later, this will turn out much easier to just use the proportionality approach rather than a full equality approach and trying to work out the integral.

## Normalizing Constants and Proportionality

The full expression for a posterior distribution of some parameter θ is given by

$$
\frac{f(x\mid\theta) f(\theta)} {\int_{-\infty}^{\infty}{f(x\mid\theta)\ f(\theta)\ d\theta}}
$$

As we will see in coming lessons, it is often more convenient to work with the numerator only: $f(\theta\mid x) \propto f(x\mid\theta)f(\theta)$, which is the likelihood times the prior. The symbol $\propto$ stands for "is proportional to." We can multiply a function of $\theta$ by any constant and maintain proportionality. For example, if $f(\theta) = 5\theta$, then $f(\theta) \propto \theta$. However, $f(\theta)$ is not proportional to $\theta + 1$. We maintain proportionality only by modifying constants which are multiplied by the entire function $f(\theta)$. Hence $5(\theta + 1) \propto \theta + 1$.

The reason we can write $f(\theta\mid x) \propto f(x \mid \theta)f(\theta)$ is because the denominator $\int_{-\infty}^{\infty} f(x\mid\theta)\ f(\theta)\ d\theta$ is free of $\theta$. It is just a normalizing constant. Therefore, we can ignore any multiplicative terms not involving θ. For example, if $θ ∼ N(m, s2)$, then

$$\begin{aligned}
f(\theta) &= \frac{1}{\sqrt{2πs^2}}
exp \left [ − \frac{1}{2s^2}(\theta − m)^2 \right ]
\\ &\propto exp \left [ − \frac{1}{2s^2}(\theta − m)^2 \right ]
\end{aligned}
$$ {#eq-normalizing-constants}

Clearly, the expression in the bottom of @eq-normalizing-constants does not integrate to 1 (it integrates to $\sqrt{2πs^2}$ ). Although it is not a PDF, it is proportional to the $N(m, s^2)$ PDF and can be normalized to represent the $N(m, s^2)$ distribution only. Likewise, the posterior $f(θ\mid x)$ maintains its uniqueness as long as we specify it up to a proportionality constant.

To evaluate posterior quantities such as posterior probabilities, we will eventually need to find the normalizing constant. If the integral required is not tractable, we can often still simulate draws from the posterior and approximate posterior quantities. In some cases, we can identify $f(x\mid θ)f(θ)$ as being proportional to the PDF of some known distribution. This will be a major topic of Lesson 6.

## Bayesian Confidence Intervals

![Bayesian Prior intervals](images/c1l05-ss-04-posterior-intervals.png){.column-margin width="200px"}


### Prior Interval Estimates

Since the prior is a proper PMF, we can perform interval probabilities as well. This is called Prior interval estimates.

$$
\begin{aligned}
\mathbb{P}r(0.025 <\theta < 0.975) &= \int_{0.025}^{0.975}{1 d \theta} \\&= \theta |_{0.025}^{0.975} \\&= 0.975-0.025 \\&= 0.95 
\end{aligned}
$$ {#eq-prior-two-tailed-interval-estimate}

$$
\mathbb{P}r(\theta > 0.05) = 1 - 0.05 = 0.95
$$ {#eq-prior-right-tailed-interval-estimate}

### Posterior Interval Estimates

Since the posterior is a proper PMF, we can perform interval probabilities as well. This is called Posterior interval estimates.

$$
\begin{aligned}
\mathbb{P}r(0.025 < \theta < 0.975) &= \int_{0.025}^{0.975}{2\theta\ d \theta} \\&= \theta^2 |_{0.025}^{0.975} \\&= (0.975)^2 - (0.025)^2 \\&= 0.95
\end{aligned}
$$ {#eq-posterior-two-tailed-interval-estimate}

$$
\mathbb{P}r(\theta > 0.05) = 1 - (0.05)^2 = 0.9975
$$ {#eq-posterior-right-tailed-interval-estimate}

These are the sort of intervals we would get from the prior and then ask what is their posterior probability.

In other cases, we may want to ask, what is the posterior interval of interest? What's an interval that contains 95% of posterior probability in some meaningful way? This would be equivalent then to a frequentest confidence interval.

We can do this in several different ways, two main ways that we make Bayesian Posterior intervals or credible intervals are:

1.  **equal-tailed intervals** and
2.  **highest posterior density intervals** HDPI.

### Equal-tailed Interval Estimates

In the case of an equal-tailed interval, we put the equal amount of probability in each tail. So to make a 95% interval we'll put 0.025 in each tail.

To be able to do this, we're going to have to figure out what the quantiles are. So we're going to need some value, $q$, so that

$$
\mathbb{P}r(\theta < q \mid Y = 1) = \int_0^9{2\theta d\theta} = q^2
$$ {#eq-posterior-ci}

$$
\mathbb{P}r(\sqrt{0.025} < \theta < \sqrt{0.975}) = \mathbb{P}r(0.158 < \theta < 0.987) = 0.95
$$ {#eq-posterior-ci-calculation}

This is an equal tailed interval in that the probability that $\theta$ is less than 0.18 is the same as the probability that $\theta$ is greater than 0.987. We can say that under the posterior, there's a 95% probability that $\theta$ is in this interval.

### Highest Posterior Density (HPD)

Here we want to ask where in the density function is it highest? Theoretically this will be the shortest possible interval that contains the given probability, in this case a 95% probability.

$$
\mathbb{P}r(\theta > \sqrt{0.05} \mid Y = 1) = \mathbb{P}r(\theta > 0.224 \mid Y = 1) = 0.95
$$ {#eq-posterior-HPD}

This is the shortest possible interval, that under the posterior has a probability 0.95. it's $\theta$ going from 0.224 up to 1.

The posterior distribution describes our understanding of our uncertainty combining our prior beliefs and the data. It does this with a probability density function, so at the end of the day, we can make intervals and talk about probabilities of data being in the interval.

This is different from the frequentest approach, where we get confidence intervals. But we can't say a whole lot about the actual parameter relative to the confidence interval. We can only make long run frequency statements about hypothetical intervals.

In this case, we can legitimately say that the posterior probability that $\theta$ is bigger than 0.05 is $0.9975$. We can also say that we believe there's a 95% probability that $\theta$ is in between 0.158 and 0.987.

### Discussion of Bayesians and Frequentist interpretation of CIs

Bayesian represent uncertainty with probabilities, so that the coin itself is a physical quantity. It may have a particular value for $\theta$.

It may be fixed, but because we don't know what that value is, we represent our uncertainty about that value with a distribution. And at the end of the day, we can represent our uncertainty, collect it with the data, and get a posterior distribution and make intuitive statements.

Frequentist confidence intervals have the interpretation that "If you were to repeat many times the process of collecting data and computing a 95% confidence interval, then on average about 95% of those intervals would contain the true parameter value; however, once you observe data and compute an interval the true value is either in the interval or it is not, but you can't tell which."

Bayesian credible intervals have the interpretation that "Your posterior probability that the parameter is in a 95% credible interval is 95%."
