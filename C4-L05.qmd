---
date: 2024-11-05
title: "Bayesian Inference in the AR(p) -  M2L5"
subtitle: Time Series Analysis
description: "The AR(P) process, its state-space representation, the characteristic polynomial, and the forecast function"
categories: 
  - Coursera 
  - notes
  - Bayesian Statistics
  - Autoregressive Models
  - Time Series
keywords: 
  - time series
  - stability
  - order of an AR process 
  - characteristic lag polynomial
  - autocorrelation function
  - ACF
  - partial autocorrelation function
  - PACF
  - smoothing
  - State Space Model
  - ARMA process
  - ARIMA
  - moving average
  - AR(p) process  
  - R code
---

## Bayesian inference in the AR(p): Reference prior, conditional likelihood ðŸŽ¥

![inference for AR(p)](images/m2_0031.png){#fig-ar-p-process .column-margin  group="slides" width="53mm"}


### Model Setup

we start with an AR(p) model as we define in the previous lesson:
$$
y_t = \phi_1 y_{t-1} + \ldots + \phi_p y_{t-p} + \varepsilon_t \qquad \varepsilon_t \stackrel{iid}{\sim} \mathcal{N}(0, v)
$$ {#eq-arp-inf-model}

but now we wish to infer:

- $\phi_i$ the *AR(p)* coefficients and 
- $v$ the innovation variance.

### Conditional likelihood

We make use of the autoregressive structure and rewrite $y_t$ conditionally on the previous $p$ values of the process and the parameters:

$$
(y_t \mid y_{t-1}, \ldots, y_{t-p}, \phi_1, \ldots, \phi_p) \sim \mathcal{N}\left (\sum_{j=1}^{p} \phi_j y_{t-j}, v\right )
$$ {#eq-arp-inf-cond}

We condition on the first $p$ values of the process. 
The conditional distribution of $y_t$ given the previous $p$ values and parameters is normal with mean given by the weighted sum $\sum \phi_j y_{t-j}$ and variance $v$.


the density for the first $p$ observations is given by:
$$
p(y_{(p+1):T}\mid y_{1:p}, \phi_1, \ldots, \phi_p, v) = \prod_{t=p+1}^{T} p(y_t | y_{t-1}, \ldots, y_{t-p}, \phi_1, \ldots, \phi_p, v)
$$ {#eq-arp-inf-full-cond-likelihood}

This product of conditionals yields the full conditional likelihood. 
Each term is Gaussian and independent, given the past values and parameters.

### Regression Formulation

This is recast as a linear regression: response vector $\mathbf{y}$ starting from $y_{p+1}$ to $y_T$, design matrix $\mathbb{X}$ built from lagged values, and $\boldsymbol\beta$ as the AR coefficients $\phi_j$.

$$
\mathbf{y}= \mathbb{X}\boldsymbol \beta + \boldsymbol \varepsilon \qquad \varepsilon \sim \mathcal{N}(0, vI)
$$ {#eq-arp-inf-regression}


$$
\mathbf{y} = \begin{pmatrix} y_{p+1} \\ y_{p+2} \\ \vdots \\ y_T \end{pmatrix}, \quad
\boldsymbol \beta = \begin{pmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \end{pmatrix}, \quad
\mathbb{X} = \begin{pmatrix} y_{p} & y_{p-1} & \cdots & y_{1} \\ y_{p+1} & y_{p} & \cdots & y_{2} \\ \vdots & \vdots & \ddots & \vdots \\ y_{T-1} & y_{T-2} & \cdots & y_{T-p} \end{pmatrix}
$$

In the design matrix $\mathbb{X}$ each row corresponds to lagged observations used to predict the next value. This setup enables applying linear regression machinery.




we can now infer the parameters by using the left generalized Moore Penrose inverse of $\mathbb{X}$ to obtain the maximum likelihood estimator (MLE) of the AR coefficients $\boldsymbol \beta$ 
Assuming full-rank $\mathbb{X}$, this is the MLE. 

$$
\boldsymbol \beta_{MLE} = (\mathbb{X}^\top \mathbb{X})^{-1} \mathbb{X}^\top \mathbf{y}
$$ {#eq-arp-inf-regression-mle}

where:

- $\mathbb{X}$ is the design matrix, 
- $\boldsymbol \beta$ is the vector of AR coefficients, and 
- $\mathbf{y}$ is the response vector.

It matches the usual OLS solution in linear regression.

### Reference prior and posterior distribution

The reference prior reflects non-informative beliefs. This yields a conjugate posterior due to the Gaussian likelihood.

$$
p(\beta,v) \propto 1/v
$$ {#eq-arp-inf-prior}

$$
(\boldsymbol \beta | \mathbf{y}_{:T}, v) \sim \mathcal{N}(\boldsymbol \beta_{MLE}, v (\mathbb{X}^\top \mathbb{X})^{-1})
$$ {#eq-arp-inf-posterior-beta}

The posterior is Gaussian with mean $\boldsymbol \beta_{MLE}$ and scaled covariance $v(\mathbb{X}^\top \mathbb{X})^{-1}$, analogous to the OLS variance.

$$
(v | \mathbf{y}_{:T}) \sim \text{Inverse-Gamma}\left(\frac{T - 2p}{2}, \frac{S^2}{2}\right)
$$ {#eq-arp-inf-posterior-v}

where:  

- $S^2 = \sum_{t=p+1}^{T} (y_t - \mathbb{X} \boldsymbol \beta_{MLE})^2$ is the unbiased estimator of the variance $v$.
- $T$ is the total number of observations.

Posterior for $v$ is inverse gamma. The shape parameter is $(T - 2p)/2$ because the number of residual degrees of freedom is $T - 2p$ (residual = $T-p$ obs minus $p$ params). Scale is half the sum of squared residuals.

### Simulation-based Inference

Posterior sampling:

- Draw $v$ from the inverse gamma posterior.
- Plug $v$ into the posterior of $\boldsymbol\beta$ and sample from the Gaussian.

This gives full Bayesian posterior samples of $(\boldsymbol\beta, v)$.


### Summary

- In this lesson, we discussed the Bayesian inference for the AR(p) process using the conditional likelihood and reference prior. 
- We have established a connection between the AR(p) process and a regression model, allowing us to use standard regression techniques to estimate the parameters. 
- The posterior distribution can be derived, and we can perform simulation-based inference to obtain samples from the posterior distribution of the AR coefficients and variance.

::: {.callout-note collapse="true"}
## Video Transcript {.unnumbered}

<!-- TODO: summarize this transcript -->

{{< include _C4-L05-T01.qmd >}}

:::

## R code: Maximum likelihood estimation, AR(p), conditional likelihood ðŸ“–

\index{Maximum Likelihood Estimation}

```{r}
#| label: ar-likelihood
  set.seed(2021)
# Simulate 300 observations from an AR(2) with one pair of complex-valued reciprocal roots 
r=0.95
lambda=12 
phi=numeric(2) 
phi[1]=2*r*cos(2*pi/lambda) 
phi[2]=-r^2
sd=1 # innovation standard deviation
T=300 # number of time points
# generate stationary AR(2) process
yt=arima.sim(n = T, model = list(ar = phi), sd = sd) 

## Compute the MLE for phi and the unbiased estimator for v using the conditional likelihood
p=2
y=rev(yt[(p+1):T]) # response
X=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));
XtX=t(X)%*%X
XtX_inv=solve(XtX)
phi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi
s2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v

cat("\n MLE of conditional likelihood for phi: ", phi_MLE, "\n",
    "Estimate for v: ", s2, "\n")
```

## Model order selection ðŸŽ¥

![model order selection](images/m2_0032.png){#fig-ar-p-model-order-selection .column-margin  group="slides" width="53mm"}

### Goal

Determine the best model order $p$ for an AR($p$) process by comparing candidate models.

### Step 1: Define a candidate set of model orders

$$
p^* \qquad p=1:p^*
$$

You choose a maximum order $p^$ (e.g., 20), then evaluate AR models of each order $p \in {1, \dots, p^}$.

### Step 2: Estimate residual variance for each model


$$
S^2_p \qquad y_{(p^*+1):T} 
$$

For each model of order $p$, compute $S_p^2$ as the residual sum of squares (RSS) using a fixed evaluation window: $t = p^*+1$ to $T$, to ensure all models are evaluated on the same data subset.

### Step 3: Compute model selection criteria

AIC balances fit (log-likelihood) with complexity (number of parameters).
The first term measures model fit using $\log(S_p^2)$, and the second term penalizes complexity with $2p$.


$$
AIC_p = (T-p^*) \log(S_p^2) + 2p
$$

BIC uses the same fit term but a heavier penalty: $p \log(T - p^*)$, which increases with sample size. This often leads BIC to favor smaller models than AIC.

$$
BIC_p = (T-p^*) \log(S_p^2) + p \log(T-p^*)
$$

::: {.callout-note collapse="true"}
## Video Transcript {.unnumbered}

{{< include _C4-L05-T02.qmd >}}

:::

## Example: Bayesian inference in the AR(p), conditional likelihood ðŸŽ¥

This video demonstrates walks through the code in the next section.

::: {.callout-note collapse="true"}
## Video Transcript {.unnumbered}

{{< include _C4-L05-T03.qmd >}}

:::


## code: Bayesian inference, AR(p), conditional likelihood ðŸ“– â„›{#sec-arp-bayesian-inference}

```{r}
#| label: ar-bayesian-inference
# Simulate 300 observations from an AR(2) with one pair of complex-valued roots 
set.seed(2021)
r=0.95
lambda=12 
phi=numeric(2) 
phi[1]=2*r*cos(2*pi/lambda) 
phi[2]=-r^2
sd=1 # innovation standard deviation
T=300 # number of time points
# generate stationary AR(2) process
yt=arima.sim(n = T, model = list(ar = phi), sd = sd) 
par(mfrow=c(1,1), mar = c(3, 4, 2, 1) )
plot(yt)

## Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood
p=2
y=rev(yt[(p+1):T]) # response
X=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));
XtX=t(X)%*%X
XtX_inv=solve(XtX)
phi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi
s2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v

#####################################################################################
### Posterior inference, conditional likelihood + reference prior via 
### direct sampling                 
#####################################################################################

n_sample=1000 # posterior sample size
library(MASS)

## step 1: sample v from inverse gamma distribution
v_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)

## step 2: sample phi conditional on v from normal distribution
phi_sample=matrix(0, nrow = n_sample, ncol = p)
for(i in 1:n_sample){
  phi_sample[i, ]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)
}

par(mfrow = c(2, 3), mar = c(3, 4, 2, 1),  cex.lab = 1.3)
## plot histogram of posterior samples of phi and v

for(i in 1:2){
  hist(phi_sample[, i], xlab = bquote(phi), 
       main = bquote("Histogram of "~phi[.(i)]),col='lightblue')
  abline(v = phi[i], col = 'red')
}

hist(v_sample, xlab = bquote(nu), main = bquote("Histogram of "~v),col='lightblue')
abline(v = sd, col = 'red')

#####################################################
# Graph posterior for modulus and period 
#####################################################
r_sample=sqrt(-phi_sample[,2])
lambda_sample=2*pi/acos(phi_sample[,1]/(2*r_sample))
hist(r_sample,xlab="modulus",main="",col='lightblue')
abline(v=0.95,col='red')
hist(lambda_sample,xlab="period",main="",col='lightblue')
abline(v=12,col='red')


```

### code: Model order selection ðŸ“– â„› {#sec-ar-2-model-order-selection} 

```{r}
#| label: ar-2-model-order-selection

###################################################
# Simulate data from an AR(2)
###################################################
set.seed(2021)
r=0.95
lambda=12 
phi=numeric(2) 
phi[1]=2*r*cos(2*pi/lambda) 
phi[2]=-r^2
sd=1 # innovation standard deviation
T=300 # number of time points
# generate stationary AR(2) process
yt=arima.sim(n = T, model = list(ar = phi), sd = sd) 

#############################################################################
######   compute AIC and BIC for different AR(p)s based on simulated data ###
#############################################################################
pmax=10 # the maximum of model order
Xall=t(matrix(yt[rev(rep((1:pmax),T-pmax)+rep((0:(T-pmax-1)),
              rep(pmax,T-pmax)))], pmax, T-pmax));
y=rev(yt[(pmax+1):T])
n_cond=length(y) # (number of total time points - the maximum of model order)

## compute MLE
my_MLE <- function(y, Xall, p){
  n=length(y)
  x=Xall[,1:p]
  a=solve(t(x) %*%x)
  a=(a + t(a))/2 # for numerical stability 
  b=a%*%t(x)%*%y # mle for ar coefficients
  r=y - x%*%b # residuals 
  nu=n - p # degrees freedom
  R=sum(r*r) # SSE
  s=R/nu #MSE
  return(list(b = b, s = s, R = R, nu = nu))
}


## function for AIC and BIC computation 
AIC_BIC <- function(y, Xall, p){
  ## number of time points
  n <- length(y)
  
  ## compute MLE
  tmp=my_MLE(y, Xall, p)
  
  ## retrieve results
  R=tmp$R
  
  ## compute likelihood
  likl= n*log(R)
  
  ## compute AIC and BIC
  aic =likl + 2*(p)
  bic =likl + log(n)*(p)
  return(list(aic = aic, bic = bic))
}
# Compute AIC, BIC 
aic =numeric(pmax)
bic =numeric(pmax)

for(p in 1:pmax){
  tmp =AIC_BIC(y,Xall, p)
  aic[p] =tmp$aic
  bic[p] =tmp$bic
  print(c(p, aic[p], bic[p])) # print AIC and BIC by model order
}

## compute difference between the value and its minimum
aic =aic-min(aic) 
bic =bic-min(bic) 

## draw plot of AIC, BIC, and the marginal likelihood
par(mfrow = c(1, 1), mar = c(3, 4, 2, 1) )
matplot(1:pmax,matrix(c(aic,bic),pmax,2),ylab='value',
        xlab='AR order p',pch="ab", col = 'black', main = "AIC and BIC")
# highlight the model order selected by AIC
text(which.min(aic), aic[which.min(aic)], "a", col = 'red') 
# highlight the model order selected by BIC
text(which.min(bic), bic[which.min(bic)], "b", col = 'red') 

########################################################
p <- which.min(bic) # We set up the moder order
print(paste0("The chosen model order by BIC: ", p))
```


## Spectral representation of the AR(p) ðŸŽ¥


### Spectral representation of the AR(p): Example ðŸŽ¥

This video walks through the code in the next section.



### Rcode: Spectral density of AR(p) â„› ðŸ“–

```{r}
#| label: ar-spectral-density
### Simulate 300 observations from an AR(2) prcess with a pair of complex-valued roots 
set.seed(2021)
r=0.95
lambda=12 
phi=numeric(2) 
phi[1]<- 2*r*cos(2*pi/lambda) 
phi[2] <- -r^2
sd=1 # innovation standard deviation
T=300 # number of time points
# sample from the AR(2) process
yt=arima.sim(n = T, model = list(ar = phi), sd = sd) 

# Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood 
p=2
y=rev(yt[(p+1):T])
X=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));
XtX=t(X)%*%X
XtX_inv=solve(XtX)
phi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi
s2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v

# Obtain 200 samples from the posterior distribution under the conditional likelihood and the reference prior 
n_sample=200 # posterior sample size
library(MASS)

## step 1: sample v from inverse gamma distribution
v_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)

## step 2: sample phi conditional on v from normal distribution
phi_sample=matrix(0, nrow = n_sample, ncol = p)
for(i in 1:n_sample){
  phi_sample[i,]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)
}


### using spec.ar to draw spectral density based on the data assuming an AR(2)
spec.ar(yt, order = 2, main = "yt")

### using arma.spec from astsa package to draw spectral density
library("astsa")

## plot spectral density of simulated data with posterior sampled 
## ar coefficients and innvovation variance
par(mfrow = c(1, 1), mar = c(3, 4, 2, 1) )
#result_MLE=arma.spec(ar=phi_MLE, var.noise = s2, log='yes',main = '')
result_MLE=arma.spec(ar=phi_MLE, var.noise = s2, main = '')
freq=result_MLE$freq
  
spec=matrix(0,nrow=n_sample,ncol=length(freq))

for (i in 1:n_sample){
result=arma.spec(ar=phi_sample[i,], var.noise = v_sample[i],# log='yes',
                 main = '',plot=FALSE)
spec[i,]=result$spec
}

plot(2*pi*freq,log(spec[1,]),type='l',ylim=c(-3,12),ylab="log spectra",
     xlab="frequency",col=0)
#for (i in 1:n_sample){
for (i in 1:2){
lines(2*pi*freq,log(spec[i,]),col='darkgray')
}
lines(2*pi*freq,log(result_MLE$spec))
abline(v=2*pi/12,lty=2,col='red')


```

### Quiz: Spectral representation of the AR(p)

Omitted due to Coursera's Honor Code

### Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p)

The dataset below corresponds to a portion of an electroencephalogram (EEG) recorded in a particular location on the scalp of an individual. The original EEG dataset was originally recorded at 256Hz but was then subsampled every sixth observations, so the resulting sampling rate is about 42.7 observations per second. The dataset below has 400 observations corresponding approximately to 9.36 seconds.

You will use an AR(8) to model this dataset and obtain maximum likelihood estimation and Bayesian inference for the parameters of the model. For this you will need to do the following:

1.  Download the dataset, and plot it in R. Upload a picture of your graph displaying the data and comment on the features of the data. Does it present any trends or quasi-periodic behavior?

2.  Modify the code below to obtain the maximum likelihood estimators (MLEs) for the AR coefficients under the conditional likelihood. For this you will assume an autoregressive model of order p=8. The parameters of the model are $\phi=(\phi_1, \ldots \phi_8)'$ snf $v$. You will compute the MLE of $\phi$ denoted as $\hat\phi$. â€‹

3.  Obtain an unbiased estimator for the observational variance of the AR(8). You will compute the unbiased estimator for $v$ denoted as $s^2$.

4.  Modify the code below to obtain 500 samples from the posterior distribution of the parameters $\phi=(\phi_1, \ldots \phi_8)'$ and $v$ under the conditional likelihood and the reference prior. You will assume an autoregressive model of order v. Once you obtain samples from the posterior distribution you will compute the posterior means of $\phi$ and $v$, denoted as $\hat\phi$. and $\hat v$, respectively.

Modify the code below to use the function polyroot and obtain the moduli and periods of the reciprocal roots of the AR polynomial evaluated at the posterior mean $\hat\phi$.

```{r}
#| label: ar-simulation

set.seed(2021)
r=0.95
lambda=12 
phi=numeric(2) 
phi[1]=2*r*cos(2*pi/lambda) 
phi[2]=-r^2
sd=1 # innovation standard deviation
T=300 # number of time points
# generate stationary AR(2) process
yt=arima.sim(n = T, model = list(ar = phi), sd = sd) 
par(mfrow=c(1,1), mar = c(3, 4, 2, 1) )
plot(yt)

## Case 1: Conditional likelihood
p=2
y=rev(yt[(p+1):T]) # response
X=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));
XtX=t(X)%*%X
XtX_inv=solve(XtX)
phi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi
s2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v

cat("\n MLE of conditional likelihood for phi: ", phi_MLE, "\n",
    "Estimate for v: ", s2, "\n")
    
#####################################################################################
##  AR(2) case 
### Posterior inference, conditional likelihood + reference prior via 
### direct sampling                 
#####################################################################################

n_sample=1000 # posterior sample size
library(MASS)

## step 1: sample v from inverse gamma distribution
v_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)

## step 2: sample phi conditional on v from normal distribution
phi_sample=matrix(0, nrow = n_sample, ncol = p)
for(i in 1:n_sample){
  phi_sample[i, ]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)
}

## plot histogram of posterior samples of phi and nu
par(mfrow = c(1, 3),  mar = c(3, 4, 2, 1), cex.lab = 1.3)
for(i in 1:2){
  hist(phi_sample[, i], xlab = bquote(phi), 
       main = bquote("Histogram of "~phi[.(i)]))
  abline(v = phi[i], col = 'red')
}

hist(v_sample, xlab = bquote(nu), main = bquote("Histogram of "~v))
abline(v = sd, col = 'red')

```

## ARIMA processes ðŸ“–

::: {.callout-note}

### ARMA Model Definition {.unnumbered}

\index{ARMA process!definition}
A time series process is a zero-mean autoregressive moving average process if it is given by

$$
\operatorname{ARMA}(p,q)= \textcolor{red}
                {\underbrace{\sum_{i=1}^{p} \phi_i y_{t-i}}_{AR(P)}}
      + 
      \textcolor{blue}
                {\underbrace{\sum_{j=1}^{q} \theta_j \epsilon_{t-j}}_{MA(Q)}} 
      + \epsilon_t 
$$ {#eq-arma-definition}

with $\epsilon_t \sim N(0, v)$.

- For $q = 0$, we get an AR(p) process.
- For $p = 0$, we get a MA(q) i.e. moving average process of order $q$.

:::

Next we will define the notions of stability and invertibility of an ARMA process.
\index{ARMA process!stability}

::: {.callout-info}
#### Stability Definition {.unnumbered}

An ARMA process is **stable** if the roots of the AR characteristic polynomial [stable]{.column-margin}

$$
\Phi(u) = 1 - \phi_1 u - \phi_2 u^2 - \ldots - \phi_p u^p
$$

lie outside the unit circle, i.e., for all $u$ such that $\Phi(u) = 0$, $|u| > 1$.

Equivalently, this happens when the reciprocal roots of the AR polynomial have moduli smaller than 1.

This condition implies stationarity.

:::

\index{ARMA process!invertibility}

::: {.callout-info}
#### Invertible ARMA Definition {.unnumbered}

An ARMA process is **invertible** if the roots of the MA **characteristic polynomial** given by [invertible]{.column-margin}

$$
\Theta(u) = 1 + \theta_1 u + \ldots + \theta_q u^q,
$$ {#eq-arma-ma-poly}

lie outside the unit circle.

:::

Note that $\Phi(B) y_t = \Theta(B) \epsilon_t$.

- When an ARMA process is **stable**, it can be written as an infinite order moving average process.
- When an ARMA process is **invertible**, it can be written as an infinite order autoregressive process.

\index{ARIMA process!definition}

::: {.callout-info}
#### ARIMA Processes {.unnumbered}

An autoregressive integrated moving average process with orders $p$, $d$, and $q$ is a process that can be written as

$$
(1 - B)^d y_t = \sum_{i=1}^{p} \phi_i y_{t-i} + \sum_{j=1}^{q} \theta_j \epsilon_{t-j} + \epsilon_t,
$$ {#eq-arima-definition}

where $B$ is the backshift operator, $d$ is the order of integration, and $\epsilon_t \sim N(0, v)$.

in other words, $y_t$ follows an ARIMA(p, d, q) if the $d$ difference of $y_t$ follows an ARMA(p, q).

:::

Estimation in ARIMA processes can be done via *least squares*, *maximum likelihood*, and also *in a Bayesian way*. We will not discuss Bayesian estimation of ARIMA processes in this course.

### Spectral Density of ARMA Processes

For a given AR(p) process with AR coefficients $\phi_1, \dots, \phi_p$ and variance $v$, we can obtain its **spectral density** as

$$
f(\omega) = \frac{v}{2\pi |\Phi(e^{-i\omega})|^2} = \frac{v}{2\pi |1 - \phi_1 e^{-i\omega} - \ldots - \phi_p e^{-ip\omega}|^2},
$$ {#eq-ar-spectral-density}

with $\omega$ a frequency in $(0, \pi)$.

The spectral density provides a frequency-domain representation of the process that is appealing because of its interpretability.

For instance, an AR(2) process that has one pair of complex-valued reciprocal roots with modulus 0.7 and a period of $\lambda = 12$, will show a mode in the spectral density located at a frequency of $2\pi/12$. If we keep the period of the process at the same value of 12 but increase its modulus to 0.95, the spectral density will continue to show a mode at $2\pi/12$, but the value of $f(2\pi/12)$ will be higher, indicating a more persistent *quasi-periodic* behavior.

Similarly, we can obtain the spectral density of an ARMA process with AR characteristic polynomial $\Phi(u) = 1 - \phi_1 u - \ldots - \phi_p u^p$ and MA characteristic polynomial $\Theta(u) = 1 + \theta_1 u + \ldots + \theta_q u^q$, and variance $v$ as

$$
f(\omega) = \frac{v}{2\pi} \frac{|\Theta(e^{-i\omega})|^2}{|\Phi(e^{-i\omega})|^2}.
$$ {#eq-arma-spectral-density}

Note that if we have posterior estimates or posterior samples of the AR/ARMA coefficients and the variance $v$, we can obtain samples from the spectral density of AR/ARMA processes using the equations above.