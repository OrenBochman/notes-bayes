---
date: 2025-07-02
subtitle: "Bayesian Statistics - Capstone Project"
description: "Capstone Project: Introduction"
categories:
  - Bayesian Statistics
  - Capstone Project
keywords:
  - Time Series
---

# Bayesian Non-Parametric Models


Here is a question I raised in my Feynman Notebook:

1. Will we learn about Gaussian Processes/Neural Networks in this course?
   - This is a type of Bayesian Non-parametric and we don't cover these in the specialization. However Abel Rodriguez, the instructor of the third course on mixture model has a [short course](https://users.soe.ucsc.edu/~abel/website/Teaching.html)
   - Herbert Lee wrote a [Bayesian Nonparametrics via Neural Networks](https://www.amazon.com/Bayesian-Nonparametrics-Networks-Statistics-Probability/dp/0898715636) on the subject.
   - [Athanasios Kottas](https://users.soe.ucsc.edu/~thanos/) of UCSC has made the following notes available on his website:   
    - Tutorial on [Nonparametric Bayesian density regression:
modeling methods and applications](https://users.soe.ucsc.edu/~thanos/BNPnet_tutorial_notes.pdf)
    - Short course on [Applied Bayesian Nonparametric Mixture Modeling](https://users.soe.ucsc.edu/~thanos/NPB_course_notes.pdf) with [references (16 pages)](https://users.soe.ucsc.edu/~thanos/NPB_course_references.pdf) with 52 of them being his own papers.


This is not available as a course on Coursera and isn't a part of the specialization which ended in the last course. So this notes are my own personal notes gathered from tutorials and courses I found on the web.
- Tamara Broderick's Gaussian Processes for Regression tutorials from 
  - 2025 [slides](https://tamarabroderick.com/tutorial_2025_asa_cirs_webinar.html) [video](https://www.youtube.com/watch?v=szuREe_brxw&ab_channel=AmstatVideos) and [code](https://github.com/tbroderick/gps_tutorial/tree/2025asa_cirs)
  - 2024 [slides](https://tamarabroderick.com/tutorial_2024_cps-fr.html) 
- Tamara Broderick

## Overview

- In this course we learn to:
  - [x] Demonstrate a wide range of skills and knowledge in Bayesian statistics.
  - [x] Explain essential concepts in Bayesian statistics.
  - [x] Apply what you know to real-world data.
- We will build the following skills:
  - Probability Distribution
  - R Programming
  - Time Series Analysis and Forecasting
  - Statistical Modeling
  - Predictive Modeling
  - Advanced Analytics
  - Bayesian Statistics
  - Sampling (Statistics)
  - Statistical Methods
  - Data Analysis
  - Technical Communication
  - Markov Model
  - four Analysis
- There are five modules in this course:
  1. **Gaussian Processes for Regression**: We will focus on Gaussian processes as a flexible prior distribution for regression problems, allowing us to capture complex relationships in the data.
     1.1 Gaussian process model [slides](https://tamarabroderick.com/files/broderick_2024_cps-fr_part_i.pdf)
     1.2 Gaussian process regression [slides](https://tamarabroderick.com/files/broderick_2024_cps-fr_part_ii.pdf)
     1.3 Squared exponential kernel and observation noise [slides](https://tamarabroderick.com/files/broderick_2024_cps-fr_part_iii.pdf)
     1.4 What uncertainty are we quantifying? [slides](https://tamarabroderick.com/files/broderick_2024_cps-fr_part_iv.pdf)
     1.5 A list of resources: [slide](https://tamarabroderick.com/files/broderick_2024_cps-fr_resources.pdf)
  2. **Dirichlet process**: We will explore the Dirichlet process as a prior distribution over probability measures, allowing for flexible modeling of unknown distributions.
  3. **Chinese restaurant process**: We will introduce the Chinese restaurant process as a metaphor for the Dirichlet process, providing an intuitive understanding of how it works.
  4. **The peer reviewed data analysis project**: We will develop the model and then evaluate other students' models.




## Prerequisite skill checklist ðŸ“–


:::: {.callout-note collapse="true"}
## Prerequisite skill checklist

### Bayesian Statistics
- [x] Interpret and specify the components of Bayesian statistical models: likelihood, prior, posterior.
- [x] Explain the basics of sampling algorithms, including sampling from
standard distributions and using MCMC to sample from non-standard
posterior distributions.
- [x] Assess the performance of a statistical model and compare competing
models using posterior samples.
- [x] Coding in R to achieve the above tasks.

### Mixture Models
- [x] Define mixture model.
- [x] Explain the likelihood function associated with a random sample
from a mixture distribution.
- [x] Derive Markov chain Monte Carlo algorithms for fitting mixture models.
- [x] Coding in R to achieve the above tasks.

### Time Series Analysis
- [x] Define time series and stochastic processes (univariate, discrete-time, equally-spaced)
- [x] Define strong and weak stationarity
- [x] Define the auto-covariance function, the auto-correlation function
(ACF), and the partial autocorrelation function (PACF)
- [x] Definition of the general class of autoregressive processes of order p.
:::

## Some References:

1. Gaussian Processes @rasmussen2006gaussian
2. Surrogates @gramacy2020Surrogates
3. A Tutorial on Bayesian Optimization @frazier2018tutorialbayesianoptimization
  - It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample
  - three common acquisition functions:
    - expected improvement
    - entropy search
    - knowledge gradient
