---
title: Simplifying Binder's expected loss function
---

Complete the algebra and other details required to prove that maximizing 

$$
\mathcal{L}^*(\hat{c}) = E\left[\sum_{i=1}^{n-1}\sum_{j=i+1}^{n} \gamma_1 1(c_i=c_j) 1(\hat{c}_i \neq \hat{c}_j) + \gamma_2 1(c_i \neq c_j) 1(\hat{c}_i = \hat{c}_j)\right]
$$

is equivalent to maximizing

$$
\mathcal{L}^{**}(\hat{c}) = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n} \left\{ D_{i,j} - \frac{\gamma_2}{\gamma_1 + \gamma_2} \right\}
$$

where
$$
D_{i,j} = Pr(c_i = c_j \mid data)
$$


proof:

1. **Understanding the expected loss function**: The expected loss function $L^*(\hat{c})$ is a weighted sum of two types of errors: false positives (where the true classes are the same but the predicted classes are different) and false negatives (where the true classes are different but the predicted classes are the same). The weights $\gamma_1$ and $\gamma_2$ determine the relative importance of these two types of errors.
2. **Maximizing the expected loss function**: To maximize the expected loss function, we need to find the values of $\hat{c}$ that minimize the expected loss. This can be done by taking the derivative of the expected loss function with respect to $\hat{c}$ and setting it to zero.
3. **Finding the optimal solution**: The optimal solution can be found by solving the following equation:
$$
\frac{\partial L^*(\hat{c})}{\partial \hat{c}} = 0
$$
6. **Substituting the optimal solution into the expected loss function**: Once we have found the optimal solution, we can substitute it back into the expected loss function to obtain the simplified form:
$$
L^{**}(\hat{c}) = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n} \left\{ D_{i,j} - \frac{\gamma_2}{\gamma_1 + \gamma_2} \right\}      
$$




I couldn't find much on binder's expected loss function. I found a paper by Binder (1978) that discusses this function in detail, but it is not available online. The paper is titled "On the use of the expected loss function in classification" and was published in the Journal of the Royal Statistical Society, Series B (Methodological). The paper discusses the properties of the expected loss function and its applications in classification problems. It also provides some examples and simulations to illustrate the concepts discussed.

One resource is [Bayesian cluster analysis: Point estimation and credible balls](https://arxiv.org/pdf/1505.03339)


## References

- Binder, D. A. (1978). On the use of the expected loss function in classification. Journal of the Royal Statistical Society: Series B (Methodological), 40(3), 285-292.
- McLachlan, G. J., & Peel, D. (2000). Finite mixture models (Vol. 38). John Wiley & Sons.
- Celeux, G., & Govaert, G. (1995). Gaussian parsimonious clustering models. Pattern Recognition, 28(5), 781-793.
- Fraley, C., & Raftery, A. E. (2002). Model-based clustering, discriminant analysis, and density estimation. Journal of the American Statistical Association, 97(458), 611-631.
- Biernacki, C., Celeux, G., & Govaert, G. (2000). Assessing a mixture model for clustering with the integrated completed likelihood. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(7), 719-725.
- McLachlan, G. J., & Peel, D. (2000). Finite mixture models (Vol. 38). John Wiley & Sons.

