---
title : "EM algorithm for fitting a location mixture of 2 Gaussian components"
---

## The mixture model

We start by simulating data from a mixture of 2 Gaussian components.
Recall that in a location mixture model, the data is generated from a mixture of Gaussian distributions with different means $\mu_1$,$\mu_2$ but the same variance $\sigma$

$$
\begin{aligned}
x_i &\sim \sum_{k=1}^{KK} w_k \mathcal{N}(\mu_k, \sigma^2) 
\\&= w_1 \mathcal{N}(\mu_1, \sigma^2) + w_2 \mathcal{N}(\mu_2, \sigma^2) 
\\&= w_1 \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x_i - \mu_1)^2}{2\sigma^2}} + w_2 \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x_i - \mu_2)^2}{2\sigma^2}}
\end{aligned}
$$ {#eq-location-mixture-model}

where $x_i$ is the $i$-th observation, $K$ is the number of components, $w_k$ are the weights associated with each component, $\mu_k$ are the means of the components, and $\sigma^2$ is the common variance of all components.


## Simulating synthetic data

we first set some parameters and hyper-parameters for the simulation

```{r}
#| label: sim_data
#### Example of an EM algorithm for fitting a location mixture of 2 Gaussian components
#### The algorithm is tested using simulated data

## Clear the environment and load required libraries
rm(list=ls())
set.seed(81196)    # So that results are reproducible (same simulated data every time)

## Generate data from a mixture with 2 components

## Ground truth parameters
KK         = 2    # Number of components of the mixture
w.true     = 0.6  # True weights associated with the components
mu.true    = rep(0, KK) # initialize the true means list
mu.true[1] = 0   # True mean for the first component
mu.true[2] = 5   # True mean for the second component
sigma.true = 1   # True standard deviation of all components
n  = 120         # Number of observations to be generated

# simulate the component indicator items
cc = sample(1:KK, n, replace=T, prob=c(w.true,1-w.true))
x  = rep(0, n) # initialize the data vector x
for(i in 1:n){
  # sample from a distribution selected by component indicator
  x[i] = rnorm(1, mu.true[cc[i]], sigma.true)
}
```

next, we plot the data and the true density of the mixture

```{r}
#| label: plot_data
# Plot the true density
par(mfrow=c(1,1))
xx.true = seq(-8,11,length=200)
yy.true = w.true     * dnorm(xx.true, mu.true[1], sigma.true) + 
          (1-w.true) * dnorm(xx.true, mu.true[2], sigma.true) 
plot(xx.true, yy.true, type="l", xlab="x", ylab="True density", lwd=2)
points(x, rep(0,n), col=cc)
```

## Run the actual EM algorithm

Now we will use the EM algorithm to estimate the parameters of the mixture from the data we generated above.

The algorithm is implemented in the following steps:

1. **Initialization**: We initialize the parameters of the model (weights, means, and standard deviations) randomly.
2. **E-step**: We compute the expected value of the full data log-likelihood function, given the current parameters and the data.

   $$
   Q(\omega,\theta \mid \omega^{(t)}, \theta^{(t)},x) = E_{c \mid \omega^{(t)},\theta^{(t)}, x} \left[ \log p(x,c \mid \omega,\theta) \right]
   $$ {#eq-q-function}


   Where $c$ is the latent variable indicating the component from which each observation was generated, $\omega$ are the weights, and $\theta$ are the parameters of the Gaussian components (means and standard deviations).

3. **M-step**: We maximize the expected log-likelihood function with respect to the parameters.

  $$
    \hat{\omega}^{(t+1)},\hat{\theta}^{(t+1)} = \arg \max_{\omega,\theta} Q(\omega,\theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)},y)
  $$ {#eq-mstep}

  where $\hat{\omega}^{(t)}$ and $\hat{\theta}^{(t)}$ are the current estimates of the parameters, and $y$ is the observed data.

  Each component is independently when conditioning on the $\omega, \theta, x$

  $$
  p(c_i=k \mid x_i, \omega, \theta) = \frac{\omega_k g_k(x_i \mid \theta_k)}{\sum_{j=1}^{K} \omega_j g_j(x_i \mid \theta_j)}= v_{ik}(\omega, \theta)
  $$

  where the value of $v_{ik}$ is interpreted as the probability that the $i$-th observation comes from the $k$-th component of the mixture assuming the population parameters $\omega$ and $\theta$.

  The M-step consists of updating the parameters of the model by maximizing the expected log-likelihood function with respect to the parameters. This is done by taking the derivative of the expected log-likelihood function with respect to each parameter and setting it to zero.

  The M-step updates the weights, means, and standard deviations of the components based on the values of $v_{ik}$ computed in the E-step.

4. **Convergence check**: We check if the algorithm has converged by comparing the current and previous values of the log-likelihood function.


```{r}
#| label: em_algorithm


## Initialize the parameters

w     = 1/2                         # Assign equal weight to each component to start with
mu    = rnorm(KK, mean(x), sd(x))   # Random cluster centers randomly spread over the support of the data
sigma = sd(x)                       # Initial standard deviation

```


```{r}
# Plot the initial guess for the density
xx = seq(-8,11,length=200)
yy = w*dnorm(xx, mu[1], sigma) + (1-w)*dnorm(xx, mu[2], sigma)
plot(xx, yy, type="l", ylim=c(0, max(yy)), xlab="x", ylab="Initial density")
points(x, rep(0,n), col=cc)
title(main="Initial estimate + Observations")
```

```{r}
#| label: em_algorithm_loop
s  = 0              # s_tep counter
sw = FALSE          # sw_itch to stop the algorithm
QQ = -Inf           # the Q function (log-likelihood function)
QQ.out = NULL       # the Q function values
epsilon = 10^(-5)   # the stopping criterion for the algorithm


##Checking convergence of the algorithm
while(!sw){
  ## E step - Compute the expected value of the log-likelihood function
  v = array(0, dim=c(n,KK))
  v[,1] = log(w) + dnorm(x, mu[1], sigma, log=TRUE)    #Compute the log of the weights
  v[,2] = log(1-w) + dnorm(x, mu[2], sigma, log=TRUE)  #Compute the log of the weights
  for(i in 1:n){
    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner
  }
  
  ## M step - Maximize the expected log-likelihood function with respect to the parameters
  # Weights
  w = mean(v[,1])
  mu = rep(0, KK)
  for(k in 1:KK){
    for(i in 1:n){
      mu[k]    = mu[k] + v[i,k]*x[i]
    }
    mu[k] = mu[k]/sum(v[,k])
  }
  # Standard deviations
  sigma = 0
  for(i in 1:n){
    for(k in 1:KK){
      sigma = sigma + v[i,k]*(x[i] - mu[k])^2
    }
  }
  sigma = sqrt(sigma/sum(v))
  
  ##Check convergence
  QQn = 0
  for(i in 1:n){
    # log is used to avoid numerical underflow
    QQn = QQn + v[i,1]*(log(w) + dnorm(x[i], mu[1], sigma, log=TRUE)) +
                v[i,2]*(log(1-w) + dnorm(x[i], mu[2], sigma, log=TRUE))
  }
  if(abs(QQn-QQ)/abs(QQn)<epsilon){
    sw=TRUE
  }
  QQ = QQn
  QQ.out = c(QQ.out, QQ)
  s = s + 1
  print(paste(s, QQn))
  
  #Plot current estimate over data for each iteration
#   layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))
#   par(mar=c(3.1,4.1,0.5,0.5))
#   plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)
  
#   par(mar=c(5,4,1.5,0.5))
#   xx = seq(-8,11,length=200)
#   yy = w*dnorm(xx, mu[1], sigma) + (1-w)*dnorm(xx, mu[2], sigma)
#   plot(xx, yy, type="l", ylim=c(0, max(c(yy,yy.true))), main=paste("s =",s,"   Q =", round(QQ.out[s],4)), lwd=2, col="red", lty=2, xlab="x", ylab="Density")
#   lines(xx.true, yy.true, lwd=2)
#   points(x, rep(0,n), col=cc)
#   legend(6,0.22,c("Truth","Estimate"),col=c("black","red"), lty=c(1,2))

}
```

```{r}
#| label: plot_final_estimate
#Plot final estimate over data
layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))
par(mar=c(3.1,4.1,0.5,0.5))
plot(QQ.out[1:s],type="l", xlim=c(1,max(10,s)), las=1, ylab="Q", lwd=2)

par(mar=c(5,4,1.5,0.5))
xx = seq(-8,11,length=200)
yy = w*dnorm(xx, mu[1], sigma) + (1-w)*dnorm(xx, mu[2], sigma)
plot(xx, yy, type="l", ylim=c(0, max(c(yy,yy.true))), main=paste("s =",s,"   Q =", round(QQ.out[s],4)), lwd=2, col="red", lty=2, xlab="x", ylab="Density")
lines(xx.true, yy.true, lwd=2)
points(x, rep(0,n), col=cc)
legend(6,0.22,c("Truth","Estimate"),col=c("black","red"), lty=c(1,2), bty="n")


```