[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Specialization Notes",
    "section": "",
    "text": "Preface\nThese are my notes on Bayesian Statistics. They are based on online courses I took after I worked as a Data Scientist for a number of years. Over this time I have often felt that if i was better at statistics I could overcome limitations in the data with mathematics, build even better models, and perhaps most importantly make better inferences using model I have already developed.\nI tried to review the main result in statistic and probability theory, and I started to get better results and so I decided to take some classes. I often felt that the material was introductory and simplistic. Real world problems are monsters and the examples are usually trivial in comparison. The issues raised in class are rarely the troubles I saw at work. But I do believe deep down that the two are somehow related. And indeed as I covered more material certain aspects began to connect.\nI noticed that like in real classes the teachers made mistakes, their motivation was not always clear and that they skipped steps or reffered to certain results. On the other hand each teacher offered different insights into this complicated area of data analysis.\nI therefore have a number main areas of Focus:\n\nWhat are the questions one should ask\nWhat are the explicit details of each examples or problem.\nWhat is the mathematical representation of the model for these\nWhat is the code representation for this in R and in Python.\nCan I find or create diagrams to make interpretation of probabilities clearer.\nCan I annotate the main equations to break them down.\nCan I keep track of the most useful Mathematical results in Probability and Statistics?\n\nI tried to keep these handy as appendices to keep the main material less cluttered. However as time goes by these keep expanding.\n\nCan I learn to communicate my intuitions of these concepts to laymen and to colleages\n\nSome courses have discussion prompts, but I tried to make the most of these, and included them in these notes.\nI realized that while I often get excited about a new paper or technique my colleges are smart and talented people and quickly ask questions that are difficult to answer. These question often indicate gaps of knowledge which can dampen the enthusiasm for implementing these new ideas.\nI found that good communicators can overcome these issues more readily and connect what they already know.\n\n\nI have often found exercises rather easy to solve and so I often breezed through them with little notice. This time I resolved to make the most of these as opportunities to get better at using and manipulating probabilities and posterior distributions. So although I could get around 75% in each exercises based on intuition I took the extra effort to understand what is realty going on here mathematically.\nAt work one of the main challenges is making the problem conform to a simple model. This can be even more challenging when the goal is a latent (unobserved) variable or when you are considering a synergy of multiple effects and you have seen and unseen confounds. In many cases it is unclear how to proceed based on the simple examples we see in these classes. However I am now able to look at the problems with more critical point of view. Also I see great advantages of a quick expositions to many new simple models. In Bayesian hierarchical framework each can become a link to adding just a little more complexity, integrating new types of prior information and so on. So I view these as jumping boards for overcoming my next challenges.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "1  About",
    "section": "",
    "text": "About this site\n\n\nCode\n1 + 1\n\n\n2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction to Bayesian Statistics",
    "section": "",
    "text": "3 Introduction\nWhat is covered is:\nWith this goal in mind, the content is divided into the following three main sections (courses).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#why-inference",
    "href": "intro.html#why-inference",
    "title": "2  Introduction to Bayesian Statistics",
    "section": "3.1 Why Inference?",
    "text": "3.1 Why Inference?\nThe purpose of the set of courses is to focus on Inferential Statistics as opposed to Descriptive Statistics.\nAll the samples in the group that we are interested in learning about make up a population. Populations can be described by parameters such as the mean and variance since they represent all of the data. Often, we do not have access to all the data in our population and have to sample from the population. The metrics of mean and variance computed from these samples are not called parameters but statistics of the data.\n\n3.1.1 Descriptive Statistics\nThis is used to summarize the data so that we have a quantitative way to understand data. This allows to understand and visualize data qualitatively. We can draw conclusions about the nature of the data. Descriptive statistics is applied to a population and hence can provide measures such as the mean and variance of the data. They do not allow us to make predictions about data that we have not analyzed.\n\n\n3.1.2 Inferential Statistics\nInferential Statistics allow us to make generalizations about the population from the samples. This process of sampling introduces errors as this is never a perfect representation of the underlying data. The statistics thus computed are supposed to be an estimate of the true population parameters. It allows you to form a distribution of the population from the sampled data by accounting for the errors in the sampling, thereby allowing you to make predictions about data that is not yet seen or sampled.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#how-is-inference-different-from-prediction",
    "href": "intro.html#how-is-inference-different-from-prediction",
    "title": "2  Introduction to Bayesian Statistics",
    "section": "3.2 How is Inference different from Prediction?",
    "text": "3.2 How is Inference different from Prediction?\nReference\n\n3.2.1 Prediction\nIf you happen to come from a background in Machine Learning, you are probably used to making predictions. This is exactly what it sounds like, you use a model to make predictions on unseen data. The predictive process involves the following steps\n\nCreate the model\nSelect the best model using performance metrics such as accuracy, F1 scores on out-of-sample data\nMake predictions on new data\n\n\n\n3.2.2 Inference\nIn Inference, you are trying to model a distribution and understand the process that generates the data. This involves the following steps\n\nCreate the model, usually involves some prior understanding of the data generation process\nSelect the model using goodness-of-fit measures such as such as residual analysis, deviance, AIC scores etc.\nPerform inference by generating distributions that describe the data, or the data generation process",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "2  Introduction to Bayesian Statistics",
    "section": "3.3 References",
    "text": "3.3 References\n\n(Casella and Berger 2002) e-book solutions\n(Spanos 2019)\n(Hobbs and Hooten 2015) ebook website\n(VanderPlas 2016) ebook notebooks\n(Bishop 2006) ebook website\n\n\n\n\n\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer (India) Private Limited. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nCasella, G., and R. L. Berger. 2002. Statistical Inference. Duxbury Advanced Series in Statistics and Decision Sciences. Thomson Learning. http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&Roger%20L.Berger--Statistical%20Inference.pdf.\n\n\nHobbs, N. Thompson, and Mevin B. Hooten. 2015. Bayesian Models: A Statistical Primer for Ecologists. STU - Student edition. Princeton University Press. http://www.jstor.org/stable/j.ctt1dr36kz.\n\n\nSpanos, A. 2019. Probability Theory and Statistical Inference. Cambridge University Press. https://books.google.co.il/books?id=9nCiDwAAQBAJ.\n\n\nVanderPlas, J. 2016. Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media. https://jakevdp.github.io/PythonDataScienceHandbook/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "C1-L01.html",
    "href": "C1-L01.html",
    "title": "3  Probability",
    "section": "",
    "text": "4 Probability\nIn these notes I supplement the course material with my own notes for establishing an axiomatic foundation for probability These were omitted from the course material, but alluded to in the course and their absence was so irksome that I decided to add them to my notes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-rules-of-probability-odds--expectation",
    "href": "C1-L01.html#sec-rules-of-probability-odds--expectation",
    "title": "3  Probability",
    "section": "4.1 Rules of Probability, Odds & Expectation",
    "text": "4.1 Rules of Probability, Odds & Expectation\n\nBackground reading: This reviews the rules of probability, odds, and expectation.\n\nFor all its discussion of different paradigms of probability, the course lacks a rigorous definition of probability.\n\nDefinition 4.1 (Sample Space and Sample Point)  Sample space \\Omega\n\n\\Omega = \\{ \\forall w \\mid w \\text { is an outcome of an experiment} \\} \\ne \\emptyset\n\\tag{4.1}\nthen \\Omega is called a sample space.\nSince\n\n\\Omega \\ne \\emptyset \\implies  \\exists\\ \\omega \\in \\Omega\n\\tag{4.2}\nthen \\omega is called a sample point  Sample point \\omega\n\n\nDefinition 4.2 (Event)  Event A\n\n\\Omega \\ne \\emptyset \\implies  \\exists \\mathcal{F} \\subset 2^\\Omega \\implies \\exists A\\in F\n\\tag{4.3}\nLet \\mathcal{F} denote a family of subsets of a sample space \\Omega, and A any such subset. Then A is called an event\n\n\nDefinition 4.3 (Elementary Event) An event composed of a single point \\omega is called an elementary event.\n\n\nDefinition 4.4 (Outcome) We say that event A happened if when conducting the experiment we got an outcome \\omega and \\omega\\in A.\n\n\nDefinition 4.5 (Certain Event) \\Omega is called the certain event.\n\n\nDefinition 4.6 (Impossible Event) \\emptyset is called the impossible event.\n\n\nDefinition 4.7 (σ-Algebra) A family of events \\mathcal{F} with the following properties:\n\n\\Omega is the universal set\n\n\\Omega \\in \\mathcal{F}\n  \\tag{4.4}\n\\mathcal{F} is closed under complement operation:\n\n\\forall A \\in \\mathcal{F} \\implies A^c \\in \\mathcal{F}\n\\tag{4.5}\n\\mathcal{F} is closed under countable unions:\n\n\\exists A_i \\in \\mathcal{F} \\quad  i \\in \\mathcal{N} \\implies \\bigcup_{n=1}^\\infty {A_i} \\in \\mathcal{F}\n\\tag{4.6}\n\nis called a \\sigma-algebra or a \\sigma-field .\n\nsome properties of \\sigma-algebra\n\nhttps://math.stackexchange.com/questions/1330649/difference-between-topology-and-sigma-algebra-axioms\nAn epsilon of room: pages from year three of a mathematical blog section 2.7\n\n\nDefinition 4.8 (Probability Measure) if \\Omega is a sample space Definition 4.1 and \\mathcal{F} a \\sigma-algebra Definition 4.7 for \\Omega then a function P: \\mathcal{f} \\to [0,1] with the following properties:\n\nTotal measure of the sample space is 1: \n     \\mathbb{P}r(\\Omega)=1\n  \\tag{4.7}\ncountably additive for pairwise disjoint countable collections of events: is called a probability measure over \\mathcal{F}. \n\\forall E_{ i \\in \\mathbb{N} } \\quad   \\mathbb{P}r(\\bigcup_{n\\in \\mathbb{N} }{E_n})=\\sum_{n\\in \\mathbb{N} } \\mathbb{P}r(E_n)\n\\tag{4.8}\nthen P is a probability measure over \\mathcal{F}\n\n\n\nDefinition 4.9 (Probability Space) If \\Omega is a sample space Definition 4.1 and \\mathcal{F} a \\sigma-algebra Definition 4.7 for \\Omega, and P a probability measure (Definition 4.8) for \\mathcal{F} then the ordered set &lt;\\Omega,\\mathcal{F},P &gt; is called a probability space\n\n\n\n\n\n\n\n\nFigure 4.1: Illustration of a probability space by Ziggystar\n\n\n\n\n\n\n\n\nTipNotation\n\n\n\nsometimes \\mathcal{F} is replaced with \\Sigma. for the \\sigma-algebra like in the figure below",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-properties-of-probability-measures",
    "href": "C1-L01.html#sec-properties-of-probability-measures",
    "title": "3  Probability",
    "section": "4.2 Properties of Probability Measures",
    "text": "4.2 Properties of Probability Measures\nThe probability of the null event is 0.\n\n\\mathbb{P}r(\\emptyset) = 0\n\\tag{4.9}\nProbabilities of all possible events (the space of all possible outcomes) must sum to one.\n\n\\mathbb{P}r(\\Omega) = 1\n\\tag{4.10}\n\nA\\cap B = \\emptyset \\implies \\mathbb{P}r(A \\cup B) = \\mathbb{P}r(A)+\\mathbb{P}r(B)\n\\tag{4.11}\n\n\n\\mathbb{P}r(A^c) =1-\\mathbb{P}r(A) \\qquad \\forall A\\in\\Omega\n\\tag{4.12}\n\nif A is an event in \\Omega then A^C is in Omega and since they are mutually exclusive by Equation 4.10\nIf S is the certain event in class C \\Omega then\nFor every event X in class \\Omega\n\n1 \\ge \\mathbb{P}r(X) \\ge 0 \\qquad \\forall X \\in \\Omega \\qquad \\text{(P1)} \\qquad\n\\tag{4.13}\nProbabilities add to one:\n\n\\sum_{i\\in \\Omega} \\mathbb{P}r(X=i)=1 \\qquad \\text{(P2)}\\qquad\n\\tag{4.14}\nThe complement of an event A is A^c\n\n\\mathbb{P}r(S) = 1\n\\tag{4.15}\nIf events A_\\lambda are mutually exclusive (only one event may happen):\n\n\\mathbb{P}r(A_1 \\cup A_2) = \\mathbb{P}r(A_1) + \\mathbb{P}r(A_2) - \\mathbb{P}r(A_1\\cap A_1)\n\\tag{4.16}\n\n\\mathbb{P}r(\\bigcup_{\\lambda\\in \\Omega} A_\\lambda)=\\sum_{\\lambda \\in \\Omega} \\mathbb{P}r(A_\\lambda)\n\\tag{4.17}\nif {B_i} is a finite or countably infinite partition of a sample space \\Omega then\n\n\\mathbb{P}r(A) = {\\sum_{i=1}^{N} \\mathbb{P}r(A \\cap B_i)}= {\\sum_{i=1}^{N} \\mathbb{P}r(A|B_i)\\mathbb{P}r(B_i)}\n\\tag{4.18}\n\n4.2.1 Odds\n\nC-3PO: Sir, the possibility of successfully navigating an asteroid field is approximately 3,720 to 1!  Han Solo: Never tell me the odds! — Star Wars Episode V: The Empire Strikes Back\n\nAnother way to think about probabilities is using odds Equation 4.19. Odds are more intuitive when we are thinking about the risk of an event happening or not happening. and when we consider the risk associated with uncertainty odds are a handy way of considering the risks.\n\nDefinition 4.10 (Odds Definitions) the odds of an event A are:\n\n\\mathcal{O}(A)  = \\frac{\\mathbb{P}r(A)}{\\mathbb{P}r(A^c)} = \\frac{ \\mathbb{P}r(A)}{1-\\mathbb{P}r(A)}\n\\tag{4.19}\n\nIt is also possible to convert odds to probabilities Equation 4.20\n\nTheorem 4.1 (Probability from odds) \n\\mathbb{P}r(A) = \\frac{ \\mathcal{O}(A)} {1+ \\mathcal{O}(A)}\n\\tag{4.20}\n\n\nProof. \n\\begin{aligned}\n& & \\mathcal{O}(A)  &= \\frac{\\mathbb{P}r(A)}{1-\\mathbb{P}r(A)} && \\text{(odds definition)}\n  \\\\&\\implies & \\mathbb{P}r(A) &= \\mathcal{O}(A) (1-\\mathbb{P}r(A))  && (\\times \\text{ denominator})\n  \\\\&\\implies &  \\mathbb{P}r(A) &= \\mathcal{O}(A) - \\mathcal{O}(A) \\mathbb{P}r(A) && \\text{(expand)}\n  \\\\&\\implies &  \\mathbb{P}r(A)(1+ \\mathcal{O}(A)) &= \\mathcal{O}(A) && \\text{(collect)}  \n  \\\\&\\implies & \\mathbb{P}r(A) &= \\frac{ \\mathcal{O}(A)} {1+ \\mathcal{O}(A)} && \\blacksquare  \n\\end{aligned}\n\n\nIf we are at the races and thinking about each horse a horse what we may care about is if it will win or lose. In such a case the odds can summarize the ratio of past successes and failures to win. Odds seem to be in line with a frequentist view summarizing ratios of success to failure. In reality, the other horses have odds as well and we may want to consider the probability of winning given the other horses in the race, and perhaps other parameters, like the track type, length of the race, jockey, and perhaps some hot tips. So let us not get ahead of ourselves\n\n\n\n\n\n\nTipData Scientist - insights.\n\n\n\nMany of these formulas are rather tedious. But, once you start to work on a data science project you will often discover that there are some problems with the data and because of that you cannot use your favorite algorithm. Or worse when you do the results are not very useful. It is at this point that the ability to think back to first principles will be very fruitful. The more of this material you can recall, the more the dots will connect, and your ability will translate into models of increasing sophistication. Luckily, the rules of probability are logical. So it is fairly easy to remember or even derive if you take some time to understand them.\nI realize that figuring out which results are more useful is easier in hindsight. And one of the reasons I am taking these courses is to annotate in my note the results I think to be most useful.\n\n\n\n\n4.2.2 Expectation\nThe expectation of a random variable (RV) X is the weighted average of the outcomes it can take weighted by their probabilities.\n\nDefinition 4.11 (Expectation for a discrete RV) \n\\mathbb{E}(x) = \\sum^N_{i=1} x_i \\times \\mathbb{P}r(X=x_i)\n\\tag{4.21}\n\n\nDefinition 4.12 (Expectation for a continuous RV) \n\\mathbb{E}(x) = \\int_{\\Omega} x \\mathbb{P}r(X=x) dx\n\\tag{4.22}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-probability-paradigms",
    "href": "C1-L01.html#sec-probability-paradigms",
    "title": "3  Probability",
    "section": "4.3 Probability Paradigms",
    "text": "4.3 Probability Paradigms\n\n\n\n\n\n\n\nFigure 4.2: Probability Paradigms\n\n\nWe start by looking at probability as defined or interpreted under three paradigms. Probability is at its root a logical and scientific approach to formalizing and modeling uncertainty.\nThe three paradigms are:\n\nDefinition 4.13 (Classical Probability) Deals primarily with cases where probabilities are distributed equally, like with dice and cards.\n\n\n\n\n\n\n\n\nFigure 4.3: Abraham De Moivre\n\n\n\n\n\n\n\n\nTipBiographical note on Abraham de Moivre\n\n\n\n\n\n\nThe Probability of an Event is greater or less, according to the number of chances by which it may happen, compared with the whole number of chances by which it may either happen or fail. — (Moivre 1718)\n\nAbraham de Moivre (1667-1754) was a prominent French mathematician known for his significant contributions to the field of probability and his work on the foundations of Bayesian statistics. His research and writings played a crucial role in establishing the mathematical principles of probability theory and laid the groundwork for future advancements in the field.\nDe Moivre is best known for his work on the theory of probability. He made significant advancements in understanding the Binomial distribution and its application to games of chance and coin tossing. In his influential book, “The Doctrine of Chances” (1718), he presented a comprehensive treatise on probability theory, providing mathematical explanations for various phenomena such as the law of large numbers and the central limit theorem. His book became a standard reference in the field and greatly influenced subsequent research on probability.\nFurthermore, de Moivre’s work laid the foundation for Bayesian statistics, although the term “Bayesian” was not coined until many years after his death. He developed a formula known as de Moivre’s theorem, which establishes a connection between the normal distribution and the binomial distribution. This theorem became a fundamental tool in probability theory and enabled the calculation of probabilities for large sample sizes. It provided a bridge between frequentist and Bayesian approaches, allowing for the estimation of parameters and the quantification of uncertainty.\n\nAnd thus in all cases it will be found, that although Chance produces irregularities, still the Odds will be infinitely great, that in process of Time, those Irregularities will bear no proportion to the recurrency of that Order which naturally results from Original Design. (Moivre 1718)\n\nHe was an active participant in scientific societies and maintained correspondence with renowned mathematicians of his time, including Isaac Newton and James Stirling. His work played a crucial role in disseminating mathematical knowledge and promoting the study of probability theory across Europe. De Moivre’s research and writings laid the groundwork for the development of probability theory and Bayesian statistics. His ideas and formulas continue to be foundational in the field, and his contributions have had a lasting impact on mathematics, statistics, and the broader scientific community.\nHis work remains an essential reference for researchers and serves as a testament to his profound understanding of probability and statistics.\n\nFurther, the same Arguments which explode the Notion of Luck, may, on the other side, be useful in some cases to establish a due comparison between Chance and Design: We may imagine Chance and Design to be, as it were, in Competition with each other, for the production of some sorts of Events, and many calculate what Probability there is, that those Events should be rather be owing to the one than to the other. (Moivre 1718)\n\n\n\n\n\nDefinition 4.14 (Frequentist Probability) Defines probabilities using long-run limits of frequencies from repeated independent sampling generated by a hypothetical infinite sequence of experiments from a population\nFrequentist probability or frequentism is an interpretation of probability; it defines an event’s probability as the limit of its relative frequency in many trials AKA long-run probability. Probabilities can be found, in principle, by a repeatable objective process and are thus ideally devoid of opinion. The continued use of frequentist methods in scientific inference, however, has been called into question.\n\nSince in reality we cannot repeat most experiments many times.\n“by definition, scientific researchers do not possess sufficient knowledge about the relevant and irrelevant aspects of their tests and populations to be sure that their replications will be equivalent to one another” - Mark Rubin 2020\n\n\n\nDefinition 4.15 (Bayesian Probability) Defines probability starting with a subjective view of the problem called a prior and updates it as evidence comes in using Bayes Rule.\n\nThe lesson and assignments test these views with examples - but the division is rather artificial to me. Not that it does not exist, but rather different authors on the subject treat it differently.\n\n\n\n\n\n\n\nVideo 4.1: Interview with Dennis Lindley, a pioneer of Bayesian statistics, discussing the history and philosophy of Bayesian methods, and his contributions to the field. He emphasizes the importance of subjective probability and the role of prior beliefs in statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-bayesian-probability-and-coherence",
    "href": "C1-L01.html#sec-bayesian-probability-and-coherence",
    "title": "3  Probability",
    "section": "4.4 Bayesian Probability and Coherence",
    "text": "4.4 Bayesian Probability and Coherence\n\n\n\n\n\n\n\nFigure 4.4: Coherence\n\n\nA notion of a fair bet - one which we would take either way for the same reward.\n\ncoherence following the rules of statistics\nincoherence or Dutch book one would be guaranteed to lose money.\n\n\n\n\n\n\n\n\nFigure 4.5: Bruno de Finetti\n\n\n\n\n\n\n\n\nTipBiographical note on Bruno de Finetti\n\n\n\n\n\n\nFrom the subjective standpoint, no assertion is possible without a priori opinion, but the variety of possible opinions makes problems depending on different opinions interesting.\n\nBruno de Finetti 1906-1985 was born in Innsbruck (Austria) to an Italian family. He studied mathematics at the University of Trieste, where he developed a keen interest in probability theory and its applications.\nAfter completing his doctoral studies in 1928, de Finetti embarked on a distinguished academic career. His first research work dealt with mathematical biology and was published, in 1926 when he was still an undergraduate. After graduation and up to 1931, he worked in the mathematical office of the Central Italian Agency for Statistics. From 1931-46, de Finetti worked in Trieste at Assicurazioni Generali, one of the most important insurance companies in Italy. In the same period, he lectured at the University of Trieste and the University of Padua.\nOne of de Finetti’s most significant contributions was his development of the theory of subjective probability, also known as the Bayesian interpretation of probability. He developed his ideas independently of F. P. Ramsey who also published on this (Ramsey 1926)\nIn his seminal work, (Finetti 1937), he proposed that probability should be interpreted as a personal measure of belief or degree of uncertainty rather than as a frequency or long-run proportion. This subjective approach allowed for the incorporation of prior information and updating of beliefs in light of new data, forming the basis of Bayesian inference.\n\nProbabilistic reasoning – always to be understood as subjective – merely stems from our being uncertain about something. (Finetti 2017 § preface)\n\nIt is impossible to summarize in a few paragraphs the scientific activity of de Finetti in the different fields of mathematics (probability), measure theory, analysis, geometry, mathematics of finance, economics, the social sciences, teaching, computer science, and biomathematics or to describe his generous and complex personality as a scientist and a humanitarian. De Finetti discussed his own life in a book edited by Gani (1982). See also the article by Lindley (1989).\n\nMy thesis, paradoxically, and a little provocatively, but nonetheless genuinely, is simply this :  PROBABILITY DOES NOT EXIST.  … Probability, too, if regarded as something endowed with some kind of objective existence, is no less a misleading misconception, an illusory attempt to exteriorize or materialize our true probabilistic beliefs. (Finetti 2017 § preface page x)\n\nde Finetti was a brilliant statistician but his books and papers have garnered a reputation of being challenging to read both in the original Italian, French and English translation. The above quote embodies his radical point of view which he challenged other statisticians to rethink their views.\nWhat I think he meant is that meant primarily was that probabilities unlike physical quantities cannot be measured in the objective sense. de Fineti was well versed with quantum mechanics, where physical quantities like the position and speed of an electron are interpreted primarily through probabilities in a wave equation, to include a discussion in the start of his second volume.\nA large part of this course is that we are inferring parameters - which are often probabilities.\nAnother milestone result by de Finetti is his theorem\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\nRepresenting uncertainty with probability: Don’t use any outside information on this question, just determine probabilities subjectively. The country of Chile is divided into 15 administrative regions. The size of the country is 756,096 square kilometers. How big do you think the region of Atacama is? Let:\n\nA_1 be the event: Atacama is less than 10,000 km^2.\nA_2 be the event: Atacama is between 10,000 and 50,000 km^2\nA_3 be the event: Atacama is between 50,000 and 100,000 km^2\nA_4 be the event: Atacama is more than 100,000 km^2 Assign probabilities to A_1 \\ldots A_4\n\n\n\n\n\n\n\n\n\nEvent\nMin km^2\nMax km^2\nP\n\n\n\n\nA_1\n0\n10k\n\\frac{1}{4}\n\n\nA_2\n10k\n50k\n\\frac{1}{4}\n\n\nA_3\n50k\n100k\n\\frac{1}{4}\n\n\nA_4\n100k\n\n\\frac{1}{4}\n\n\n\n\nWhat do I know at this point?\n\nThe expected area for the region is \\frac{750,000}{15}=50,000\\ km^2 .\n\nWhat Do I believe?\n\nI believe that the administrative regions have significantly different sizes\nfrom my familiarity with some other countries.\nAs I don’t know if Atacama is large or small my best bet is to assign equal probabilities to each event.\n\n\n\n\n\n\n\n\nNoteMore information 1\n\n\n\nAtacama is the fourth largest of 15 regions. Using this information, I revised my probabilities as follows:\n\n\n\n\n\n\n\nEvent\nMin km^2\nMax km^2\nP\n\n\n\n\nA_1\n\n10k\n\\frac{1}{16}\n\n\nA_2\n10k\n50k\n\\frac{3}{16}\n\n\nA_3\n50k\n100k\n\\frac{6}{16}\n\n\nA_4\n100k\n\n\\frac{6}{16}\n\n\n\n\nWhat do I know?\n\nThe expected area is \\frac{750,000}{15}=50,000\\ km^2 .\nI know that Atacama is the Fourth largest.\n\nWhat Do I believe?\n\nI believe that the administrative regions have significantly different sizes - from my familiarity with some other countries.\n\nHow do I revise my guesstimate?\n\nIf the region sizes are equally sized I should gamble mostly on A_2 and A_3 .\nBut I think there are a few large regions and many smaller ones.\nAlso I know that 11 regions are smaller than Atacama and that 3 three are larger.\nNone of the events can yet be ruled out.\nA_1 seems extremely unlikely as it necessitates the top three regions account for almost all of the area of the country. \\frac{750,000 - 14 * 10,000}{3} = 203,333.3 that’s about 4 times the average for each state.\nA_2 is fairly unlikely to require the top three regions to account for \\frac{(750,000-14*20000)}{3}=170,000 each that’s more than 3 times the average.\n\n\n\n\n\n\n\n\nNoteMore information 2\n\n\n\nThe smallest region is the capital region, Santiago Metropolitan, which has an area of 15,403 km^2. Using this information, I revised my probabilities as follows:\n\n\n\n\n\n\n\nEvent\nMin km^2\nMax km^2\nP\n\n\n\n\nA_1\n\n10k\n0\n\n\nA_2\n10k\n50k\n\\frac{1}{8}\n\n\nA_3\n50k\n100k\n\\frac{4}{8}\n\n\nA_4\n100k\n\n\\frac{3}{8}\n\n\n\nWhat do I know?\n\nThe expected area is \\frac{750,000}{15}=50,000 \\quad km^2\n\\mathbb{P}r(A_1)=0 since the smallest region is $ 15,403 km^2$.\nI believe that the administrative regions have significantly different sizes - from my familiarity with some other countries.\nI know that Atacama is the Fourth largest.\n\nIf the region sizes are equally sized I should gamble mostly on A_2 and A_3.\nBut I think there are a few large regions and many smaller ones.\nAlso I know that 11 regions are smaller than Atacama and that 3 three are larger.\nNone of the events can yet be ruled out. But A1 and A2 are now very unlikely as they would require the top three regions to account for almost all of the area of the country.\n\n\n\n\n\n\n\n\nNoteMore information 3\n\n\n\nThe third largest region is Aysén del General Carlos Ibáñez del Campo, which has an area of 108,494 km^2.\nUsing this information, I revised my probabilities as follows:\n\n\n\n\n\n\n\nEvent\nMin km^2\nMax km^2\nP\n\n\n\n\nA_1\n\n10K\n0\n\n\nA_2\n10k\n50K\n\\frac{1}{8}\n\n\nA_3\n50k\n100K\n\\frac{6}{8}\n\n\nA_4\n100k\n\n\\frac{1}{8}\n\n\n\n\nThe expected area is \\frac{750,000}{15}=50,000 \\quad km^2\n\\mathbb{P}r(A1)=0 since the smallest region is $15,403 km^2 $ .\nI believe that the administrative regions have significantly different sizes - from my familiarity with some other countries.\nI know that Atacama is the Fourth largest.\n\nIf the region sizes are equally sized I should gamble mostly on A_2 and A_3 .\nBut I think there are a few large regions and many smaller ones.\nAlso I know that 11 regions are smaller than Atacama and that 3 three are larger.\nNone of the events can yet be ruled out. But A1 and A2 are now very unlikely as they would require the top three regions to account for almost all of the area of the country.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-discussions-objectivity",
    "href": "C1-L01.html#sec-discussions-objectivity",
    "title": "3  Probability",
    "section": "4.5 Discussions: Objectivity",
    "text": "4.5 Discussions: Objectivity\n\n\n\n\n\n\nTipDiscussion: Objectivity\n\n\n\nIn what ways could the frequentist paradigm be considered objective? In what ways could the Bayesian paradigm be considered objective? Identify ways in which each paradigm might be considered subjective.\n\nFrequentist:\n\nThe orthodox approach is statisticians should establish an objective statistical methodology and field researchers should then use it to solve their problems. This leads to following flow charts for analysis and tests without fully understanding the model and how it works. At best one makes mistakes due to misunderstanding. But we can see that there is a systematic gaming of this methodology using p-hacking, multiple hypotheses, and hiding failed experiments leading to the publication of outrageously good results, which then cannot be replicated.\nThe analysis is done on data that is supposedly sampled from a population. But the same data may belong to different populations (the city, the country, etc) each with different statistics. We should assume the same long-run frequencies would converge to different to each one of these statistics if we repeat the experiment enough times.\nThe sample size, or how long we run the experiment is a tricky decision to make in advance and without prior knowledge. And if we do not decide in advance, but periodically as the data comes in. It turns out that this can completely change the outcomes of the experiment - even if both approaches have the same data.\nThe choice of H_0 and H_1 is often subjective and each hypothesis can lead to yet another.\nThe choice of the confidence level 95%, 99%, etc. used for statistical significance is subjective.\nIf an effect size is considered large is subjective and depends on the field one studies.\n\nBayesian:\n\nthe prior should be highly informative and therefore subjective. But it can be\nuninformative and hence more objective.\nit can be difficult to decide what impact the prior should have on the posterior. Ideally, we can quantify the effective sample size for the prior data and we can understand how much information each contributes to the posterior.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-expected-values",
    "href": "C1-L01.html#sec-expected-values",
    "title": "3  Probability",
    "section": "4.6 Expected values",
    "text": "4.6 Expected values\nThe expectation of an RV is a measure of its central tendency.\nThe expected value, also known as the expectation or mean, of a random variable X is denoted \\mathbb{E}[X]. It is the weighted average of all values X could take, weighted by their probabilities.\n\n\n\n\n\n\nTip\n\n\n\n\nI looked this up and found the following answer, see (https://math.stackexchange.com/users/25097/autolatry) (n.d.).\nThe RV X is a function whereas the Expectation is a Functional (a mapping from a function to a number). Mathematicians adopt the use of square brackets for functionals.\nSee Wikipedia contributors (2023) for more information on what a Functional is.\n\n\nWhy Square Brackets for Expectation\n4.6.1 Expectation of a discrete random variable\nIf X is a discrete-valued random variable then its expectation is defined by(?eq-expectation-discrete-RV)\n\n\\mathbb{E}[X]=\\sum^N_{i=1} x_i \\cdot \\mathbb{P}r(X=x_i) = \\sum^N_{i=1} x_i \\cdot f(x)\n\\tag{4.23}\nwhere f(x) is the probability mass function (PMF) of X.\n\n\n4.6.2 Expectation of a continuous random variable\nIf X is a continuous random variable then its expectation is defined by(?eq-expectation-continuous-RV)\n\n\\mathbb{E}[X]=\\int_{-\\infty}^{\\infty} x \\cdot f(x) dx\n\\tag{4.24}\nwhile the mean is an important descriptive statistic for central tendencies, we often prefer the median which is robust to outliers, and pick the mode as a representative if we need a value in the data set.\n\n\n4.6.3 Properties of Expectation\nSum and integral are linear operators so the Expectation is also a linear operator\n\n\\mathbb{E}[c]= c\n\\tag{4.25}\n\n\\mathbb{E}[aX+bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\n\\tag{4.26}\n\n\\mathbb{E}[g[X]]  = \\int{g(x)f(x)dx}\n\\tag{4.27}\nwhere g[X] is a function of the random variable X.\nIf X & Y are independent\n\n\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]\n\\tag{4.28}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-variance",
    "href": "C1-L01.html#sec-variance",
    "title": "3  Probability",
    "section": "4.7 Variance",
    "text": "4.7 Variance\nVariance is the dispersion of a distribution about the mean.\n\nDefinition 4.16 For a discrete random variable, the Variance is defined using (Equation 4.29)\n\n\\mathbb{V}ar(X)=\\sum^N_{i=1} (x_i-\\mu)^2 \\mathbb{P}r(X=x_i)\n\\tag{4.29}\n\n\nDefinition 4.17 For a continuous random variable, the Variance is defined using (Equation 4.30)\n\n\\mathbb{V}ar[X]=\\int_{- \\infty}^{\\infty} (x-\\mu)^2 f(x)dx\n\\tag{4.30}\n\n\n4.7.1 Properties of Variance\n\n\\mathbb{V}ar[c] = 0\n\\tag{4.31}\nif X and Y are independent then\n\n\\mathbb{V}ar[aX+by] = a^2\\mathbb{V}ar[X] +b^2\\mathbb{V}ar[Y]\n\\tag{4.32}\notherwise\n\n\\mathbb{V}ar[aX+by] = a^2\\mathbb{V}ar[X] +b^2\\mathbb{V}ar[Y] + 2ab\\mathbb{C}ov(X,Y)\n\\tag{4.33}\nwhere \\mathbb{C}ov(X,Y) is the covariance of X and Y.\nHere is one of the most useful identities (Equation 4.34) for wrangling with variance using the expectation of X and X^2.\n\n\\begin{aligned}\n    \\mathbb{V}ar[X] &= \\mathbb{E}[(X- \\mathbb{E}[X])^2]\n    \\\\&= \\mathbb{E}[X^2] − (\\mathbb{E}[X])^2\n\\end{aligned}\n\\tag{4.34}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-covariance",
    "href": "C1-L01.html#sec-covariance",
    "title": "3  Probability",
    "section": "4.8 Covariance",
    "text": "4.8 Covariance\nCovariance is a measure of the joint variability of two random variables. It indicates the direction of the linear relationship between the variables.\nIf X and Y are two random variables, the covariance of X and Y is defined as:\n\n\\begin{aligned}\n\\mathrm{Cov}(X,Y) &= \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n\\\\ &= \\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]\n\\end{aligned}\n\\tag{4.35}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-correlation",
    "href": "C1-L01.html#sec-correlation",
    "title": "3  Probability",
    "section": "4.9 Correlation",
    "text": "4.9 Correlation\nCorrelation is a standardized measure of the linear relationship between two random variables. It is a dimensionless quantity that ranges from -1 to 1.\nThe correlation coefficient \\rho_{XY} is defined as the covariance of X and Y divided by the product of their standard deviations:\n\n\\rho_{XY} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X\\sigma_Y}\n\\tag{4.36}\n\n\n\n\n\n\nFinetti, Bruno de. 1937. “La Prévision: Ses Lois Logiques, Ses Sources Subjectives.” Annales de l’Institut Henri Poincaré 7 (1): 1–68.\n\n\n———. 2017. “Theory of Probability.” Edited by Antonio Machí and Adrian Smith. Wiley Series in Probability and Statistics, January. https://doi.org/10.1002/9781119286387.\n\n\n(https://math.stackexchange.com/users/25097/autolatry), Autolatry. n.d. “Why Square Brackets for Expectation.” Mathematics Stack Exchange. https://math.stackexchange.com/q/1302543.\n\n\nMoivre, Abraham De. 1718. The Doctrine of Chances. H. Woodfall. https://tellingstorieswithdata.com.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” In The Foundations of Mathematics and Other Logical Essays, edited by R. B. Braithwaite, 156–98. McMaster University Archive for the History of Economic Thought. https://EconPapers.repec.org/RePEc:hay:hetcha:ramsey1926.\n\n\nWikipedia contributors. 2023. “Functional (Mathematics) — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Functional_(mathematics)&oldid=1148699341.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "C1-L02.html",
    "href": "C1-L02.html",
    "title": "4  Bayes’ Theorem",
    "section": "",
    "text": "4.1 Bayes’ Theorem",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  },
  {
    "objectID": "C1-L02.html#sec-conditional-probability",
    "href": "C1-L02.html#sec-conditional-probability",
    "title": "4  Bayes’ Theorem",
    "section": "4.2 Conditional Probability",
    "text": "4.2 Conditional Probability\n\n\n\n\n\n\n\nFigure 4.1: conditional probability\n\n\n\n\\mathbb{P}r(A \\mid B)=\\frac{\\mathbb{P}r(A \\cap B)}{\\mathbb{P}r(B)}\n\\tag{4.1}\nindependence\n\n\\mathbb{P}r(A \\mid B) = \\mathbb{P}r(A) \\implies \\mathbb{P}r(A \\cap B) = \\mathbb{P}r(A)\\mathbb{P}r(B)\n\\tag{4.2}\n\n4.2.1 Conditional Probability Example - Female CS Student\nSuppose there are 30 students, 9 of whom are female. Of the 30 students, 12 are computer science majors. 4 of those 12 computer science majors are female.\nWe want to estimate what is the probability of a student being female given that she is a computer science major\nWe start by writing the above in the language of probability by converting frequencies to probabilities. We start with the marginal.\nFirst, the probability of a student being female from the data given above.\n\n\\mathbb{P}r(Female) = \\frac{9}{30} = \\frac{3}{10}\n\nNext, we estimate the probability of a student being a computer science major again just using the data given above.\n\n\\mathbb{P}r(CS) = \\frac{12}{30} = \\frac{2}{5}\n\nNext, we can estimate the joint probability, i.e. the probability of being female and being a CS major. Again we have been given the numbers in the data above.\n\n\\mathbb{P}r(F\\cap CS) = \\frac{4}{30} = \\frac{2}{15}\n\nFinally, we can use the definition of conditional probability and substitute the above\n\n\\mathbb{P}r(F \\mid CS) = \\frac{\\mathbb{P}r(F \\cap CS)}{\\mathbb{P}r(CS)} = \\frac{2/15}{2/5} = \\frac{1}{3}\n\n\nAn intuitive way to think about a conditional probability is that we’re looking at a sub-segment of the original population, and asking a probability question within that segment\n\n\\mathbb{P}r(F \\mid CS^c) = \\frac{\\mathbb{P}r(F\\cap CS^c)}{PS(CS^c)} = \\frac{5/30}{18/30} = \\frac{5}{18}\n\nThe concept of independence is when one event does not depend on another.\n\nA \\perp \\!\\!\\! \\perp B \\iff \\mathbb{P}r(A \\mid B) = \\mathbb{P}r(A)\n\nIt doesn’t matter that B occurred.\nIf two events are independent then the following is true:\n\nA \\perp \\!\\!\\! \\perp B \\iff \\mathbb{P}r(A\\cap B) = \\mathbb{P}r(A)\\mathbb{P}r(B)\n\\tag{4.3}\nThis can be derived from the conditional probability equation.\n\n4.2.2 Inverting Conditional Probabilities\nIf we don’t know \\mathbb{P}r(A \\mid B) but we do know the inverse probability \\mathbb{P}r(B \\mid A) is. We can then rewrite \\mathbb{P}r(A \\mid B) in terms of \\mathbb{P}r(B \\mid A)\n\n\\mathbb{P}r(A \\mid B) = \\frac{\\mathbb{P}r(B \\mid A)\\mathbb{P}r(A)}{\\mathbb{P}r(B \\mid A)\\mathbb{P}r(A) + \\mathbb{P}r(B \\mid A^c)\\mathbb{P}r(A^c)}\n\\tag{4.4}\n\n\n4.2.3 Conditional Probability Example - ELISA HIV test\n\n\n\n\n\n\n\nVideo 4.1\n\n\nLet’s look at an example of an early test for HIV antibodies known as the ELISA test. - The test has a true positive rate of 0.977. - It has a true negative rate of 0.926. - The incidence of HIV in North America is .0026.\nNow we want to know the probability of an individual having the disease given that they tested positive \\mathbb{P}r(HIV | +).\nThis is the inverse probability of the true positive, so we will need to use Bayes’ theorem.\nWe start by encoding the above using mathematical notation, so we know what to substitute into Bayes’ theorem.\nThe true positive rate is:\n\n\\mathbb{P}r(+ \\mid HIV) = 0.977\n\nThe true negative rate is:\n\n\\mathbb{P}r(- \\mid NO\\_HIV) = 0.926\n\nThe probability of someone in North America having this disease was\n\n\\mathbb{P}r(HIV) = .0026\n\nwhat we want is: \\mathbb{P}r(HIV \\mid +)\n\n\\begin{aligned}\n\\mathbb{P}r(HIV \\mid +) &= \\frac{\\mathbb{P}r(+ \\mid HIV)\\mathbb{P}r(HIV)}{\\mathbb{P}r(+ \\mid HIV)\\mathbb{P}r(HIV) + \\mathbb{P}r(+ \\mid NO\\_HIV){\\mathbb{P}r(NO\\_HIV)}}  \\\\ &= \\frac{(.977)(.0026)}{(.977)(.0026) + (1-.977)(1-.0026)}  \\\\ &=  0.033\n\\end{aligned}\n\nThis is a bit of a surprise - although the test has 90% + true and false accuracy - taking it once is only valid 3% of the time. How is this possible?\nWhat happens in Bayes law is that we are updating probabilities. And since we started with such a low probability of .0026, Bayesian updating only brings it up to 0.03.\n\n\\begin{aligned}\n\\mathbb{P}r(A \\mid B) = \\frac{\\mathbb{P}r(B \\mid A_1){(A_1)}}{\\sum_{i=1}^{n}{\\mathbb{P}r(B \\mid A_i)}\\mathbb{P}r(A_i)} \\end{aligned}\n\n\nNote: (McElreath (2015)) discusses how this can be presented less surprisingly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  },
  {
    "objectID": "C1-L02.html#sec-bayes-theorem-video",
    "href": "C1-L02.html#sec-bayes-theorem-video",
    "title": "4  Bayes’ Theorem",
    "section": "4.3 Bayes’ theorem (Video)",
    "text": "4.3 Bayes’ theorem (Video)\n\n\n\n\n\n\n\nVideo 4.2: Bayes Theorem in Probability with Examples\n\n\n\n\n\n\n\n\nFigure 4.2: Bayes theorem\n\n\n\nHere are a few formulations of Bayes’ theorem. We denote H for our hypothesis and E as our evidence i.e. the data!\nWe start by using the definition of conditional probability:\n\n\\mathbb{P}r(A \\mid B) = \\frac{ \\mathbb{P}r(A \\cap B)}{\\mathbb{P}r(B)} \\quad \\text{(conditional probability)}\n\n\n\\begin{aligned}\n{\\color{orange} \\overbrace{\\color{orange} \\mathbb{P}r(H|E)}^{\\text{Posterior}}} &= \\frac{  {\\color{pink} \\overbrace{\\color{pink} \\mathbb{P}r(H \\cap E)}^{\\text{Joint}}}  } {  {\\color{green} \\underbrace{{\\color{green} \\mathbb{P}r(\\text{E})}}_{\\text{Marginal Evidence}}} } \\\\ &= \\frac{  {\\color{red} \\overbrace{\\color{red} P (\\text{H})}^{\\text{Prior}}} \\cdot  {\\color{blue} \\overbrace{\\color{blue} P (E \\mid H)}^{\\text{Likelihood}}} } { {\\color{green} \\underbrace{{\\color{green} \\mathbb{P}r(E)}}_{\\text{Marginal Evidence}}} } \\\\ &= \\frac{  {\\color{red} \\overbrace{\\color{red} P (H)}^{\\text{Prior}}} \\cdot {\\color{blue} \\overbrace{\\color{blue} P (E \\mid H)}^{\\text{Likelihood}}} }{  {\\color{green} \\underbrace{\\color{green} \\mathbb{P}r(E \\mid H) \\mathbb{P}r(H) + \\mathbb{P}r(E \\mid H^c) \\mathbb{P}r(H^c)  }_{\\text{Marginal Evidence}}}}\n\\end{aligned}\n\nWe can extend Bayes theorem to cases with multiple mutually exclusive events:\n\n\n\n\n\n\n\nFigure 4.3: mutually exclusive events\n\n\nif H_1 \\ldots H_n are mutually exclusive events that sum to 1:\n\n\\begin{aligned} \\mathbb{P}r(H_1 \\mid E)\n  & = \\frac{\\mathbb{P}r(E \\mid H)\\mathbb{P}r(H_1)}{\\mathbb{P}r(E \\mid H_1)\\mathbb{P}r(H_1) +\\ldots  + \\mathbb{P}r(E \\mid H_n)\\mathbb{P}r(H_N)} \\\\\n  & = \\frac{\\mathbb{P}r(E \\mid H)\\mathbb{P}r(H_1)}{\\sum_{i=1}^{N} \\mathbb{P}r(E \\mid H_i)\\mathbb{P}r(H_i)} \\end{aligned}\n\nwhere we used the law of total probability in the denominator\nif \\{B_i\\} is a finite or countably finite partition of a sample space then\n\n\\mathbb{P}r(A) = {\\sum_{i=1}^{N} \\mathbb{P}r(A \\cup B_i)}= {\\sum_{i=1}^{N} \\mathbb{P}r(A \\mid B_i)\\mathbb{P}r(B_i)}\n\n\n{\\color{orange} P (\\text{H} \\mid \\text{E})} = \\frac {{\\color{red} \\mathbb{P}r(\\text{H})} \\times {\\color{blue}\\mathbb{P}r(\\text{E} \\mid \\text{H})}} {\\color{gray} {\\mathbb{P}r(\\text{E})}}\n\n\n{\\color{orange} \\overbrace{\\color{orange} P (\\text{Unknown} \\mid \\text{Data})}^{\\text{Posterior}}} = \\frac {{\\color{red} \\overbrace{\\color{red} P (\\text{Unknown})}^{\\text{Prior}}} \\times {\\color{blue} \\overbrace{\\color{blue} P (\\text{Data} \\mid \\text{Unknown})}^{\\text{Likelihood}}}} {{\\color{green} \\underbrace{{\\color{green} \\mathbb{P}r(\\text{E})}}_{\\text{Average likelihood}}}}\n\nThe following is a video explaining Bayes law.\n\n\n\n\n\n\n\nVideo 4.3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  },
  {
    "objectID": "C1-L02.html#bayes-theorem-for-continuous-distributions",
    "href": "C1-L02.html#bayes-theorem-for-continuous-distributions",
    "title": "4  Bayes’ Theorem",
    "section": "4.4 Bayes’ Theorem for continuous distributions",
    "text": "4.4 Bayes’ Theorem for continuous distributions\nWhen dealing with a continuous random variable \\theta, we can write the conditional density for \\theta given y as:\n\nf(\\theta \\mid y) =\\frac{f(y\\mid\\theta)f(\\theta)}{\\int f(y\\mid\\theta) f(\\theta) d\\theta }\n\\tag{4.5}\nThis expression does the same thing that the versions of Bayes’ theorem from Lesson 2 do. Because \\theta is continuous, we integrate over all possible values of \\theta in the denominator rather than take the sum over these values. The continuous version of Bayes’ theorem will play a central role from Lesson 5 on.\n\n\n\n\n\n\n\nFigure 4.4: Rev. Thomas Bayes by Mark Riehl\n\n\n\n\n\n\n\n\nTipHistorical Note on The Reverend Thomas Bayes\n\n\n\nBayes Rule is due to Thomas Bayes (1701-1761) who was an English statistician, philosopher and Presbyterian minister. Although Bayes never published what would become his most famous accomplishment; his notes were edited and published posthumously by Richard Price.\n\n\n\n\n\n\n\n\nMcElreath, Richard. 2015. Statistical Rethinking, a Course in r and Stan.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes' Theorem</span>"
    ]
  },
  {
    "objectID": "C1-L03.html",
    "href": "C1-L03.html",
    "title": "5  Distributions",
    "section": "",
    "text": "5.1 Distributions",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-bernoulli--binomial-distribution",
    "href": "C1-L03.html#sec-the-bernoulli--binomial-distribution",
    "title": "5  Distributions",
    "section": "5.2 The Bernoulli & Binomial Distribution",
    "text": "5.2 The Bernoulli & Binomial Distribution\n\n\n\n\n\n\n\nFigure 5.1: Bernoulli and Binomial Distributions\n\n\nThese two distributions are built on a trial of a coin toss (possibly biased).\n\nWe use the Bernoulli distribution to model a random variable for the probability of such a coin toss trial.\nWe use the Binomial distribution to model a random variable for the probability of getting k heads in N independent trials.\n\n\n5.2.1 The Bernoulli Distribution\nArises when modeling events with two possible outcomes, Success and Failure for a coin toss these can be Heads and Tails\n\nX \\sim \\text{Bernoulli}(p) =\n\\begin{cases}\n   \\mathbb{P}r(X=1) = p & \\text{success} \\\\\n   \\mathbb{P}r(X=0)=1-p & \\text{failure}\n\\end{cases}\n\\tag{5.1}\nWhere parameter p is the probability of getting heads.\nThe probability for the two events is:\nNotation:\n\nwe use (Roman) p if its value is known.\n\nwe use (Greek) \\theta when its value is unknown.\n\nThis is a probability mass function since it is discrete. But we call it a Probability Density Function (PDF) in the measure-theoretic sense.\n\nf(X=x\\mid p) = p^x(1-p)^x \\mathbb{I}_{[0,1]}(x)\n\\tag{5.2}\n\n\\mathbb{E}(x)= p\n\\tag{5.3}\n\n\\text{Var}(x)= \\mathbb{P}r(1-p)\n\\tag{5.4}\n\n\nCode\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=0.30, var=0.21, skew=0.87, kurt=-1.24\n\n\nCode\nx = np.arange(bernoulli.ppf(0.01, p),\n              bernoulli.ppf(0.99, p))\nax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\n\nrv = bernoulli(p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\nBernoulli distribution\n\n\n\n\nCode\n## Generate random numbers\nr = bernoulli.rvs(p, size=10)\nr\n\n\narray([0, 0, 0, 1, 0, 1, 0, 0, 0, 1])\n\n\n\n\n\n\n\n\n\nFigure 5.2: Jacob Bernoulli\n\n\n\n\n\n\n\n\nTipBiographical note on Jacob Bernoulli\n\n\n\n\nIt seems that to make a correct conjecture about any event whatever, it is necessary to calculate exactly the number of possible cases and then to determine how much more likely it is that one case will occur than another. (Bernoulli 1713)\n\nThe Bernoulli distribution as well as The Binomial distribution are due to Jacob Bernoulli (1655-1705) who was a prominent mathematicians in the Bernoulli family. He discovered the fundamental mathematical constant e. However, his most important contribution was in the field of probability, where he derived the first version of the law of large numbers.\nfor a fuller biography see\n\n\n\n\n5.2.2 The Binomial Distribution\n\n\\overbrace{\\underbrace{\\fbox{0}\\ \\ldots \\fbox{0}}_{N_0}\\ \\underbrace{\\fbox{1}\\ \\ldots \\fbox{1}}_{N_1}}^N\n\\tag{5.5}\nThe Binomial distribution models counts of successes in independent Bernoulli trials . It arises when we need to consider the summing N independent and identically distributed Bernoulli RV with the same probability of success \\theta.\n\n\n\n\n\n\nTipConditions\n\n\n\n\nDiscrete data\nTwo possible outcomes for each trial\nEach trial is independent\nThe probability of success/failure is the same in each trial\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial reparams mindmap\n\n\n\n\n\nX \\sim Bin[n,p]\n\\tag{5.6}\nthe probability function\n\nf(X=x \\mid \\theta) = {n \\choose x} \\theta^x(1-\\theta)^{n-x}\n\\tag{5.7}\n\n\\mathcal{L}(\\theta)=\\prod_{i=1}^{n} {n\\choose x_i}  \\theta ^ {x_i} (1− \\theta) ^ {(n−x_i)}\n\\tag{5.8}\n\n\\begin{aligned}\n\\ell( \\theta) &= \\log \\mathcal{L}( \\theta) \\\\\n              &= \\sum_{i=1}^n \\left[\\log {n\\choose x_i} + x_i \\log  \\theta + (n-x_i)\\log (1- \\theta) \\right]\n\\end{aligned}\n\\tag{5.9}\n\n\\mathbb{E}[X]= N \\times  \\theta\n\\tag{5.10}\n\n\\mathbb{V}ar[X]=N \\cdot \\theta \\cdot (1-\\theta)\n\\tag{5.11}\n\n\\mathbb{H}(X) = \\frac{1}{2}\\log_2 \\left (2\\pi n \\theta(1 - \\theta)\\right) + O(\\frac{1}{n})\n\\tag{5.12}\n\n\\mathcal{I}(\\theta)=\\frac{n}{ \\theta \\cdot (1- \\theta)}\n\\tag{5.13}\n\n5.2.2.1 Relationships\n\n\n\n\n\n\n\nFigure 5.3: binomial distribution relations\n\n\nThe Binomial Distribution is related to:\n\nthe Geometric distribution,\nThe Multinomial distribution with two categories is the binomial.\nthe Poisson distribution distribution. If X \\sim \\mathrm{Binomial}(n, p) rv and Y \\sim \\mathrm{Poisson}(np) distribution then \\mathbb{P}r(X = n) \\approx \\mathbb{P}r(Y = n) for large n and small np.\nthe Bernoulli distribution If X \\sim \\mathrm{Binomial}(n, p) RV with n = 1, X \\sim Bernoulli(p) RV.\nthe Normal distribution If X \\sim \\mathrm{Binomial}(n, p) RV and Y \\sim Normal(\\mu=np,\\sigma=n\\mathbb{P}r(1-p)) then for integers j and k, \\mathbb{P}r(j \\le X \\le k) \\approx \\mathbb{P}r(j – {1 \\over 2} \\le Y \\le k + {1 \\over 2}). The approximation is better when p ≈ 0.5 and when n is large. For more information, see normal approximation to binomial\nHypergeometric: The difference between a binomial distribution and a hypergeometric distribution is the difference between sampling with replacement and sampling without replacement. As the population size increases relative to the sample size, the difference becomes negligible. So If X \\sim Binomial(n, p) RV and Y \\sim HyperGeometric(N,a,b) then\n\n\n\\lim_{n\\to \\infty} X = Y\n\n\n\nCode\nimport numpy as np\nfrom scipy.stats import binom\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\nn, p = 5, 0.4\nmean, var, skew, kurt = binom.stats(n, p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=2.00, var=1.20, skew=0.18, kurt=-0.37\n\n\nCode\nx = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p))\nax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\nax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\nrv = binom(n, p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n## generate random numbers\nr = binom.rvs(n, p, size=10)\nr\n\n\narray([2, 1, 0, 1, 3, 3, 2, 3, 0, 4])\n\n\n\n\n\n\n The Bernoulli Distribution\n\n\nVideo 5.1\n\n\n\n\n\n5.2.3 The Discrete Uniform Distribution\n\nX \\sim U[0,1]\n\\tag{5.14}\n\n    f(x)=\n    \\begin{cases}\n      1, & \\text{if}\\ x \\in [0,1] \\\\\n      0, & \\text{otherwise}\n    \\end{cases}\n    = \\mathbb{I}_{\\{0 \\le x \\le 1\\}}(x)\n\\tag{5.15}\n\n\nCode\nimport numpy as np\nfrom scipy.stats import uniform\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = uniform.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=0.50, var=0.08, skew=0.00, kurt=-1.20\n\n\nCode\n# we use ppf to get the domain from a range of (0.01,0.99)\nx = np.linspace(uniform.ppf(0.01), uniform.ppf(0.99), 100)\nax.plot(x, uniform.pdf(x), 'r-', lw=5, alpha=0.6, label='uniform pdf')\nrv = uniform()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\n## generate random numbers\nr = uniform.rvs(size=1000)\n\n# And compare the histogram:\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n\n(array([1.05660998, 1.13365445, 0.85849561, 0.9795655 , 0.92453373,\n       1.04560362, 0.99057185, 1.06761633, 0.90252102, 1.00157821,\n       1.04560362]), array([1.68257843e-04, 9.10248673e-02, 1.81881477e-01, 2.72738086e-01,\n       3.63594696e-01, 4.54451305e-01, 5.45307915e-01, 6.36164524e-01,\n       7.27021134e-01, 8.17877743e-01, 9.08734353e-01, 9.99590962e-01]), [&lt;matplotlib.patches.Polygon object at 0x6fffbaa4b9a0&gt;])\n\n\nCode\nax.set_xlim([x[0], x[-1]])\n\n\n(0.01, 0.99)\n\n\nCode\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.2.4 The Continuous Uniform Distribution\n\nX \\sim \\mathrm{Uniform}[\\theta_1,\\theta_2]\n\\tag{5.16}\n\nf(x)= \\frac{1}{\\theta_2-\\theta_1} \\mathbb{I}_{\\{\\theta_1 \\le x \\le \\theta_2\\}}(x)\n\\tag{5.17}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-normal-z-t-distributions",
    "href": "C1-L03.html#sec-the-normal-z-t-distributions",
    "title": "5  Distributions",
    "section": "5.3 The Normal, Z, t Distributions",
    "text": "5.3 The Normal, Z, t Distributions\n The normal, AKA Gaussian distribution is one of the most important distributions in statistics.\nIt arises as the limiting distribution of sums (and averages) of random variables. This is due to the Section 32.1. Because of this property, the normal distribution is often used to model the “errors,” or unexplained variations of individual observations in regression models.\n\n5.3.1 The Standard Normal distribution\n The standard normal distribution is given by\n\n\\mathcal{Z} \\sim \\mathcal{N}[1,0]\n\\tag{5.18}\n\nf(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}}\n\\tag{5.19}\n\n\\mathbb{E}[\\mathcal{Z}] = 0\n\\tag{5.20}\n\n\\mathbb{V}ar[\\mathcal{Z}]= 1\n\\tag{5.21}\n\n\n5.3.2 The Normal distribution\n Now consider X = \\sigma \\mathcal{Z}+\\mu where \\sigma &gt; 0 and \\mu is any real constant. Then \\mathbb{E}(X) = \\mathbb{E}(\\sigma \\mathcal{Z}+\\mu) = \\sigma \\mathbb{E}(\\mathcal{Z}) + \\mu = \\sigma \\times 0 + \\mu = \\mu and Var(X) = Var(\\sigma^2 + \\mu) = \\sigma^2 Var(\\mathcal{Z}) + 0 = \\sigma^2 \\cdot 1 = \\sigma^2\nThen, X follows a normal distribution with mean \\mu and variance \\sigma^2 (standard deviation \\sigma) denoted as\n\nX \\sim \\mathcal{N}[\\mu,\\sigma^2]\n\\tag{5.22}\n\nf(x\\mid \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}  e^{-\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}(x-\\mu)^2}\n\\tag{5.23}\n\n\\mathbb{E}[x]= \\mu\n\\tag{5.24}\n\n\\mathbb{V}ar[x]= \\sigma^2\n\\tag{5.25}\n\nThe normal distribution is symmetric about the mean \\mu and is often described as a bell-shaped curve.\nAlthough X can take on any real value (positive or negative), more than 99% of the probability mass is concentrated within three standard deviations of the mean.\n\nThe normal distribution has several desirable properties.\nOne is that if X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2_1) and X_2 \\sim \\mathcal{N}(\\mu_2, \\sigma^2_2) are independent, then X_1+X_2 \\sim \\mathcal{N}(\\mu_1+\\mu_2, \\sigma^2_1+\\sigma^2_2).\nConsequently, if we take the average of n Independent and Identically Distributed (IID) normal random variables we have\n\n\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})\n\\tag{5.26}\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = norm.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=0.00, var=1.00, skew=0.00, kurt=0.00\n\n\nCode\nx = np.linspace(norm.ppf(0.01),\n                norm.ppf(0.99), 100)\nax.plot(x, norm.pdf(x),\n       'r-', lw=5, alpha=0.6, label='norm pdf')\n\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nr = norm.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n\n(array([0.00374289, 0.00748577, 0.0299431 , 0.04117176, 0.07111486,\n       0.11228662, 0.14971549, 0.17965858, 0.30317386, 0.36680294,\n       0.32937407, 0.38177449, 0.39300315, 0.34060273, 0.35931717,\n       0.21334457, 0.1759157 , 0.10854373, 0.05614331, 0.05988619,\n       0.02245732, 0.01871444, 0.00748577, 0.        , 0.01122866]), array([-3.12530463, -2.8581312 , -2.59095777, -2.32378434, -2.05661091,\n       -1.78943749, -1.52226406, -1.25509063, -0.9879172 , -0.72074377,\n       -0.45357034, -0.18639691,  0.08077651,  0.34794994,  0.61512337,\n        0.8822968 ,  1.14947023,  1.41664366,  1.68381708,  1.95099051,\n        2.21816394,  2.48533737,  2.7525108 ,  3.01968423,  3.28685765,\n        3.55403108]), [&lt;matplotlib.patches.Polygon object at 0x6fffbaf84ee0&gt;])\n\n\nCode\nax.set_xlim([x[0], x[-1]])\n\n\n(-2.3263478740408408, 2.3263478740408408)\n\n\nCode\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 The t-Distribution\n If we have normal data, we can use (Equation 29.32) to help us estimate the mean \\mu. Reversing the transformation from the previous section, we get\n\n\\frac {\\hat X - \\mu}{\\sigma / \\sqrt(n)} \\sim N(0, 1)\n\\tag{5.27}\nHowever, we may not know the value of \\sigma. If we estimate it from data, we can replace it with S = \\sqrt{\\sum_i \\frac{(X_i-\\hat X)^2}{n-1}}, the sample standard deviation. This causes the expression (Equation 29.33) to no longer be distributed as a Standard Normal; but as a standard t-distribution with ν = n − 1 degrees of freedom\n\nX \\sim t[\\nu]\n\\tag{5.28}\n\nf(t\\mid\\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}}\\left (1 + \\frac{t^2}{\\nu}\\right)^{-(\\frac{\\nu+1}{2})}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{5.29}\n\n\\text{where }\\Gamma(w)=\\int_{0}^{\\infty}t^{w-1}e^{-t}\\mathrm{d}t \\text{ is the gamma function}\n\n\nf(t\\mid\\nu)={\\frac {1}{{\\sqrt {\\nu }}\\,\\mathrm {B} ({\\frac {1}{2}},{\\frac {\\nu }{2}})}}\\left(1+{\\frac {t^{2}}{\\nu }}\\right)^{-(\\nu +1)/2}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{5.30}\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the beta function}\n\n\n\\mathbb{E}[Y] = 0 \\qquad \\text{ if } \\nu &gt; 1\n\\tag{5.31}\n\n\\mathbb{V}ar[Y] = \\frac{\\nu}{\\nu - 2} \\qquad \\text{ if } \\nu &gt; 2\n\\tag{5.32}\nThe t distribution is symmetric and resembles the Normal Distribution but with thicker tails. As the degrees of freedom increase, the t distribution looks more and more like the standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#the-exponential-distribution",
    "href": "C1-L03.html#the-exponential-distribution",
    "title": "5  Distributions",
    "section": "5.4 The Exponential Distribution",
    "text": "5.4 The Exponential Distribution\n The Exponential distribution models the waiting time between events for events with a rate \\lambda. Those events, typically, come from a Poisson process.\nThe Exponential distribution is often used to model the waiting time between random events. Indeed, if the waiting times between successive events are independent then they form an \\exp(r(\\lambda) distribution. Then for any fixed time window of length t, the number of events occurring in that window will follow a Poisson distribution with mean t\\lambda.\n\nX \\sim Exp[\\lambda]\n\\tag{5.33}\n\nf(x \\mid \\lambda) = \\frac{1}{\\lambda} e^{- \\frac{x}{\\lambda}}(x)\\mathbb{I}_{\\lambda\\in\\mathbb{R}^+ } \\mathbb{I}_{x\\in\\mathbb{R}^+_0 } \\quad \\text{(PDF)}\n\\tag{5.34}\n\n\\mathbb{E}(x)= \\lambda\n\\tag{5.35}\n\n\\mathbb{V}ar[X]= \\lambda^2\n\\tag{5.36}\n\n\nCode\nimport numpy as np\nfrom scipy.stats import expon\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = expon.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=1.00, var=1.00, skew=2.00, kurt=6.00\n\n\nCode\nx = np.linspace(expon.ppf(0.01), expon.ppf(0.99), 100)\nax.plot(x, expon.pdf(x), 'r-', lw=5, alpha=0.6, label='expon pdf')\n\nrv = expon()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\nr = expon.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n\n(array([0.93590651, 0.74480518, 0.61250426, 0.54390378, 0.42630296,\n       0.25480177, 0.22540157, 0.17640123, 0.2303016 , 0.10780075,\n       0.11760082, 0.08330058, 0.07350051, 0.06860048, 0.04900034,\n       0.07840055, 0.01960014, 0.0294002 , 0.0147001 , 0.0147001 ,\n       0.01960014, 0.02450017, 0.        , 0.00980007, 0.00980007,\n       0.00490003, 0.00490003, 0.        , 0.        , 0.00490003,\n       0.00980007, 0.00490003]), array([2.80515731e-03, 2.06885371e-01, 4.10965584e-01, 6.15045797e-01,\n       8.19126011e-01, 1.02320622e+00, 1.22728644e+00, 1.43136665e+00,\n       1.63544686e+00, 1.83952708e+00, 2.04360729e+00, 2.24768750e+00,\n       2.45176772e+00, 2.65584793e+00, 2.85992814e+00, 3.06400836e+00,\n       3.26808857e+00, 3.47216878e+00, 3.67624900e+00, 3.88032921e+00,\n       4.08440942e+00, 4.28848964e+00, 4.49256985e+00, 4.69665006e+00,\n       4.90073028e+00, 5.10481049e+00, 5.30889070e+00, 5.51297092e+00,\n       5.71705113e+00, 5.92113134e+00, 6.12521156e+00, 6.32929177e+00,\n       6.53337198e+00]), [&lt;matplotlib.patches.Polygon object at 0x6fffbaaf1090&gt;])\n\n\nCode\nax.set_xlim([x[0], x[-1]])\n\n\n(0.010050335853501442, 4.605170185988091)\n\n\nCode\nax.legend(loc='best', frameon=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-geometric-distribution",
    "href": "C1-L03.html#sec-the-geometric-distribution",
    "title": "5  Distributions",
    "section": "6.1 The Geometric Distribution",
    "text": "6.1 The Geometric Distribution\n The Geometric distribution arises when we want to know “What is the number of Bernoulli trials required to get the first success?”, i.e., the number of Bernoulli events until a success is observed, such as the probability of getting the first head when flipping a coin. It takes values on the positive integers starting with one (since at least one trial is needed to observe a success).\n\n\n\n\n\n\n\nVideo 6.1: The Geometric Distribution\n\n\n\nX \\sim \\mathrm{Geo}(p)\n\\tag{6.1}\n\n\\mathbb{P}r(X = x\\mid p) = \\mathbb{P}r(1-p)^{x-1} \\qquad \\forall x \\in N;\\quad 0\\le p \\le 1\n\\tag{6.2}\n\n\\mathbb{E}[X] = \\frac{1}{p}\n\\tag{6.3}\n\n\\mathbb{V}ar[X]=\\frac{1-p}{p^2}\n\\tag{6.4}\n\n\\mathbb{M}_X[t] = \\frac{pe^t}{1-(1-p)e^t} \\qquad t &lt; -log(1-p)\n\\tag{6.5}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-multinomial-distribution",
    "href": "C1-L03.html#sec-the-multinomial-distribution",
    "title": "5  Distributions",
    "section": "6.2 The Multinomial Distribution",
    "text": "6.2 The Multinomial Distribution\nAnother generalization of the Bernoulli distribution and the Binomial distribution is the Multinomial distribution , which sums the successes of Bernoulli trials when there are n different possible outcomes. Suppose we have n trials and there are k different possible outcomes that occur with probabilities p_1, \\ldots, p_k. For example, we are rolling a six-sided die that might be loaded so that the sides are not equally likely, then n is the total number of rolls, k = 6, p_1 is the probability of rolling a one, and we denote by x_1, \\ldots, x_6 a possible outcome for the number of times we observe rolls of each of one through six, where\n\nX \\sim \\mathrm{Multinomial}(p_1,...p_k)\n\n\nP (X = x \\mid p_1,\\ldots,p_k) = \\frac{n!}{x_1! \\cdot \\cdot \\cdot x_k! } \\prod_i p_i^{x_i}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-poisson-distribution",
    "href": "C1-L03.html#sec-the-poisson-distribution",
    "title": "5  Distributions",
    "section": "6.3 The Poisson Distribution",
    "text": "6.3 The Poisson Distribution\nThe Poisson distribution arises when modeling count data. The parameter \\lambda &gt; 0 is the rate at which we expect to observe the thing we are counting. We write this as X \\sim \\mathrm{Poisson}(\\lambda)\n\n\\mathbb{P}r(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{−\\lambda}}{x!} \\qquad \\forall x \\in \\mathbb{N}_0 \\qquad \\text{PDF}\n\\tag{6.6}\n\n\\mathbb{E}[X] = \\lambda \\qquad \\text{Expectation}\n\\tag{6.7}\n\n\\mathbb{V}ar[X] = \\lambda \\qquad \\text{Variance}\n\\tag{6.8}\n\n\\mathbb{M}_X(t) = \\exp[\\lambda(e^t-1)] \\qquad \\text{Moment Generating fn.}\n\\tag{6.9}\n\n\\mathcal{I}_X(t) = \\frac{1}{\\lambda}\n\\tag{6.10}\n\n6.3.1 Relations\n\n\n\n\n\n\n\nFigure 6.1: Relations of the Poisson distribution\n\n\n A Poisson process is a process wherein events occur on average at rate \\mathbb{E}, events occur one at a time, and events occur independently of each other.\n\n\n\n\n\n\n\nFigure 6.2: Siméon Denis Poisson\n\n\n\n\n\n\n\n\nTipBiographical Note on The Siméon Denis Poisson\n\n\n\nThe Poisson distribution is due to Baron Siméon Denis Poisson (1781-1840) see (Poisson 2019, 205–7) was a French mathematician and physicist who worked on statistics, complex analysis, partial differential equations, the calculus of variations, analytical mechanics, electricity and magnetism, thermodynamics, elasticity, and fluid mechanics.\nfor a fuller biography see",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#hypergeometric-distribution",
    "href": "C1-L03.html#hypergeometric-distribution",
    "title": "5  Distributions",
    "section": "6.4 Hypergeometric Distribution",
    "text": "6.4 Hypergeometric Distribution\n\nConsider an urn with a white balls and b black balls. Draw N balls from this urn without replacement. The number white balls drawn, n is Hypergeometrically distributed.\n\nX \\sim \\mathrm{Hypergeometric}(n \\mid N,a,b)\n\n\n\\mathrm{Hypergeometric}(n\\mid N,a,b) = \\frac{\\normalsize{\\binom{a}{n} \\binom{b}{N - n}}} {\\normalsize{\\binom{a + b}{N}}} \\quad \\text{(PDF)}\n\\tag{6.11}\n\n\\mathbb{E}[X]=N\\frac{a}{a+b} \\qquad \\text{(expectation)}\n\\tag{6.12}\n\n\\mathbb{V}ar[X]=N\\,\\frac{ab}{(a + b)^2}\\,\\frac{a+b-N}{a+b-1} \\qquad \\text{(variance)}\n\\tag{6.13}\n\n\n\n\n\n\nBernoulli, J. 1713. Ars Conjectandi [the Art of Conjecturing]. Impensis Thurnisiorum. https://books.google.co.il/books?id=Ba5DAAAAcAAJ.\n\n\nPoisson, S. -D. 2019. “English Translation of Poisson’s \"Recherches Sur La Probabilité Des Jugements En Matière Criminelle Et En Matière Civile\" / \"Researches into the Probabilities of Judgements in Criminal and Civil Cases\".” https://arxiv.org/abs/1902.02782.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "C1-L04.html",
    "href": "C1-L04.html",
    "title": "6  Frequentist Inference",
    "section": "",
    "text": "6.1 Confidence Intervals\nA brief review of the frequentist approach to inference will be useful for contrasting with the Bayesian approach. (Kruschke 2011) Chapter 2 suggests that CI provides the basis for a Bayesian workflow and that the rest of the text fills in the missing pieces.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Frequentist Inference</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-confidence-intervals",
    "href": "C1-L04.html#sec-confidence-intervals",
    "title": "6  Frequentist Inference",
    "section": "",
    "text": "Figure 6.1: frequentist approach to confidence intervals\n\n\n\n\n\n\n\n\n\nImportantFrequentist paradigm\n\n\n\nUnder the frequentist paradigm, one views the data as a random sample from some larger, potentially hypothetical population. We can then make probability statements i.e. long-run frequency statements based on this larger population.\n\n\n\nExample 6.1 (Coin Flip Example - Central Limit Theorem) Suppose we flip a coin 100 times. And we get 44 heads and 56 tails. We can view these 100 flips as a random sample from a much larger infinite hypothetical population of flips from this coin. We can say that each flip is X_i an RV which follows a Bernoulli distribution with some probability p. In this case p is unknown, but we can assume it is fixed since we are using a specific physical coin.\n\nX_i \\sim B(p)\n\\tag{6.1}\nWe ask :\n\nWhat is our best estimate of p the probability of getting a head?\nHow confident are we in the estimate of p?\n\nTo estimate p we will apply the Central limit theorem c.f. Theorem 32.1 which states that the mean of a large number of IID RV with mean \\mu and variance \\sigma^2 is approximately N(\\mu,\\sigma^2).\n\n\\sum^{100}_{i=1} X_i\\mathrel{\\dot \\sim } N(100 \\enspace p, 100 \\enspace \\mathbb{P}r(1-p))\n\\tag{6.2}\nGiven that this is a Normal distribution, we can use the empirical rule often called the 68-95-99.7 rule see (Wikipedia contributors 2023), that says 95% of the time we will get a result is in within 1.96 standard deviations of the mean. This is referred to as a Confidence Interval or (CI).\n\n95\\% \\: \\text{CI}= 100 \\: \\hat{p} \\pm 1.96\\sqrt{100 \\: \\hat{p}(1-\\hat{p})}\n\\tag{6.3}\nSince we observed 44 heads we can estimate \\hat{p} as\n\n\\hat p = \\frac{44}{100} = .44\n\\tag{6.4}\nThis answers our first questions. Now we want to quantify our uncertainty.\n\\begin{aligned}\n95\\% \\: \\text{CI for 100 tosses} &= 100 \\: (.44) \\pm 1.96\\sqrt{100(0.44)(1-0.44)} \\\\ &= 44 \\pm 1.96\\sqrt{(44) (0.56)} \\\\ &= 44 \\pm 1.96\\sqrt{23.64} \\\\ &= (34.27,53.73) \\end{aligned}\n\\tag{6.5}\nWe can be 95% confident that 100\\times \\hat{p} \\in [34.3,53.7] We can say that we’re 95% confident that the true probability p \\in (.343, .537).\nIf one were to ask do I think this coin is Fair ? This is a reasonable hypothesis, since 0.5 \\in [.343,.537].\nBut we can also step back and say what does this interval mean when we say we’re 95% confident? Under the frequentist paradigm, we have to think back to our infinite hypothetical sequence of events, were we to repeat this trial an arbitrarily large number of times and each time create a confidence interval, then on average 95% of the intervals we make will contain the true value of p. This makes senses as a long run frequency explanation.\nOn the other hand, we might want to know something about this particular interval. Does this interval contain the true p. What’s the probability that this interval contains a true p? Well, we don’t know for this particular interval. But under the frequentist paradigm, we’re assuming that there is a fixed right answer for p. Either p is in that interval or it’s not in that interval. And so technically, from a frequentist perspective, the probability that p is in this interval is either 0 or 1. This is not a particularly satisfying explanation. In the other hand when we get to the Bayesian approach we will be able to compute an interval and actually say there is probably a p is in this interval is 95% based on a random interpretation of an unknown parameter.\n\n\n\n\n\n\n\nTip\n\n\n\nIn this example of flipping a coin 100 times, observing 44 heads resulted in the following 95% confidence interval for p: (.343, .537). From this, we concluded that it is plausible that the coin may be fair because p=.5 is in the interval.\nSuppose instead that we flipped the coin 100,000 times, observing 44,000 heads (the same percentage of heads as before). Then using the method just presented, the 95% confidence interval for p is (.437, .443). Is it still reasonable to conclude that this is a fair coin with 95% confidence?\nNo Because 0.5 \\not\\in (.437, .443), we must conclude that p=.5 is not a plausible value for the population mean . Observing 100,000 flips increases the power of the experiment, leading to a more precise estimate with a narrower CI, due to the law of large numbers.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Frequentist Inference</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-likelihood-function-and-MLE",
    "href": "C1-L04.html#sec-likelihood-function-and-MLE",
    "title": "6  Frequentist Inference",
    "section": "6.2 Likelihood function and MLE",
    "text": "6.2 Likelihood function and MLE\n\n\n\n\n\n\n\nFigure 6.2: Likelihood fn and MLE\n\n\n\nExample 6.2 (Heart Attack Patients - MLE) Consider a hospital where 400 patients are admitted over a month for heart attacks, and a month later 72 of them have died and 328 of them have survived.\nwhat’s our estimate of the mortality rate?\n\n\n\n\n\n\nWarningReference Population\n\n\n\nUnder the frequentist paradigm, we must first establish our reference population. This is the cornerstone of our thinking as we are considering how the sample parameter approximates the population statistic. What do we think our reference population is here?\n\nRef Pop 1: Heart attack patients in the region.\nRef Pop 2: Heart attack patients that are admitted to this hospital, but over a longer period.\nRef Pop 3: The people in the region who might have a heart attack and might get admitted to this hospital.\n\nBoth Ref Pop 1 and Ref Pop 2 seem like viable options. Unfortunately, in our data is not a random sample drawn from either. We could pretend they are and move on, or we could also try to think harder about what our data is sampled from, perhaps Ref Pop 3.\nThis is an odd hypothetical situation, and so there are some philosophical issues with the setup of this whole problem within the frequentist paradigm\n\n\n\nY_i \\sim Bernulli(p)\n\\tag{6.6}\nSince this is a Bernoulli trial we need to specify what we interpret as the success . In this case, the success is a mortality.\n\n\\mathbb{P}r(Y_i=1) = \\theta\n\\tag{6.7}\nThe PDF for the dataset can be written in vector form. \\mathbb{P}r(\\vec{Y}=\\vec{y} \\mid \\theta) is the Probability of all the Y’s take some value little y given a value of theta.\n\n\\begin{aligned}\n\\mathbb{P}r(\\vec{Y}=\\vec{y} \\mid \\theta) &= \\mathbb{P}r(Y_1=y,\\dots,Y_n=y_n \\mid \\theta) && \\text{(joint probability)}\n\\\\&= \\mathbb{P}r(Y_1=y_1 \\mid \\theta) \\cdots \\mathbb{P}r(Y_n=y_n \\mid \\theta)            && \\text {(independence)}\n\\\\&= \\prod^n_{i=1} \\mathbb{P}r(Y_i=y_i \\mid \\theta)                            && \\text {(product notation)}\n\\\\&= \\prod^n_{i=1} \\theta^{y_i} (1-\\theta)^{1-y_i}                   && \\text {(Bernoulli PMF)}\n\\end{aligned}\n\\tag{6.8}\nWe now cal the expression for \\mathbb{P}r(\\vec{Y}=\\vec{y} \\mid \\theta) above the likelihood function L(\\theta \\mid \\vec{y} ):\n  \n\\mathcal{L}(\\theta\\mid\\vec{y}) = \\prod^n_{i=1} \\theta^{y_i} (1-\\theta)^{1-y_i}\n\\tag{6.9}\nRecall that we want to find the mortality rate parameter \\theta for our Sample \\vec Y.\nSince it is a probability, it has a range of values from 0 to 1. One way to estimate it is that there should be one value that maximizes (Equation 6.9). It makes the data the most likely to occur for the particular data we observed. This is referred to as the maximum likelihood estimate (MLE).\n\n\\mathop{\\mathrm{MLE}}(\\hat \\theta) = \\mathop{\\mathrm{argmax}} \\; \\mathcal{L}(\\theta\\mid y)\n\nAlthough we are trying to find the \\theta that maximizes the likelihood, in practice, it’s usually easier to maximize the natural logarithm of the likelihood, commonly referred to as the log-likelihood.\n \\begin{aligned}\n  \\mathcal{L}(\\theta) &= \\log(L(\\theta|\\vec{y}))  &&\n\\\\        &= \\log(\\prod^n_{i=1} {\\theta^{y_i} (1-\\theta)^{1-y_i}})  && \\text{subst. liklihood}\n\\\\        &= \\sum^n_{i=1}{ \\log(\\theta^{y_i}) + \\log(1-\\theta)^{1-y_i}} && \\text{log product rule}\n\\\\        &= \\sum^n_{i=1}{ y_i \\log(\\theta) + (1-y_i) \\log(1-\\theta)} && \\text{log power rule}\n\\\\        &= \\log(\\theta) \\sum^n_{i=1}{  y_i + \\log(1-\\theta)} \\sum^n_{i=1}{  (1-y_i) }&& \\text{extracting logs}\n\\end{aligned}\n\\tag{6.10}\nWhat is the interpretation of the MLE of \\theta in the context of the heart attack example?\nIf \\hat \\theta is the MLE for \\theta, the 30-day mortality rate, then all possible values of θ produce a lower value of the likelihood than \\hat \\theta.\nTo calculate the MLE one should differentiate \\mathcal{L}(\\theta) w.r.t. \\theta and then set it equal to 0.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Frequentist Inference</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-computing-the-MLE",
    "href": "C1-L04.html#sec-computing-the-MLE",
    "title": "6  Frequentist Inference",
    "section": "6.3 Computing the MLE",
    "text": "6.3 Computing the MLE\n\n\n\n\n\n\n\nFigure 6.3: Computing the MLE\n\n\n\n\\begin{aligned}\n   && \\mathcal{L}'(\\theta)=& \\frac{1}{\\theta}\\sum_{i=1}^n y_i-\\frac{1}{1-\\theta}\\sum_{i=1}^n 1-y_i \\stackrel{\\text{set}}{=}0  \\text {set derivative to 0}\n\\\\ & \\implies   & \\frac{1}{\\hat \\theta}\\sum_{i=1}^n y_i & = \\frac{1}{1- \\hat \\theta}\\sum_{i=1}^n 1 - y_i\n\\\\ & \\implies   & (1 -\\hat \\theta) \\sum_{i=1}^n{y_i}    &= \\hat\\theta \\sum_{i=1}^n {1-y_i}  \n\\\\ & \\implies   & 1 \\sum_{i=1}^n{y_i} - \\cancel{ \\hat \\theta \\sum_{i=1}^{n}{y_i}} &= \\hat\\theta \\sum_{i=1}^n {1} - \\cancel{\\hat\\theta \\sum_{i=1}^n {y_i}}  \n\\\\ & \\implies   & \\sum_{i=1}^n{y_i}  &= \\hat\\theta N\n\\\\ & \\implies   &  \\hat \\theta &= \\frac{1}{N} \\sum_{i=1}^n y_i  = \\hat p = \\frac{72}{400}=.18\n\\end{aligned}\n\nMaximum Likelihood Estimates (MLEs) possess the following favorable properties:\n\nUnbiased - Thus given sufficient data the MLE will converge to the true value. As a consequence, MLEs are asymptotically unbiased. As we will see in the examples they can still be biased in finite samples.\nconsistent - One important property of maximum likelihood is that it produces consistent estimates.\ninvariant - The invariance principle states that the MLE is invariant against reparameterization.\n\nusing the Central Limit theorem (see Theorem 32.1).\n\n\\hat \\theta \\pm 1.96\\sqrt\\frac{\\hat \\theta(1-\\hat \\theta)}{n}\n\n\n\\hat \\theta \\simeq \\mathcal{N}(\\theta,\\frac{1}{\\mathcal{I} (\\hat \\theta)})\n\nwhere \\mathcal{I} is the Fischer information which for the Bernoulli distribution is:\n\n\\mathcal{I}( \\hat \\theta) = \\frac{1}{\\theta(1-\\theta)}\n\nNote: The Fischer information is a measure of how much information about theta is in each data point!\n\n\n\n\n\n\nTipExplainable AI (XAI) & Fischer information\n\n\n\nIn XAI we use discuss local and global explanations.\n\nGlobal explanations explain a black box model’s predictions based on each feature, via its parameters.\nLocal explanations explain the prediction of a specific datum from its features.\n\nsince Fischer information quantifies the information in a data point on a parameter we should be able to use it to produce local and perhaps even global explanations for Bayesian models.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Frequentist Inference</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-computing-the-MLE-examples",
    "href": "C1-L04.html#sec-computing-the-MLE-examples",
    "title": "6  Frequentist Inference",
    "section": "6.4 Computing the MLE: examples",
    "text": "6.4 Computing the MLE: examples\nSome more examples of maximum likelihood estimators.\n\n6.4.1 Computing the MLE for Exponential RV\n\n\n\n\n\n\n\nFigure 6.4: computing the MLE for Exponential RV\n\n\nLet’s say X_i are exponential distributed\n\nX_i \\sim Exp(\\lambda)\n\nLet’s say the data is independent and identically distributed, therefore making the overall density function\n\n\\begin{aligned}\n  f(x \\mid \\lambda) &= \\prod_{i = 1}^n{\\lambda e^{-\\lambda x_i}} && \\text {(simplifying)}\n  \\\\ &= \\lambda^ne^{-\\lambda \\sum{x_i}}\n\\end{aligned}\n\\tag{6.11}\nNow the likelihood function is\n\nL(\\lambda \\mid x)=\\lambda^ne^{-\\lambda \\sum{x_i}}\n\\tag{6.12}\nthe log likelihood is\n\n\\mathcal{L}(\\lambda) = n\\ln{\\lambda} - \\lambda\\sum_i{x_i}\n\\tag{6.13}\nTaking the derivative\n\n\\begin{aligned}\n  \\mathcal{L}^\\prime(\\lambda) &= \\frac{n}{\\lambda} - \\sum_i{x_i} \\stackrel{\\text{set}}{=}0 && \\text {(set derivative = 0)}\n\\\\ \\implies \\frac{n}{\\hat{\\lambda}} &= \\sum_i{x_i} && \\text{(rearranging)}\n\\end{aligned}\n\\tag{6.14}\n\n\\hat{\\lambda} = \\frac{n}{\\sum_i{x_i}} = \\frac{1}{\\bar{x}}\n\\tag{6.15}\n\n\n6.4.2 Computing the MLE for Uniform RV\n\n\n\n\n\n\n\nFigure 6.5: computing the MLE for Uniform RV\n\n\n\nX_i \\sim  \\mathrm{Uniform}[0, \\theta]\n\\tag{6.16}\n\nf(x \\mid \\theta) = \\prod_{i = 1}^n{\\frac{1}{\\theta}\\mathbb{I}_{0 \\le x_i \\le \\theta}}\n\\tag{6.17}\nCombining all the indicator functions, for this to be a 1, each of these has to be true. These are going to be true if all the observations are bigger than 0, as in the minimum of the x is bigger than or equal to 0. The maximum of the x’s is also less than or equal to \\theta.\n\n\\mathcal{L}(\\theta|x) = \\theta^{-1} \\mathbb{I}_{0\\le min(x_i) \\le max(x_i) \\le \\theta}\n\\tag{6.18}\n\n\\mathcal{L}^\\prime(\\theta) = -n\\theta^{-(n + 1)}\\mathbb{I}_{0 \\le min(x_i) \\le max(x_i)\\le \\theta}\n\\tag{6.19}\nWe ask, can we set this equal to zero and solve for \\theta? It turns out, this is not equal to zero for any \\theta positive value. We need \\theta to be strictly larger than zero. But for \\theta positive, this will always be negative. The derivative is negative, that says this is a decreasing function. Therefore this function will be maximized when we pick \\theta as small as possible. What’s the smallest possible value of \\theta we can pick? Well we need in particular for \\theta to be larger than all of the X_i. And so, the maximum likelihood estimate is the maximum of X_i\n\n\\hat{\\theta} = max(x_i)\n\\tag{6.20}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Frequentist Inference</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-cumulative-distribution-function",
    "href": "C1-L04.html#sec-cumulative-distribution-function",
    "title": "6  Frequentist Inference",
    "section": "6.5 Cumulative Distribution Function",
    "text": "6.5 Cumulative Distribution Function\nThe cumulative distribution function (CDF) exists for every distribution. We define it as F(x) = \\mathbb{P}r(X \\le x) for random variable X.\nIf X is discrete-valued, then the CDF is computed with summation F(x) = \\sum_{t = -\\infty}^x {f(t)}. where f(t) = \\mathbb{P}r(X = t) is the probability mass function (PMF) which we’ve already seen.\nIf X is continuous, the CDF is computed with an integral F(x) = \\int_{-\\infty}^x{f(t)dt}\nThe CDF is convenient for calculating probabilities of intervals. Let a and b be any real numbers with a &lt; b. Then the probability that X falls between a and b is equal to \\mathbb{P}r(a &lt; X &lt; b) = \\mathbb{P}r(X \\le b) - \\mathbb{P}r(X \\le a) = F(b) - F(a)\n\n\n\n\n\n\nFigure 6.6: Illustration of using the CDF to calculate the probability of an interval for continuous random variable X. Probability values are represented with shaded regions in the graphs.\n\n\n\n\nExample 6.3 (CDF example 1) Suppose X ∼ Binomial(5, 0.6). Then\n\n  \\begin{aligned}\n  F(1) &= \\mathbb{P}r(X \\le 1)\n\\\\ &= \\sum_{−∞}^1 f(t)\n\\\\ &= \\sum_{t=−∞}^{-1} 0 + \\sum_{t=0}^1 {5 \\choose t} 0.6^t (1 − 0.6)^{5−t}\n\\\\ &= {5 \\choose 0} 0.6^0 (1 − 0.6)5−0 +{5 \\choose 1} 0.6^1 (1 − 0.6)^{5−1}\n\\\\ &= (0.4)^5 + 5(0.6)(0.4)^4\n\\\\ &≈ 0.087\n\\end{aligned}\n\\tag{6.21}\n\n\nExample 6.4 (CDF example 1) Example: Suppose Y ∼ Exp(1). Then\n\n\\begin{aligned}\nF(2) &= \\mathbb{P}r(Y \\le 2)\n\\\\ &= \\int^{2}_{−∞} e^{−t}\\mathbb{I}_{(t≥0)} dt\n\\\\ &= \\int^{2}_{0} e^{−t}dt\n\\\\ &= −e^{−t}|^2_0\n\\\\ &= −(e^{−2} − e^0)\n\\\\ &= 1−e^{−2}\n\\\\ &≈ 0.865\n\\end{aligned}\n\\tag{6.22}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Frequentist Inference</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-quantile-function",
    "href": "C1-L04.html#sec-quantile-function",
    "title": "6  Frequentist Inference",
    "section": "6.6 Quantile Function",
    "text": "6.6 Quantile Function\nThe CDF takes a value for a random variable and returns a probability. Suppose instead we start with a number between 0 and 1, which we call p, and we wish to find a value x so that \\mathbb{P}r(X \\le x) = p. The value x which satisfies this equation is called the p quantile. (or 100p percentile) of the distribution of X.\n\nExample 6.5 (Quantile Function example 1) In a standardized test, the 97th percentile of scores among all test-takers is 23. Then 23 is the score you must achieve on the test in order to score higher than 97% of all test-takers. We could equivalently call q = 23 the .97 quantile of the distribution of test scores.\n\n\nExample 6.6 (Quantile Function example 2) The middle 50% of probability mass for a continuous random variable is found between the .25 and .75 quantiles of its distribution. If Z \\sim N(0, 1), then the .25 quantile is −0.674 and the .75 quantile is 0.674. Therefore, \\mathbb{P}r(−0.674 &lt;Z &lt;0.674) = 0.5.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Frequentist Inference</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-plotting-the-likelihood-function-in-r",
    "href": "C1-L04.html#sec-plotting-the-likelihood-function-in-r",
    "title": "6  Frequentist Inference",
    "section": "7.1 Plotting the likelihood function in R",
    "text": "7.1 Plotting the likelihood function in R\nGoing back to the hospital example\n\n\nCode\nlikelihood = function(n, y, theta) {\n  return(theta^y * (1 - theta)^(n - y))\n}\ntheta = seq(from = 0.01, to = 0.99, by = 0.01)\nplot(theta, likelihood(400, 72, theta))\n\n\n\n\n\n\n\n\n\nYou can also do this with log likelihoods. This is typically more numerically stable to compute\n\n\nCode\nloglike = function(n, y, theta) {\n  return(y * log(theta) + (n - y) * log(1 - theta))\n}\nplot(theta, loglike(400, 72, theta))\n\n\n\n\n\n\n\n\n\nHaving these plotted as points makes it difficult to see, let’s plot it as lines\n\n\nCode\nplot(theta, loglike(400, 72, theta), type = \"l\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Frequentist Inference</span>"
    ]
  },
  {
    "objectID": "C1-L05.html",
    "href": "C1-L05.html",
    "title": "7  Bayesian Inference",
    "section": "",
    "text": "7.1 Inference example: frequentist\nWe’ll start by defining the unknown parameter \\theta, this is either that the coin is fair or it’s a loaded coin.\n\\theta = \\{\\text{fair},\\ \\text{loaded}\\} \\qquad \\text{(parameter)}\n\\tag{7.1}\nwe get to flip it five times but we do not know what kind of coin it is\nX \\sim Bin(5, \\theta) \\qquad \\text{(model)}\n\\tag{7.2}\neach value of theta gives us a competing binomial likelihood:\nf(x\\ mid\\theta) = \\begin{cases}\n      {5 \\choose x}(\\frac{1}{2})^5            & \\theta = \\text{fair}\n\\\\    {5 \\choose x} (.7)^x (.3)^{5 - x}       & \\theta = \\text{loaded}\n   \\end{cases} \\qquad \\text{(likelihood)}\n\\tag{7.3}\nWe can also rewrite the likelihood f(x \\mid \\theta) using indicator functions\nf(x\\mid\\theta) = {5\\choose x}(.5)^5\\mathbb{I}_{\\{\\theta= \\text{fair}\\}} + {5 \\choose x}(.7)^x(.3)^{5 - x}\\mathbb{I}_{\\{\\theta = \\text{loaded}\\}} \\qquad \\text{(likelihood)}\n\\tag{7.4}\nIn this case, we observed that x = 2\nf(\\theta \\mid x = 2) = \\begin{cases}\n    0.3125 & \\theta = \\text{fair} \\\\\n    0.1323 & \\theta = \\text{loaded}\n\\end{cases} \\qquad \\text{(sub. x=2)}\n\\tag{7.5}\n\\therefore \\hat{\\theta} = \\text{fair} MLE\n\\tag{7.6}\nThat’s a good point estimate, but then how do we answer the question, how sure are you?\nThis is not a question that’s easily answered in the frequentest paradigm. Another question is that we might like to know what is the probability that theta equals fair, give, we observe two heads.\n\\mathbb{P}r(\\theta = \\text{fair} \\mid x = 2) = ?\n\\tag{7.7}\nIn the frequentest paradigm, the coin is a physical quantity. It’s a fixed coin, and therefore it has a fixed probability of coining up heads. It is either the fair coin, or it’s the loaded coin.\n\\mathbb{P}r(\\theta = \\text{fair}) = \\{0,1\\}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-inference-example-frequentist",
    "href": "C1-L05.html#sec-inference-example-frequentist",
    "title": "7  Bayesian Inference",
    "section": "",
    "text": "Figure 7.1: coin probability inference\n\n\n\nExample 7.1 (Two Coin Example) Suppose your brother has a coin that you know to be loaded so that it comes up heads 70% of the time. He then comes to you with some coin, you’re not sure which one and he wants to make a bet with you. Betting money that it’s going to come up heads.\nYou’re not sure if it’s the loaded coin or if it’s just a fair one. So he gives you a chance to flip it 5 times to check it out.\nYou flip it five times and get 2 heads and 3 tails.\nWhich coin do you think it is and how sure are you about that?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-bayesian-approach-to-the-problem",
    "href": "C1-L05.html#sec-bayesian-approach-to-the-problem",
    "title": "7  Bayesian Inference",
    "section": "7.2 Bayesian Approach to the Problem",
    "text": "7.2 Bayesian Approach to the Problem\n\n\n\n\n\n\n\nFigure 7.2: Bayesian coin probability inference\n\n\nAn advantage of the Bayesian approach is that it allows you to easily incorporate prior information when you know something in advance of looking at the data. This is difficult to do under the frequentist paradigm.\nIn this case, we’re talking about your brother. You probably know him pretty well. So suppose you think that before you’ve looked at the coin, there’s a 60% probability that this is the loaded coin.\nIn this case, we put this into our prior. Our prior belief is that the probability the coin is loaded is 0.6. We can update our prior beliefs with the data to get our posterior beliefs, and we can do this using the Bayes theorem.\n\n\\begin{aligned}\n  \\mathbb{P}r(\\text{loaded}) &= 0.6\\ && \\text{(prior)}\n\\\\ f(\\theta \\mid x) &= \\frac{f(x \\mid \\theta)f(\\theta)}{\\sum_\\theta{f(x \\mid \\theta)f(\\theta)}} && \\text{(Bayes)}\n\\\\ f(\\theta\\mid x=2)&= \\frac{{5\\choose x} \\left [(\\frac{1}{2})^5(1-0.6)\\ \\mathbb{I}_{(\\theta = \\text{fair})} + (.7)^x (.3)^{5-x}(.6)\\ \\mathbb{I}_{(\\theta = \\text{loaded})}  \\right] } {{5\\choose x} \\left [(\\frac{1}{2})^5(.4) + (.7)^x (.3)^{5-x}(0.6)  \\right] }&& \\text{(sub. x=2)}\n\\\\ &= \\frac{0.0125\\ \\mathbb{I}_{(\\theta = \\text{fair})}  + 0.0079\\ \\mathbb{I}_{(\\theta = \\text{loaded})} }{0.0125+0.0079}&& \\text{(normalize)}\n\\\\ &= \\textbf{0.612}\\ \\mathbb{I}_{(\\theta=\\text{fair})} + 0.388\\ \\mathbb{I}_{(\\theta = \\text{loaded})} && \\text{(MLE)}\n\\end{aligned}\n\\tag{7.8}\nAs you can see in the calculation Equation 7.8, we have the likelihood times the prior in the numerator, and a normalizing constant in the denominator. When we divide the two, we’ll get an answer that adds up to 1. These numbers match exactly in this case because it’s a very simple problem.\nThis is a concept that we will revisit — what’s in the denominator here is always a normalizing constant.\n\n\\mathbb{P}r(\\theta = loaded \\mid x = 2) = 0.388\n\nThis here updates our beliefs after seeing some data about what the probability might be.\nWe can also examine what would happen under different choices of prior.\n\n\\mathbb{P}r(\\theta = loaded) = \\frac{1}{2} \\implies \\mathbb{P}r(\\theta = loaded \\mid x = 2) = 0.297\n\n\n\\mathbb{P}r(\\theta = loaded) = 0.9 \\implies \\mathbb{P}r(\\theta = loaded \\mid x = 2) = 0.792\n\nIn this case, the Bayesian approach is inherently subjective. It represents your perspective, and this is an important part of the paradigm. If you have a different perspective, you will get different answers, and that’s okay. It’s all done in a mathematically vigorous framework, and it’s all mathematically consistent and coherent.\nAnd in the end, we get interpretable results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-continuous-version-of-bayes-theorem",
    "href": "C1-L05.html#sec-continuous-version-of-bayes-theorem",
    "title": "7  Bayesian Inference",
    "section": "7.3 Continuous version of Bayes’ theorem",
    "text": "7.3 Continuous version of Bayes’ theorem\n\n\n\n\n\n\n\nFigure 7.3: Continuous version of Bayes’ theorem",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-posterior-intervals",
    "href": "C1-L05.html#sec-posterior-intervals",
    "title": "7  Bayesian Inference",
    "section": "7.4 Posterior Intervals",
    "text": "7.4 Posterior Intervals\n\n\n\n\n\n\n\nFigure 7.4: Posterior Intervals",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-discussion-cis",
    "href": "C1-L05.html#sec-discussion-cis",
    "title": "7  Bayesian Inference",
    "section": "7.5 Discussion CIs",
    "text": "7.5 Discussion CIs\n\nFrequentist confidence intervals have the interpretation that “If you were to repeat many times the process of collecting data and computing a 95% confidence interval, then on average about 95% of those intervals would contain the true parameter value; however, once you observe data and compute an interval the true value is either in the interval or it is not, but you can’t tell which.”\nBayesian credible intervals have the interpretation that “Your posterior probability that the parameter is in a 95% credible interval is 95%.”\nBayesian intervals treat their bounds as fixed and the estimated parameter as a random variable.\nFrequentist confidence intervals treat their bounds as random variables and the parameter as a fixed value.\n\n\n\n\n\n\n\nNoteDiscussion Under what circumstances would you prefer a frequentist CI or a Bayesian CI?\n\n\n\n7.5.1 Focusing on Bayesian / Frequentist paradigms\n\nA Frequentist CI might be preferred if:\n\nI had plenty of data to support a frequentist construction of frequentist CI and\nI was doing research and refining or refuting a result that has been established using frequentist hypothesis testing.\n\nI would want to show that for H_1 against some null hypothesis, H_0 the parameters have a certain p-value for some effect.\nParticularly when we are interested in the inference and are less interested in using the value of the parameter.\n\nI cannot justify introducing some subjective priors.\n\nA Bayesian CI might be better if:\n\nMy dataset is too small.\nWhat I care about is the parameter’s value and less about hypothesis testing\nI need an estimate of uncertainty for the parameter for the inference it is used in.\nI had subjective reasons to introduce a prior:\n\nI know about constraints\nI have access to expert knowledge\n\nI wish to introduce pooling between groups, to share information for reducing uncertainty.\nMy results are in a Bayesian-oriented domain.\n\n\n\n\n\n\n\n\n\n\nNoteDiscussion: Under what circumstances would you prefer a frequentist CI or a Bayesian CI?\n\n\n\n7.5.2 Focusing on the CI choices\nLet’s point out that this is what we call a loaded question, as it has a bias against the frequentist approach by stating one of its shortcomings when it is still possible to get a point estimate for the parameter and compare it to the CI. Typically one will have already done it say using regression before considering the CI.\nNext, we have all the standard reasons for choosing between the Frequentist and the Bayesian paradigms. I could list them but I don’t think that is the real point of this question, but rather what would we prefer if both were viable options and why?\nCIs are primarily a tool for understanding uncertainties about parameters that encode effects. In the parametric Bayesian approach we are learning the distribution of our parameters so they have uncertainties baked into them. In the Frequentist approach, we look for the least squares point estimates for our parameters and consider using the CI to approximate the long-run uncertainty due to sampling.\nFrequentist CI might be preferable if I am worried about Aletoric uncertainty due to sampling i.e. to what degree can I be certain my experimental outcomes are not due to chance? I would feel this way since I am a classical physicist or a botanist studying a predominately deterministic effect and I see errors in estimating the parameters as artifacts of sampling that can be made smaller till the parameters will converge with the true population statistics and the error will become vanishingly small.\nGiven that I did my best to get a good data sample I just need to check how sure to decide the cardinal question do I publish or do I perish? I need to decide that the result is due to the effect and not due to some conspiracy bad samples.\nBayesian CIs are just a result of using Bayesian analysis which is a requirement to investigate what are predominately random effects that are the domain of quantum physicists, an ecologist, or a geneticist. Since almost everything I study is predominantly random and I need random variables and Bayes law to get to my results. I also need to report confidence intervals for my work when I publish - but if one is a Bayesian, one will use a Bayesian credible interval?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "C1-L06.html",
    "href": "C1-L06.html",
    "title": "8  Priors",
    "section": "",
    "text": "8.1 Priors and prior predictive distributions\nIn this section, we will delve more deeply into choices of Priors and how they influence Bayesian CI by developing the prior predictive (Definition 8.1) and posterior predictive (Definition 8.2) intervals.\nTheoretically, we’re defining a cumulative distribution function for the parameter\n\\mathbb{P}r(\\theta \\le c) \\qquad \\forall c \\in \\mathbb{R}\nWe need to do this for an infinite number of possible sets but it isn’t practical to do, and it would be very difficult to do it coherently so that all the probabilities were consistent. Therefore in practice, we tend to work with a convenient family that is flexible enough for members to represent our beliefs.\nGenerally if one has enough data, the information in the data will overwhelm the information in the prior. This makes it seem like the prior is less important in terms of the form and substance of the posterior. Once the prior is overwhelmed, any reasonable choice of prior will lead to approximately the same posterior. This is a point where the Bayesian approach should converge to the frequentist and can be shown to be more or less objective.\nOn the other hand choices of priors can be important because even with masses of data, groups and items can be distributed very sparsely in which case priors can have a lasting impact on the posteriors. Secondly, we can decide to pick priors that have a long-lasting impact on operating as regularizing constraints within our models. In such cases, the impact of the prior can be significant.\nOne of our guiding questions will be to consider how much information the prior and the data contribute to the posterior. We will consider the effective sample size of different priors.\nFinally, a bad choice of priors can lead to specific issues.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Priors</span>"
    ]
  },
  {
    "objectID": "C1-L06.html#sec-priors-and-prior-predictive-distributions",
    "href": "C1-L06.html#sec-priors-and-prior-predictive-distributions",
    "title": "8  Priors",
    "section": "",
    "text": "ImportantChoosing a prior\n\n\n\nHow should we choose a prior?\n\nOur prior needs to represent our perspectives, beliefs, and our uncertainties.\nIt should encode any constraints on the data or parameters.\n\nage is positive and less than 120\n\nIt can regularize the data\nIt could encode expert knowledge we have elicited from domain experts.\nIt should prefer informative priors over uninformative ones.\n\n\n\n\n\n\n\n\n\n\n\nExample 8.1 (Example of Bad Prior) Suppose we chose a prior that says the probability of \\mathbb{P}r(\\theta = \\frac{1}{2}) = \\delta( \\frac{1}{2})= 1\nAnd thus, the probability of \\theta equaling any other value is 0. If we do this, our data won’t make a difference since we only put a probability of 1 at a single point.\n\nf(\\theta \\mid y) \\propto f(y \\mid\\theta)f(\\theta) = f(\\theta) = \\delta(\\theta)\n\\tag{8.1}\n\n\n\n\n\n\n\nCautionAvoid priors that assign 0 or 1\n\n\n\n\nEvents with a prior probability of 0 will always have a posterior probability of 0 because f(\\theta)=0 in (Equation 8.1) the product will and therefore the posterior be 0\nEvents with a prior probability of 1, will always have a posterior probability of 1. This is a little harder to see. In this case f(\\theta^c)=0 in (Equation 8.1) so that the posterior will again be zero elsewhere.\n\n\n\n\nIt is good practice to avoid assigning a probability of 0 or 1 to any event that has already occurred or is already known not to occur.\nIf the priors avoid 0 and 1 values the information within the data will eventually overwhelm the information within the prior.\n\n\n8.1.1 Calibration - making priors precise\n\n\nQ. How do we calibrate our prior probability to reality?\nCalibration of predictive intervals is a useful concept in terms of choosing priors. If we make an interval where we’re saying we predict 95% of new data points will occur in this interval. It would be good if, in reality, 95% of new data points did fall in that interval. This is a frequentist concept but this is important for practical statistical purposes so that our results reflect reality.\n\n\n\n\n\n\n\nFigure 8.1: Prior Predictive Distribution\n\n\nWe can compute a predictive interval. This is an interval such that 95% of new observations are expected to fall into it. It’s an interval for the data rather than an interval for \\theta\n\nDefinition 8.1 (Prior Predictive Distribution) The prior predictive distribution expresses our uncertainty about a parameter, i.e. the distribution of its possible values before we observe any data.\n\n\\begin{aligned}\nf(y) &= \\int{f(y \\mid\\theta)f(\\theta)d\\theta} &&\\text {by Bayes theorem}\n\\\\&= \\int{f(y, \\theta)d\\theta} && \\text{the joint probability}\n\\end{aligned}\n\\tag{8.2}\n\nf(y,\\theta) is the joint density of y and \\theta.\nIf we are integrating out \\theta, we will end up with a marginalized probability distribution of the data.\nHowever, we may well decide to not integrate out \\theta completely, so we will end up with a predictive interval.\nBut no data y has been observed, so this is the prior predictive before any data is observed.\nIt is used in prior predictive checks to assess whether the choice of prior distribution captures our prior beliefs.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Priors</span>"
    ]
  },
  {
    "objectID": "C1-L06.html#sec-prior-predictive-binomial-example",
    "href": "C1-L06.html#sec-prior-predictive-binomial-example",
    "title": "8  Priors",
    "section": "8.2 Prior Predictive: Binomial Example",
    "text": "8.2 Prior Predictive: Binomial Example\n\n\n\n\n\n\n\nFigure 8.2: Prior Predictive Distribution Binomial Example\n\n\nSuppose we’re going to flip a coin 10 times and count the number of heads we see. But we are thinking about this in advance of actually doing it, and we are interested in the predictive distribution\n\n\nQ. How many heads do we predict we’re going to see?\nQ. What’s the probability that it shows up heads?\nSo, we’ll need to choose a prior.\n\nN=10 \\qquad \\text {number of coin flips}\n\nWhere Y_i represents individual coin flips. with Head being a success\n\nY \\sim \\text{Bernoulli}(\\theta)\n\nOur data is the count of successes (heads) in N flips.\n\nX = \\sum_{i=0}^N Y_i \\qquad\n\nIf we think that all possible coins or all possible probabilities are equally likely, then we can put a prior for \\theta that’s flat over the interval from 0 to 1. That is the Uniform prior (Equation 5.17):\n\nf(\\theta)=\\mathbb{I}_{[0 \\le \\theta \\le 1]}\n\nThe predictive probability is a binomial likelihood times the prior = 1\n\nf(x) = \\int f(x \\mid\\theta) f(\\theta) d\\theta = \\int_0^1 \\frac{10!}{x!(10-x)!} \\theta^x(1-\\theta)^{10-x}(1) d \\theta\n\nNote that because we’re interested in X at the end, it’s important that we distinguish between a Binomial density and a Bernoulli density. Here we just care about the total count rather than the exact ordering which would be Bernoulli.\nFor most of the analyses, we’re doing, where we’re interested in \\theta rather than x, the binomial and the Bernoulli are interchangeable because the part in here that depends on \\theta is the same.\nTo solve this integral let us recall that:\n\nn! =\\Gamma(n+1)\n\\tag{8.3}\nand\n\nZ \\sim \\text{Beta}(\\alpha,\\beta)\n\nThe PDF for the beta distribution is given as:\n\nf(z)= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} z^{\\alpha−1}(1−z)^{\\beta−1}I_{(0 &lt; z &lt;1)}\n\nwhere \\alpha&gt;0 and \\beta&gt;0.\n\n\\begin{aligned}\n  f(x) &= \\int f(x \\mid\\theta) f(\\theta) d\\theta && \\text {prior predictive dfn}\n\\\\ &= \\int_0^1 \\frac{10!}{x!(10-x)!} \\theta^x(1-\\theta)^{10-x}( \\mathbb{I_{[0,1]}}) d \\theta && \\text {subst. Binomial, } \\mathbb{I_{[0,1]}}\n\\\\ &= \\int_0^1 \\frac{\\Gamma(11)}{\\Gamma(x+1)\\Gamma(11-x)} \\theta^{(x+1)-1}(1-\\theta)^{(11-x)-1}(1) d \\theta && \\text {convert to Beta(x+1,11-x), }\n\\\\ &=\\frac{\\Gamma(11)}{\\Gamma(12)}\n\\cancel{\n  \\int_0^1 \\frac{\\Gamma(12)}{\\Gamma(x+1)\\Gamma(11-x)}\\theta^{(x+1)-1}(1-\\theta)^{(11-x)-1}(1)d \\theta\n} && \\text {integrating PDF=1 }\n\\\\ &=\\frac{\\Gamma(11)}{\\Gamma(12)} \\times 1\n= \\frac{10!}{11!}\n=\\frac{1}{11} && \\forall x \\in \\{1,2,\\dots,10\\}\n\\end{aligned}\n\nThus we see that if we start with a uniform prior, we then end up with a discrete uniform predictive density for X. If all possible \\theta probabilities are equally likely, then all possible sums X outcomes are equally likely.\nThe integral above is a beta density, all integrals of valid beta densities equal one.\n\nf(x) = \\frac{\\Gamma(11)}{\\Gamma(12)} = \\frac{10!}{11!} = \\frac{1}{11}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Priors</span>"
    ]
  },
  {
    "objectID": "C1-L06.html#posterior-predictive-distribution",
    "href": "C1-L06.html#posterior-predictive-distribution",
    "title": "8  Priors",
    "section": "8.3 Posterior Predictive Distribution",
    "text": "8.3 Posterior Predictive Distribution\n\n\n\n\nPosterior Predictive Distribution\n\nWhat about after we’ve observed data? What’s our posterior predictive distribution?\nGoing from the previous example, let us observe after one flip that we got a head.\nWe want to ask, what’s our predictive distribution for the second flip, given we saw a head on the first flip?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Priors</span>"
    ]
  },
  {
    "objectID": "C1-L06.html#posterior-predictive-distribution-1",
    "href": "C1-L06.html#posterior-predictive-distribution-1",
    "title": "8  Priors",
    "section": "8.4 Posterior Predictive Distribution",
    "text": "8.4 Posterior Predictive Distribution\nThe posterior predictive distribution is produced analogously to the posterior predictive distribution by marginalizing the posterior with respect to the parameter.\n\nDefinition 8.2 (Posterior Predictive Distribution) \n\\begin{aligned}\nf(y_2 \\mid y_1) &= \\text{likelihood}\\times \\text{posterior} \\\\\n&= \\int{f(y_2 \\mid \\theta,y_1) \\; f(\\theta \\mid y_1)}d\\theta\n\\end{aligned}\n\\tag{8.4}\n\n\n\n\n\n\n\nTipMarginalizing distribution\n\n\n\nSuppose we have an experiment with events based on two RVs: - (C) a coin toss - (D) and a dice toss. And we call this event X = \\mathbb{P}r(C,D) = \\mathbb{P}r(C) \\times \\mathbb{P}r(D)\n\n\n\nC D\n1\n2\n3\n4\n5\n6\n\\mathbb{P}r(C)\n\n\n\n\nH\n1/12\n1/12\n1/12\n1/12\n1/12\n1/12\n6/12\n\n\nT\n1/12\n1/12\n1/12\n1/12\n1/12\n1/12\n6/12\n\n\n\\mathbb{P}r(D)\n2/12\n2/12\n2/12\n2/12\n2/12\n2/12\n1\n\n\n\nWe can recover the \\mathbb{P}r(C) coin’s distribution or the dice distribution \\mathbb{P}r(D) by marginalization. \\mathbb{P}r(X) This is done by summing over the row or columns.\nThe marginal distribution let us subset a joint distribution. The marginal distribution has removed the uncertainty due to a parameter.\nwe use three terms interchangeably :\n\nmarginalizing the posterior w.r.t. \\theta\nintegrating/summing over \\theta\nintegrating \\theta out\n\nThe first is the real idea, the others are the techniques being used to do it. For a predictive distribution we may want to marginalize all the parameters so we end up with the RV we wish to predict.\n\n\nWe’re going to assume that Y_2 is independent of Y_1. Therefore,\n\nf(y_2 \\mid y_1) = \\int{f(y_2 \\mid \\theta)f(\\theta \\mid y_1)d\\theta}\n\nSuppose we’re thinking of a uniform distribution for \\theta and we observe the first flip is a “head”. What do we predict for the second flip?\nThis is no longer going to be a uniform distribution like it was before because we have some data. We’re going to think it’s more likely that we’re going to get a second head. We think this because since we observed a head \\theta is now likely to be at least \\frac{1}{2} possibly larger.\n\nf(y_2 \\mid Y_1 = 1) = \\int_0^1{\\theta^{y_2}(1-\\theta)^{1-y_2}2\\theta d\\theta}\n\n\nf(y_2 \\mid Y_1 = 1) = \\int_0^1{2\\theta^{y_2 + 1}(1-\\theta)^{1-y_2}d\\theta}\n\nWe could work this out in a more general form, but in this case, Y_2 has to take the value 0 or 1. The next flip is either going to be heads or tails so it’s easier to just plop in a particular example.\n\n\\mathbb{P}r(Y_2 = 1 \\mid Y_1 = 1) = \\int_\\theta^1 {2 \\theta^2 d \\theta} = \\frac{2}{3}\n\n\n\\mathbb{P}r(Y_2 = 0 \\mid Y_1 = 1) = 1 - \\mathbb{P}r(Y_2 = 1 \\mid Y_1 = 1) = 1 - \\frac{2}{3} = \\frac{1}{3}\n\nWe can see here that the posterior is a combination of the information in the prior and the information in the data. In this case, our prior is like having two data points, one head and one tail.\nSaying we have a uniform prior for \\theta is equivalent in an information sense to saying “we have observed one ‘Head’ and one ‘Tail’”.\nSo then when we observe one head, it’s like we now have seen two heads and one tail. So our predictive distribution for the second flip says if we have two heads and one tail, then we have a \\frac{2}{3} probability of getting another head and a \\frac{1}{3} probability of getting another tail.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Priors</span>"
    ]
  },
  {
    "objectID": "C1-L07.html",
    "href": "C1-L07.html",
    "title": "9  Binomial Data",
    "section": "",
    "text": "9.1 Bernoulli/Binomial likelihood with a uniform prior\nWhen we use a uniform prior for a Bernoulli likelihood, we get a beta posterior.\nThe Bernoulli likelihood of \\vec Y \\mid \\theta is\n{\\color{green}f(\\vec Y \\mid \\theta) = {\\theta^{\\sum{y_i}}(1-\\theta)^{n - \\sum{y_i}}}} \\qquad \\text{Bernoulli Likelihood}\nOur prior for \\theta is just a Uniform distribution\n{\\color{red}f(\\theta) = I_{\\{0 \\le \\theta \\le 1\\}} }\\qquad \\text {Uniform prior}\nThus our posterior for \\theta is \n\\begin{aligned}\nf(\\theta \\mid y) & = \\frac{f(y \\mid \\theta) f(\\theta)}{\\int f(y \\mid \\theta)f(\\theta) \\, d\\theta} & \\text{Bayes law} \\\\\n& = \\frac{\\theta^{\\sum{y_i}} (1 - \\theta)^{n - \\sum{y_i}} \\mathbb{I}_{\\{0 \\le \\theta \\le 1\\}}}{\\int_0^1 \\theta^{\\sum{y_i}}(1 - \\theta)^{n - \\sum{y_i}} \\mathbb{I}_{\\{0 \\le \\theta \\le 1\\}} \\, d\\theta} & \\text{subst. Likelihood \\& Prior} \\\\\n& = \\frac{\\theta^{\\sum{y_i}} (1-\\theta)^{n - \\sum{y_i}} \\mathbb{I}_{\\{0 \\le \\theta \\le 1\\}}}{\\frac{\\Gamma(\\sum{y_i} + 1)\\Gamma(n - \\sum{y_i} + 1)}{\\Gamma(n + 2)} \\cancel{\\int_0^1 \\frac{\\Gamma(n + 2)}{\\Gamma(\\sum{y_i} + 1) \\Gamma(n - \\sum{y_i} + 1)} \\theta^{\\sum{y_i}} (1 - \\theta)^{n - \\sum{y_i}} \\, d\\theta}} & \\text{Beta PDF integrates to 1} \\\\\n& = \\frac{\\Gamma(n + 2)}{\\Gamma(\\sum{y_i}+ 1) \\Gamma(n - \\sum{y_i}+ 1)} \\theta^{\\sum{y_i}}(1 - \\theta)^{n - \\sum{y_i}} \\mathbb{I}_{\\{0 \\le \\theta \\le 1\\}} & \\text{simplifying} \\\\\n& = \\mathrm{Beta} \\left (\\sum{y_i} + 1, n - \\sum{y_i} + 1 \\right )\n\\end{aligned}\nWhere we used a trick of recognizing the denominator as a Beta distribution (Equation 29.7) we then manipulate it to take the exact form of Beta. We can then cancel it since the beta density integrates to 1, we can simplify this as From here we can see that the posterior follows a beta distribution\n\\theta | y \\sim Beta(\\sum{y_i} + 1, n - \\sum{y_i} + 1)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07.html#bernoullibinomial-likelihood-with-a-uniform-prior",
    "href": "C1-L07.html#bernoullibinomial-likelihood-with-a-uniform-prior",
    "title": "9  Binomial Data",
    "section": "",
    "text": "Figure 9.1: Binomial likelihood with a Uniform prior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipHistorical Note on R.A. Fisher\n\n\n\n R.A. Fisher’s objection to the Bayesian approach is that “The theory of inverse probability is founded upon an error, and must be wholly rejected” (Fisher 1925) was specifically referring to this example of a”Binomial with a Uniform prior”. The gist of it is that the posterior depends on the parametrization of the prior.(Aldrich 2008). R.A. Jeffry who corresponded with Fisher went on to develop his eponymous priors which were invariant to reparametrization. Which we will consider in Section 13.2",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07.html#conjugate-priors",
    "href": "C1-L07.html#conjugate-priors",
    "title": "9  Binomial Data",
    "section": "9.2 Conjugate Priors",
    "text": "9.2 Conjugate Priors\n\n\n\n\nConjugate Priors\n\nThe uniform distribution is Beta(1, 1)\nAny beta distribution is conjugate for the Bernoulli distribution. Any beta prior will give a beta posterior.\n\nf(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta -1}\\mathbb{I}_{\\{\\theta \\le \\theta \\le 1\\}}\n\n\nf(\\theta \\mid y) \\propto f(y \\mid \\theta)f(\\theta) = \\theta^{\\sum{y_i}}(1-\\theta)^{n - \\sum{y_i}}\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\\mathbb{I}_{\\{\\theta \\le \\theta \\le 1\\}}\n\n\nf(y \\mid\\theta)f(\\theta) \\propto \\theta^{\\alpha + \\sum{y_i}-1}(1-\\theta)^{\\beta + n - \\sum{y_i} - 1}\n\nThus we see that this is a beta distribution\n\n\\theta \\mid y \\sim \\mathrm{Beta}(\\alpha + \\sum{y_i}, \\beta + n - \\sum{y_i})\n\nWhen \\alpha and \\beta are one like in the uniform distribution, then we get the same result as earlier.\nThis whole process where we choose a particular form of prior that works with a likelihood is called using a conjugate family.\nA family of distributions is referred to as conjugate if when you use a member of that family as a prior, you get another member of that family as your posterior.\nThe beta distribution is conjugate for the Bernoulli distribution. It’s also conjugate for the binomial distribution. The only difference in the binomial likelihood is that there is a combinatorics term. Since that does not depend on \\theta, we get the same posterior.\nWe often use conjugate priors because they make life much simpler, sticking to conjugate families allows us to get closed-form solutions easily.\nIf the family is flexible enough, then you can find a member of that family that closely represents your beliefs.\n\nthe Uniform distribution can be written as the Beta(1,1) prior.\nAny Beta prior will give a Beta posterior.\nBeta is conjugate for Binomial and for Bernoulli",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07.html#posterior-mean-and-effective-sample-size",
    "href": "C1-L07.html#posterior-mean-and-effective-sample-size",
    "title": "9  Binomial Data",
    "section": "9.3 Posterior mean and effective sample size",
    "text": "9.3 Posterior mean and effective sample size\n\n\n\n\nEffective Sample Size\n\nReturning to the beta posterior model it is clear how both the prior and the data contribute to the posterior.\nFor a prior Beta(\\alpha,\\beta) we can say that the effective sample size of the prior is\n\n\\alpha + \\beta \\qquad \\text {(ESS)}\n\\tag{9.1}\nRecall that the expected value or mean of a Beta distribution is \\frac{\\alpha}{\\alpha + \\beta}\nTherefore we can derive the posterior mean as\n\n\\begin{aligned}\n   posterior_{mean} &= \\frac{\\alpha + \\sum{y_i}}{\\alpha + \\sum{y_i}+\\beta + n - \\sum{y_i}}\n\\\\                  &= \\frac{\\alpha+\\sum{y_i}}{\\alpha + \\beta + n}\n\\\\                  &= \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\frac{\\alpha}{\\alpha + \\beta} + \\frac{n}{\\alpha + \\beta + n}\\frac{\\sum{y_i}}{n}\n\\\\ &= (\\text{prior weight} \\times \\text{prior mean}) + (\\text{data weight} \\times \\text{data mean})\n\\end{aligned}\n\\tag{9.2}\ni.e. The posterior mean is a weighted average of the prior mean and the data mean.\nThis effective sample size gives you an idea of how much data you would need to make sure that your prior does not have much influence on your posterior.\nIf \\alpha + \\beta is small compared to n then the posterior will largely just be driven by the data. If \\alpha + \\beta is large relative to n then the posterior will be largely driven by the prior.\nWe can make a 95% credible interval using our posterior distribution for \\theta . We can find an interval that has 95 \\% probability of containing \\theta.\nUsing Bayesian Statistics we can do sequential analysis by doing a sequential update every time we get new data. We can get a new posterior, and we just use our previous Posterior as a Prior for doing another update using Bayes’ theorem.\n\nfor a Beta prior, its effective sample size is a + b\nif n &gt;&gt; \\alpha+\\beta the posterior will be predominantly determined by the prior\nif n &lt;&lt; \\alpha+\\beta the posterior will be predominantly determined by the data\nthe idea of an effective sample size of the prior is a useful concept to work with.\n(Wiesenfarth and Calderazzo 2020)\n\nEffective Sample Size (ESS)\nEffective Current Sample size (ECSS)\n\n\n\nESS algorithms\n\n\nwith (Morita, Thall, and Müller 2008) on the left and ECSS on the right\n\n\n\nExercise 9.1 (Discussion on Prior elicitation) Suppose we are interested in global temperatures, and that we have a summary measure that represents the average global temperature for each year. Now we could ask “What is the probability that next year will have a higher average global temperature than this year?” What would be your choice of prior and why? Be specific about the distribution and its parameters. You may use any other information that you want to bring into this problem.\n\nSolution. It is possible to get historical estimates using:\n\nmeteorological and satellites for the last 200 years. \nice cores for the last 800,000 years \ndeep sea sediment oxygen 18 isotope fractation for the last 5 million years.  or yearly temperature data from 1850 till today based on meteorological readings. We can also consider Greenland ice core data covering 800,000 years.\n\nOne simple way is to model the yearly temperature as a random walk\ni.e. Each year is a Bernoulli trial where success is the temperature getting warmer. We can then use the historical data since 1800 to estimate theta the probability that we get warmer.\nI suppose we can use a Binomial prior with parameters for alpha the count of years the temperature increased and N for the total number of years and p the probability the a given year is hotter than the previous.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07.html#data-analysis-example-in-r",
    "href": "C1-L07.html#data-analysis-example-in-r",
    "title": "9  Binomial Data",
    "section": "9.4 Data Analysis Example in R",
    "text": "9.4 Data Analysis Example in R\nSuppose we’re giving two students a multiple-choice exam with 40 questions, where each question has four choices. We don’t know how much the students have studied for this exam, but we think that they’ll do better than just guessing randomly\n\nWhat are the parameters of interest?\n\nThe parameters of interests are \\theta_1 = true the probability that the first student will answer a question correctly, \\theta_2 = true the probability that the second student will answer a question correctly.\n\nWhat is our likelihood?\n\nThe likelihood is \\mathrm{Binomial}(40, \\theta) if we assume that each question is independent and that the probability a student gets each question right is the same for all questions for that student.\n\nWhat prior should we use?\n\nThe Conjugate Prior is a Beta Distribution. We can plot the density with dbeta\n\n\nCode\ntheta = seq(from = 0, to = 1, by = 0.1)\n# Uniform\nplot(theta, dbeta(theta, 1, 1), type = 'l')\n\n\n\n\n\n\n\n\n\nCode\n# Prior mean 2/3\nplot(theta, dbeta(theta, 4, 2), type = 'l')\n\n\n\n\n\n\n\n\n\nCode\n# Prior mean 2/3 but higher effect size (more concentrated at mean)\nplot(theta, dbeta(theta, 8, 4), type = 'l')\n\n\n\n\n\n\n\n\n\n\nWhat are the prior probabilities \\mathbb{P}r(\\theta &gt; 0.25)? \\mathbb{P}r(\\theta &gt; 0.5)? \\mathbb{P}r(\\theta &gt; 0.8)?\n\n\n\nCode\n1 - pbeta(0.25, 8, 4)\n\n\n[1] 0.9988117\n\n\nCode\n#[1] 0.998117\n1 - pbeta(0.5, 8, 4)\n\n\n[1] 0.8867188\n\n\nCode\n#[1] 0.8867188\n1 - pbeta(0.8, 8, 4)\n\n\n[1] 0.1611392\n\n\nCode\n#[1] 0.16113392\n\n\n\nSuppose the first student gets 33 questions right. What is the posterior distribution for \\theta_1 ? \\mathbb{P}r(\\theta &gt; 0.25) ? \\mathbb{P}r(\\theta &gt; 0.5) ? \\mathbb{P}r(\\theta &gt; 0.8) ? What is the 95% posterior credible interval for \\theta_1?\n\n\\text{Posterior} \\sim Beta(8 + 33, 4 + 40 - 33) = Beta(41, 11)\n\nWith a posterior mean of \\frac{41}{41+11} = \\frac{41}{52}\n\nWe can plot the posterior distribution with the prior\n\n\nCode\nplot(theta, dbeta(theta, 41, 11), type = 'l')\nlines(theta, dbeta(theta, 8 ,4), lty = 2) #Dashed line for prior\n\n\n\n\n\n\n\n\n\nPosterior probabilities\n\n\nCode\n1 - pbeta(0.25, 41, 11)\n\n\n[1] 1\n\n\nCode\n#[1] 1\n1 - pbeta(0.5, 41, 11)\n\n\n[1] 0.9999926\n\n\nCode\n#[1] 0.9999926\n1 - pbeta(0.8, 41, 11)\n\n\n[1] 0.4444044\n\n\nCode\n#[1] 0.4444044\n\n\nEqual-tailed 95% credible interval\n\n\nCode\nqbeta(0.025, 41, 11)\n\n\n[1] 0.6688426\n\n\nCode\n#[1] 0.6688426\nqbeta(0.975, 41, 11)\n\n\n[1] 0.8871094\n\n\nCode\n#[1] 0.8871094\n\n\n95% confidence that \\theta_1 is between 0.67 and 0.89\n\nSuppose the second student gets 24 questions right. What is the posterior distribution for \\theta_2? \\mathbb{P}r(\\theta &gt; 0.25)? \\mathbb{P}r(\\theta &gt; 0.5)? \\mathbb{P}r(\\theta &gt; 0.8)? What is the 95% posterior credible interval for \\theta_2\n\n\n\\text{Posterior} \\sim Beta(8 + 24, 4 + 40 - 24) = Beta(32, 20)\n\nWith a posterior mean of \\frac{32}{32+20} = \\frac{32}{52}\nWe can plot the posterior distribution with the prior\n\n\nCode\nplot(theta, dbeta(theta, 32, 20), type = 'l')\nlines(theta, dbeta(theta, 8 ,4), lty = 2) #Dashed line for prior\n\n\n\n\n\n\n\n\n\nPosterior probabilities\n\n\nCode\n1 - pbeta(0.25, 32, 20)\n\n\n[1] 1\n\n\nCode\n#[1] 1\n1 - pbeta(0.5, 32, 20)\n\n\n[1] 0.9540427\n\n\nCode\n#[1] 0.9540427\n1 - pbeta(0.8, 32, 20)\n\n\n[1] 0.00124819\n\n\nCode\n#[1] 0.00124819\n\n\nEqual-tailed 95% credible interval\n\n\nCode\nqbeta(0.025, 32, 20)\n\n\n[1] 0.4808022\n\n\nCode\n#[1] 0.4808022\nqbeta(0.975, 32, 20)\n\n\n[1] 0.7415564\n\n\nCode\n#[1] 0.7415564\n\n\n95% confidence that \\theta_1 is between 0.48 and 0.74\n\nWhat is the posterior probability that \\theta_1 &gt; \\theta_2?\n\ni.e., that the first student has a better chance of getting a question right than the second student?\nEstimate by simulation: draw 1,000 samples from each and see how often we observe \\theta_1 &gt; \\theta_2\n\n\nCode\ntheta1 = rbeta(100000, 41, 11)\ntheta2 = rbeta(100000, 32, 20)\nmean(theta1 &gt; theta2)\n\n\n[1] 0.97565\n\n\nCode\n#[1] 0.975\n\n\n\n\n\n\n\n\nAldrich, John. 2008. “R. A. Fisher on Bayes and Bayes’ Theorem.” Bayesian Analysis 3 (March). https://doi.org/10.1214/08-BA306.\n\n\nFisher, R. A. 1925. Statistical Methods for Research Workers. 1st ed. Edinburgh Oliver & Boyd.\n\n\nMorita, Satoshi, Peter F Thall, and Peter Müller. 2008. “Determining the Effective Sample Size of a Parametric Prior.” Biometrics 64 (2): 595–602.\n\n\nWiesenfarth, Manuel, and Silvia Calderazzo. 2020. “Quantification of Prior Impact in Terms of Effective Current Sample Size.” Biometrics 76 (1): 326–36. https://doi.org/https://doi.org/10.1111/biom.13124.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L08.html",
    "href": "C1-L08.html",
    "title": "10  Poisson Data",
    "section": "",
    "text": "Figure 10.1: Poisson likelihood with a Gamma prior\n\n\n\n\n10.0.1 Poisson - Chocolate Chip Cookie\nIn mass-produced chocolate chip cookies, they make a large amount of dough; mix in a large number of chips; then chunk out the individual cookies. In this process, the number of chips per cookie approximately follows a Poisson distribution.\nIf we were to assume that chips have no volume, then this would be exactly a Poisson process and follow exactly a Poisson distribution. In practice, since chips are not that small, so they follow approximately a Poisson distribution for the number of chips per cookie.\n\n\n\nY_i \\sim \\mathrm{Poisson}(\\lambda)\n\\tag{10.1}\n The likelihood of the data is given by the Poisson distribution.What is the likelihood of the data?\n\n\\begin{aligned}\n{\\color{red}f(y \\mid \\lambda) = \\frac{\\lambda^{\\sum{y_i}}e^{-n\\lambda}}{\\prod_{i = 1}^n{y_i!}}} \\quad \\forall (\\lambda &gt; 0) && \\text{ Poisson Likelihood }\n\\end{aligned}\n\n It would be convenient if we could put a conjugate prior. What distribution looks like \\lambda raised to a power and e raised to a negative power?What type of prior should we put on \\lambda ?\nFor this, we’re going to use a Gamma prior.\n\n\\begin{aligned} \\lambda &\\sim \\mathrm{Gamma}(\\alpha, \\beta) && \\text{Gamma Prior} \\\\ \\color{green}{ f(\\lambda)} &= \\color{green}{\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha - 1}e^{-\\beta\\lambda}} && \\text{subst. Gamma PDF} \\end{aligned}\n\\tag{10.2}\n We can use Bayes theorem to find the posterior.What is the posterior?\n\n\\begin{aligned} {\\color{blue}f(\\lambda \\mid y)} &\\propto \\color{red}{ f(y \\mid \\lambda)} \\color{green}{ f(\\lambda)} && \\text{Bayes without the denominator} \\\\ &\\propto \\color{red}{\\lambda^{\\sum{y_i}}e^{-n\\lambda}}\\color{green}{\\lambda^{\\alpha - 1}e^{-\\beta \\lambda} } && \\text{subst. Likelihood and Prior}\n\\\\ & \\propto { \\color{blue} \\lambda^{\\alpha + \\sum{y_i} - 1}e^{-(\\beta + n)\\lambda} } && \\text{collecting terms}\n\\\\ & \\propto { \\color{blue} \\mathrm{Gamma}(\\alpha + \\sum{y_i}, \\beta + n)}\n\\end{aligned}\n\\tag{10.3}\n The posterior is a Gamma distribution with parameters \\alpha + \\sum{y_i} and \\beta + n.What is the posterior distribution?\nThus we can see that the posterior is a Gamma Distribution\n\n\\lambda \\mid y \\sim \\mathrm{Gamma}(\\alpha + \\sum{y_i}, \\beta + n)\n\\tag{10.4}\n The posterior mean of a Gamma distribution is given byWhat is the posterior mean?\nThe mean of Gamma under this parameterization is: \\frac{\\alpha}{\\beta}\nThe posterior mean is going to be\n\n\\begin{aligned}\n{\\color{blue}\\mu_{\\lambda}} &= \\frac{\\alpha + \\sum{y_i}}{\\beta + n} && \\text{(Posterior Mean)} \\\\\nposterior_{\\mu}\n&= \\frac{\\alpha + \\sum{y_i}}{\\beta + n} \\\\\n&= \\frac{\\beta}{\\beta + n}\\frac{\\alpha}{\\beta} + \\frac{n}{\\beta + n}\\frac{\\sum{y_i}}{n} \\\\\n& \\propto  \\beta \\cdot \\mu_\\text{prior} + n\\cdot \\mu_\\text{data}\n\\end{aligned}\n\\tag{10.5}\n The posterior variance of a Gamma distribution is given byWhat is the posterior variance?\nAs you can see here the posterior mean of the Gamma distribution is also the weighted average of the prior mean and the data mean.\nTherefore, the effective sample size (ESS) of the Gamma prior is \\beta\n\n\n\n\n\n\nTipPrior Elicitation of Gamma Hyper-parameters\n\n\n\nHere are two strategies for choose the hyper-parameters \\alpha and \\beta\n\nAn informative prior with a prior mean guess of \\mu=\\frac{a}{b} e.g. what is the average number of chips per cookie?\n\nNext we need another piece of knowledge to pinpoint both parameters.\nCan you estimate the error for the mean? I.e. what do you think the standard deviation is? Since for the Gamma prior\nWhat is the effective sample size \\text{ESS}=\\beta ?\nHow many units of information do you think we have in our prior v.s. our data points ? \\sigma = \\frac{ \\sqrt{\\alpha} }{\\beta}\n\nA vague prior refers to one that’s relatively flat across much of the space.\n\nFor a Gamma prior we can choose \\Gamma(\\epsilon, \\epsilon) where \\epsilon is small and strictly positive. This would create a distribution with a \\mu = 1 and a huge \\sigma stretching across the whole space. And the effective sample size will also be \\epsilon Hence the posterior will be largely driven by the data and very little by the prior.\n\n\n\n\nThe first strategy with a mean and an ESS will be used in numerous models going forward so it is best to remember these two strategies!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Poisson Data</span>"
    ]
  },
  {
    "objectID": "C1-L09.html",
    "href": "C1-L09.html",
    "title": "11  Exponential Data",
    "section": "",
    "text": "Figure 11.1: Exponential likelihood with a Gamma prior\n\n\n\nExample 11.1 (Time between buses - Exponential) Time between buses\nSuppose you’re waiting for a bus that you think comes on average once every 10 minutes, but you’re not sure exactly how often it comes.\n\n\nY \\sim \\mathrm{Exp}(\\lambda)\n\\tag{11.1}\nYour waiting time has a prior expectation of \\frac{1}{\\lambda}\nIt turns out the gamma distribution is conjugate for an exponential likelihood. We need to specify a prior, or a particular gamma in this case. If we think that the buses come on average every ten minutes, that’s a rate of one over ten.\n\nprior_{\\mu} = \\frac{1}{10}\n\nThus, we’ll want to specify a gamma distribution so that the first parameter dived by the second parameter is 1 \\over 10\nWe can now think about our variability. Perhaps you specify\n\n\\mathrm{Gamma}(100, 1000)\n\nThis will indeed have a prior mean of 1 \\over 10 and it’ll have a standard deviation of 1 \\over 100. If you want to have a rough estimate of our mean plus or minus two standard deviations then we have the following\n\n0.1 \\pm 0.02\n\nSuppose that we wait for 12 minutes and a bus arrives. Now you want to update your posterior for \\lambda about how often this bus will arrive.\n\nf(\\lambda \\mid y) \\propto f(y\\mid \\lambda)f(\\lambda)\n\n\nf(\\lambda \\mid y) \\propto \\lambda e^{-\\lambda y}\\lambda^{\\alpha - 1}e^{-\\beta \\lambda}\n\n\nf(\\lambda \\mid y)  \\propto \\lambda^{(\\alpha + 1) - 1}e^{-(\\beta + y)\\lambda}\n\n\n\\lambda \\mid y \\sim \\mathrm{Gamma}(\\alpha + 1, \\beta + y)\n\nPlugging in our particular prior gives us a posterior for \\lambda which is\n\n\\lambda \\mid y \\sim \\mathrm{Gamma}(101, 1012)\n\nThus our posterior mean is going to be \\frac{101}{1012} = 0.0998.\nThis one observation does not contain a lot of data under this likelihood. When the bus comes and it takes 12 minutes instead of 10, it barely shifts our posterior mean up.\nOne data point does not have a big impact here.\n\nExercise 11.1 We can generalize the result from the lesson to more than one data point.\n\nSuppose\n\nY_1, \\ldots, Y_n \\stackrel{iid}\\sim Exp(\\lambda)=\\lambda e^{-\\lambda x}\\mathbb{I}_{x\\ge0}\n\nwith mean\n\n\\mathbb{E}[Y]=\\frac{1}{\\lambda}\n\nand assume a\n\nf(\\lambda)= \\mathrm{Gamma}(\\alpha, \\beta) \\qquad (\\text{prior for }\\lambda)\n\nThe likelihood is then:\n\nf(y \\mid \\lambda) = \\prod \\lambda e^{-\\lambda x}\\mathbb{I}_{x\\ge0} =  \\lambda ^ n e^{− \\lambda \\sum y_i}\\cdot1\n\nand we can follow the same steps from the lesson to obtain the posterior distribution (try to derive it yourself):\n\n\\lambda \\mid y ∼ \\mathrm{Gamma}(\\alpha + n, \\beta + \\sum y_i)\n\n What is the prior effective sample size (ess) in this model?\n\nSolution. The data sample size n is added to \\alpha to update the first parameter. Thus \\alpha can be interpreted as the sample size equivalent in the prior.\n\nIt might be helpful to think about a related problems…\n\nWe are waiting at a bus stop with 1 bus line, the information at the bus stop say that the bus comes on average every 10 minutes at this time. How long do we expect to wait for the bus?\nwhat if we have waited for k minutes and the bus has not arrived yet? How long do we expect to wait for the bus?\nWhile we are waiting more people arrive at the bus stop. You notice the bus stop features a digital counter and a display with long term mean E and V variance of the number of people at the bus stop. Can we use this information to get a better estimate of our bus arrival time?\nIf we wait at a bus stop with K different bus lines each with the same lambda, and we see a L people waiting. Can we get a better estimate of our bus arrival time?\nWhat if more people come. And we know the mean and variance of the people waiting at the bus stop?\nWhat if a different bus line arrives and the number of people waiting is now M?\nWhat if each bus line has a different lambda, but we know the mean and variance of the people waiting at the bus stop?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exponential Data</span>"
    ]
  },
  {
    "objectID": "C1-L10.html",
    "href": "C1-L10.html",
    "title": "12  Normally distributed Data",
    "section": "",
    "text": "12.1 Normal Likelihood with known variance\nNormally distributed data is not that common. However, modeling using a normal RV is second to none.(Hoff 2009, 75). The CLT is the primary reason that the normal is a good approximation if there are enough IID samples. We will look at two types of conjugate normal priors, and in the next unit we will consider two more uninformative priors for Normally distributed data.\nCharles Zaiontz provides pro types of conjugate priors for normally distributed data:\nIn each case, the unknown refer to population statistics. Since we are able to estimate sample parameters such as the mean and variance quite easily. A key question to consider is how well does our posterior distribution of the parameter representative of the unknown population statistic?\nIdeally, I will update the notes below with proofs of conjugate, prior and posterior and marginal distribution.\nSome of the proofs are in here as well\nSee (Hoff 2009, sec. 5.2)\nLet’s suppose the standard deviation or variance \\sigma^2 is known and we’re only interested in learning about the mean. This is a situation that often arises in monitoring industrial production processes.\nX_i \\stackrel{iid}\\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\tag{12.1}\nIt turns out that the Normal distribution is conjugate for itself when looking for the mean parameter\nPrior\n\\mu \\sim \\mathcal{N}(m_0,S_0^2)\n\\tag{12.2}\nBy Bayes rule:\nf(\\mu \\mid x ) \\propto f(x \\mid \\mu)f(\\mu)\n\\mu \\mid x \\sim \\mathcal{N} \\left (\\frac{\\frac{n\\bar{x}}{\\sigma_0^2} + \\frac{m_0}{s_0^2} }{\\frac{n}{\\sigma_0^2} +\\frac{1}{s_0^2}}, \\frac{1}{\\frac{n}{\\sigma_0^2} + \\frac{1}{s_0^2}}\\right )\n\\tag{12.3}\nwhere:\nLet’s look at the posterior mean\n\\begin{aligned}\nposterior_{\\mu} &= \\frac{\n          \\frac{n}{\\sigma_0^2}}\n       {\\frac{n}{\\sigma_0^2}s + \\frac{1}{s_0^2}}\\bar{x} +     \n          \\frac{ \\frac{1}{s_0^2} }{ \\frac{n}{\\sigma_0^2} + \\frac{1}{s_0^2}\n        }m\n\\\\ &= \\frac{n}{n + \\frac{\\sigma_0^2}{s_0^2} }\\bar{x} + \\frac{ \\frac{\\sigma_0^2}{s_0^2} }{n + \\frac{\\sigma_0^2}{s_0^2}}m\n\\end{aligned}\n\\tag{12.4}\nThus we see, that the posterior mean is a weighted average of the prior mean and the data mean. And indeed that the effective sample size for this prior is the ratio of the variance for the data to the variance in the prior.\nPrior\\ ESS= \\frac{\\sigma_0^2}{s_0^2}\n\\tag{12.5}\nThis makes sense, because the larger the variance of the prior, the less information that’s in it.\nThe marginal distribution for Y is\n\\mathcal{N}(m_0, s_0^2 + \\sigma^2)\n\\tag{12.6}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Normally distributed Data</span>"
    ]
  },
  {
    "objectID": "C1-L10.html#sec-normal-likelihood-with-unknown-mean",
    "href": "C1-L10.html#sec-normal-likelihood-with-unknown-mean",
    "title": "12  Normally distributed Data",
    "section": "",
    "text": "Figure 12.1: Normal likelihood with variance known\n\n\n\n\n\n\n\n\n\n\n\n\n\nn is the sample size\n\\bar{x}=\\frac{1}{n}\\sum x_i is the sample mean\n\\sigma =\\frac{1}{n} \\sum (x_i-\\bar{x})^2 is the sample variance\nindexing parameters with 0 seems to be a convention that they are from the prior:\ns_0 is the prior variance\nm_0 is the prior mean\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nshould we use n-1 in the sample variance?\n\n\n\n\n\n\n\n\n12.1.1 Prior (and posterior) predictive distribution\nThe prior (and posterior) predictive distribution for data is particularly simple in the conjugate normal model .\nIf \ny \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^2)\n and \n\\theta \\sim \\mathcal{N}(m, s_0^2)\n\nthen the marginal distribution for Y, obtained as\n\n\\int f(y,\\theta) d\\theta = \\mathcal{N}(m_0,s_0^2)\n\\tag{12.7}\n\nExample 12.1 Suppose your data are normally distributed with \\mu=\\theta and \\sigma^2=1.\n\ny \\mid \\theta \\sim \\mathcal{N}(\\theta,1)\n\nYou select a normal prior for \\theta with mean 0 and variance 2.\n\n\\theta \\sim \\mathcal{N}(0, 2)\n\nThen the prior predictive distribution for one data point would be N(0, a). What is the value of a?\nSince, m_0 =0, and s^2_0=2 and \\sigma^2=1, the predictive distribution is N(0,2+1).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Normally distributed Data</span>"
    ]
  },
  {
    "objectID": "C1-L10.html#sec-normal-likelihood-with-expectation-and-variance-unknown",
    "href": "C1-L10.html#sec-normal-likelihood-with-expectation-and-variance-unknown",
    "title": "12  Normally distributed Data",
    "section": "12.2 Normal likelihood with expectation and variance unknown",
    "text": "12.2 Normal likelihood with expectation and variance unknown\n\n\n\n\n\n\n\nFigure 12.2: Normal likelihood with a unknown variance\n\n\n\n\n\n\n\n\nTipChallenging\n\n\n\nThis section is challenging.\n\nThe updating derivation is skipped,\nthe posterior\nupdating rule values are introduced without motivations and explanation.\nThe model is also the most complicated in the course, the note at the end says this can be extended hierarchically if we want to specify hyper priors for m, w and \\beta\nOther text discuss this case using a inverse chi squared distribution\n\nIf we can understand the model the homework is going to make sense. Also this is probably the level needed for the other courses in the specialization.\nIt can help to review some of the books:\n\nSee (Hoff 2009, sec. 5.3) which has some R examples.\nSee (Gelman et al. 2013, sec. 5)\n\n\n\nIf both \\mu and \\sigma^2 are unknown, we can specify a conjugate prior in a hierarchical fashion.\n\nX_i \\mid \\mu, \\sigma^2 \\stackrel{iid}\\sim \\mathcal{N}(\\mu, \\sigma^2) \\qquad \\text{(the data given the params) }\n\n\nThis is the level 1 hierarchically model - X_i model our observations.\nWe state on the left, that the RV X is conditioned on the \\mu and \\sigma^2.\nBut the variables \\mu and \\sigma^2 are unknown population statistics which we will need to infer from the data. We can call them latent variables.\n\nNext we add a prior from \\mu conditional on the value for \\sigma^2\n\n\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(m, \\frac{\\sigma^2}{w}) \\qquad \\text{(prior of the mean conditioned on the variance)}\n\nwhere:\n\nw is going to be the ratio of \\sigma^2 and some variance for the Normal distribution. This is the effective sample size of the prior.\nWhy is the mean conditioned on the variance. We can have a model where they are independent too?\nlater on (in the homework) we are told that w can express the confidence in the prior.\nI think this means that Since this is a knowledge of m, i.e. giving w a weight of 1/10 expresses that we value\n\nPerhaps this is due to CLT ?\nThis is level 2 of the model\n\nFinally, the last step is to specify a prior for \\sigma^2. The conjugate prior here is an inverse gamma distribution with parameters \\alpha and \\beta.\n\n\\sigma^2 \\sim \\mathrm{Gamma}^{-1}(\\alpha, \\beta)  \\qquad \\text{prior of the variance}\n\nAfter many calculations… we get the posterior distribution\n\n\\sigma^2 \\mid x \\sim \\mathrm{Gamma}^{-1}(\\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2}\\sum_{i = 1}^n{(x-\\bar{x}^2 + \\frac{nw}{2(n+2)}(\\bar{x} - m)^2)})\n\\tag{12.8}\n\n\\mu \\mid \\sigma^2,x \\sim \\mathcal{N}(\\frac{n\\bar{x}+wm}{n+w}, \\frac{\\sigma^2}{n + w})\n\\tag{12.9}\nWhere the posterior mean can be written as the weighted average of the prior mean and the data mean.\n\n\\frac{n\\bar{x}+wm}{n+w} = \\frac{w}{n + w}m + \\frac{n}{n + w}\\bar{x} \\qquad \\text{post. mean}\n\\tag{12.10}\nIn some cases, we only care about \\mu. We want some inference on \\mu and we may want it such that it does not depend on \\sigma^2. We can marginalize that \\sigma^2 integrating it out. The posterior for \\mu marginally follows a t distribution.\n\n\\mu \\mid x \\sim t\n\nSimilarly, the posterior predictive distribution also is a t distribution.\nFinally, note that we can extend this in various directions, this can be extended to the multivariate normal case that requires matrix vector notations and can be extended hierarchically if we want to specify priors for m, w, \\beta\n\n\n\n\n\n\nGelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis. https://books.google.co.il/books?id=ZXL6AQAAQBAJ.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Springer New York. https://doi.org/10.1007/978-0-387-92407-6.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Normally distributed Data</span>"
    ]
  },
  {
    "objectID": "C1-L11.html",
    "href": "C1-L11.html",
    "title": "13  Non-Informative Priors",
    "section": "",
    "text": "13.1 Non-Informative Priors\nWe’ve seen examples of choosing priors that contain a significant amount of information. We’ve also seen some examples of choosing priors where we’re attempting to not put too much information in to keep them vague.\nAnother approach is referred to as objective Bayesian statistics or inference where we explicitly try to minimize the amount of information that goes into the prior.\nThis is an attempt to have the data have maximum influence on the posterior\nLet’s go back to coin flipping\nY_i \\sim B(\\theta)\nHow do we minimize our prior information in \\theta? One obvious intuitive approach is to say that all values of \\theta are equally likely. So we could have a prior for \\theta which follows a uniform distribution on the interval [0, 1]\nSaying all values of \\theta are equally likely seems like it would have no information in it.\nRecall however, that a Uniform(0, 1) is the same as Beta(1, 1)\nThe effective sample size of a beta prior is the sum of its two parameters. So in this case, it has an effective sample size of 2. This is equivalent to data, with one head and one tail already in it.\nSo this is not a completely non-informative prior.\nWe could think about a prior that has less information. For example Beta(\\frac{1}{2}, \\frac{1}{2}), this would have half as much information with an effective sample size of one.\nWe can take this even further. Think about something like Beta(0.001, 0.001) This would have much less information, with the effective sample size fairly close to zero. In this case, the data would determine the posterior and there would be very little influence from the prior.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Non-Informative Priors</span>"
    ]
  },
  {
    "objectID": "C1-L11.html#sec-non-informative-priors",
    "href": "C1-L11.html#sec-non-informative-priors",
    "title": "13  Non-Informative Priors",
    "section": "",
    "text": "Figure 13.1: Non-Informative Priors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.1.1 Improper priors\nCan we go even further? We can think of the limiting case. Let’s think of Beta(0,0), what would that look like?\n\nf(\\theta) \\propto \\theta^{-1}(1-\\theta)^{-1}\n\nThis is not a proper density. If you integrate this over (0,1), you’ll get an infinite integral, so it’s not a true density in the sense of it not integrating to 1.\nThere’s no way to normalize it, since it has an infinite integral. This is what we refer to as an improper prior.\nIt’s improper in the sense that it doesn’t have a proper density. But it’s not necessarily improper in the sense that we can’t use it. If we collect data, we use this prior and as long as we observe one head and one tail, or at least one success and one failure. Then we can get a posterior\n\nf(\\theta\\mid y) \\propto \\theta^{y-1}(1-\\theta)^{n-y-1} \\sim Beta(y, n-y)\n\nWith a posterior mean of \\frac{y}{n} =\\hat{\\theta}, which you should recognize as the maximum likelihood estimate. So by using this improper prior, we get a posterior which gives us point estimates exactly the same as the frequentist approach.\nBut in this case, we can also think of having a full posterior. From this, we can make interval statements, and probability statements, and we can actually find an interval and say that there’s 95\\% probability that \\theta is in this interval. This is not something you can do under the frequentist approach even though we may get the same exact interval.\n\n\n13.1.2 Statements about improper priors\nImproper priors are okay as long as the posterior itself is proper. There may be some mathematical things that need to be checked and you may need to have certain restrictions on the data. In this case, we needed to make sure that we observed at least one head and one tail to get a proper posterior.\nBut as long as the posterior is proper, we can go forwards and do Bayesian inference even with an improper prior.\nThe second point is that for many problems there does exist a prior, typically an improper prior that will lead to the same point estimates as you would get under the frequentist paradigm. So we can get very similar results, results that are fully dependent on the data, under the Bayesian approach.\nBut in this case, we can also continue to have a posterior and make posterior interval estimates and talk about the posterior probabilities of the parameter.\n\n\n13.1.3 Normal Case\nAnother example is thinking about the normal case.\n\nY_i \\stackrel{iid}\\sim \\mathcal{N}(\\mu, \\sigma^2)\n\nLet’s start off by assuming that \\sigma^2 is known and we’ll just focus on the mean \\mu.\nWe can think about a vague prior like before and say that\n\n\\mu \\sim N(0, 1000000^2)\n\nThis would just spread things out across the real line. That would be a fairly non-informative prior covering a lot of possibilities. We can then think about taking the limit, what happens if we let the variance go to \\infty. In that case, we’re spreading out this distribution across the entire real number line. We can say that the density is just a constant across the whole real line.\n\nf(\\mu) \\propto 1\n\nThis is an improper prior because if you integrate the real line you get an infinite answer. However, if we go ahead and find the posterior\n\nf(\\mu \\mid y) \\propto f(y \\mid \\mu) f(\\mu) \\propto \\exp \\left (-\\frac{1}{2\\sigma^2}\\sum{(y_i - \\mu)^2} \\right ) (1)\n\n\nf(\\mu \\mid y) \\propto exp(-\\frac{1}{2\\sigma^2/n}(\\mu - \\bar{y})^2)\n\n\n\\mu \\mid y \\sim N(\\bar{y}, \\frac{\\sigma^2}{n})\n\nThis should look just like the maximum likelihood estimate.\n\n\n13.1.4 Normal with unknown Variance\nIn the case that \\sigma^2 is unknown, the standard non-informative prior is\n\nf(\\sigma^2) \\propto \\frac{1}{\\sigma^2}\n\n\n\\sigma^2 \\sim \\Gamma^{-1}(0,0)\n\nThis is an improper prior and it’s uniform on the log scale of \\sigma^2.\nIn this case, we’ll end up with a posterior for \\sigma^2\n\n\\sigma^2 \\mid y \\sim \\Gamma^{-1}\\left (\\frac{n-1}{2}, \\frac{1}{2}\\sum{(y_i - \\bar{y})^2}\\right)\n\nThis should also look reminiscent of the quantities we get as a frequentist. For example, the samples standard deviation",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Non-Informative Priors</span>"
    ]
  },
  {
    "objectID": "C1-L11.html#sec-jeffreys-prior",
    "href": "C1-L11.html#sec-jeffreys-prior",
    "title": "13  Non-Informative Priors",
    "section": "13.2 Jeffrey’s Prior",
    "text": "13.2 Jeffrey’s Prior\n\n\n\n\n\n\n\nFigure 13.2: Jeffrey’s Prior\n\n\nChoosing a uniform prior depends upon the particular parameterization.\nSuppose I used a prior which is uniform on the log scale for \\sigma^2\n\nf(\\sigma^2) \\propto \\frac{1}{\\sigma^2}\n\nSuppose somebody else decides, that they just want to put a uniform prior on \\sigma^2 itself.\n\nf(\\sigma^2) \\propto 1\n\nThese are both uniform on certain scales or certain parameterizations, but they are different priors. So when we compute the posteriors, they will be different as well.\nThe key thing is that uniform priors are not invariant with respect to transformation. Depending on how you parameterize the problem, you can get different answers by using a uniform prior\nOne attempt to round this out is to use Jeffrey’s Prior\nJeffrey’s Prior is defined as the following\n\nf(\\theta) \\propto \\sqrt{\\mathcal{I(\\theta)}}\n\nWhere \\mathcal{I}(\\theta) is the fisher information of \\theta.\nIn most cases, this will be an improper prior.\n\n\n\n\nHarold Jeffreys\n\n\n\n\n\n\n\nTipHistorical Note on Sir Harold Jeffreys\n\n\n\nJeffreys’ Prior is due to Sir Harold Jeffreys (1891-1989) a British geophysicist who who used sophisticated mathematical models to study the Earth and solar system. His hypotheses were uncertain, requiring revision in the face of incoming results, Jeffreys tried to construct a formal theory of scientific reasoning based on Bayesian probability. He made significant contributions to mathematics and statistics. His book, Theory of Probability (Jeffreys 1983), first published in 1939, played an important role in the revival of the objective Bayesian view of probability.\nInductive and Reductive Inference\n“The fundamental problem of scientific progress, and a fundamental one of everyday life, is that of learning from experience. Knowledge obtained in this way is partly merely description of what we have already observed, but part consists of making inferences from past experience to predict future experience. This part may be called generalization or induction.”\nJEFFREYS’ RULES FOR A THEORY OF INDUCTIVE INFERENCE\n\nAll hypotheses used must be explicitly stated and the conclusions must follow from the hypotheses.\nA theory of induction must be self-consistent; that is, it must not be possible to derive contradictory conclusions from the postulates and any given set of observational data.\nAny rule given must be applicable in practice. A definition is useless unless the thing defined can be recognized in terms of the definition when it occurs. The existence of a thing or the estimate of a quantity must not involve an impossible experiment.\nA theory of induction must provide explicitly for the possibility that inferences made by it may turn out to be wrong.\nA theory of induction must not deny any empirical proposition a priori; any precisely stated empirical proposition must be formally capable of being accepted in the sense of the last rule, given a moderate amount of relevant evidence.\nThe number of postulates should be reduced to a minimum. (Occam’s Razor)\nAlthough we do not regard the human mind as a perfect reasoner, we must accept it as a useful one and the only one available. The theory need not represent actual thought processes in detail but should agree with them in outline.\nIn view of the greater complexity of induction, we cannot hope to develop it more thoroughly than deduction. We therefore take it as a rule that an objection carries no weight if an analogous objection invalidates part of generally accepted pure mathematics.\n\n\n\n\n13.2.1 Normal Data\nFor the example of Normal Data\n\nY_i \\sim N(\\mu, \\sigma^2)\n\n\nf(\\mu) \\propto 1\n\n\nf(\\sigma^2) \\propto \\frac{1}{\\sigma^2}\n\nWhere \\mu is uniform and \\sigma^2 is uniform on the log scale.\nThis prior will then be transformation invariant. We will end up putting the same information into the prior even if we use a different parameterization for the Normal.\n\n\n13.2.2 Binomial\n\nY_i \\sim B(\\theta)\n\n\nf(\\theta) \\propto \\theta^{-\\frac{1}{2}}(1-\\theta)^{-\\frac{1}{2}} \\sim \\mathrm{Beta}(\\frac{1}{2},\\frac{1}{2})\n\nThis is a rare example of where the Jeffrey’s prior turns out to be a proper prior.\nYou’ll note that this prior actually does have some information in it. It’s equivalent to an effective sample size of one data point. However, this information will be the same, not depending on the parameterization we use.\nIn this case, we have \\theta as a probability, but another alternative which is sometimes used is when we model things on a logistics scale.\nBy using the Jeffreys prior, we’ll maintain the exact same information.\n\n\n13.2.3 Closing information about priors\nOther possible approaches to objective Bayesian inference include priors such as reference priors and maximum entropy priors.\nA related concept to this is called empirical Bayesian analysis. The idea in empirical Bayes is that you use the data to help inform your prior; such as by using the mean of the data to set the mean of the prior distribution. This approach often leads to reasonable point estimates in your posterior. However, it’s sort of cheating since you’re using your data twice and as a result may lead to improper uncertainty estimates.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Non-Informative Priors</span>"
    ]
  },
  {
    "objectID": "C1-L11.html#sec-fisher-information",
    "href": "C1-L11.html#sec-fisher-information",
    "title": "13  Non-Informative Priors",
    "section": "13.3 Fisher Information",
    "text": "13.3 Fisher Information\nThe Fisher information (for one parameter) is defined as\n\n\\mathcal{I}(\\theta) = E\\left[\\left(\\frac{d}{d\\theta}log{(f(X \\mid \\theta))}\\right)^2\\right]\n\nWhere the expectation is taken with respect to X which has PDF f(X \\mid \\theta). This quantity is useful in obtaining estimators for \\theta with good properties, such as low variance. It is also the basis for Jeffrey’s prior.\n\n\n\n\n\n\nTipJeffreys prior violates the likelihood principle.\n\n\n\nUse of the Jeffreys prior violates the strong version of the likelihood principle. Which proposes that, given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function. When using the Jeffreys prior, inferences about \\theta depend not just on the probability of the observed data as a function of \\theta, but also on the universe of all possible experimental outcomes, as determined by the experimental design, because the Fisher information is computed from an expectation over the chosen universe. Accordingly, the Jeffreys prior, and hence the inferences made using it, may be different for two experiments involving the same \\theta parameter even when the likelihood functions for the two experiments are the same a violation of the strong likelihood principle.\n\n\n\nExample 13.1 (Jeffreys prior) Let\n\nX \\mid \\theta \\sim N(\\theta, 1)\n\nThen we have\n\nf(x \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}}\\exp[-\\frac{1}{2}(x-\\theta)^2]\n\n\n\\log{(f(x \\mid \\theta))} = -\\frac{1}{2}\\log{(2\\pi)}-\\frac{1}{2}(x-\\theta)^2\n\n\n\\left ( \\frac{d}{d\\theta}log{(f(x \\mid \\theta))} \\right )^2 = (x-\\theta)^2\n\nand so\n\n\\mathcal{I}(\\theta) = \\mathbb{E}[(X - \\theta)^2] = \\mathbb{V}ar[X] = 1",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Non-Informative Priors</span>"
    ]
  },
  {
    "objectID": "C1-L11.html#sensitivity-analysis-of-priors",
    "href": "C1-L11.html#sensitivity-analysis-of-priors",
    "title": "13  Non-Informative Priors",
    "section": "13.4 Sensitivity analysis of priors",
    "text": "13.4 Sensitivity analysis of priors\nThe general approach to using priors in models is to start with some justification for a prior, run the analysis, then come up with competing priors and reexamine the conclusions under the alternative priors. The results of the final model and the analysis of the sensitivity of the analysis to the choice of prior are written up as a package.\nFor a discussion of steps and methods to use in a sensitivity analysis, see: (Gelman et al. 2013, page: 38) which discusses two approaches:\nMany times we choose priors out of convenience. How to judge when assumptions of convenience can be made safely is a central task of Bayesian sensitivity analysis.\n\nAnalysis using different conjugate prior distributions.\n\n\nStarting with a uniform prior\nMore informative priors are tested and the 95% posterior CI is compared against the posterior mean and the prior mean.\nA table of prior mean, prior effective sample size , posterior mean and posterior 95 CI is created for the results\nWe are interested primarily to see how well the the posterior CI can excludes the prior mean even for priors with large effective sample size.\n\n\nAnalysis using a non-conjugate prior distribution follows the same approach but uses non conjugate prior. The comparisons described in 1. can be carried out using sampling.\n\n(Gelman et al. 2013, pages: 141)\n\n\n\n\n\n\nGelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis. https://books.google.co.il/books?id=ZXL6AQAAQBAJ.\n\n\nJeffreys, H. 1983. Theory of Probability. International Series of Monographs on Physics. Clarendon Press.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Non-Informative Priors</span>"
    ]
  },
  {
    "objectID": "C1-L12.html",
    "href": "C1-L12.html",
    "title": "14  Brief Review of Regression",
    "section": "",
    "text": "14.1 Conjugate Modeling\nRecall that linear regression is a model for predicting a response or dependent variable (Y, also called an output) from one or more covariates or independent variables (X, also called explanatory variables, inputs, or features). For a given value of a single x, the expected value of y is\n\\mathbb{E}[y] = \\beta_0 + \\beta_1x\nor we could say that\nY \\sim \\mathcal{N}(\\beta_0 + \\beta_1x, \\sigma^2)\nFor data (x_1, y_1), \\dots , (x_n, y_n), the fitted values for the coefficients, \\hat{\\beta_0} and \\hat{\\beta_1} are those that minimize the sum of squared errors \\sum_{i = 1}^n{(y_i - \\hat{y_i})^2}, where the predicted values for the response are \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x. We can get these values from R. These fitted coefficients give the least-squares line for the data.\nThis model extends to multiple covariates, with one \\beta_j for each k covariates\n\\mathbb{E}[y_i] = \\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_kx_{ik}\nOptionally, we can represent the multivariate case using vector-matrix notation.\nIn the Bayesian framework, we treat the \\beta parameters as unknown, put a prior on them, and then find the posterior. We might treat \\sigma^2 as fixed and known, or we might treat it as an unknown and also put a prior on it. Because the underlying assumption of a regression model is that the errors are independent and identically normally distributed with mean 0 and variance \\sigma^2, this defines a normal likelihood.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Brief Review of Regression</span>"
    ]
  },
  {
    "objectID": "C1-L12.html#conjugate-modeling",
    "href": "C1-L12.html#conjugate-modeling",
    "title": "14  Brief Review of Regression",
    "section": "",
    "text": "14.1.1 \\sigma^2 known\nSometimes we may know the value of the error variance \\sigma^2 . This simplifies calculations. The conjugate prior for the \\beta is a normal prior. In practice, people typically use a non-informative prior, i.e., the limit as the variance of the normal prior goes to infinity, which has the same mean as the standard least-squares estimates. If we are only estimating \\beta and treating \\sigma^2 as known, then the posterior for \\beta is a (multivariate) normal distribution. If we just have a single covariate, then the posterior for the slope is:\n\n\\beta_1 \\mid y \\sim N\\left(\\frac{\\sum_{i = 1}^n{(x_i-\\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n{(x_i-\\bar{x})^2}}, \\frac{\\sigma^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}\\right)\n\nIf we have multiple covariates, then using a matrix-vector notation, the posterior for the vector of coefficients is \n\\beta \\mid y \\sim N((X^tX)^{-1}X^ty, (X^tX)^{-1}\\sigma^2)\n\nwhere X denotes the design matrix and X^t is the transpose of X. The intercept is typically included in X as a column of 1’s. Using an improper prior requires us to have at least as many data points as we have parameters to ensure that the posterior is proper.\n\n\n14.1.2 \\sigma^2 Unknown\nIf we treat both \\beta and \\sigma^2 as unknown, the standard prior is the non-informative Jeffreys prior, f(\\beta, \\sigma^2) \\propto \\frac{1}{\\sigma^2} . Again, the posterior mean for \\beta will be the same as the standard least-squares estimates. The posterior for \\beta conditional on \\sigma^2 is the same normal distributions as when \\sigma^2 is known, but the marginal posterior distribution for \\beta, with \\sigma^2 integrated out is a t distribution, analogous to the t tests for significance in standard linear regression. The posterior t distribution has mean (X^tX)^{-1}X^ty and scale matrix (related to the variance matrix) s^2(X^tX)^{-1} , where s^2 = \\sum_{i = 1}^n{(y_i - \\hat{y_i})^2/(n - k - 1)} . The posterior distribution for \\sigma^2 is an inverse gamma distribution\n\n\\sigma^2 | y \\sim \\Gamma^{-1}(\\frac{n - k - 1}{2}, \\frac{n - k - 1}{2}s^2)\n\nIn the simple linear regression case (single variable), the marginal posterior for \\beta is a t distribution with mean \\frac{\\sum_{i = 1}^n{(x_i-\\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n{(x_i-\\bar{x})^2}} and scale \\frac{s^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}} . If we are trying to predict a new observation at a specified input x^* , that predicted value has a marginal posterior predictive distribution that is a t distribution, with mean \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x^* and scale se_r\\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n - 1)s_x^2}} . se_r is the residual standard error of the regression, which can be found easily in R. s_x^2 is the sample variance of x. Recall that the predictive distribution for a new observation has more variability than the posterior distribution for \\hat{y}, because individual observations are more variable than the mean.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Brief Review of Regression</span>"
    ]
  },
  {
    "objectID": "C1-L12.html#linear-regression",
    "href": "C1-L12.html#linear-regression",
    "title": "14  Brief Review of Regression",
    "section": "14.2 Linear Regression",
    "text": "14.2 Linear Regression\n\n14.2.1 Single Variable Regression\nWe’ll be looking at the Challenger dataset. It contains 23 past launches where it has the temperature at the day of launch and the O-ring damage index\nChallenger dataset\nRead in the data https://pdixon.stat.iastate.edu/stat511/datasets/challenger2.txt\n\n\nCode\noring=read.table(\"data/challanger.txt\", header=T)\n# Note that attaching this masks T which is originally TRUE\nattach(oring)\n\n\n\n\nCode\nhead(oring)\n\n\n   t  i\n1 53 11\n2 57  4\n3 58  4\n4 63  2\n5 66  0\n6 67  0\n\n\nNow we’ll see the plot\n\n\nCode\nplot(t,i)\n\n\n\n\n\n\n\n\n\nFit a linear model\n\n\nCode\noring.lm = lm(i ~ t)\nsummary(oring.lm)\n\n\n\nCall:\nlm(formula = i ~ t)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3025 -1.4507 -0.4928  0.7397  5.5337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.36508    4.43859   4.138 0.000468 ***\nt           -0.24337    0.06349  -3.833 0.000968 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.102 on 21 degrees of freedom\nMultiple R-squared:  0.4116,    Adjusted R-squared:  0.3836 \nF-statistic: 14.69 on 1 and 21 DF,  p-value: 0.0009677\n\n\nAdd the fitted line into the scatter plot\n\n\nCode\nplot(t,i)\nlines(t,fitted(oring.lm))     \n\n\n\n\n\n\n\n\n\nCreate a 95% posterior interval for the slope\n\n\nCode\n-0.24337 - 0.06349*qt(.975,21)\n\n\n[1] -0.3754047\n\n\n\n\nCode\n-0.24337 + 0.06349*qt(.975,21)\n\n\n[1] -0.1113353\n\n\nNote: These are the same as the frequentist confidence intervals\nIf the challenger launch was at 31 degrees Fahrenheit, how much O-Ring damage would we predict?\n\n\nCode\ncoef(oring.lm)[1] + coef(oring.lm)[2]*31  \n\n\n(Intercept) \n   10.82052 \n\n\nCode\n# [1] 10.82052 \n\n\nLet’s make our posterior prediction interval\n\n\nCode\npredict(oring.lm,data.frame(t=31),interval=\"predict\")\n\n\n       fit      lwr      upr\n1 10.82052 4.048269 17.59276\n\n\nWe can calculate the lower bound through the following formula\n\n\nCode\n10.82052-2.102*qt(.975,21)*sqrt(1+1/23+((31-mean(T))^2/22/var(t)))\n\n\n[1] 4.850937\n\n\nWhat’s the posterior probability that the damage index is greater than zero?\n\n\nCode\n1-pt((0-10.82052)/(2.102*sqrt(1+1/23+((31-mean(T))^2/22/var(T)))),21)\n\n\n[1] NA\n\n\n\n\n14.2.2 Multivariate Regression\nWe’re looking at Galton’s seminal data predicting the height of children from the height of the parents.\n\n\n  Family Father Mother Gender Height Kids\n1      1   78.5   67.0      M   73.2    4\n2      1   78.5   67.0      F   69.2    4\n3      1   78.5   67.0      F   69.0    4\n4      1   78.5   67.0      F   69.0    4\n5      2   75.5   66.5      M   73.5    4\n6      2   75.5   66.5      M   72.5    4\n7      2   75.5   66.5      F   65.5    4\n8      2   75.5   66.5      F   65.5    4\n\n\nWhat are the columns in the dataset?\n\n\nCode\nnames(heights)\n\n\n[1] \"Family\" \"Father\" \"Mother\" \"Gender\" \"Height\" \"Kids\"  \n\n\nCode\n# [1] \"Family\" \"Father\" \"Mother\" \"Gender\" \"Height\" \"Kids\"  \n\n\nexplanation of the columns:\n\nFamily: the family the child is from\nFather: height of the father\nMother: height of the mother\nKids: count of children in the family\nGender: the gender of the child\nHeight: the height the child\n\nThe Height is out target variables.\nLet’s look at the relationship between the different variables\n\n\nCode\npairs(heights)\n\n\n\n\n\n\n\n\n\nPair plots are a great tool for doing EDA in R. You need to get used read them.\nWe care primarily about the Height so we can should first consider the row of the height. The other rows can inform us if there is a relation between other variables.\n\nthe Father and Mother are correlated with height.\nGender male children are generally taller.\nKids and Family don’t seem to have a clear pattern.\n\nFirst let’s start by creating a linear model taking all of the columns into account\n\n\nCode\nsummary(lm(Height~Father+Mother+Gender+Kids))\n\n\n\nCall:\nlm(formula = Height ~ Father + Mother + Gender + Kids)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.4748 -1.4500  0.0889  1.4716  9.1656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.18771    2.79387   5.794 9.52e-09 ***\nFather       0.39831    0.02957  13.472  &lt; 2e-16 ***\nMother       0.32096    0.03126  10.269  &lt; 2e-16 ***\nGenderM      5.20995    0.14422  36.125  &lt; 2e-16 ***\nKids        -0.04382    0.02718  -1.612    0.107    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.152 on 893 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6391 \nF-statistic: 398.1 on 4 and 893 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can see here, the Kids column is not statistically significant. Let’s look at a model with it removed.\n\n\nCode\nheights.lm=lm(Height~Father+Mother+Gender)\nsummary(heights.lm)\n\n\n\nCall:\nlm(formula = Height ~ Father + Mother + Gender)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.523 -1.440  0.117  1.473  9.114 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15.34476    2.74696   5.586 3.08e-08 ***\nFather       0.40598    0.02921  13.900  &lt; 2e-16 ***\nMother       0.32150    0.03128  10.277  &lt; 2e-16 ***\nGenderM      5.22595    0.14401  36.289  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.154 on 894 degrees of freedom\nMultiple R-squared:  0.6397,    Adjusted R-squared:  0.6385 \nF-statistic:   529 on 3 and 894 DF,  p-value: &lt; 2.2e-16\n\n\nThis model looks good. We can tell from the summary that:\n\neach extra inch of the father’s height contributes an extra 0.4 inches height of the child.\neach extra inch of the mother’s height contributes an extra 0.3 inches height of the child.\nmale gender contributes 5.2 inches to the height of the child.\n\nLet’s create a 95% posterior interval for the difference in height by gender\n\n\nCode\n5.226 - 0.144 * qt(.975,894)\n\n\n[1] 4.943383\n\n\n\n\nCode\n5.226 + 0.144 * qt(.975,894)\n\n\n[1] 5.508617\n\n\nLet’s make a posterior prediction interval for a male and female with a father whose 68 inches and a mother whose 64 inches.\n\n\nCode\npredict(heights.lm,data.frame(Father=68,Mother=64,Gender=\"M\"), interval=\"predict\")\n\n\n       fit      lwr     upr\n1 68.75291 64.51971 72.9861\n\n\n\n\nCode\npredict(heights.lm,data.frame(Father=68,Mother=64,Gender=\"F\"), interval=\"predict\")\n\n\n       fit      lwr      upr\n1 63.52695 59.29329 67.76062",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Brief Review of Regression</span>"
    ]
  },
  {
    "objectID": "C2-L01.html",
    "href": "C2-L01.html",
    "title": "15  Statistical Modeling and Monte Carlo estimation",
    "section": "",
    "text": "15.1 Objectives\nThis course is about statistical modelling which falls under the analyzing data objective.\nFor what kinds of problems might we use a statistical model?",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistical Modeling and Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L01.html#objectives",
    "href": "C2-L01.html#objectives",
    "title": "15  Statistical Modeling and Monte Carlo estimation",
    "section": "",
    "text": "What are the objectives of statistical models?\nWhat can they accomplish and where do they fit in the broader field of data science?\n\n\n\nSo what is a statistical model?\nA statistical model will be a mathematical structure used to imitate, And approximate, the data generating process. It typically describes relationships among variables while accounting for uncertainty and variability in the data.\n\n\n\nQuantifying uncertainty:\n\nare relationships between variables we cannot measure?\nhow many peoeple were polled?\nhow were they chosen?\nhow would the data change if we repeated the poll.\n\nInference\n\nExtend the result and infer what percentage of the total population supports the candidate?\nWe may also have other demographic information about each person in the poll.\nA statistical model might allow us to see how these other variables relate to a person’s likelihood of supporting the candidate.\n\nMeasure support for hypothesis\n\nDoes the evidence support a hypothesis that the candidate is more popular with men than women?\n\nPrediction\n\nGiven demographic information on a voter we could use the model to predict her vote.\nAlso important for machine learning.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistical Modeling and Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L01.html#a-poll-for-a-political-candidate",
    "href": "C2-L01.html#a-poll-for-a-political-candidate",
    "title": "15  Statistical Modeling and Monte Carlo estimation",
    "section": "15.2 A Poll for a political candidate",
    "text": "15.2 A Poll for a political candidate\n\n57% for a candidate\nthe 99% CI (51,63)\ndemographics:\n\n55% women\n63% men",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistical Modeling and Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L01.html#modeling-process",
    "href": "C2-L01.html#modeling-process",
    "title": "15  Statistical Modeling and Monte Carlo estimation",
    "section": "15.3 Modeling Process",
    "text": "15.3 Modeling Process\n\n\n\n\nstatistical modeling process\n\nBuilding statistical models is a process, and each step should be taken carefully. Here we outline the general process and offer some practical advice. We’ll call this the statistical modeling process.\n The first step in this process is to understand the problem. This may seem obvious, but understanding the problem and context is critical to success. A sophisticated model might be useless if it is applied inappropriately.understand the problem\n\nExample 15.1 (international stores) For example, suppose you have revenue data from several different locations of a store chain at unknown locations.\n\nIt seems reasonable to average these revenue numbers as a summary of how the store is doing.\nSuppose you discover that the stores are located in different countries and reported revenues in different currencies.\nNow that average doesn’t seem to have much meaning unless, of course, we get the revenue numbers converted to the same scale.\n\n\n The second step is to plan and properly collect relevant data. There may be multiple quantities that you could potentially measure to help answer your question. In this step, you decide what information will be most useful to solving your problem. How to collect the data and how many data points to collect. The quality of your data collection plan determines the value of your data.plan and collect data\n\nFor example, if you conduct a survey of peers in your workplace. Your results would likely not generalize to all workers in the company, especially if there are multiple work sites. If you want generalizable results, a better plan would be to select a random sample among all employees to participate in your survey.\n\nThe step is addressed in detail in most introductory statistics courses.\n The third step in this process is to explore your data. In this step, you should ensure that the data collection plan was followed. And that the data were recorded accurately. If there are major surprises in that data, verified that they are not errors. In this step, you’ll often want to visualize the data to gain a basic understanding of the relationships among your variables. This can help you decide what kinds of models might be appropriate. Finally, the practice of snooping around or mining your data, looking for interesting hypothesis to test can potentially invalidate your statistical modeling results. If you want to mine your data, and test your findings, it is usually best to randomly split your data into two parts. With one part, you can look for interesting things to test and fit different potential models. With the other, you can fit the model you chose using the first part to validate or see if the results can be replicated on other data.explore your data\n The fourth step in this process is to postulate a model. After gaining an understanding of how your data are structured, choose a model that can appropriately approximate or summarize the interesting content of the data. This might be an off the shelf statistical model like a regression or it could be based on a scientific theory such as a growth model. It could also be multiple models.postulate a model\nGenerally, it is desirable to find a model where the parameters we estimate can be interpreted in the context of the original problem. You might also have to strike a balance between model complexity, and model generalizability. This is often referred to as the bias variance trade-off. Large complex models, might be able to fit your particular dataset very well. But may fail to generalize to future data.\n\nExample 15.2 (overfitting) Let’s look at an example of this. Let’s suppose your data looked like  where x is your explanatory variable, y is your response variable. And you have points like these. One possible model you could fit would be just a linear regression going through the points.\nAnother possibility for model you could fit here would be essentially an interpolator that makes sure it goes through every single point. Now, consider a future scenario where you’ve got another dataset just like this one with a new cloud of points. You can imagine that perhaps this interpolated model, which fit the original dataset perfectly, might struggle on a future dataset.\n\n The fifth step in our statistical modeling process is to fit the model. In this step we need to estimate the parameters of the model using the data. In this particular class, we’re going to take a Bayesian approach to this step.fit the model\n The sixth step in our statistical modeling process is to check the model. Here we want to check to see if the model adequately imitates the data generating process. Are predictions from the model realistic? Does it fit well to your data? Or does it completely miss some of the features? We’ll look into some the techniques for doing this, including residual analysis and predictive checks later in the course. In this step, we may also compare a competing models according to some criteria.check the model\n The seventh step in our statistical modeling process is to iterate. That is, return, possibly, to steps 4 through 6. If the model you have already fit is, for some reason, inadequate, we should return to step 4 and proceed through step 6 again with a new, and hopefully better, model that would address or correct the deficiencies from your previous model.iterate\n The eighth and final step in our statistical modeling process is to use the model. If we’ve iterated through these enough times and decided that the model is good, or that we have selected an appropriate model, we can use the results to answer your original research questions and arrive at conclusions. In this course, we are going to focus on steps 4 through 8. But this does not mean steps 1 through 3 should be ignored in any analysis. In fact, the importance of steps 1 through 3 cannot be overstated. The validity of you final results depends on them. That is why most introductory statistics courses will emphasize these steps. If you’ll not explore this issues in an introductory statistics course, we highly recommend you do so. We hope you’ll refer to this outline, the statistical modeling process often as you begin modeling data.use the model\n\n15.3.1 Process outline:\n\nunderstand the problem.\nplan and collect data.\nexplore the data.\npostulate the model.\nfit the model.\ncheck the model.\niterate be going beck to step 4.\nuse the model.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Statistical Modeling and Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L02.html",
    "href": "C2-L02.html",
    "title": "16  Bayesian Modeling",
    "section": "",
    "text": "17 Notes - Bayesian Modeling",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#components-of-a-bayesian-model",
    "href": "C2-L02.html#components-of-a-bayesian-model",
    "title": "16  Bayesian Modeling",
    "section": "17.1 Components of a Bayesian Model",
    "text": "17.1 Components of a Bayesian Model\n\n\n\n\na Bayesian Model\n\nIn lesson one, we defined a statistical model as a mathematical structure used to imitate or approximate the data generating process. It incorporates uncertainty and variability using the theory of probability. A model could be very simple, involving only one variable.\n\nSuppose our data consists of the heights of N=15 adult men. . Clearly it would be very expensive or even impossible to collect the genetic information that fully explains the variability in these men’s heights. We only have the height measurements available to us. To account for the variability, we might assume that the men’s heights follow a normal distribution.heights of N=15 men\nSo we could write the model like this:  where y_i will represent the height for person i, i will be our index. This will be equal to a constant, a number \\mu which will represents the mean for all men plus \\epsilon_i. the individual error term for individual i.y_i= \\mu + \\epsilon_i\n We’re going to assume that \\epsilon_i comes from a normal distribution with mean zero and variance \\sigma^2. We are also going to assume that these epsilons are independent and identically distributed from this normal distribution. This is also for i equal to 1 up to N which will be 15 in our case. Equivalently we could write this model directly for the y_i themselves.\\epsilon_i \\stackrel{iid}\\sim N(0,\\sigma^2) \\  i\\in 1 \\dots N\n So each y_i comes from a normal distribution independent and identically distributed with the normal distribution. With mean \\mu and variance \\sigma^2. This specifies a probability distribution and a model for the data.y_i \\stackrel{iid}\\sim N(\\mu,\\sigma^2) \\ i \\in 1 \\dots N\n\n\n\n\n\n\nNoteheights of men\n\n\n\n\n\\begin{aligned}\ny_i&= \\mu+\\epsilon_i,\n\\\\ \\epsilon_i &\\stackrel{iid}\\sim N(0,\\sigma^2)\n\\end{aligned}\n another way to write this:\n\n\\begin{aligned}\ny_i &\\stackrel{iid}\\sim N(\\mu,\\sigma^2)\n\\end{aligned}\n\n\n\nIf we know the values of \\mu and \\sigma. It also suggests how we might generate more fake data that behaves similarly to our original data set.\n\nA model can be as simple as the one right here or as complicated and sophisticated as we need to capture the behavior of the data. So far, this model is the same for Frequentists and Bayesians.\nAs you may recall from the previous course. The frequentist approach to fitting this model right here would be to consider \\mu and \\sigma to be fixed but unknown constants, and then we would estimate them. To calculate our uncertainty in those estimates a frequentist approach would consider how much the estimates of \\mu and \\sigma might change if we were to repeat the sampling process and obtain another sample of 15 men, over, and over.\n\n\n\n\nComponents of a Bayesian Model\n\nThe Bayesian approach, the one we’re going to take in this class. Tackles our uncertainty in \\mu and \\sigma^2 with probability directly. By treating them as random variables with their own probability distributions. These are often called priors, and they complete a Bayesian model.\nIn the rest of this segment, we’re going to review three key components of Bayesian models. That were used extensively in the previous course The three primary components of Bayesian models that we often work with are the likelihood, the prior and the posterior.\n The likelihood is the probabilistic model for the data. It describes how, given the unknown parameters, the data might be generated. We’re going to call unknown parameter theta right here. Also, in this expression, you might recognize this from the previous class, as describing a probability distribution.\\mathbb{P}r(y\\mid \\theta)\\ \\text{(likelihood)}\n The prior, the next step, is the probability distribution that characterizes our uncertainty with the parameter theta. We’re going to write it as \\mathbb{P}r(\\theta). It’s not the same distribution as this one. We’re just using this notation p to represent the probability distribution of theta. By specifying a likelihood and a prior.\\mathbb{P}r(\\theta)\\ \\text{(prior)}\n We now have a joint probability model for both the knowns, the data, and the unknowns, the parameters. We can see this by using the chain rule of probability. If we wanted the joint distribution of both the data and the parameters theta. Using the chain rule of probability, we could start with the distribution of \\theta. And multiply that by the probability or the distribution of y given theta. That gives us an expression for the joint distribution. However if we’re going to make inferences about data and we already know the values of y, we don’t need the joint distribution, what we need is the posterior distribution.\\mathbb{P}r(y,\\theta) = \\mathbb{P}r(\\theta)\\mathbb{P}r(y\\mid\\theta) \\ \\text{(joint probability)}\n The posterior distribution is the distribution of \\mathbb{P}r(\\theta \\mid y), i.e. \\theta given y. We can obtain this expression using the laws of conditional probability and specifically using Bayes’ theorem.\\mathbb{P}r(\\theta \\mid y)\\ \\text{(posterior)}\n\n\\begin{aligned}\n\\mathbb{P}r(\\theta \\mid y) &= \\frac{\\mathbb{P}r(\\theta,y)}{\\mathbb{P}r(y)}\n\\\\ &= \\frac{\\mathbb{P}r(\\theta,y)}{\\int \\mathbb{P}r(\\theta,y)}\n\\\\ &= \\frac{\\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)}{\\int \\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)\\ d\\theta}\n\\end{aligned}\n\nWe start with the definition of conditional probability (1). The conditional distribution, \\mathbb{P}r(\\theta \\mid y) is the ratio of the joint distribution of \\theta and y, i.e. \\mathbb{P}r(\\theta,y); with the marginal distribution of y, \\mathbb{P}r(y).\n We start with the joint distribution like we have on top, and we integrate out or marginalize over the values of theta (2)How do we get the marginal distribution of y?\nTo make this look like the Bayes theorem that we’re familiar with the joint distribution can be rewritten as the product of the prior and the likelihood. We start with the likelihood, because that’s how we usually write Bayes’ theorem. We have the same thing in the denominator here. But we’re going to integrate over the values of theta. These integrals are replaced by summations if we know that \\theta is a discrete random variable. The marginal distribution is another important piece which we may use when we more advanced Bayesian modeling.\nThe posterior distribution is our primary tool for achieving the statistical modeling objectives from lesson one.\n\n\n\n\n\n\nNoteanatomy of a posterior probability\n\n\n\n\n  \\begin{aligned}\n  &\\mathbb{P}r(y\\mid \\theta) && (likelihood)\n\\\\ &  \\mathbb{P}r(\\theta) && (prior)\n\\\\   \\mathbb{P}r(y,\\theta) &= \\mathbb{P}r(\\theta)\\mathbb{P}r(y|\\theta) &&(joint\\ distribution)\n\\\\   \\mathbb{P}r(\\theta \\mid y) &= \\frac{\\mathbb{P}r(\\theta,y)}{\\mathbb{P}r(y)} && (conditional\\ probability)\n\\\\ &= \\frac{\\mathbb{P}r(\\theta,y)}{\\int \\mathbb{P}r(\\theta,y)}\n\\\\ &= \\frac{\\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)}{\\int \\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)\\ d\\theta}\n\\end{aligned}\n\\tag{17.1}\n\n\n\nWhereas non-Bayesian approaches consider a probability model for the data only, the hallmark characteristic of Bayesian models is that they specify a joint probability distribution for both data and parameters. How does the Bayesian paradigm leverage this additional assumption?\n\n\n\nThis allows us to make probabilistic assessments about how likely our particular data outcome is under any parameter setting.\nThis allows us to select the most accurate prior distribution.\nThis allows us to make probabilistic assessments about hypothetical data outcomes given particular parameter values.\nThis allows us to use the laws of conditional probability to describe our updated information about parameters given the data.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#model-specification",
    "href": "C2-L02.html#model-specification",
    "title": "16  Bayesian Modeling",
    "section": "17.2 Model Specification",
    "text": "17.2 Model Specification\n Before fitting any model we first need to specify all of its components.\n\n\n\n\n\n\n\n\nFigure 17.1: The graphical model specification for the height model\n\n\n\n\n\n\n17.2.1 Hierarchical representation\nOne convenient way to do this is to write down the hierarchical form of the model. By hierarchy, we mean that the model is specified in steps or in layers. We usually start with the model for the data directly, or the likelihood. Let’s write, again, the model from the previous lesson.\nWe had the height for person i, given our parameters \\mu and \\sigma^2, so conditional on those parameters, y_i came from a normal distribution that was independent and identically distributed, where the normal distribution has mean \\mu and variance \\sigma^2, and we’re doing this for individuals 1 up to N, which was 15 in this example. y_i | \\mu,\\sigma^2 \\stackrel{iid}\\sim N(\\mu,\\sigma^2) \\ for\\ i \\in 1,\\dots,15\nThe next level that we need is the prior distribution from \\mu and \\sigma^2. For now we’re going to say that they’re independent priors. So that our prior from \\mu and \\sigma^2 is going to be able to factor Into the product of two independent priors.  We can assume independents in the prior and still get dependents in the posterior distribution.\\mathbb{P}r(\\mu,\\sigma^2)~=~\\mathbb{P}r(\\mu)\\mathbb{P}r(\\sigma^2)\\ (independence)\nIn the previous course we learned that the conjugate prior for \\mu, if we know the value of \\sigma^2, is a normal distribution, and that the conjugate prior for \\sigma^2 when \\mu is known is the inverse gamma distribution.\nLet’s suppose that our prior distribution for \\mu is a normal distribution where mean will be \\mu_0.  This is just some number that you’re going to fill in here when you decide what the prior should be. Mean \\mu_0, and less say \\sigma^2_0 would be the variance of that prior.\\mu \\sim N(\\mu_0,\\sigma^2_0)\nThe prior for \\sigma^2 will be inverse gamma  which has two parameters:\\sigma^2 \\sim IG(\\nu_0,\\beta_0)\n\nIt has a shape parameter, we’re going to call that \\nu_0, and\nIt has a scale parameter, we’ll call that \\beta_0.\n\nWe need to choose values for these hyper-parameters here. But we do now have a complete Bayesian model.\nWe now introduce some new ideas that were not presented in the previous course.\n\n\n\n\n\n\nNoteHierarchical representation\n\n\n\nBy hierarchy, we mean that the model is specified in steps or in layers.\n\nstart with the model for the data, or the likelihood.\nwrite the priors\nadd hyper-priors for the parameters of the priors.\n\nMore details can be seen on this wikipedia article and on this one\n\n\n\n\n17.2.2 Graphical representation\nAnother useful way to write out this model Is using what’s called a graphical representation. To write a graphical representation, we’re going to do the reverse order, we’ll start with the priors and finish with the likelihood.\nIn the graphical representation we draw what are called nodes so this would be a node for mu. The circle means that the this is a random variable that has its own distribution. So \\mu with its prior will be represented with that. And then we also have \\sigma^2. The next part of a graphical model is showing the dependence on other variables. Once we have the parameters, we can generate the data.\nFor example we have y_1, \\dots y_n. These are also random variables, so we’ll create these as nodes. And I’m going to double up the circle here to indicate that these nodes are observed, you see them in the data. So we’ll do this for all of the ys here. And to indicate the dependence of the distributions of the ys on \\mu and \\sigma^2, we’re going to draw arrows. So \\mu influences the distribution of y for each one of these ys. The same is true for sigma squared, the distribution of each y depends on the distribution of \\sigma^2. Again, these nodes right here, that are double-circled, mean that they’ve been observed. If they’re shaded, which is the usual case, that also means that they’re observed. The arrows indicate the dependence between the random variables and their distributions.\nNotice that in this hierarchical representation, I wrote the dependence of the distributions also. We can simplify the graphical model by writing exchangeable random variables and I’ll define exchangeable later.\nWe’re going to write this using a representative of the ys here on what’s called the plate. So I’m going to re draw this hierarchical structure, we have \\mu and \\sigma^2. And we don’t want to have to write all of these notes again. So I’m going to indicate that there are n of them, And I’m just going to draw one representative, y_i. And they depend on \\mu and \\sigma^2. To write a model like this, we must assume that the ys are exchangeable. That means that the distribution for the ys does not change if we were to switch the index label like the i on the y there. So, if for some reason, we knew that one of the ys was different from the other ys in its distribution, and if we also know which one it is, then we would need to write a separate node for it and not use a plate like we have here.\n\n\n\n\n\n\nNoteGraphical representation\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.2: pgm-posterior\n\n\n\n\n\nIn the graphical representation we start at the top by drawing:\n\ncircle nodes for the hyper-paramers.\narrows indicating that they determine the\nnodes for the priors.\nnodes for the RVs (doubled circles)\nplates (rectangles) indicating RVs that are exchangeable. We add an index to the corner of the plate to indicate the amount of replicated RVs\n\nMore details can be seen on this wikipedia article\n\n\nBoth the hierarchical and graphical representations show how you could hypothetically simulate data from this model. You start with the variables that don’t have any dependence on any other variables. You would simulate those, and then given those draws, you would simulate from the distributions for these other variables further down the chain.\nThis is also how you might simulate from a prior predictive distribution.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#posterior-derivation",
    "href": "C2-L02.html#posterior-derivation",
    "title": "16  Bayesian Modeling",
    "section": "17.3 Posterior derivation",
    "text": "17.3 Posterior derivation\n\n\n\n\nPosterior derivation\n\nSo far, we’ve only drawn the model with two levels. But in reality, there’s nothing that will stop us from adding more layers.\nFor example, instead of fixing the values for the hyper parameters in the previous segment, those hyper parameters were the \\mu_0, the \\sigma_0, the \\nu_0 and the \\beta_0.\nWe could specify just fixed numeric values for those, or we could learn them from the data and model them using additional prior distributions for those variables to make this a hierarchical model.\nOne reason we might do this is if the data are hierarchically organized so that the observations are naturally grouped together. We will examine these types of hierarchical models in depth later in the course.\nAnother simple example of a hierarchical model is one you saw already in the previous course.\nLet’s write it as y_i \\mid \\mu,\\sigma^2, so this is just like the model from the previous lesson, will be independent and identically distributed normal with a mean \\mu and a variance, \\sigma^2. The next step, instead of doing independent priors for \\mu and \\sigma^2, we’re going to have the prior for \\mu depend on the value of \\sigma^2. That is given \\sigma^2, \\mu follows a normal distribution with mean \\mu naught, just some hyper parameter that you’re going to chose. And the variance of this prior will be \\sigma^2, this parameter, divided by omega naught. Another hyper parameter that will scale it.\nWe now have a joint distribution of y and \\mu given \\sigma^2 So finally, we need to complete the model with the prior for \\sigma^2. We’ll use our standard inverse gamma with the same hyper parameters as last time. This model has three layers. And \\mu depends on sigma right here. The graphical representation for this model looks like this. We start with the variables that don’t depend on anything else. So that would be \\sigma^2 and move down the chain.\nSo here, the next variable is \\mu which depends on \\sigma^2. And then dependent on both, we have the yi’s. We use a double circle because the yi’s are observed, their data, and we’re going to assume that they’re exchangeable. So let’s put them on a plate here for i in 1 to n The distribution of yi depends on both \\mu and \\sigma^2, so we’ll draw curves connecting those pieces there. To simulate hypothetical data from this model, we would have to first draw from the distribution of the prior for \\sigma^2. Then the distribution for mu which depends on \\sigma^2. And once we’ve drawn both of these, then we can draw random draws from the y’s, which of course depends on both of those. With multiple levels, this is an example of a hierarchical model. Once we have a model specification, we can write out what the full posterior distribution for all the parameters given the data looks like. Remember that the numerator in Bayes’ theorem is the joint distribution of all random quantities, all the nodes in this graphical representation over here from all of the layers. So for this model that we have right here, we have a joint distribution that’ll look like this. We’re going to write the joint distribution of everything y1 up to yn, \\mu and \\sigma^2, Using the chain rule of probability, we’re going to multiply all of the distributions in the hierarchy together. So let’s start with the likelihood piece. And we’ll multiply that by the next layer, the distribution of mu, given \\sigma^2. And finally, with the prior for sigma squared. So what do these expressions right here look like? The likelihood right here in this level because they’re all independent will be a product of normal densities. So we’re going to multiply the normal density for each yi, Given those parameters. This, again, is shorthand right here for the density of a normal distribution. So that represents this piece right here. The conditional prior of \\mu given sigma squared is also a normal. So we’re going to multiply this by a normal distribution of mu, where its parameters are \\mu naught and sigma squared over omega naught. And finally, we have the prior for sigma squared. We’ll multiply by the density of an inverse gamma for \\sigma^2 given the hyper parameters \\mu naught, sorry, that is given, the hyper parameters \\mu naught and and beta naught. What we have right here is the joint distribution of everything. It is the numerator in Bayes theorem. Let’s remind ourselves really fast what Bayes theorem looks like again. We have that the posterior distribution of the parameter given the data is equal to the likelihood, Times the prior. Over the same thing again. So this gives us in the numerator the joint distribution of everything which is what we’ve written right here.\nIn Bayes theorem, the numerator and the denominator are the exact same expression accept that we integrate or marginalize over all of the parameters.\nBecause the denominator is a function of the y’s only, which are known values, the denominator is just a constant number. So we can actually write the posterior distribution as being proportional to, this symbol right here represents proportional to. The joint distribution of the data and parameters, or the likelihood times the prior. The poster distribution is proportional to the joint distribution, or everything we have right here. In other words, what we have already written for this particular model is proportional to the posterior distribution of \\mu and \\sigma^2, given all of the data. The only thing missing in this expression right here is just some constant number that causes the expression to integrate to 1. If we can recognize this expression as being proportional to a common distribution, then our work is done, and we know what our posterior distribution is. This was the case for all models in the previous course. However, if we do not use conjugate priors or if the models are more complicated, then the posterior distribution will not have a standard form that we can recognize. We’re going to explore a couple of examples of this issue in the next segment.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#non-conjugate-models",
    "href": "C2-L02.html#non-conjugate-models",
    "title": "16  Bayesian Modeling",
    "section": "17.4 Non-conjugate models",
    "text": "17.4 Non-conjugate models\n\n\n\n\nNon-conjugate models\n\nWe’ll first look at an example of a one parameter model that is not conjugate.\n\n\n17.4.0.1 Company Personnel\nSuppose we have values that represent the percentage change in total personnel from last year to this year for, we’ll say, ten companies. These companies come from a particular industry. We’re going to assume for now, that these are independent measurements from a normal distribution with a known variance equal to one, but an unknown mean.\nSo we’ll say the percentage change in the total personnel for company I, given the unknown mean \\mu will be distributed normally with mean \\mu, and we’re just going to use variance 1.\nIn this case, the unknown mean could represent growth for this particular industry.\nIt’s the average of the growth of all the different companies. The small variance between the companies and percentage growth might be appropriate if the industry is stable.\nWe know that the conjugate prior for \\mu in this location would be a normal distribution.\nBut suppose we decide that our prior believes about \\mu are better reflected using a standard t distribution with one degree of freedom. So we could write that as the prior for \\mu is a t distribution with a location parameter 0. That’s where the center of the distribution is. A scale parameter of 1 to make it the standard t-distribution similar to a standard normal, and 1 degree of freedom.\nThis particular prior distribution has heavier tails than the conjugate and normal distribution, which can more easily accommodate the possibility of extreme values for mu. It is centered on zero so, that apriori, there is a 50% chance that the growth is positive and a 50% chance that the growth is negative.\n\n\nRecall that the posterior distribution of \\mu is proportional to the likelihood times the prior. Let’s write the expression for that in this model. That is the posterior distribution for \\mu given the data y_1 \\dots y_n is going to be proportional to the likelihood.\nIt is a product from i equals 1 to n, in this case that’s 10.\nDensities from a normal distribution.\nLet’s write the density from this particular normal distribution.\nIs 1 over the square root of 2 pi.\nE to the negative one-half.\nYi minus the mean squared, this is the normal density for each individual Yi and we multiplied it for likelihood.\nThe density for this t prior looks like this.\nIt’s 1 over pi times 1 plus \\mu squared.\nThis is the likelihood times the prior.\nIf we do a little algebra here, first of all, we’re doing this up to proportionality.\nSo, constants being multiplied by this expression are not important.\nThe square root of 2 pi being multiplied n times, is just a constant number, and \\pi creates a constant number. So we will drop them in our next step.\nSo this is now proportional too, we’re removing this piece and now we’re going to use properties of exponents.\nThe product of exponents is the sum of the exponentiated pieces.\nSo we have the exponent of negative one-half times the sum from i equals 1 to n, of Yi minus \\mu squared.\nAnd then we’re dropping the pie over here, so times 1 plus \\mu squared.\nWe’re going to do a few more steps of algebra here to get a nicer expression for this piece.\nBut we’re going to skip ahead to that.\nWe’ve now added these last two expressions.\nTo arrive at this expression here for the posterior, or what’s proportional to the posterior distribution.\nThis expression right here is almost proportional to a normal distribution except we have this 1 plus \\mu squared term in the denominator.\nWe know the posterior distribution up to a constant but we don’t recognize its form as a standard distribution.\nThat we can integrate or simulate from, so we’ll have to do something else.\nLet’s move on to our second example. For a two parameter example, we’re going to return to the case where we have a normal likelihood.\nAnd we’re now going to estimate \\mu and \\sigma^2, because they’re both unknown.\nRecall that if \\sigma^2 were known, the conjugate prior from \\mu would be a normal distribution.\nAnd if \\mu were known, the conjugate prior we could choose for \\sigma^2 would be an inverse gamma.\nWe saw earlier that if you include \\sigma^2 in the prior for \\mu, and use the hierarchical model that we presented earlier, that model would be conjugate and have a closed form solution. However, in the more general case that we have right here, the posterior distribution does not appear as a distribution that we can simulate or integrate.\nChallenging posterior distributions like these ones and most others that we’ll encounter in this course kept Bayesian in methods from entering the main stream of statistics for many years. Since only the simplest problems were tractable. However, computational methods invented in the 1950’s, and implemented by statisticians decades later, revolutionized the field. We do have the ability to simulate from the posterior distributions in this lesson as well as for many other more complicated models.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L03.html",
    "href": "C2-L03.html",
    "title": "17  Monte Carlo estimation",
    "section": "",
    "text": "17.1 Monte Carlo Integration\nBefore we learn how to simulate from complicated posterior distributions, let’s review some of the basics of Monte Carlo estimation.\nMonte Carlo estimation refers to simulating hypothetical draws from a probability distribution in order to calculate important quantities. By “important quantities,” we mean things like the mean, the variance, or the probability of some event or distributional property.\nAll of these calculations involve integration, which except for the simplest distributions, may range from very difficult to impossible :-) .\nSuppose we have a random variable \\theta that follows a \\Gamma distribution\n\\theta \\sim \\mathrm{Gamma}(a,b) \\qquad\n\\tag{17.1}\nLet’s say a=2 and b=\\frac{1}{3} , where a is the shape parameter and b is the rate parameter.\na=2 \\qquad b=1/3 \\qquad\n\\tag{17.2}\nTo calculate the mean of this distribution, we would need to compute the following integral. It is possible to compute this integral, and the answer is \\frac{a}{b} (6 in this case).\n\\mathbb{E}[\\theta] = \\int_0^\\infty \\theta f(\\theta) d\\theta = \\int_0^\\infty \\theta \\frac{b^a}{\\Gamma(a)}\\theta^{a-1}e^{-b\\theta} d\\theta = \\frac{a}{b} \\qquad\n\\tag{17.3}\nHowever, we could verify this answer through Monte Carlo estimation.\nTo do so, we would simulate a large number of draws (call them \\theta^∗_i \\quad (i=1,\\ldots ,m) ) from this gamma distribution and calculate their sample mean.\nWhy can we do this?\nRecall from the previous course that if we have a random sample from a distribution, the average of those samples converges in probability to the true mean of that distribution by the Law of Large Numbers.\nFurthermore, by the Central Limit Theorem (CLT), this sample mean \\bar{\\theta}^* = \\frac{1}{m}\\sum_{i=1}^m \\theta_i^* approximately follows a normal distribution with mean \\mathbb{E}[\\theta] and variance \\mathbb{V}ar[\\theta]/m .\nThe theoretical variance of \\theta is the following integral:\n\\text{Var}[\\theta] = \\int_0^\\infty (\\theta-\\mathbb{E}(\\theta))^2 f(\\theta) d\\theta \\qquad\n\\tag{17.4}\nJust like we did with the mean, we could approximate this variance with the sample variance\n\\text{Var}[\\theta^*] = \\frac{1}{m}\\sum_{i=1}^m (\\theta_i^* - \\bar{\\theta}^*)^2 \\qquad\n\\tag{17.5}",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#sec-monte-carlo-integration",
    "href": "C2-L03.html#sec-monte-carlo-integration",
    "title": "17  Monte Carlo estimation",
    "section": "",
    "text": "Monte Carlo Integration",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#calculating-probabilities",
    "href": "C2-L03.html#calculating-probabilities",
    "title": "17  Monte Carlo estimation",
    "section": "17.2 Calculating probabilities",
    "text": "17.2 Calculating probabilities\n\n\n\n\nMonte Carlo Integration\n\nThis method can be used to calculate many different integrals. Say h(\\theta) is any function and we want to calculate\n\n\\int h(\\theta) \\mathbb{P}r(\\theta) d\\theta = \\mathbb{E}(h(\\theta)) \\approx \\frac{1}{m}\\sum_{i=1}^m h(\\theta_i^*) \\qquad\n\\tag{17.6}\nwhere \\mathbb{P}r(\\theta) is the probability density function of \\theta and h(\\theta) is any function of \\theta.\nThis integral is precisely what is meant by \\mathbb{E}[h(\\theta)] , so we can conveniently approximate it by taking the sample mean of h(\\theta_i^*). That is, we apply the function h to each simulated sample from the distribution, and take the average of all the results.\nOne extremely useful example of an h function is is the indicator I_A(\\theta) where A is some logical condition about the value of \\theta. To demonstrate, suppose h(\\theta)=I_{[\\theta&lt;5]}(\\theta), which will give a 1 if \\theta &lt;5 and return a 0 otherwise.\nWhat is \\mathbb{E}(h(\\theta))?\nThis is the integral:\n\n\\begin{aligned}\n\\mathbb{E}[h(\\theta)] &= \\int_0^\\infty \\mathbb{I}_{[\\theta&lt;5]}(\\theta) \\mathbb{P}r(\\theta) d\\theta \\\\\n&= \\int_0^5 1 \\cdot \\mathbb{P}r(\\theta) d\\theta + \\int_5^\\infty 0 \\cdot \\mathbb{P}r(\\theta) d\\theta \\\\\n&= \\mathbb{P}r(\\theta &lt; 5) \\qquad\n\\end{aligned}\n\\tag{17.7}\nSo what does this mean?\nIt means we can approximate the probability that \\theta &lt; 5 by drawing many samples \\theta^∗_i , and approximating this integral with \\frac{1}{m} \\sum_{i=1}^m I_{\\theta^* &lt; 5} (\\theta_i^*). This expression is simply counting how many of those samples come out to be less than 5 , and dividing by the total number of simulated samples.\nThat’s convenient!\nLikewise, we can approximate quantiles of a distribution. If we are looking for the value z such that \\mathbb{P}r(\\theta &lt; z) = 0.9 , we simply arrange the samples \\theta^∗_i in ascending order and find the smallest drawn value that is greater than 90% of the others.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#monte-carlo-error-and-marginalization",
    "href": "C2-L03.html#monte-carlo-error-and-marginalization",
    "title": "17  Monte Carlo estimation",
    "section": "17.3 Monte Carlo Error and Marginalization",
    "text": "17.3 Monte Carlo Error and Marginalization\n\n\n\n\nMonte Carlo Error and Marginalization\n\nHow good is an approximation by Monte Carlo sampling?\nAgain we can turn to the CLT, which tells us that the variance of our estimate is controlled in part by m. For a better estimate, we want larger m.\nFor example, if we seek \\mathbb{E}[\\theta] , then the sample mean \\bar\\theta^∗ approximately follows a normal distribution with mean \\mathbb{E}[\\theta] and variance Var[\\theta]/m .\nThe variance tells us how far our estimate might be from the true value.\nOne way to approximate Var[\\theta] is to replace it with the sample variance.\nThe standard deviation of our Monte Carlo estimate is the square root of that, or the sample standard deviation divided by \\sqrt{m} .\nIf m is large, it is reasonable to assume that the true value will likely be within about two standard deviations of your Monte Carlo estimate.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#marginalization",
    "href": "C2-L03.html#marginalization",
    "title": "17  Monte Carlo estimation",
    "section": "17.4 Marginalization",
    "text": "17.4 Marginalization\nWe can also obtain Monte Carlo samples from hierarchical models.\nAs a simple example, let’s consider a binomial random variable where y \\mid \\phi \\sim \\mathrm{Bin}(10,\\phi) and further suppose \\phi is random (as if it had a prior) and is distributed beta \\phi \\sim \\mathrm{Beta}(2,2) .\nGiven any hierarchical model, we can always write the joint distribution of y and \\phi as \\mathbb{P}r(y,\\phi) = \\mathbb{P}r(y \\mid \\phi)\\mathbb{P}r(\\phi) using the chain rule of probability.\nTo simulate from this joint distribution, repeat these steps for a large number m :\n\nSimulate \\phi^∗_i from its Beta(2,2) distribution.\nGiven the drawn \\phi^∗_i , simulate y^∗_i from Bin(10,\\phi^*_i) .\n\nThis will produce m independent pairs of (y^∗,\\phi^∗)_i drawn from their joint distribution.\nOne major advantage of Monte Carlo simulation is that marginalizing is easy. Calculating the marginal distribution of y , \\mathbb{P}r(y)=\\int^1_0 \\mathbb{P}r(y,\\phi)d\\phi, might be challenging. But if we have draws from the joint distribution, we can just discard the \\phi^∗_i draws and use the y^∗_i as samples from their marginal distribution.\nThis is also called the prior predictive distribution introduced in the previous course.\nIn the next segment, we will demonstrate some of these principles.\nRemember, we do not yet know how to sample from the complicated posterior distributions introduced in the previous lesson.\nBut once we learn that, we will be able to use the principles from this lesson to make approximate inferences from those posterior distributions.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#computing-examples",
    "href": "C2-L03.html#computing-examples",
    "title": "17  Monte Carlo estimation",
    "section": "17.5 Computing Examples",
    "text": "17.5 Computing Examples\nMonte Carlo simulation from the most common distributions is very straightforward in R.\nLet’s start with the example from the previous segment, where \\theta \\sim Gamma(a,b) with a=2, b=1/3 . This could represent the posterior distribution of \\theta if our data came from a Poisson distribution with mean \\theta and we had used a conjugate gamma prior. Let’s start with m=100 .\n\n\nCode\nset.seed(32) # Initializes the random number generator so we can replicate these results. To get different random numbers, change the seed. \nm = 100 \na = 2.0 \nb = 1.0 / 3.0 \n\n\nTo simulate m independent samples, use the rgamma function.\n\n\nCode\ntheta &lt;- rgamma(n=m, shape = a, rate=b) \n\n\nWe can plot a histogram of the generated data, and compare that to the true density.\n\n\nCode\nhist(theta, freq=FALSE) \ncurve(dgamma(x=x, shape=a, rate=b), col=\"blue\", add=TRUE)\n\n\n\n\n\n\n\n\nFigure 17.1: Histogram of simulated gamma samples with true density\n\n\n\n\n\nTo find our Monte Carlo approximation to \\mathbb{E}(\\theta) , let’s take the average of our sample (and compare it with the truth).\n\n\nCode\nsum(theta) / m # sample mean \n\n\n[1] 5.514068\n\n\n\n\nCode\nmean(theta) # sample mean \n\n\n[1] 5.514068\n\n\n\n\nCode\na / b # true expected value\n\n\n[1] 6\n\n\nNot bad, but we can do better if we increase m to say, 10,000.\n\n\nCode\nm = 1e4 \ntheta = rgamma(n=m, shape=a, rate=b) \nmean(theta)\n\n\n[1] 6.023273\n\n\nHow about the variance of \\theta ?\n\n\nCode\nvar(theta) # sample variance\n\n\n[1] 18.04318\n\n\n\n\nCode\na / b^2 # true variance of Gamma(a,b) \n\n\n[1] 18\n\n\nWe can also approximate the probability that \\theta &lt; 5 .\n\n\nCode\nind = theta &lt; 5.0 # set of indicators, TRUE if theta_i &lt; 5 \nmean(ind)         # automatically converts FALSE/TRUE to 0/1 \n\n\n[1] 0.497\n\n\n\n\nCode\npgamma(q=5.0, shape=a, rate=b) # true value of Pr( theta &lt; 5 )\n\n\n[1] 0.4963317\n\n\nWhat is the 0.9 quantile (90th percentile) of \\theta ? We can use the quantile function which will order the samples for us and find the appropriate sample quantile.\n\n\nCode\nquantile(x=theta, probs=0.9) \n\n\n     90% \n11.74338 \n\n\n\n\nCode\nqgamma(p=0.9, shape=a, rate=b) # true value of 0.9 quantile\n\n\n[1] 11.66916",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#monte-carlo-error",
    "href": "C2-L03.html#monte-carlo-error",
    "title": "17  Monte Carlo estimation",
    "section": "17.6 Monte Carlo error",
    "text": "17.6 Monte Carlo error\nWe can use the CLT to approximate how accurate our Monte Carlo estimates are. For example, if we seek E(\\theta) , then the sample mean \\bar\\theta^∗ approximately follows a normal distribution with mean \\mathbb{E}(\\theta) and variance Var(\\theta)/m . We will use the sample standard deviation divided by the square root of m to approximate the Monte Carlo standard deviation.\n\n\nCode\nse = sd(theta) / sqrt(m) \n2.0 * se # we are reasonably confident that the Monte Carlo estimate is no more than this far from the truth\n\n\n[1] 0.08495454\n\n\nThese numbers give us a reasonable range for the quantity we are estimating with Monte Carlo. The same applies for other Monte Carlo estimates, like the probability that \\theta &lt; 5.\n\n\nCode\nind = theta &lt; 5.0 \nse = sd(ind) / sqrt(m)\n2.0 * se # we are reasonably confident that the Monte Carlo estimate is no more than this far from the truth \n\n\n[1] 0.01000032",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#marginalization-1",
    "href": "C2-L03.html#marginalization-1",
    "title": "17  Monte Carlo estimation",
    "section": "17.7 Marginalization",
    "text": "17.7 Marginalization\nLet’s also do the second example of simulating a hierarchical model. In our example from the previous segment, we had a binomial random variable where y \\mid \\phi \\overset{\\text{iid}}{\\sim}\\text{Binomial}(10,\\phi), and \\phi \\sim Beta(2,2). To simulate from this joint distribution, repeat these steps for a large number m :\n\nSimulate \\phi_i from its Beta(2,2) distribution.\nGiven the drawn \\phi_i , simulate y_i from Bin(10,\\phi_i) .\n\n\n\nCode\nm = 10e4\n\ny = numeric(m) # create the vectors we will fill in with simulations \nphi = numeric(m)\n\nfor (i in 1:m) {\n  phi[i] = rbeta(n=1, shape1=2.0, shape2=2.0)\n  y[i] = rbinom(n=1, size=10, prob=phi[i]) \n} \n# which is equivalent to the following 'vectorized' code \nphi = rbeta(n=m, shape1=2.0, shape2=2.0) \ny = rbinom(n=m, size=10, prob=phi)\n\n\nIf we are interested only in the marginal distribution of y , we can just ignore the draws for \\phi and treat the draws of y as a sample from its marginal distribution.\n\n\nCode\nmean(y) \n\n\n[1] 5.00008\n\n\n\n\nCode\nplot(prop.table(table(y)), ylab=\"Pr(y)\", main=\"Marginal distribution of y\")\n\n\n\n\n\n\n\n\nFigure 17.2",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#definition",
    "href": "C2-L03.html#definition",
    "title": "17  Monte Carlo estimation",
    "section": "18.1 Definition",
    "text": "18.1 Definition\nIf we have a sequence of random variables X_1,X_2,\\dots X_n where the indices 1,2,\\dots,n represent successive points in time, we can use the chain rule of probability to calculate the probability of the entire sequence:\n\n\\mathbb{P}r(X_1, X_2, \\ldots X_n) = \\mathbb{P}r(X_1) \\cdot \\mathbb{P}r(X_2 \\mid X_1) \\cdot \\mathbb{P}r(X_3 \\mid X_2, X_1) \\cdot \\ldots \\cdot \\mathbb{P}r(X_n \\mid X_{n-1}, X_{n-2}, \\ldots, X_2, X_1) \\qquad\n\\tag{18.1}\nMarkov chains simplify this expression by using the Markov assumption. The assumption is that given the entire past history, the probability distribution for the random variable at the next time step only depends on the current variable. Mathematically, the assumption is written like this:\n\n\\mathbb{P}r(X_{t+1} \\mid X_t, X_{t-1}, \\ldots, X_2, X_1 ) = \\mathbb{P}r(X_{t+1} \\mid X_t) \\qquad\n\\tag{18.2}\nfor all t=2,\\dots,n. Under this assumption, we can write the first expression as\n\n\\mathbb{P}r(X_1, X_2, \\ldots X_n) = \\mathbb{P}r(X_1) \\cdot \\mathbb{P}r(X_2 \\mid X_1) \\cdot \\mathbb{P}r(X_3 \\mid X_2) \\cdot \\mathbb{P}r(X_4 \\mid X_3) \\cdot \\ldots \\cdot \\mathbb{P}r(X_n \\mid X_{n-1}) \\qquad\n\\tag{18.3}\nwhich is much simpler than the original. It consists of an initial distribution for the first variable, \\mathbb{P}r(X_1), and n−1 transition probabilities. We usually make one more assumption: that the transition probabilities do not change with time. Hence, the transition from time t to time t+1 depends only on the value of Xt.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#examples-of-markov-chains",
    "href": "C2-L03.html#examples-of-markov-chains",
    "title": "17  Monte Carlo estimation",
    "section": "18.2 Examples of Markov chains",
    "text": "18.2 Examples of Markov chains\n\n18.2.1 Discrete Markov chain\nSuppose you have a secret number (make it an integer) between 1 and 5. We will call it your initial number at step 1. Now for each time step, your secret number will change according to the following rules:\n\nFlip a coin.\n\nIf the coin turns up heads, then increase your secret number by one (5 increases to 1).\nIf the coin turns up tails, then decrease your secret number by one (1 decreases to 5).\n\nRepeat n times, and record the evolving history of your secret number.\n\nBefore the experiment, we can think of the sequence of secret numbers as a sequence of random variables, each taking on a value in \\{1,2,3,4,5\\}. Assume that the coin is fair, so that with each flip, the probability of heads and tails are both 0.5.\nDoes this game qualify as a true Markov chain? Suppose your secret number is currently 4 and that the history of your secret numbers is (2,1,2,3). What is the probability that on the next step, your secret number will be 5? What about the other four possibilities? Because of the rules of this game, the probability of the next transition will depend only on the fact that your current number is 4. The numbers further back in your history are irrelevant, so this is a Markov chain.\nThis is an example of a discrete Markov chain, where the possible values of the random variables come from a discrete set. Those possible values (secret numbers in this example) are called states of the chain. The states are usually numbers, as in this example, but they can represent anything. In one common example, the states describe the weather on a particular day, which could be labeled as 1-fair, 2-poor.\n\n\n18.2.2 Random walk (continuous)\nNow let’s look at a continuous example of a Markov chain. Say X_t=0 and we have the following transition model:\n\n\\mathbb{P}r(X_{t+1}\\mid X_t=x_t)=N(x_t,1) \\qquad\n\\tag{18.4}\nThat is, the probability distribution for the next state is Normal with variance 1 and mean equal to the current state. This is often referred to as a “random walk.” Clearly, it is a Markov chain because the transition to the next state Xt+1 only depends on the current state Xt.\nThis example is straightforward to code in R:\n\n\nCode\nset.seed(34)\n\nn = 100\nx = numeric(n)\n\nfor (i in 2:n) {\n  x[i] = rnorm(1, mean=x[i-1], sd=1.0)\n}\n\nplot.ts(x)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#transition-matrix",
    "href": "C2-L03.html#transition-matrix",
    "title": "17  Monte Carlo estimation",
    "section": "18.3 Transition matrix",
    "text": "18.3 Transition matrix\nLet’s return to our example of the discrete Markov chain. If we assume that transition probabilities do not change with time, then there are a total of 25 (52) potential transition probabilities. Potential transition probabilities would be from State 1 to State 2, State 1 to State 3, and so forth. These transition probabilities can be arranged into a matrix Q:\n\nQ =\n\\begin{pmatrix}\n0 & .5 & 0 & 0 & .5 \\\\\n.5 & 0 & .5 & 0 & 0 \\\\\n0 & .5 & 0 & .5 & 0 \\\\\n0 & 0 & .5 & 0 & .5 \\\\\n.5 & 0 & 0 & .5 & 0 \\\\\n\\end{pmatrix} \\qquad\n\\tag{18.5}\nwhere the transitions from State 1 are in the first row, the transitions from State 2 are in the second row, etc. For example, the probability \\mathbb{P}r(X_{t+1}=5\\mid X_t=4) can be found in the fourth row, fifth column.\nThe transition matrix is especially useful if we want to find the probabilities associated with multiple steps of the chain. For example, we might want to know \\mathbb{P}r(X_{t+2}=3 \\mid X_t=1), the probability of your secret number being 3 two steps from now, given that your number is currently 1. We can calculate this as \\sum_{k=15} \\mathbb{P}r(X_t+2=3 \\mid X_t+1=k) \\cdot \\mathbb{P}r(X_{t+1}=k \\mid X_t=1), which conveniently is found in the first row and third column of Q_2.\nWe can perform this matrix multiplication easily in R:\n\n\nCode\nQ = matrix(c(0.0, 0.5, 0.0, 0.0, 0.5,\n             0.5, 0.0, 0.5, 0.0, 0.0,\n             0.0, 0.5, 0.0, 0.5, 0.0,\n             0.0, 0.0, 0.5, 0.0, 0.5,\n             0.5, 0.0, 0.0, 0.5, 0.0), \n           nrow=5, byrow=TRUE)\n\nQ %*% Q # Matrix multiplication in R. This is Q^2.\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,] 0.50 0.00 0.25 0.25 0.00\n[2,] 0.00 0.50 0.00 0.25 0.25\n[3,] 0.25 0.00 0.50 0.00 0.25\n[4,] 0.25 0.25 0.00 0.50 0.00\n[5,] 0.00 0.25 0.25 0.00 0.50\n\n\n\n\nCode\n(Q %*% Q)[1,3]\n\n\n[1] 0.25\n\n\nTherefore, if your secret number is currently 1, the probability that the number will be 3 two steps from now is .25.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#stationary-distribution",
    "href": "C2-L03.html#stationary-distribution",
    "title": "17  Monte Carlo estimation",
    "section": "18.4 Stationary distribution",
    "text": "18.4 Stationary distribution\nSuppose we want to know the probability distribution of the your secret number in the distant future, say \\mathbb{P}r(X_{t+h} \\mid X_t) where h is a large number. Let’s calculate this for a few different values of h.\n\n\nCode\nQ5 = Q %*% Q %*% Q %*% Q %*% Q # h=5 steps in the future\nround(Q5, 3)\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 0.062 0.312 0.156 0.156 0.312\n[2,] 0.312 0.062 0.312 0.156 0.156\n[3,] 0.156 0.312 0.062 0.312 0.156\n[4,] 0.156 0.156 0.312 0.062 0.312\n[5,] 0.312 0.156 0.156 0.312 0.062\n\n\n\n\nCode\nQ10 = Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q # h=10 steps in the future\nround(Q10, 3)\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 0.248 0.161 0.215 0.215 0.161\n[2,] 0.161 0.248 0.161 0.215 0.215\n[3,] 0.215 0.161 0.248 0.161 0.215\n[4,] 0.215 0.215 0.161 0.248 0.161\n[5,] 0.161 0.215 0.215 0.161 0.248\n\n\n\n\nCode\nQ30 = Q\nfor (i in 2:30) {\n  Q30 = Q30 %*% Q\n}\nround(Q30, 3) # h=30 steps in the future\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 0.201 0.199 0.200 0.200 0.199\n[2,] 0.199 0.201 0.199 0.200 0.200\n[3,] 0.200 0.199 0.201 0.199 0.200\n[4,] 0.200 0.200 0.199 0.201 0.199\n[5,] 0.199 0.200 0.200 0.199 0.201\n\n\nNotice that as the future horizon gets more distant, the transition distributions appear to converge. The state you are currently in becomes less important in determining the more distant future. If we let h get really large, and take it to the limit, all the rows of the long-range transition matrix will become equal to (.2,.2,.2,.2,.2). That is, if you run the Markov chain for a very long time, the probability that you will end up in any particular state is 1/5=.2 for each of the five states. These long-range probabilities are equal to what is called the stationary distribution of the Markov chain.\nThe stationary distribution of a chain is the initial state distribution for which performing a transition will not change the probability of ending up in any given state. That is,\n\n\nCode\nc(0.2, 0.2, 0.2, 0.2, 0.2) %*% Q\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.2  0.2  0.2  0.2  0.2\n\n\nOne consequence of this property is that once a chain reaches its stationary distribution, the stationary distribution will remain the distribution of the states thereafter.\nWe can also demonstrate the stationary distribution by simulating a long chain from this example.\n\n\nCode\nn = 5000\nx = numeric(n)\nx[1] = 1 # fix the state as 1 for time 1\nfor (i in 2:n) {\n  x[i] = sample.int(5, size=1, prob=Q[x[i-1],]) # draw the next state from the intergers 1 to 5 with probabilities from the transition matrix Q, based on the previous value of X.\n}\n\n\nNow that we have simulated the chain, let’s look at the distribution of visits to the five states.\n\nCode\ntable(x) / n\n\n\n\n\nTable 18.1\n\n\n\nx\n     1      2      3      4      5 \n0.1996 0.2020 0.1980 0.1994 0.2010 \n\n\n\n\nThe overall distribution of the visits to the states is approximately equal to the stationary distribution.\nAs we have just seen, if you simulate a Markov chain for many iterations, the samples can be used as a Monte Carlo sample from the stationary distribution. This is exactly how we are going to use Markov chains for Bayesian inference. In order to simulate from a complicated posterior distribution, we will set up and run a Markov chain whose stationary distribution is the posterior distribution.\nIt is important to note that the stationary distribution doesn’t always exist for any given Markov chain. The Markov chain must have certain properties, which we won’t discuss here. However, the Markov chain algorithms we’ll use in future lessons for Monte Carlo estimation are guaranteed to produce stationary distributions.\n\n18.4.1 Continuous example\nThe continuous random walk example we gave earlier does not have a stationary distribution. However, we can modify it so that it does have a stationary distribution.\nLet the transition distribution be \\mathbb{P}r(X_{t + 1}\\mid X_t = x_t)=N(\\phi x_t,1) where -1 &lt; \\phi &lt; 1. That is, the probability distribution for the next state is Normal with variance 1 and mean equal to ϕ times the current state. As long as \\phi is between −1 and 1, then the stationary distribution will exist for this model.\nLet’s simulate this chain for \\phi=−0.6.\n\n\nCode\nset.seed(38)\n\nn = 1500\nx = numeric(n)\nphi = -0.6\n\nfor (i in 2:n) {\n  x[i] = rnorm(1, mean=phi*x[i-1], sd=1.0)\n}\n\nplot.ts(x)\n\n\n\n\n\n\n\n\nFigure 18.1: Simulated AR(1) process with phi=-0.6\n\n\n\n\n\nThe theoretical stationary distribution for this chain is normal with mean 0 and variance 1/(1−\\phi^2), which in our example approximately equals 1.562. Let’s look at a histogram of our chain and compare that with the theoretical stationary distribution.\n\n\\text{Var}_{\\text{stationary}} = \\frac{1}{1-\\phi^2} \\qquad\n\\tag{18.6}\n\n\nCode\nhist(x, freq=FALSE)\ncurve(dnorm(x, mean=0.0, sd=sqrt(1.0/(1.0-phi^2))), col=\"red\", add=TRUE)\nlegend(\"topright\", legend=\"theoretical stationary\\ndistribution\", col=\"red\", lty=1, bty=\"n\")\n\n\n\n\n\n\n\n\nFigure 18.2: Histogram of simulated AR(1) process with theoretical stationary distribution\n\n\n\n\n\nIt appears that the chain has reached the stationary distribution. Therefore, we could treat this simulation from the chain like a Monte Carlo sample from the stationary distribution, a normal with mean 0 and variance 1.562.\nBecause most posterior distributions we will look at are continuous, our Monte Carlo simulations with Markov chains will be similar to this example.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Monte Carlo estimation</span>"
    ]
  },
  {
    "objectID": "C2-L04.html",
    "href": "C2-L04.html",
    "title": "18  Metropolis-Hastings",
    "section": "",
    "text": "18.1 The Metropolis-Hastings Algorithm\nMetropolis-Hastings (M-H) is an algorithm that allows us to sample from a generic probability distribution (which we will call the target distribution), even if we do not know the normalizing constant. To do this, we construct and sample from a Markov chain whose stationary distribution is the target distribution. It consists of picking an arbitrary starting value and iteratively accepting or rejecting candidate samples drawn from another distribution, one that is easy to sample.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#the-metropolis-hastings-algorithm",
    "href": "C2-L04.html#the-metropolis-hastings-algorithm",
    "title": "18  Metropolis-Hastings",
    "section": "",
    "text": "ImportantWhy use M-H or MCMC?\n\n\n\nWe will use M-H or other MCMC methods if there is no easy way to simulate independent draws from the target distribution. This can be due to non-conjugate priors, challenges in evaluating the normalizing constant or multiple explanatory variables.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#the-m-h-algorithm",
    "href": "C2-L04.html#the-m-h-algorithm",
    "title": "18  Metropolis-Hastings",
    "section": "18.2 The M-H Algorithm",
    "text": "18.2 The M-H Algorithm\n\n\n\n\nThe Metropolis-Hastings Algorithm\n\n Let’s say we wish to produce samples from a target distribution \\mathbb{P}r(\\theta) \\propto g(\\theta), where we don’t know the normalizing constant (since \\int g(\\theta)d\\theta is hard or impossible to compute), so we only have g(\\theta), the unnormalized joint probability to work with. The Metropolis-Hastings algorithm proceeds as follows.\n\nSelect an initial value \\theta_0.\nFor i=1,\\dots,m repeat the following steps:\n\nDraw a candidate sample \\theta^∗ from a proposal distribution  q(\\theta^* \\mid \\theta_{i−1}) .\nCompute the ratio \\alpha = \\frac{g(\\theta^*) / q(\\theta^* \\mid \\theta_{i-1}) }{g(\\theta_{i-1}) / q(\\theta_{i-1} \\mid \\theta^*)} = \\frac{g(\\theta^*)q(\\theta_{i-1} \\mid \\theta^*)}{g(\\theta_{i-1})q(\\theta^* \\mid \\theta_{i-1})}\n\nIf \\alpha\\ge 1, then accept \\theta^∗ and set \\theta_i=\\theta^∗.\nIf 0&lt;\\alpha&lt;1:\n\naccept \\theta^∗ and set \\theta_i=\\theta^∗ with probability \\alpha,\nreject \\theta^∗ and set \\theta_i=\\theta_{i−1} with probability 1−\\alpha.\n\n\n\n\nproposal distribution q\n\n\n\n\n\nImportantCorrection to the proposal distribution\n\n\n\nSteps 2.b and 2.c act as a correction  since the proposal distribution is not the target distribution. At each step in the chain, we draw a random candidate value of the parameter and decide whether to “move” the chain there or remain where we are. If the proposed move to the candidate is “advantageous,” (\\alpha \\ge 1) we “move” there and if it is not “advantageous,” we still might move there, but only with probability \\alpha. Since our decision to “move” to the candidate only depends on where the chain currently is, this is a Markov chain.\n\n\ncorrection",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#proposal-distribution-q",
    "href": "C2-L04.html#proposal-distribution-q",
    "title": "18  Metropolis-Hastings",
    "section": "18.3 Proposal distribution q",
    "text": "18.3 Proposal distribution q\nOne careful choice we must make is the candidate generating distribution q(\\theta^∗\\mid\\theta_{i−1}). It may or may not depend on the previous iteration’s value of \\theta.\n\n\n\n\n\n\nImportantIndependent Metropolis-Hastings\n\n\n\nThe simpler case is when the proposal distribution q does not depend on the previous value. We then write it as q(\\theta^∗). This arises if it is always the same distribution. We call this case independent Metropolis-Hastings. If we use independent M-H, q(\\theta) should be as similar as possible to \\mathbb{P}r(\\theta).\n\n\n\n\n\n\n\n\nImportantRandom-Walk Metropolis-Hastings\n\n\n\nIn the more general case, the proposal distribution takes the form q(\\theta^∗\\mid\\theta_{i−1}) with dependence on the previous iteration, is Random-Walk Metropolis-Hastings. Here, the proposal distribution is centered on \\theta_{i−1}.\nFor instance, it might be a Normal distribution with mean \\theta_{i−1}. Because the Normal distribution is symmetric, this example comes with another advantage: q(\\theta^* \\mid \\theta_{i−1})=q(\\theta_{i−1}∣\\theta^*) causing it to cancel out when we calculate \\alpha.\nThus, in Random-Walk M-H where the candidate is drawn from a Normal with mean \\theta_{i−1} and constant variance, the acceptance ratio is simply \\alpha=g(\\theta^∗)/g(\\theta_{i−1}).",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#acceptance-rate-α",
    "href": "C2-L04.html#acceptance-rate-α",
    "title": "18  Metropolis-Hastings",
    "section": "18.4 Acceptance rate α",
    "text": "18.4 Acceptance rate α\nClearly, not all candidate draws are accepted, so our Markov chain sometimes “stays” where it is, possibly for many iterations. How often you want the chain to accept candidates depends on the type of algorithm you use. If you approximate \\mathbb{P}r(\\theta) with q(\\theta^∗) and always draw candidates from that, accepting candidates often is good; it means q(\\theta^∗) is approximating \\mathbb{P}r(\\theta) well. However, you still may want q to have a larger variance than p and see some rejection of candidates as an assurance that q is covering the space well.\nAs we will see in coming examples, a high acceptance rate for the Random-Walk Metropolis-Hastings sampler is not a good thing. If the random walk is taking too small of steps, it will accept often but will take a very long time to fully explore the posterior. If the random walk is taking too large of steps, many of its proposals will have a low probability and the acceptance rate will be low, wasting many draws. Ideally, a random walk sampler should accept somewhere between 23% and 50% of the candidates proposed.\nIn the next segment, we will see a demonstration of this algorithm used in a discrete case, where we can show mathematically that the Markov chain converges to the target distribution. In the following segment, we will demonstrate coding a Random-Walk Metropolis-Hastings algorithm in R to solve one of the problems from the end of Lesson 2.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#demonstration-of-a-discrete-case",
    "href": "C2-L04.html#demonstration-of-a-discrete-case",
    "title": "18  Metropolis-Hastings",
    "section": "18.5 Demonstration of a Discrete case",
    "text": "18.5 Demonstration of a Discrete case\n\n\n\n\nMCMC Coin Flip Example\n\nThe following segment is by Herbert Lee, a professor of statistics and applied mathematics at the University of California, Santa Cruz.\nThe following is a demonstration of using Markov chain Monte Carlo, used to estimate posterior probabilities in a simplified case, where we can actually work out the correct answer in closed form. We demonstrate that the Metropolis-Hastings algorithm is indeed working, and giving us the right answer.\nIf you recall from the previous course, the example where your brother or maybe your sister, has a loaded coin that you know will come up heads 70% of the time. But they come to you with some coin, you’re not sure if it’s the loaded coin or a fair coin, and they want to make a bet with you. And you have to figure out which coin this is.\nSuppose you have a prior probability that it’s a 60% probability, that they’ll bring a loaded coin to you. They let you flip it five times, and you get two heads and three tails.\nAnd then you need to figure out, what’s your posterior probability that this is a loaded coin.\nOur unknown parameter \\theta, can either take the values fair or loaded.\n\n\\theta = \\{\\text{fair, loaded} \\}\n\\tag{18.1}\nOur prior for \\theta is the probability of theta equals loaded, is 0.6.\n\n\\mathbb{P}r(\\theta=\\text{loaded})=0.6 \\qquad  \\text{(prior)}\n\\tag{18.2}\nOur likelihood will follow a Binomial distribution, depending upon the value of \\theta.\n\nf(x\\mid \\theta) = {5 \\choose x} \\frac{1}{2}^5\\mathbb{I}_{\\theta=\\text{fair}}+  {5 \\choose x} (.7)^x(.3)^{5-x}\\mathbb{I}_{\\theta=\\text{loaded}}  \\qquad  \\text{(likelihood)}\n\\tag{18.3}\nOur posterior then, we can look at posterior for theta, given that we saw x=2 equals two heads, posterior is the likelihood times the prior, divided by a normalizing constant.\n\n  \\begin{aligned}\n    f(\\theta \\mid X=2) &=\n      \\frac{ \\frac{1}{2}^5(0.4)\\mathbb{I}_{(\\theta=\\text{fair})} + (.7)^2(.3)^{3}(.6)\\mathbb{I}_{(\\theta=\\text{loaded})}}\n           { \\frac{1}{2}^5(0.4) + (.7)^2(.3)^{3}(.6)}  \n  \\\\&=\\frac{ 0.0125 \\mathbb{I}_{(\\theta=\\text{fair})} + 0.00794 \\mathbb{I}_{(\\theta=\\text{loaded})}}\n           { 0.0125 + 0.00794}\n  \\\\&= 0.612 \\mathbb{I}_{(\\theta=\\text{fair})} + 0.388 \\mathbb{I}_{(\\theta=\\text{loaded})}\n  \\qquad  \\text{(posterior) }\n  \\end{aligned}\n\\tag{18.4}\nIn this case, we can work out the binomial and our prior. And we see that we get these expressions at the end. We get posterior probability of \\theta is loaded given that we saw two heads, to be 0.388.\n\n\\therefore \\mathbb{P}r(\\theta=\\text{loaded}\\mid X=2) = 0.388 \\qquad  \\text{(posterior conditional probability ) }\n\\tag{18.5}\nThis is all review from the previous course so far.\nBut suppose we had a more complicated problem, where we couldn’t work this all out in closed form? We’ll know the likelihood and the prior, but we may not be able to get this normalizing constant. Can we instead do this by simulation? And indeed, yes we can.\nWe can do this with Markov chain Monte Carlo. In particular, using the Metropolis-Hastings algorithm. What we’ll do is, we’ll set up a Markov chain whose equilibrium distribution has this posterior distribution. So we’ll consider a Markov chain with two states, theta equals fair and theta equals loaded. And we’ll allow the chain to move between those two states, with certain transition probabilities. We set this up using this using the Metropolis-Hastings algorithm.\n\n\n\n\nThe Metropolis-Hastings Algorithm\n\nSo under the Metropolis-Hastings algorithm, step one is we start at an arbitrary location. And in this case, we can\n\nstart at either \\theta \\ne \\text{fair}, or \\theta \\ne \\text{loaded}.\n\nIt does not really matter where we start, we’ll be moving back and forth and we’re going to look at the long-term running average, the long-term simulations.\nSo the key is we’ll be simulating.\n\nRun m simulations and in each iteration, we’ll propose a candidate and either accept it or reject it.\n\n\nSo the first part is we’re proposing a new candidate. We’ll call this candidate \\theta^*, and we’re going to propose it be the other state compared to where we are now. Where we are now is \\theta_{i-1}, and so we’ll propose to move to \\theta^*.\n\nIf our current state is fair, we’ll propose \\theta^*=\\text{loaded}.\nIf our current state is loaded, we’ll propose \\theta^*=\\text{fair}.\n\n\nwhat’s our acceptance probability alpha?\nThe general form for \\alpha is:\n\n\\begin {aligned}\n\\alpha &= {\n            { g(\\theta^*)     / q(\\theta^*     \\mid  \\theta_{i-1}) }\n      \\over {g(\\theta_{i-1}) / q(\\theta_{i-1} \\mid  \\theta^*)     }\n      }\n\\\\      &= {\n            { f(x=2 \\mid \\theta^*) f(\\theta^*)     / 1 }\n      \\over { f(x=2 \\mid \\theta_{i-1})f(\\theta_{i-1}) / 1    }\n} \\qquad \\text {(sub. g,q)}\n\\end{aligned}\n\\tag{18.6}\nIn this case,\n\ng() is our un-normalized likelihood times prior\nq(), the proposal distribution, is, in this case, since we always accept the opposite state deterministically i.e. \\theta^*=\\neg \\theta{i_1} with P=1\nIf \\theta^* = \\text{loaded} \\implies \\alpha = {0.00794 \\over 0.0125}=0.635\nIf \\theta^* = \\text{fair} \\implies \\alpha = { 0.0125 \\over 0.00794}=1.574\n\n\n\n\n\nThe Metropolis-Hastings Algorithm\n\nGiven these probabilities, we then can do the acceptance or rejection step.\n\n\\begin{cases}\n\\text{ accept } \\theta^* \\text { and set } \\theta_i=\\text{fair} & \\text{If } \\theta^*=\\text{fair,  } \\alpha&gt;1\n\\\\ \\begin {cases}\n   \\text{ accept } \\theta^* \\text{  and set } \\theta_i=\\text{loaded} &  \\text{ With probability } 0.635\n\\\\ \\text{ reject } \\theta^* \\text{ and set } \\theta_i=\\text{fair}     &  \\text{ Otherwise }\n\\end{cases} & \\text{If } \\theta^*=\\text{loaded, } \\alpha=.635\n\\end{cases}\n\nIf the \\theta^*=\\text{loaded} \\implies \\alpha=0.635. So we accept theta star with probability 0.635. And if we accept it. Set \\theta_i=\\text{loaded} Otherwise, set \\theta_i = \\theta_{i- 1}, if we do not accept, it stays in that same old fair state.\nWe can draw this out as a Markov chain with two states, Fair and ‘loaded’. If it’s in the ‘loaded’ state, it will move with probability one to the fair state. If it’s in the fair state, it will move with a probability of 0.635 to the ‘loaded’ state. And with a probability of 0.365 it will stay in the fair state.\n\n\n\n\nstate diagram\n\nAnd so here’s a little diagram for this Markov chain with two states. In which case it will move back and forth with certain probabilities.\nThus, if we wanted to find our posterior probability , f(\\theta=\\text{loaded} \\mid x=2). We can simulate from this Markov chain using these transition probabilities. And observe the fraction of time that it spends in the state theta equals ‘loaded’. And this gives us a good estimate of the posterior probability that it’s the ‘loaded’ coin. In this particular case, we can also show that this gives us the theoretical right answer.\nIf you’ve seen a little bit of the theory of Markov chains. We can say that a Markov chain with transition probability capital P, has stationary distribution \\Pi.\n\n\\pi P = \\pi \\qquad \\text{(def. stationary distribution)}\n\\tag{18.7}\nHere we have a transition probability matrix P, where we can think about ‘fair’ and ‘loaded’. Moving from the ‘fair’ state, remaining in the ‘fair’ state happens with a probability of 0.365 and it moves from ‘fair’ to ‘loaded’, with a probability of 0.635. If it’s in the ‘loaded’ state, we’ll move to the ‘fair’ state with probability one, and it will stay in the ‘loaded’ state with probability 0.\n\nP=\\begin{bmatrix}\n   0.365 & 0.635\n\\\\ 1 & 0\n\\end{bmatrix}\n\nIn this case, we want our stationary distribution to be the posterior probabilities.\n\n\\Pi=\\begin{bmatrix}\n0.612 & 0.388 \\\\\n\\end{bmatrix}\n\nWhich you can recall are 0.612 of being ‘fair’ and 0.388 of being ‘loaded’. And so indeed, if you do just the minimal amount of matrix algebra, you can see that 0.612, 0.388 Multiplied by this matrix, 0.365, 0.635, 1, 0, does indeed give you 0.612 and 0.388, at least to within rounding error.\n\n\\begin{aligned}\n  \\Pi P &=\n  \\begin{bmatrix} 0.612 & 0.388 \\end{bmatrix}\n  \\begin{bmatrix} 0.365 & 0.635 \\\\ 1 & 0 \\end{bmatrix}\n  \\\\&= \\begin{bmatrix}0.612 & 0.388 \\end{bmatrix}\n  \\\\&= \\Pi\n\\end{aligned}\n\\tag{18.8}\nThus in this case we can see, that we do get the correct stationary distribution for the Markov chain using the Metropolis–Hastings algorithm. And that when we simulate it, we do get correct estimates then of the posterior probabilities.\nThis is a nice simple example where we can work out the posterior probabilities in closed form. We don’t need to run Markov chain Monte Carlo. But this method is very powerful because all we need is to be able to evaluate the likelihood and the prior, we don’t need to evaluate the full posterior and get that normalizing constant. And so this applies to a much broader range of more complicated problems. Where we can use Markov chain Monte Carlo to simulate, to be able to get these probabilities. We’ll make good use of this in the rest of this course.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#random-walk-with-normal-likelihood-t-prior",
    "href": "C2-L04.html#random-walk-with-normal-likelihood-t-prior",
    "title": "18  Metropolis-Hastings",
    "section": "18.6 Random walk with Normal likelihood, t prior",
    "text": "18.6 Random walk with Normal likelihood, t prior\nRecall the model from the last segment of Lesson 2 where the data are the percent change in total personnel from last year to this year for n=10 companies. We used a normal likelihood with known variance and t distribution for the prior on the unknown mean. Suppose the values are y=(1.2,1.4,−0.5,0.3,0.9,2.3,1.0,0.1,1.3,1.9). Because this model is not conjugate, the posterior distribution is not in a standard form that we can easily sample. To obtain posterior samples, we will set up a Markov chain whose stationary distribution is this posterior distribution.\nRecall that the posterior distribution is\n\n\\mathbb{P}r(\\mu \\mid y_1, \\ldots, y_n) \\propto \\frac{\\exp[ n ( \\bar{y} \\mu - \\mu^2/2)]}{1 + \\mu^2}\n\nThe posterior distribution on the left is our target distribution and the expression on the right is our g(\\mu).\nThe first thing we can do in R is write a function to evaluate g(\\mu). Because posterior distributions include likelihoods (the product of many numbers that are potentially small), g(\\mu) might evaluate to such a small number that to the computer, it is effectively zero. This will cause a problem when we evaluate the acceptance ratio \\alpha. To avoid this problem, we can work on the log scale, which will be more numerically stable. Thus, we will write a function to evaluate\n\n\\log(g(\\mu)) = n ( \\bar{y} \\mu - \\mu^2/2) - \\log(1 + \\mu^2)\n\nThis function will require three arguments, \\mu, \\bar{y}, and n.\n\n\nCode\nlg = function(mu, n, ybar) {\n  mu2 = mu^2\n  n * (ybar * mu - mu2 / 2.0) - log(1 + mu2)\n}\n\n\nNext, let’s write a function to execute the Random-Walk Metropolis-Hastings sampler with Normal proposals.\n\n\nCode\nmh = function(n, ybar, n_iter, mu_init, cand_sd) {\n  ## Random-Walk Metropolis-Hastings algorithm\n  \n  ## Step 1, initialize\n  mu_out = numeric(n_iter)\n  accpt = 0\n  mu_now = mu_init\n  lg_now = lg(mu=mu_now, n=n, ybar=ybar)\n  \n  ## Step 2, iterate\n  for (i in 1:n_iter) {\n    ## step 2a\n    mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate\n    \n    ## Step 2b\n    lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate\n    lalpha = lg_cand - lg_now # log of acceptance ratio\n    alpha = exp(lalpha)\n    \n    ## step 2c\n    u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)\n    if (u &lt; alpha) { # then accept the candidate\n      mu_now = mu_cand\n      accpt = accpt + 1 # to keep track of acceptance\n      lg_now = lg_cand\n    }\n    \n    ## collect results\n    mu_out[i] = mu_now # save this iteration's value of mu\n  }\n  \n  ## return a list of output\n  list(mu=mu_out, accpt=accpt/n_iter)\n}\n\n\nNow, let’s set up the problem.\n\n\nCode\ny = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)\nybar = mean(y)\nn = length(y)\nhist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data\ncurve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu\npoints(y, rep(0,n), pch=1) # individual data points\npoints(ybar, 0, pch=19) # sample mean\n\n\n\n\n\n\n\n\n\nFinally, we’re ready to run the sampler! Let’s use m=1000 iterations and proposal standard deviation (which controls the proposal step size) 3.0, and initial value at the prior median 0.\n\n\nCode\nset.seed(43) # set the random seed for reproducibility\npost = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=3.0)\nstr(post)\n\n\nList of 2\n $ mu   : num [1:1000] -0.113 1.507 1.507 1.507 1.507 ...\n $ accpt: num 0.122\n\n\n\n\nCode\nlibrary(\"coda\")\ntraceplot(as.mcmc(post$mu))\n\n\n\n\n\n\n\n\n\nThis last plot is called a trace plot. It shows the history of the chain and provides basic feedback about whether the chain has reached its stationary distribution.\nIt appears our proposal step size was too large (acceptance rate below 23%). Let’s try another.\n\n\nCode\npost = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.05)\npost$accpt\n\n\n[1] 0.946\n\n\n\n\nCode\ntraceplot(as.mcmc(post$mu))\n\n\n\n\n\n\n\n\n\nOops, the acceptance rate is too high (above 50%). Let’s try something in between.\n\n\nCode\npost = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.9)\npost$accpt\n\n\n[1] 0.38\n\n\n\n\nCode\ntraceplot(as.mcmc(post$mu))\n\n\n\n\n\n\n\n\n\nWhich looks good. Just for fun, let’s see what happens if we initialize the chain at some far-off value.\n\n\nCode\npost = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=30.0, cand_sd=0.9)\npost$accpt\n\n\n[1] 0.387\n\n\n\n\nCode\ntraceplot(as.mcmc(post$mu))\n\n\n\n\n\n\n\n\n\nIt took awhile to find the stationary distribution, but it looks like we succeeded! If we discard the first 100 or so values, it appears like the rest of the samples come from the stationary distribution, our posterior distribution! Let’s plot the posterior density against the prior to see how the data updated our belief about \\mu.\n\n\nCode\npost$mu_keep = post$mu[-c(1:100)] # discard the first 200 samples\nplot(density(post$mu_keep, adjust=2.0), main=\"\", xlim=c(-1.0, 3.0), xlab=expression(mu)) # plot density estimate of the posterior\ncurve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu\npoints(ybar, 0, pch=19) # sample mean\n\ncurve(0.017*exp(lg(mu=x, n=n, ybar=ybar)), from=-1.0, to=3.0, add=TRUE, col=\"blue\") # approximation to the true posterior in blue\n\n\n\n\n\n\n\n\n\nThese results are encouraging, but they are preliminary. We still need to investigate more formally whether our Markov chain has converged to the stationary distribution. We will explore this in a future lesson.\nObtaining posterior samples using the Metropolis-Hastings algorithm can be time-consuming and require some fine-tuning, as we’ve just seen. The good news is that we can rely on software to do most of the work for us. In the next couple of videos, we’ll introduce a program that will make posterior sampling easy.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#setup",
    "href": "C2-L04.html#setup",
    "title": "18  Metropolis-Hastings",
    "section": "19.1 Setup",
    "text": "19.1 Setup\n\n19.1.1 Introduction to JAGS\nThere are several software packages available that will handle the details of MCMC for us. See the supplementary material for a brief overview of options.\nThe package we will use in this course is JAGS (Just Another Gibbs Sampler) by Martyn Plummer. The program is free, and runs on Mac OS, Windows, and Linux. Better yet, the program can be run using R with the rjags and R2jags packages.\nIn JAGS, we can specify models and run MCMC samplers in just a few lines of code; JAGS does the rest for us, so we can focus more on the statistical modeling aspect and less on the implementation. It makes powerful Bayesian machinery available to us as we can fit a wide variety of statistical models with relative ease.\n\n\n19.1.2 Installation and setup\nThe starting place for JAGS users is mcmc-jags.sourceforge.net. At this site, you can find news about the features of the latest release of JAGS, links to program documentation, as well as instructions for installation.\nThe documentation is particularly important. It is available under the files page link in the Manuals folder.\nAlso under the files page, you will find the JAGS folder where you can download and install the latest version of JAGS. Select the version and operating system, and follow the instructions for download and installation.\nOnce JAGS is installed, we can immediately run it from R using the rjags package. The next segment will show how this is done.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#modeling-in-jags",
    "href": "C2-L04.html#modeling-in-jags",
    "title": "18  Metropolis-Hastings",
    "section": "19.2 Modeling in JAGS",
    "text": "19.2 Modeling in JAGS\nThere are four steps to implementing a model in JAGS through R:\n\nSpecify the model.\nSet up the model.\nRun the MCMC sampler.\nPost-processing.\n\nWe will demonstrate these steps with our running example with the data are the percent change in total personnel from last year to this year for n=10 companies. We used a normal likelihood with known variance and t distribution for the prior on the unknown mean.\n\n19.2.1 1. Specify the model\nIn this step, we give JAGS the hierarchical structure of the model, assigning distributions to the data (the likelihood) and parameters (priors). The syntax for this step is very similar to R, but there are some key differences.\n\n\nCode\nlibrary(\"rjags\")\n\nmod_string = \" model {\n  for (i in 1:n) {\n    y[i] ~ dnorm(mu, 1.0/sig2)\n  }\n  mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom\n  sig2 = 1.0\n} \"\n\n\nOne of the primary differences between the syntax of JAGS and R is how the distributions are parameterized. Note that the normal distribution uses the mean and precision (instead of variance). When specifying distributions in JAGS, it is always a good idea to check the JAGS user manual here in the chapter on Distributions.\n\n\n19.2.2 2. Set up the model\n\n\nCode\nset.seed(50)\ny = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)\nn = length(y)\n\ndata_jags = list(y=y, n=n)\nparams = c(\"mu\")\n\ninits = function() {\n  inits = list(\"mu\"=0.0)\n} # optional (and fixed)\n\nmod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 10\n   Unobserved stochastic nodes: 1\n   Total graph size: 15\n\nInitializing model\n\n\nThere are multiple ways to specify initial values here. They can be explicitly set, as we did here, or they can be random, i.e., list(\"mu\"=rnorm(1)). Also, we can omit the initial values, and JAGS will provide them.\n\n\n19.2.3 3. Run the MCMC sampler\n\n\nCode\nupdate(mod, 500) # burn-in\n\nmod_sim = coda.samples(model=mod, variable.names=params, n.iter=1000)\n\n\nWe will discuss more options to the coda.samples function in coming examples.\n\n\n19.2.4 4. Post-processing\n\n\nCode\nsummary(mod_sim)\n\n\n\nIterations = 1501:2500\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 1000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       0.89477        0.32375        0.01024        0.01261 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n0.2710 0.6536 0.8878 1.1394 1.4989 \n\n\n\n\nCode\nlibrary(\"coda\")\nplot(mod_sim)\n\n\n\n\n\n\n\n\n\nWe will discuss post processing further, including convergence diagnostics, in a coming lesson.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Metropolis-Hastings</span>"
    ]
  },
  {
    "objectID": "C2-L05.html",
    "href": "C2-L05.html",
    "title": "19  Gibbs sampling",
    "section": "",
    "text": "19.1 Multiple parameter sampling and full conditional distributions\nGibbs sampling is a Gibbs sampling is named after the physicist Josiah Willard Gibbs, in reference to an analogy between the sampling algorithm and statistical physics. The algorithm was described in (Geman and Geman 1984) by brothers Stuart and Donald Geman, and became popularized in the statistics community for calculating marginal probability distribution, especially the posterior distribution. Gibbs sampling is better suited for sampling from models with many variables by sampling them one at a time from a full conditional distribution.\nSo far, we have demonstrated MCMC for a single parameter.\nWhat if we seek the posterior distribution of multiple parameters, and that posterior distribution does not have a standard form?\nOne option is to perform Metropolis-Hastings (M-H) by sampling candidates for all parameters at once, and accepting or rejecting all of those candidates together. While this is possible, it can get complicated.\nAnother (simpler) option is to sample the parameters one at a time.\nAs a simple example, suppose we have a joint posterior distribution for two parameters \\theta and \\phi, written p(\\theta, \\phi \\mid y) \\propto g(\\theta, \\phi). If we knew the value of \\phi, then we would just draw a candidate for \\theta and use g(\\theta, \\phi) to compute our Metropolis-Hastings ratio, and possibly accept the candidate. Before moving on to the next iteration, if we don’t know \\phi, then we can perform a similar update for it. Draw a candidate for \\phi using some proposal distribution and again use g(\\theta, \\phi) to compute our Metropolis-Hastings ratio. Here we pretend we know the value of \\theta by substituting its current iteration from the Markov chain. Once we’ve drawn for both \\theta and \\phi, that completes one iteration and we begin the next iteration by drawing a new \\theta. In other words, we’re just going back and forth, updating the parameters one at a time, plugging the current value of the other parameter into g(\\theta, \\phi).\nThis idea of one-at-a-time updates is used in what we call Gibbs sampling, which also produces a stationary Markov chain (whose stationary distribution is the posterior). If you recall, this is the namesake of JAGS, “just another Gibbs sampler.”",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Gibbs sampling</span>"
    ]
  },
  {
    "objectID": "C2-L05.html#multiple-parameter-sampling-and-full-conditional-distributions",
    "href": "C2-L05.html#multiple-parameter-sampling-and-full-conditional-distributions",
    "title": "19  Gibbs sampling",
    "section": "",
    "text": "Multiple parameter sampling and full conditional distributions\n\n\n\n\n\n\n\n\n19.1.1 Full conditional distributions\nBefore describing the full Gibbs sampling algorithm , there’s one more thing we can do. Using the chain rule of probability, we have\n\np(\\theta, \\phi \\mid y) = p(\\theta \\mid \\phi, y) \\cdot p(\\phi \\mid y)\n\nNotice that the only difference between p(\\theta, \\phi \\mid y) and p(\\theta \\mid \\phi, y) is multiplication by a factor that doesn’t involve \\theta. Since the g(\\theta, \\phi) function above, when viewed as a function of \\theta is proportional to both these expressions, we might as well have replaced it with p(\\theta \\mid \\phi, y) in our update for \\theta.\nThis distribution p(\\theta \\mid \\phi, y) is called the full conditional distribution  for \\theta. Full conditional distribution\nWhy use p(\\theta \\mid \\phi, y) instead of g(\\theta, \\phi)?\nIn some cases, the full conditional distribution is a standard distribution we know how to sample. If that happens, we no longer need to draw a candidate and decide whether to accept it. In fact, if we treat the full conditional distribution as a candidate proposal distribution, the resulting Metropolis-Hastings acceptance probability becomes exactly 1.\nGibbs samplers require a little more work up front because you need to find the full conditional distribution for each parameter. The good news is that all full conditional distributions have the same starting point: the full joint posterior distribution. Using the example above, we have\n\np(\\theta \\mid \\phi, y) \\propto p(\\theta, \\phi \\mid y)\n\nwhere we simply now treat \\phi as a known number. Likewise, the other full conditional is p(\\phi \\mid \\theta, y) \\propto p(\\theta, \\phi \\mid y) where here, we consider \\theta to be a known number. We always start with the full posterior distribution. Thus, the process of finding full conditional distributions is the same as finding the posterior distribution of each parameter, pretending that all other parameters are known.\n\n\n19.1.2 Gibbs sampler\n\n\n\n\n\n\nNote\n\n\n\nThe idea of Gibbs sampling is that we can update multiple parameters by sampling just one parameter at a time, cycling through all parameters and repeating. To perform the update for one particular parameter, we substitute in the current values of all other parameters.\n\n\nHere is the algorithm. Suppose we have a joint posterior distribution for two parameters \\theta and \\phi, written p(\\theta, \\phi \\mid y). If we can find the distribution of each parameter at a time, i.e., p(\\theta \\mid \\phi, y) and p(\\phi \\mid \\theta, y), then we can take turns sampling these distributions like so:\n\nUsing \\phi_{i-1}, draw \\theta_i from p(\\theta \\mid \\phi = \\phi_{i-1}, y).\nUsing \\theta_i, draw \\phi_i from p(\\phi \\mid \\theta = \\theta_i, y).\n\nTogether, steps 1 and 2 complete one cycle of the Gibbs sampler and produce the draw for (\\theta_i, \\phi_i) in one iteration of a MCMC sampler. If there are more than two parameters, we can handle that also. One Gibbs cycle would include an update for each of the parameters.\nIn the following segments, we will provide a concrete example of finding full conditional distributions and constructing a Gibbs sampler.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Gibbs sampling</span>"
    ]
  },
  {
    "objectID": "C2-L05.html#conditionally-conjugate-prior-example-with-normal-likelihood",
    "href": "C2-L05.html#conditionally-conjugate-prior-example-with-normal-likelihood",
    "title": "19  Gibbs sampling",
    "section": "19.2 Conditionally conjugate prior example with Normal likelihood",
    "text": "19.2 Conditionally conjugate prior example with Normal likelihood\n\n19.2.1 Normal likelihood, unknown mean and variance\n\n\n\n\nNormal likelihood, unknown mean and variance\n\n\n\n\nNormal likelihood conjugate prior\n\n\nLet’s return to the example at the end of Lesson 2 where we have normal likelihood with unknown mean and unknown variance. The model is:\n\n\\begin{aligned}\ny_i \\mid \\mu, \\sigma^2 &\\overset{\\text{iid}}{\\sim} \\text{N} ( \\mu, \\sigma^2 ), \\quad i=1,\\ldots,n \\\\\n\\mu &\\sim \\text{N}(\\mu_0, \\sigma_0^2) \\\\\n\\sigma^2 &\\sim \\text{IG}(\\nu_0, \\beta_0)  \\, .\n\\end{aligned}\n\nWe chose a normal prior for \\mu because, in the case where \\sigma^2 is known, the normal is the conjugate prior for \\mu. Likewise, in the case where \\mu is known, the inverse-gamma is the conjugate prior for \\sigma^2. This will give us convenient full conditional distributions in a Gibbs sampler.\nLet’s first work out the form of the full posterior distribution. When we begin analyzing data, the JAGS software will complete this step for us. However, it is extremely valuable to see and understand how this works.\n\n\\begin{aligned}\np( \\mu, \\sigma^2 \\mid y_1, y_2, \\ldots, y_n ) &\\propto\np(y_1, y_2, \\ldots, y_n \\mid \\mu, \\sigma^2) p(\\mu) p(\\sigma^2)\n\\\\ &= \\prod_{i=1}^n \\text{N} ( y_i \\mid \\mu, \\sigma^2 ) \\times \\text{N}( \\mu \\mid \\mu_0, \\sigma_0^2) \\times \\text{IG}(\\sigma^2 \\mid \\nu_0, \\beta_0)\n\\\\ &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[ -\\frac{(y_i - \\mu)^2}{2\\sigma^2} \\right]\n\\\\ & \\qquad \\times\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp \\left[ -\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} \\right]\n\\\\ & \\qquad \\times \\frac{\\beta_0^{\\nu_0}}{\\Gamma(\\nu_0)}(\\sigma^2)^{-(\\nu_0 + 1)} \\exp \\left[ -\\frac{\\beta_0}{\\sigma^2} \\right] \\mathbb{I}_{\\sigma^2 &gt; 0}(\\sigma^2)\n\\\\ &\\propto (\\sigma^2)^{-n/2} \\exp \\left[ -\\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2\\sigma^2} \\right]\n\\\\ & \\qquad \\times \\exp \\left[ -\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} \\right] (\\sigma^2)^{-(\\nu_0 + 1)}\n\\\\ & \\qquad \\times \\exp \\left[ -\\frac{\\beta_0}{\\sigma^2} \\right] \\mathbb{I}_{\\sigma^2 &gt; 0}(\\sigma^2)\n\\end{aligned}\n\nFrom here, it is easy to continue on to find the two full conditional distributions we need.\nFirst let’s look at \\mu, assuming \\sigma^2 is known (in which case it becomes a constant and is absorbed into the normalizing constant):\n\n\\begin{aligned}\np(\\mu \\mid \\sigma^2, y_1, \\ldots, y_n) &\\propto p( \\mu, \\sigma^2 \\mid y_1, \\ldots, y_n )\n\\\\ &\\propto \\exp \\left[ -\\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2\\sigma^2} \\right] \\exp \\left[ -\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} \\right]\n\\\\ &\\propto \\exp \\left[ -\\frac{1}{2} \\left( \\frac{ \\sum_{i=1}^n (y_i - \\mu)^2}{2\\sigma^2} + \\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} \\right) \\right]\n\\\\ &\\propto \\text{N} \\left( \\mu \\mid \\frac{n\\bar{y}/\\sigma^2 + \\mu_0/\\sigma_0^2}{n/\\sigma^2 + 1/\\sigma_0^2} \\frac{1}{n/\\sigma^2 + 1/\\sigma_0^2} \\right)\n\\end {aligned}\n\\tag{19.1}\nwhich we derived in the supplementary material of the last course. So, given the data and \\sigma^2, \\mu follows this normal distribution.\nNow let’s look at \\sigma^2, assuming \\mu is known:\n\n\\begin{aligned}\np(\\sigma^2 \\mid \\mu, y_1, \\ldots, y_n) & \\propto p( \\mu, \\sigma^2 \\mid y_1, \\ldots, y_n )\n\\\\ &\\propto (\\sigma^2)^{-n/2} \\exp \\left[ -\\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2\\sigma^2} \\right] (\\sigma^2)^{-(\\nu_0 + 1)} \\exp \\left[ -\\frac{\\beta_0}{\\sigma^2} \\right] I_{\\sigma^2 &gt; 0}(\\sigma^2)\n\\\\ &\\propto (\\sigma^2)^{-(\\nu_0 + n/2 + 1)} \\exp \\left[ -\\frac{1}{\\sigma^2} \\left( \\beta_0 + \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2} \\right) \\right] I_{\\sigma^2 &gt; 0}(\\sigma^2)\n\\\\ &\\propto \\text{IG}\\left( \\sigma^2 \\mid \\nu_0 + \\frac{n}{2}, \\, \\beta_0 + \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2} \\right)\n\\end{aligned}\n\\tag{19.2}\nThese two distributions provide the basis of a Gibbs sampler to simulate from a Markov chain whose stationary distribution is the full posterior of both \\mu and \\sigma^2. We simply alternate draws between these two parameters, using the most recent draw of one parameter to update the other.\nWe will do this in R in the next segment.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Gibbs sampling</span>"
    ]
  },
  {
    "objectID": "C2-L05.html#computing-example-with-normal-likelihood",
    "href": "C2-L05.html#computing-example-with-normal-likelihood",
    "title": "19  Gibbs sampling",
    "section": "19.3 Computing example with Normal likelihood",
    "text": "19.3 Computing example with Normal likelihood\nTo implement the Gibbs sampler we just described, let’s return to our running example where the data are the percent change in total personnel from last year to this year for n=10 companies.  We’ll still use a normal likelihood, but now we’ll relax the assumption that we know the variance of growth between companies, \\sigma^2, and estimate that variance. Instead of the t prior from earlier, we will use the conditionally conjugate priors, normal for \\mu and inverse-gamma for \\sigma^2.Company personnel\nThe first step will be to write functions to simulate from the full conditional distributions we derived in the previous segment. The full conditional for \\mu, given \\sigma^2 and data is\n\n19.3.1 conditionally conjugate priors for the mean\n\n\\text{N} \\left( \\mu \\mid \\frac{n\\bar{y}/\\sigma^2 + \\mu_0/\\sigma_0^2}{n/\\sigma^2 + 1/\\sigma_0^2}, \\, \\frac{1}{n/\\sigma^2 + 1/\\sigma_0^2} \\right)\n\\tag{19.3}\n\n\nCode\n#' update_mu\n#'\n#' @param n - sample size\n#' @param ybar - sample mean\n#' @param sig2 - current sigma squared\n#' @param mu_0 - mean hyper-parameter\n#' @param sig2_0 - variance  hyper-parameter\n#' \n#' @output - updated  value for mu the mean\n1update_mu = function(n, ybar, sig2, mu_0, sig2_0) {\n2  sig2_1 = 1.0 / (n / sig2 + 1.0 / sig2_0)\n3  mu_1 = sig2_1 * (n * ybar / sig2 + mu_0 / sig2_0)\n4  rnorm(n=1, mean=mu_1, sd=sqrt(sig2_1))\n}\n\n\n\n1\n\nwe don’t need the data y\n\n2\n\nwhere:  sig2_1 is \\sigma^2_1 the right term in Equation 19.3  sig2 is the current \\sigma_2 which we update in update_sigma below using Equation 19.4  sig2_0 is the hyper parameter for \\sigma^2_0\n\n3\n\nmu_1 is \\sigma^2_1 the left term in Equation 19.3 which uses sig2_1 we just computed\n\n4\n\nwe now draw from the a N(\\mu_1,\\sigma_1^2) for update_sig2 and the trace.\n\n\n\n\n\n\n19.3.2 conditionally conjugate priors for the variance\nThe full conditional for \\sigma^2 given \\mu and data is\n\n\\text{IG}\\left( \\sigma^2 \\mid \\nu_0 + \\frac{n}{2}, \\, \\beta_0 + \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2} \\right)\n\\tag{19.4}\n\n\nCode\n#' update_sig2\n#'\n#' @param n - sample size\n#' @param y - the data\n#' @param nu_0 - nu hyper-parameter\n#' @param beta_0 - beta hyper-parameter\n#' \n#' @output - updated  value for sigma2 the variance\n1update_sig2 = function(n, y, mu, nu_0, beta_0) {\n2  nu_1 = nu_0 + n / 2.0\n3  sumsq = sum( (y - mu)^2 )\n4  beta_1 = beta_0 + sumsq / 2.0\n5  out_gamma = rgamma(n=1, shape=nu_1, rate=beta_1)\n6  1.0 / out_gamma\n}\n\n\n\n1\n\nwe need the data to update beta\n\n2\n\nnu_1 the left term in Equation 19.4\n\n3\n\nvectorized\n\n4\n\nbeta_1 the right term in Equation 19.4\n\n5\n\ndraw a gamma sample with updated rate for \\text{Gamma}() is shape for \\text{IG}() inv-gamma\n\n6\n\nsince there is no rinvgamma in R we use the reciprocal of a gamma random variable which is distributed inv-gamma\n\n\n\n\nWith functions for drawing from the full conditionals, we are ready to write a function to perform Gibbs sampling.\n\n\n19.3.3 Gibbs sampler in R\n\n\nCode\ngibbs = function(y, n_iter, init, prior) {\n  ybar = mean(y)\n  n = length(y)\n  \n  ## initialize\n  mu_out = numeric(n_iter)\n  sig2_out = numeric(n_iter)\n  \n  mu_now = init$mu\n  \n  ## Gibbs sampler\n  for (i in 1:n_iter) {\n    sig2_now = update_sig2(n=n, y=y, mu=mu_now, nu_0=prior$nu_0, beta_0=prior$beta_0)\n    mu_now = update_mu(n=n, ybar=ybar, sig2=sig2_now, mu_0=prior$mu_0, sig2_0=prior$sig2_0)\n    \n    sig2_out[i] = sig2_now\n    mu_out[i] = mu_now\n  }\n  \n1  cbind(mu=mu_out, sig2=sig2_out)\n}\n\n\n\n1\n\ncbind for column bind will take a lists of list and convert them into a matrix of collumns.\n\n\n\n\nNow we are ready to set up the problem in R.\n\n\\begin{aligned}\n  y_i \\mid \\mu, \\sigma &\\stackrel {iid} \\sim \\text{N}(\\mu,\\sigma^2), \\quad i=1,\\ldots,n\n  \\\\ \\mu &\\sim \\text{N}(\\mu_0,\\sigma^2_0)\n  \\\\ \\sigma^2 & \\sim \\text{IG}(\\nu,\\beta_0)\n\\end{aligned}\n\\tag{19.5}\nWe also need to create the prior hyperparameters for \\sigma^2, \\nu_0 and \\beta_0. If we chose these hyperperameters carefully, they are interpretable as a prior guess for sigma squared, as well as a prior effective sample size to go with that guess.\nThe prior effective sample size. Which we’ll call n_0, is two times this \\nu_0 parameter. So in other words, the nu parameter will be the prior sample size Divided by 2. We’re also going to create an initial guess for sigma squared, let’s call it s^2_0. The relationship between \\beta_0 and these two numbers is the following: It is the prior sample size times the prior guess divided by 2.\nThis particular parameterization of the Inverse gamma distribution is called the Scaled Inverse Chi Square Distribution, where the two parameters are n_0 and s^2_0.\n\n\nCode\ny = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)\nybar = mean(y)\nn = length(y)\n\n## prior\nprior = list()\nprior$mu_0 = 0.0\nprior$sig2_0 = 1.0\nprior$n_0 = 2.0 # prior effective sample size for sig2\nprior$s2_0 = 1.0 # prior point estimate for sig2\nprior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma\nprior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma\n\nhist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data\ncurve(dnorm(x=x, mean = prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu\npoints(y, rep(0,n), pch=1) # individual data points\npoints(ybar, 0, pch=19) # sample mean\n\n\n\n\n\n\n\n\n\nFinally, we can initialize and run the sampler!\n\n\nCode\nset.seed(53)\n\ninit = list()\ninit$mu = 0.0\n\npost = gibbs(y=y, n_iter=1e3, init=init, prior=prior)\n\n\n\n\nCode\nhead(post)\n\n\n            mu      sig2\n[1,] 0.3746992 1.5179144\n[2,] 0.4900277 0.8532821\n[3,] 0.2536817 1.4325174\n[4,] 1.1378504 1.2337821\n[5,] 1.0016641 0.8409815\n[6,] 1.1576873 0.7926196\n\n\n\n\nCode\nlibrary(\"coda\")\nplot(as.mcmc(post))\n\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(as.mcmc(post))\n\n\n\nIterations = 1:1000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 1000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n       Mean     SD Naive SE Time-series SE\nmu   0.9051 0.2868  0.00907        0.00907\nsig2 0.9282 0.5177  0.01637        0.01810\n\n2. Quantiles for each variable:\n\n       2.5%    25%    50%   75% 97.5%\nmu   0.3024 0.7244 0.9089 1.090 1.481\nsig2 0.3577 0.6084 0.8188 1.094 2.141\n\n\nAs with the Metropolis-Hastings example, these chains appear to have converged. In the next lesson, we will discuss convergence in more detail.\n\n\n\n\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.” IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-6 (6): 721–41. https://doi.org/10.1109/tpami.1984.4767596.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Gibbs sampling</span>"
    ]
  },
  {
    "objectID": "C2-L06.html",
    "href": "C2-L06.html",
    "title": "20  Assessing Convergence",
    "section": "",
    "text": "21 Notes - Assessing Convergence",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Assessing Convergence</span>"
    ]
  },
  {
    "objectID": "C2-L06.html#convergence-diagnostics",
    "href": "C2-L06.html#convergence-diagnostics",
    "title": "20  Assessing Convergence",
    "section": "21.1 Convergence diagnostics",
    "text": "21.1 Convergence diagnostics\nIn the previous two lessons, we have demonstrated ways you can simulate a Markov chain whose stationary distribution is the target distribution (usually the posterior). Before using the simulated chain to obtain Monte Carlo estimates, we should first ask ourselves: Has our simulated Markov chain converged to its stationary distribution yet? Unfortunately, this is a difficult question to answer, but we can do several things to investigate.\n\n21.1.1 Trace plots\nOur first visual tool for assessing chains is the trace plot. A trace plot shows the history of a parameter value across iterations of the chain. It shows you precisely where the chain has been exploring.\nFirst, let’s talk about what a chain should look like. Here is an example of a chain that has most likely converged.\n\n\nCode\nsource('mh.r')\n\n\nList of 2\n $ mu   : num [1:1000] 0 0 0 0 0 ...\n $ accpt: num 0.14\n\n\nCode\nlibrary(\"coda\")\nset.seed(61)\npost0 = mh(n=n, ybar=ybar, n_iter=10e3, mu_init=0.0, cand_sd=0.9)\ncoda::traceplot(as.mcmc(post0$mu[-c(1:500)]))\n\n\n\n\n\n\n\n\n\nIf the chain is stationary, it should not be showing any long-term trends. The average value for the chain should be roughly flat. It should not be wandering as in this example:\n\n\nCode\nset.seed(61)\npost1 = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.04)\ncoda::traceplot(as.mcmc(post1$mu[-c(1:500)]))\n\n\n\n\n\n\n\n\n\nIf this is the case, you need to run the chain many more iterations, as seen here:\n\n\nCode\nset.seed(61)\npost2 = mh(n=n, ybar=ybar, n_iter=100e3, mu_init=0.0, cand_sd=0.04)\ncoda::traceplot(as.mcmc(post2$mu))\n\n\n\n\n\n\n\n\n\nThe chain appears to have converged at this much larger time scale.\n\n\n21.1.2 Monte Carlo effective sample size\nOne major difference between the two chains we’ve looked at is the level of autocorrelation in each. Autocorrelation is a number between -1 and +1 which measures how linearly dependent the current value of the chain is on past values (called lags). We can see this with an autocorrelation plot:\n\n\nCode\ncoda::autocorr.plot(as.mcmc(post0$mu))\n\n\n\n\n\n\n\n\n\n\n\nCode\ncoda::autocorr.plot(as.mcmc(post1$mu))\n\n\n\n\n\n\n\n\n\n\n\nCode\ncoda::autocorr.diag(as.mcmc(post1$mu))\n\n\n            [,1]\nLag 0  1.0000000\nLag 1  0.9850078\nLag 5  0.9213126\nLag 10 0.8387333\nLag 50 0.3834563\n\n\nAutocorrelation is important because it tells us how much information is available in our Markov chain. Sampling 1000 iterations from a highly correlated Markov chain yields less information about the stationary distribution than we would obtain from 1000 samples independently drawn from the stationary distribution.\nAutocorrelation is a major component in calculating the Monte Carlo effective sample size of your chain. The Monte Carlo effective sample size is how many independent samples from the stationary distribution you would have to draw to have equivalent information in your Markov chain. Essentially it is the m (sample size) we chose in the lesson on Monte Carlo estimation.\n\n\nCode\nstr(post2) # contains 100,000 iterations\n\n\nList of 2\n $ mu   : num [1:100000] -0.0152 -0.1007 -0.0867 -0.1092 -0.0811 ...\n $ accpt: num 0.958\n\n\n\n\nCode\ncoda::effectiveSize(as.mcmc(post2$mu)) # effective sample size of ~350\n\n\n   var1 \n373.858 \n\n\n\n\nCode\n## thin out the samples until autocorrelation is essentially 0. This will leave you with approximately independent samples. The number of samples remaining is similar to the effective sample size.\ncoda::autocorr.plot(as.mcmc(post2$mu), lag.max=500)\n\n\n\n\n\n\n\n\n\n\n\nCode\nthin_interval = 400 # how far apart the iterations are for autocorrelation to be essentially 0.\nthin_indx = seq(from=thin_interval, to=length(post2$mu), by=thin_interval)\nhead(thin_indx)\n\n\n[1]  400  800 1200 1600 2000 2400\n\n\n\n\nCode\npost2mu_thin = post2$mu[thin_indx]\ntraceplot(as.mcmc(post2$mu))\n\n\n\n\n\n\n\n\n\n\n\nCode\ntraceplot(as.mcmc(post2mu_thin))\n\n\n\n\n\n\n\n\n\n\n\nCode\ncoda::autocorr.plot(as.mcmc(post2mu_thin), lag.max=10)\n\n\n\n\n\n\n\n\n\n\n\nCode\neffectiveSize(as.mcmc(post2mu_thin))\n\n\nvar1 \n 250 \n\n\n\n\nCode\nlength(post2mu_thin)\n\n\n[1] 250\n\n\n\n\nCode\nstr(post0) # contains 10,000 iterations\n\n\nList of 2\n $ mu   : num [1:10000] 0 0 0.315 0.315 0.949 ...\n $ accpt: num 0.382\n\n\n\n\nCode\ncoda::effectiveSize(as.mcmc(post0$mu)) # effective sample size of ~2,500\n\n\n    var1 \n2537.924 \n\n\n?effectiveSize\nThe chain from post0 has 10,000 iterations, but an effective sample size of about 2,500. That is, this chain essentially provides the equivalent of 2,500 independent Monte Carlo samples.\nNotice that the chain from post0 has 10 times fewer iterations than for post2, but its Monte Carlo effective sample size is about seven times greater than the longer (more correlated) chain. We would have to run the correlated chain for 700,000+ iterations to get the same amount of information from both chains.\nIt is usually a good idea to check the Monte Carlo effective sample size of your chain. If all you seek is a posterior mean estimate, then an effective sample size of a few hundred to a few thousand should be enough. However, if you want to create something like a 95% posterior interval, you may need many thousands of effective samples to produce a reliable estimate of the outer edges of the distribution. The number you need can be quickly calculated using the Raftery and Lewis diagnostic.\nraftery.diag(as.mcmc(post0$mu))\n\n\nCode\nraftery.diag(as.mcmc(post0$mu), q=0.005, r=0.001, s=0.95)\n\n\n\nQuantile (q) = 0.005\nAccuracy (r) = +/- 0.001\nProbability (s) = 0.95 \n\nYou need a sample size of at least 19112 with these values of q, r and s\n\n\n\n\nCode\n## \n## Quantile (q) = 0.005\n## Accuracy (r) = +/- 0.001\n## Probability (s) = 0.95 \n## \n## You need a sample size of at least 19112 with these values of q, r and s\n\n\n\n\nCode\n?raftery.diag\n\n\nIn the case of the first chain from post0, it looks like we would need about 3,700 effective samples to calculate reliable 95% intervals. With the autocorrelation in the chain, that requires about 13,200 total samples. If we wanted to create reliable 99% intervals, we would need at least 19,100 total samples.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Assessing Convergence</span>"
    ]
  },
  {
    "objectID": "C2-L06.html#burn-in",
    "href": "C2-L06.html#burn-in",
    "title": "20  Assessing Convergence",
    "section": "21.2 Burn-in",
    "text": "21.2 Burn-in\nWe have also seen how the initial value of the chain can affect how quickly the chain converges. If our initial value is far from the bulk of the posterior distribution, then it may take a while for the chain to travel there. We saw this in an earlier example.\n\n\nCode\nset.seed(62)\npost3 = mh(n=n, ybar=ybar, n_iter=500, mu_init=10.0, cand_sd=0.3)\ncoda::traceplot(as.mcmc(post3$mu))\n\n\n\n\n\n\n\n\n\nClearly, the first 100 or so iterations do not reflect draws from the stationary distribution, so they should be discarded before we use this chain for Monte Carlo estimates. This is called the “burn-in” period. You should always discard early iterations that do not appear to be coming from the stationary distribution. Even if the chain appears to have converged early on, it is safer practice to discard an initial burn-in.\n\n21.2.1 Multiple chains, Gelman-Rubin\nIf we want to be more confident that we have converged to the true stationary distribution, we can simulate multiple chains, each with a different starting value.\n\n\nCode\nset.seed(61)\n\nnsim = 500\npost1 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=15.0, cand_sd=0.4)\npost1$accpt\n\n\n[1] 0.616\n\n\n\n\nCode\npost2 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=-5.0, cand_sd=0.4)\npost2$accpt\n\n\n[1] 0.612\n\n\n\n\nCode\npost3 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=7.0, cand_sd=0.1)\npost3$accpt\n\n\n[1] 0.844\n\n\n\n\nCode\npost4 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=23.0, cand_sd=0.5)\npost4$accpt\n\n\n[1] 0.53\n\n\n\n\nCode\npost5 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=-17.0, cand_sd=0.4)\npost5$accpt\n\n\n[1] 0.618\n\n\n\n\nCode\npmc = mcmc.list(as.mcmc(post1$mu), as.mcmc(post2$mu), \n                as.mcmc(post3$mu), as.mcmc(post4$mu), as.mcmc(post5$mu))\nstr(pmc)\n\n\nList of 5\n $ : 'mcmc' num [1:500] 14.8 14 14 13.8 13.8 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n $ : 'mcmc' num [1:500] -5 -5 -5 -5 -4.89 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n $ : 'mcmc' num [1:500] 7 7 7 6.94 6.94 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n $ : 'mcmc' num [1:500] 23 21.9 21.9 21.8 21.8 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n $ : 'mcmc' num [1:500] -17 -17 -16.9 -16.2 -15.7 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n - attr(*, \"class\")= chr \"mcmc.list\"\n\n\n\n\nCode\ncoda::traceplot(pmc)\n\n\n\n\n\n\n\n\n\nIt appears that after about iteration 200, all chains are exploring the stationary (posterior) distribution. We can back up our visual results with the Gelman Rubin diagnostic. This diagnostic statistic calculates the variability within chains, comparing that to the variability between chains. If all chains have converged to the stationary distribution, the variability between chains should be relatively small, and the potential scale reduction factor, reported by the the diagnostic, should be close to one. If the values are much higher than one, then we would conclude that the chains have not yet converged.\n\n\nCode\ncoda::gelman.diag(pmc)\n\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\n[1,]       1.01       1.02\n\n\n\n\nCode\ncoda::gelman.plot(pmc)\n\n\n\n\n\n\n\n\n\n\n\nCode\n?gelman.diag\n\n\n From the plot, we can see that if we only used the first 50 iterations, the potential scale reduction factor or “shrink factor” would be close to 10, indicating that the chains have not converged. But after about iteration 300, the “shrink factor” is essentially one, indicating that by then, we have probably reached convergence. Of course, we shouldn’t stop sampling as soon as we reach convergence. Instead, this is where we should begin saving our samples for Monte Carlo estimation.\n\n\n21.2.2 Monte Carlo estimation\nIf we are reasonably confident that our Markov chain has converged, then we can go ahead and treat it as a Monte Carlo sample from the posterior distribution. Thus, we can use the techniques from Lesson 3 to calculate posterior quantities like the posterior mean and posterior intervals from the samples directly.\n\n\nCode\nnburn = 1000 # remember to discard early iterations\npost0$mu_keep = post0$mu[-c(1:1000)]\nsummary(as.mcmc(post0$mu_keep))\n\n\n\nIterations = 1:9000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 9000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n      0.889449       0.304514       0.003210       0.006295 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n0.2915 0.6825 0.8924 1.0868 1.4890 \n\n\n\n\nCode\nmean(post0$mu_keep &gt; 1.0) # posterior probability that mu  &gt; 1.0\n\n\n[1] 0.3554444",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Assessing Convergence</span>"
    ]
  },
  {
    "objectID": "C2-L07.html",
    "href": "C2-L07.html",
    "title": "21  Notes - Linear regression",
    "section": "",
    "text": "21.1 Introduction to linear regression\nWe discussed linear regression briefly in the previous course. And we fit a few models with non-informative priors. Here, we’ll provide a brief review, demonstrate fitting linear regression models in JAGS And discuss a few practical skills that are helpful when fitting linear models in general.\nThis is not meant to be a comprehensive treatment of linear models, which you can find in numerous courses and textbooks.\nLinear regression is perhaps the simplest way to relate a continuous response variable to multiple explanatory variables.\nThis may arise from observing several variables together and investigating which variables correlate with the response variable. Or it could arise from conducting an experiment, where we carefully assign values of explanatory variables to randomly selected subjects. And try to establish a cause-and-effect relationship.\nA linear regression model has the following form:\ny_i=\\beta_0+\\beta_1 x_i +\\ldots+ \\beta_k x_k + \\epsilon_i\n\\\\ \\epsilon_i \\stackrel {iid} \\sim N(0,\\sigma^2)\n\\tag{21.1}\nThis describes the mean, and then we would also add an error, individual term for each observation. We would assume that the errors are IID from a normal distribution means 0 variance \\sigma^2 for observations 1 \\ldots k.\nEquivalently we can write this model for y_i directly as y_i given all of the x_i values, betas and a constant variance \\sigma^2. Again, k is the number of predictor variables.\ny_i\\mid x_i,\\beta_i,\\sigma^2 \\sim N(\\beta_0+\\beta_1 x_i +\\ldots+ \\beta_k x_k, \\sigma^2)\n\\\\ \\beta_i \\sim \\mathbb{P}r(\\beta_i)\n\\\\ \\sigma^2 \\sim \\mathbb{P}r(\\sigma^2)\n\\tag{21.2}\nThis yields the following graphical model structure.\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeOneSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeTwoSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeThreeSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeFourSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeFiveSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['cmtt10'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['cmb10'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['cmss10'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['DejaVu Sans Display'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\n\nFigure 21.1: The graphical model for linear regression\nThe terms of a linear model are always linearly related because of the structure of the model.\nBut the model does not have to be linear necessarily in the xy relationship. For example, it may be that y is related linearly to x^2. Hence we could transform the x and y variables to get new x’s and new y’s but we would still have a linear model. However, in that case, if we transform the variables, we must be careful about how this changes the final interpretation of the results.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes - Linear regression</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-introduction-to-linear-regression",
    "href": "C2-L07.html#sec-introduction-to-linear-regression",
    "title": "21  Notes - Linear regression",
    "section": "",
    "text": "Introduction to linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantUnderstanding the Graphical Models\n\n\n\n\nThis graphical model uses plate notation\nWe’ll start with a plate for all of our different y variables,\n\nIt is repeated i = 1 \\ldots N times\n\ny_i, are random variable - (indicated by a circle)\n\nthey are observed - indicated by a filled shape.\n\nX_i variables.\n\nthey are drawn as squares around to indicate that they are constants and not random variables.\nWe’re always conditioning on the Xs. So they’ll just be constants.\nthey are observed, so they are filled in.\n\nThe y_i depend on the values of the x and the values of these parameters. So, we have \\beta_0, \\ldots, \\beta_k.\nSigma squared.\nSince the y_i depend on all of these, so this would be the graphical model representation.\n\n\n\n\n\n\n\n\n\n\n\nImportantInterpreting Coefficients\n\n\n\nThe basic interpretation of the \\beta_i coefficients is:\nWhile holding all other X variables constant, if we increases X_i by one then the mean of \\bar{y} is expected to increase by \\beta_i .\nThat is \\beta_i describes how the \\bar{y} changes with changes in X_i, while accounting for all the other X variables.\n\n\\beta \\approx  \\frac{\\partial \\bar{y} }{\\partial x_i}\n\\tag{21.3}\nThat’s true for all of the x variables.\n\n\n\n\n\n\n\n\nWarningRegression assumptions\n\n\n\nWe’re going to assume that\n\nThe ys are independent of each other, given the xs.\nThe y_is have the same variance.\nThe residuals are normally distributed with mean 0 and variance \\sigma^2.\n\nThese are actually strong assumptions that are not often not realistic in many situations.\nThere are many statistical models to address that.\nWe’ll look at some hierarchical methods in the coming lessons.\n\n\n\n21.1.1 Priors\nThe model is not complete until we add the prior distributions.\nSo we might say \\beta_0 comes from its prior.\n\\beta_1 would come from its prior, and so forth for all the \\betas. And sigma squared would come from its prior.\nThe most common choice for prior on the \\betas, is a Normal distribution. Or we can do a Multivariate normal for all of the betas at once.\nThis is conditionally conjugate and allows us to do Gibbs sampling.\nIf we want to be non-informative, we can choose Normal(0,\\sigma^2=1e6) priors with very large variance. Which are practically flat for realistic values of beta. The non-informative priors used in the last class are equivalent to using normal priors with infinite variance.\nWe can also use the conditionally conjugate InverseGamma() prior for \\sigma^2 that we’re familiar with.\nAnother common prior for the betas is Double exponential, or the Laplace prior, or Laplace distribution. \nThe Laplace prior has this density:\n\nf(x\\mid \\mu,\\beta)=\\frac{1}{2\\beta} e^{|\\frac{x-\\mu}{\\beta}|}\n\\tag{21.4}\nwhere:\n\n\\mu is the location parameter and\n\\beta is the scale parameter.\n\nThe case where \\mu = 0 and \\beta = 1 is called the standard double exponential distribution\n\nf(x)=\\frac{1}{2} e^{|x|}\n\\tag{21.5}\nAnd the density looks like this.\n\n\n\n\n\n\n\nFigure 21.2: The Double Exponential Distribution\n\n\n\n\n\n\n\n\n\n\nFigure 21.3: The Double Exponential Distribution\n\n\n\n\nRPython\n\n\n\n\nCode\n# Grid of X-axis values\nx &lt;- seq(-10, 10, 0.1)\nplot(x,  ddexp(x, 0, 2), type = \"l\", ylab = \"\", lwd = 2, col = \"red\")\nlines(x, ddexp(x, 0, 1.5), type = \"l\", ylab = \"\", lwd = 2, col = \"green\")\nlines(x, ddexp(x, 0, 1), type = \"l\", ylab = \"\", lwd = 2, col = \"blue\")\nlegend(\"topright\",\n       c(expression(paste(, beta)), \"1.5\",\"1\", \"2\"),\n       lty = c(0, 1, 1, 1),\n       col = c(\"red\",\"green\", \"blue\"), box.lty = 0, lwd = 2\n      )\n\n#x &lt;- rdexp(500, location = 2, scale = 1)\n#de_sample=ddexp(x, 2, 1)\n#CDF &lt;- ecdf(de_sample )\n#plot(CDF)\n\n\n\n\n\n\n\nCode\nloc, scale = 0., 1.\ns = np.random.laplace(loc, scale, 1000)\n\ncount, bins, ignored = plt.hist(s, 30, density=True)\nx = np.arange(-8., 8., .01)\npdf = np.exp(-abs(x-loc)/scale)/(2.*scale)\nplt.plot(x, pdf);\n\ng = (1/(scale * np.sqrt(2 * np.pi)) *\n     np.exp(-(x - loc)**2 / (2 * scale**2)))\nplt.plot(x,g);\n\n\n\n\n\n\nIt’s called double exponential because it looks like the exponential distribution except it’s been reflected over the y axis. It has a sharp peak at x equals 0, or beta equals 0 in this case, which can be useful if we want to do variable selection among our x’s. Because it’ll favor values in your 0 for these betas.\nThis is related to the popular regression technique known as the LASSO. \nMore information is available from:\n\nNIST\nWikipedia",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes - Linear regression</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#eda",
    "href": "C2-L07.html#eda",
    "title": "21  Notes - Linear regression",
    "section": "22.1 EDA",
    "text": "22.1 EDA\n\n\nCode\n1pairs(Leinhardt)\n\n\n\n1\n\nUsing pairs to investigate the marginal relationships between each of the four variables.\n\n\n\n\n\n\n\n\n\n\n\n\n22.1.0.1 Simple linear Model\nWe’ll start with a simple linear regression model that relates infant mortality to per capita income.\n\n\nCode\nplot(infant ~ income, data=Leinhardt)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhist(Leinhardt$infant)\n\n\n\n\n\n\n\n\n\nthis is right-skewed (many small values and a number of much larger one)\n\n\nCode\nhist(Leinhardt$income)\n\n\n\n\n\n\n\n\n\nalso right-skewed.\nthis indicates that we may do better if we do a log transform on these two variables.\n\n\n22.1.0.2 Log-Log Linear Model\n\n\nCode\n1Leinhardt$loginfant = log(Leinhardt$infant)\n2Leinhardt$logincome = log(Leinhardt$income)\n\n\n3plot(loginfant ~ logincome,data=Leinhardt)\n\n\n\n1\n\nlog transform infant column.\n\n2\n\nlog transform income column.\n\n3\n\nscatter plot of the log log transformed data.\n\n\n\n\n\n\n\n\n\n\nFigure 22.1: log log transformed infant mortality vs income\n\n\n\n\n\nSince infant mortality and per capita income are positive and right-skewed quantities, we consider modeling them on the logarithmic scale. A linear model appears much more appropriate on this scale.\n\n\nCode\n1scatterplot(loginfant ~ logincome,data=Leinhardt)\n\n\n\n1\n\nscatterplot with a regression fit and uncertainty for the data\n\n\n\n\n\n\n\n\n\n\nFigure 22.2: log log transformed infant mortality vs income scatterplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n22.1.1 Modeling\nThe reference Bayesian analysis (with a non-informative prior) is available directly in R.\n\n\nCode\nlmod0 = lm(loginfant ~ logincome, data=Leinhardt)\nsummary(lmod0)\n\n\n\n\nregression output\nlm(formula = loginfant ~ logincome, data = Leinhardt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.66694 -0.42779 -0.02649  0.30441  3.08415 \n\nCoefficients: \n            Estimate Std. Error t value Pr(&gt;|t|)    \n1(Intercept)  7.14582    0.31654  22.575   &lt;2e-16 ***\n2logincome   -0.51179    0.05122  -9.992   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n3Residual standard error: 0.6867 on 99 degrees of freedom\n4  (4 observations deleted due to missingness)\n5Multiple R-squared:  0.5021,    Adjusted R-squared:  0.4971\nF-statistic: 99.84 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\n1\n\nintercept is \\gg its error so it appears statistically significant (***)\n\n2\n\nposterior mean logincome too\n\n3\n\nResidual standard error gives us an estimate of the left over variance after fitting the model.\n\n4\n\n4 rows were dropped due to missing values\n\n5\n\nAdjusted R-squared is the explained variance adjusted for degrees of freedom",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes - Linear regression</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-residual-checks",
    "href": "C2-L07.html#sec-residual-checks",
    "title": "21  Notes - Linear regression",
    "section": "24.1 Residual checks",
    "text": "24.1 Residual checks\n\n\n\n\n\n\nImportant\n\n\n\nAnalysis gets complicated quickly when we have multiple models. What we shall soon see is how to get residuals from the Bayesian model in Stan so we can compare it visually with the reference model we got using LM.\n\n\nChecking residuals (the difference between the response and the model’s prediction for that value) is important with linear models since residuals can reveal violations of the assumptions we made to specify the model. In particular, we are looking for any sign that the model is not linear, normally distributed, or that the observations are not independent (conditional on covariates).\nFirst, let’s look at what would have happened if we fit the reference linear model to the un-transformed variables.\n\n\nCode\nlmod0 = lm(infant ~ income, data=Leinhardt)\nplot(resid(lmod0)) # to check independence (looks okay)\n\n\n\n\n\n\n\n\n\nthere should not be a pattern - but we can see an increase. This is not an issue and due to the data being presorted.\n\n\nCode\nplot(predict(lmod0), resid(lmod0)) # to check for linearity, constant variance (looks bad)\n\n\n\n\n\n\n\n\n\nafter 80 the variance starts increasing\n\n\nCode\nqqnorm(resid(lmod0)) # to check Normality assumption (we want this to be a straight line)\n\n\n\n\n\n\n\n\n\nCode\n#?qqnorm\n\n\nThis looks good except for the last few points.\nNow let’s return to our model fit to the log-transformed variables. In a Bayesian model, we have distributions for residuals, but we’ll simplify and look only at the residuals evaluated at the posterior mean of the parameters.\n\n\nCode\nX = cbind(rep(1.0, data1_jags$n), data1_jags$log_income)\nhead(X)\n\n\n     [,1]     [,2]\n[1,]    1 8.139149\n[2,]    1 8.116716\n[3,]    1 8.115521\n[4,]    1 8.466110\n[5,]    1 8.522976\n[6,]    1 8.105308\n\n\n\n\nCode\n1(pm_params1 = colMeans(mod1_csim))\n\n\n\n1\n\nposterior mean - using (var = expr) forces R to return the value of var\n\n\n\n\n      b[1]       b[2]        sig \n 7.1527311 -0.5128688  0.9711458 \n\n\n\n\nCode\n1yhat1 = drop(X %*% pm_params1[1:2])\n2resid1 = data1_jags$y - yhat1\n3plot(resid1)\n\n\n\n1\n\nwe are evaluating \\\\hat{y} = b_0 \\times 1 + b_1 \\times x_1 via matrix multiplication of [1, data1_jags$log_income] *[b_0,b_1]\n\n2\n\nres_i = y_i- \\hat y = y_i - (b_0 \\times 1 + b_1 \\times x_{1,i})\n\n3\n\nplots the residual against the data index\n\n\n\n\n\n\n\n\n\n\n\nSo to get the residuals from Stan we extract the b parameter.\nAlthough we did not discuss it we could estimate \\hat y by drawing K predictions for each x_i and look at res_i=\\frac{1}{K}\\sum_k|y_i -\\hat y_{i,k}| and plot upper and fit a line as well as lower and upper bounds as well. Also I’m not sure but I guess we can also do with using the predictive posterior dist. Anyhow here is a link to something like this: Extracting and visualizing tidy residuals from Bayesian models -jk\n\n\nCode\nplot(yhat1, resid1) # against predicted values\n\n\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(resid1) # checking normality of residuals\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(predict(lmod0), resid(mod1)) # to compare with reference linear model\n\n\n\n\n\n\n\n\n\n\n\nCode\nrownames(dat)[order(resid1, decreasing=TRUE)[1:5]] # which countries have the largest positive residuals?\n\n\n[1] \"Saudi.Arabia\" \"Libya\"        \"Zambia\"       \"Brazil\"       \"Afganistan\"  \n\n\nThe residuals look pretty good here (no patterns, shapes) except for two strong outliers, Saudi Arabia and Libya. When outliers appear, it is a good idea to double check that they are not just errors in data entry. If the values are correct, you may reconsider whether these data points really are representative of the data you are trying to model. If you conclude that they are not (for example, they were recorded on different years), you may be able to justify dropping these data points from the data set.\nIf you conclude that the outliers are part of data and should not be removed, we have several modeling options to accommodate them. We will address these in the next segment.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes - Linear regression</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-additional-covariates",
    "href": "C2-L07.html#sec-additional-covariates",
    "title": "21  Notes - Linear regression",
    "section": "25.1 Additional covariates",
    "text": "25.1 Additional covariates\nThe first approach is to look for additional covariates that may be able to explain the outliers. For example, there could be a number of variables that provide information about infant mortality above and beyond what income provides.\nLooking back at our data, there are two variables we haven’t used yet: region and oil. The oil variable indicates oil-exporting countries. Both Saudi Arabia and Libya are oil-exporting countries, so perhaps this might explain part of the anomaly.\n\n\nCode\nlibrary(\"rjags\")\n\nmod2_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dnorm(mu[i], prec)\n1        mu[i] = b[1] + b[2]*log_income[i] + b[3]*is_oil[i]\n    }\n    \n2    for (i in 1:3) {\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n    prec ~ dgamma(5/2.0, 5*10.0/2.0)\n    sig = sqrt( 1.0 / prec )\n} \"\n\n\nset.seed(73)\ndata2_jags = list(y=dat$loginfant, log_income=dat$logincome,\n3                  is_oil=as.numeric(dat$oil==\"yes\"))\ndata2_jags$is_oil\n\nparams2 = c(\"b\", \"sig\")\n\ninits2 = function() {\n4    inits = list(\"b\"=rnorm(3,0.0,100.0), \"prec\"=rgamma(1,1.0,1.0))\n}\n\nmod2 = jags.model(textConnection(mod2_string), data=data2_jags, inits=inits2, n.chains=3)\nupdate(mod2, 1e3) # burn-in\n\nmod2_sim = coda.samples(model=mod2,\n                        variable.names=params2,\n                        n.iter=5e3)\n\nmod2_csim = as.mcmc(do.call(rbind, mod2_sim)) # combine multiple chains\n\n\n\n1\n\nwe add the is_oil indicator parameter\n\n2\n\nwe increment the number of parameters\n\n3\n\nencode the is_oil from text to be binary\n\n4\n\ndraw another var for b.\n\n\n\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 101\n   Unobserved stochastic nodes: 4\n   Total graph size: 507\n\nInitializing model\n\n\nAs usual, check the convergence diagnostics.\n\n\nCode\npar(mar = c(2., 1, 2., 1))\nplot(mod2_sim)\n\n\n\n\n\n\n\n\n\n\n\nCode\ngelman.diag(mod2_sim)\n\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\nb[1]       1.01       1.04\nb[2]       1.01       1.04\nb[3]       1.00       1.00\nsig        1.00       1.00\n\nMultivariate psrf\n\n1.01\n\n\nCode\nautocorr.diag(mod2_sim)\n\n\n             b[1]       b[2]          b[3]         sig\nLag 0  1.00000000 1.00000000  1.0000000000 1.000000000\nLag 1  0.94844101 0.94931156  0.0784208911 0.029391843\nLag 5  0.77226611 0.77479049  0.0001413888 0.002160251\nLag 10 0.60494733 0.60618211 -0.0044699615 0.017232919\nLag 50 0.03579873 0.03801448 -0.0083875041 0.002220839\n\n\n\n\nCode\n#autocorr.plot(mod2_sim,auto.layout=FALSE )\nautocorr.plot(mod2_csim,auto.layout=FALSE )\n\n\n\n\n\n\n\n\nFigure 25.1: auto-correlation plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.2: auto-correlation plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.3: auto-correlation plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 25.4: auto-correlation plot\n\n\n\n\n\n\n\nCode\neffectiveSize(mod2_sim)\n\n\n      b[1]       b[2]       b[3]        sig \n  390.7554   377.4225 13115.1456 14394.8304 \n\n\nWe can get a posterior summary of the parameters in our model.\n\n\nCode\nsummary(mod2_sim)\n\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean      SD  Naive SE Time-series SE\nb[1]  7.1358 0.42232 0.0034482      0.0213351\nb[2] -0.5204 0.06840 0.0005585      0.0035159\nb[3]  0.7900 0.35786 0.0029219      0.0031256\nsig   0.9529 0.06716 0.0005484      0.0005743\n\n2. Quantiles for each variable:\n\n         2.5%     25%     50%     75%   97.5%\nb[1]  6.30789  6.8560  7.1291  7.4148  7.9725\nb[2] -0.65672 -0.5657 -0.5195 -0.4754 -0.3866\nb[3]  0.08295  0.5548  0.7889  1.0278  1.4923\nsig   0.83110  0.9060  0.9500  0.9955  1.0950\n\n\nIt looks like there is a positive relationship between oil-production and log-infant mortality. Because these data are merely observational, we cannot say that oil-production causes an increase in infant mortality (indeed that most certainly isn’t the case), but we can say that they are positively correlated.\nNow let’s check the residuals.\n\n\nCode\nX2 = cbind(rep(1.0, data1_jags$n), data2_jags$log_income, data2_jags$is_oil)\nhead(X2)\n\n\n     [,1]     [,2] [,3]\n[1,]    1 8.139149    0\n[2,]    1 8.116716    0\n[3,]    1 8.115521    0\n[4,]    1 8.466110    0\n[5,]    1 8.522976    0\n[6,]    1 8.105308    0\n\n\n\n\nCode\n(pm_params2 = colMeans(mod2_csim)) # posterior mean\n\n\n      b[1]       b[2]       b[3]        sig \n 7.1357634 -0.5204325  0.7900293  0.9529081 \n\n\n\n\nCode\nyhat2 = drop(X2 %*% pm_params2[1:3])\nresid2 = data2_jags$y - yhat2\nplot(resid2) # against data index\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(yhat2, resid2) # against predicted values\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(yhat1, resid1) # residuals from the first model\n\n\n\n\n\n\n\n\n\n\n\nCode\nsd(resid2) # standard deviation of residuals\n\n\n[1] 0.6488702\n\n\nThese look much better, although the residuals for Saudi Arabia and Libya are still more than three standard deviations away from the mean of the residuals. We might consider adding the other covariate region, but instead let’s look at another option when we are faced with strong outliers.\n\n25.1.1 Student-t likelihood\n\nLet’s consider changing the likelihood.\nThe normal likelihood has thin tails (almost all of the probability is concentrated within the first few standard deviations from the mean).\nThis does not accommodate outliers well.\nConsequently, models with the normal likelihood might be overly-influenced by outliers.\nRecall that the t distribution is similar to the normal distribution, but it has thicker tails which can accommodate outliers.\n\nThe t linear model might look something like this. Notice that the t distribution has three parameters, including a positive “degrees of freedom” parameter. The smaller the degrees of freedom, the heavier the tails of the distribution. We might fix the degrees of freedom to some number, or we can assign it a prior distribution.\n\n\nCode\ncurve(dnorm(x), from = -5, to = 5)\ncurve(dt(x,1), from = -5, to = 5,col=\"red\", add = TRUE)\n\n\n\n\n\n\nnormal and t distributions\n\n\n\n\n\nCode\nmod3_string = \" model {\n1    for (i in 1:length(y)) {\n        y[i] ~ dt( mu[i], tau, df )\n        mu[i] = b[1] + b[2]*log_income[i] + b[3]*is_oil[i]\n    }\n    \n    for (i in 1:3) {\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n2    nu ~ dexp(1.0)\n3    df = nu + 2.0\n    \n4    tau ~ dgamma(5/2.0, 5*10.0/2.0)\n5    sig = sqrt( 1.0 / tau * df / (df - 2.0) )\n}\"\n\n\n\n1\n\nwe replaced normal likelihood with a student t likelihood which has thicker tails\n\n2\n\n\\nu nu is the degrees of freedom (dof) but the outcome can be 0 or 1\n\n3\n\nwe force the degrees of freedom dof&gt;2 to guarantee the existence of mean and variance in the t dist.\n\n4\n\n\\tau tau is the inverse scale is close to, but not equal to the precision from above so we use the same prior as we used for precision.\n\n5\n\n\\sigma sig standard deviation of errors is a deterministic function of tau, and df\n\n\n\n\nWe fit this model.\n\n\nCode\nset.seed(73)\ndata3_jags = list(y=dat$loginfant, log_income=dat$logincome,\n                  is_oil=as.numeric(dat$oil==\"yes\"))\n\nparams3 = c(\"b\", \"sig\")\n\ninits3 = function() {\n    inits = list(\"b\"=rnorm(3,0.0,100.0), \"prec\"=rgamma(1,1.0,1.0))\n}\n\nmod3 = jags.model(textConnection(mod3_string), data=data3_jags, inits=inits3, n.chains=3)\nupdate(mod3, 1e3) # burn-in\n\nmod3_sim = coda.samples(model=mod3,\n                        variable.names=params3,\n                        n.iter=5e3)\n\nmod3_csim = as.mcmc(do.call(rbind, mod3_sim)) # combine multiple chains\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 101\n   Unobserved stochastic nodes: 5\n   Total graph size: 512\n\nInitializing model\n\n\ncheck MCMC convergence visually\n\n\nCode\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod3_sim)\n\n\n\n\n\n\n\n\n\ncheck MCMC convergence quantitatively using Rubin Gelman\n\n\nCode\ngelman.diag(mod3_sim)\n\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\nb[1]       1.01       1.03\nb[2]       1.01       1.03\nb[3]       1.00       1.00\nsig        1.02       1.02\n\nMultivariate psrf\n\n1.01\n\n\n\n\nCode\neffectiveSize(mod3_sim)\n\n\n      b[1]       b[2]       b[3]        sig \n  298.2639   302.3867  8186.2690 10071.0089",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes - Linear regression</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-compare-models-using-deviance-information-criterion-dic",
    "href": "C2-L07.html#sec-compare-models-using-deviance-information-criterion-dic",
    "title": "21  Notes - Linear regression",
    "section": "25.2 Compare models using Deviance information criterion (DIC)",
    "text": "25.2 Compare models using Deviance information criterion (DIC)\n We have now proposed three different models. How do we compare their performance on our data? In the previous course, we discussed estimating parameters in models using the maximum likelihood method. Similarly, we can choose between competing models using the same idea.\nWe will use a quantity known as the deviance information criterion (DIC). It essentially calculates the posterior mean of the log-likelihood and adds a penalty for model complexity.\nLet’s calculate the DIC for our first two models:\nthe simple linear regression on log-income,\n\n\nCode\ndic.samples(mod1, n.iter=1e3)\n\n\nMean deviance:  231.3 \npenalty 2.997 \nPenalized deviance: 234.3 \n\n\nand the second model where we add oil production.\n\n\nCode\ndic.samples(mod2, n.iter=1e3)\n\n\nMean deviance:  225.1 \npenalty 4.013 \nPenalized deviance: 229.1 \n\n\nand the second model where we introduce the Student t likelihood.\n\n\nCode\ndic.samples(mod3, n.iter=1e3)\n\n\nMean deviance:  230.5 \npenalty 3.929 \nPenalized deviance: 234.4 \n\n\nThe first number is the Monte Carlo estimated posterior mean deviance, which equals -2 times the log-likelihood (plus a constant that will be irrelevant for comparing models). Because of that -2 factor, a smaller deviance means a higher likelihood.\nNext, we are given a penalty for the complexity of our model. This penalty is necessary because we can always increase the likelihood of the model by making it more complex to fit the data exactly. We don’t want to do this because over-fit models generalize poorly. This penalty is roughly equal to the effective number of parameters in your model. You can see this here. With the first model, we had a variance parameter and two betas, for a total of three parameters. In the second model, we added one more beta for the oil effect.\nWe add these two quantities to get the DIC (the last number). The better-fitting model has a lower DIC value. In this case, the gains we receive in deviance by adding the is_oil covariate outweigh the penalty for adding an extra parameter. The final DIC for the second model is lower than for the first, so we would prefer using the second model.\nWe encourage you to explore different model specifications and compare their fit to the data using DIC. Wikipedia provides a good introduction to DIC and we can find more details about the JAGS implementation through the rjags package documentation by entering ?dic.samples in the R console.\n\n\nCode\n#?dic.samples",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes - Linear regression</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-regression-diagnostics",
    "href": "C2-L07.html#sec-regression-diagnostics",
    "title": "21  Notes - Linear regression",
    "section": "25.3 Regression Diagnostics",
    "text": "25.3 Regression Diagnostics\nIn production we want to flag regression issues in an automated fashion. However while we develop models we should try to examine these issues visually.\nRegression diagnostics help identify:\n\nshortcoming of our model and the preferred ways to improve them\n\ntransforms of variables\ndifferent likelihood\nadding missing covariate relations to remove patterns in the residuals\nincreasing interpretability by removing covariates that do not contribute to the fit.\n\nissues in the data\n\ntransformation\n\n\nwe should consider the following issues: 1. testing heteroscedasticity with the Breusch-Pagan test\nLet’s try to cover the diagnostic plots which help us validate a regression model.\n\n25.3.1 Residuals vs Fitted\n\nThe “residuals versus fits plot” is the most first diagnostic tool we\nshould look at to determine if the regression is valid. If the regression assumptions are violated we should be able to identify the issues and possibly correct them.\nIt is a scatter plot of residuals on the y axis and fitted values (estimated responses) on the x axis.\nThe plot can be used to detect:\n\nnon-linearity,\nunequal error variances, and\noutliers.\n\n\n\n\nCode\nplot(lmod0, 1)\n\n\n\n\n\nResiduals vs Fitted plot\n\n\n\n\nResiduals will enable us to assess visually whether an appropriate model has been fit to the data no matter how many predictor variables are used. We can checking the validity of a linear regression model by plotting residuals versus x and look for patterns. - Lack of a discernible pattern is indicative of a valid model. - A pattern is is indicative that a function or transformation of X is missing from the model.\n\n\n\n\n\n\nImportantWhat to look for\n\n\n\nLook for patterns that can indicate non-linearity,\n\nthat the residuals all are high in some areas and low in others. Change in variability as X changes - U shape missing quadratic term · we can get this plot as follows.\n\nThe blue line is there to aid the eye - it should ideally be relatively close to a straight line (in this case, it isn’t perfectly straight, which could indicate a mild non-linearity).\n\n\n\n\n25.3.2 QQ plot of the residuals\nThis plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely?\nThe regression is valid if the residuals are lined well on the straight dashed line.\nwe can get this plot as follows\n\n\nCode\nplot(lmod0, 2)\n\n\n\n\n\n\n\n\n\nnotice that the two outliers are labeled and should be reviewed for - removal - more robust likelihood\nfor more info see understanding QQ plots\n\n\n25.3.3 Scale Location plot\nThis plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.\n\n\nCode\nplot(lmod0, 3)\n\n\n\n\n\n\n\n\n\nin this case: - most of the points are to the right - the red line is almost flat which is good - there is increasing variance after 80\n\n\n25.3.4 Cook’s Distance\n\nOriginally introduced in (Cook 1977a) Cook’s Distance is an estimate of the influence of a data point.\nIt takes into account both the leverage and residual of each observation.\nCook’s Distance is a summary of how much a regression model changes when the ith observation is removed.\nWhen it comes to outliers we care about outliers that have a high Cook’s distance as they can have a large impact on the regression model. by shifting the sample fit from the population fit.\nAnother aspect of Cook’s distance is it can be used to identify regions of the design space where the model is poorly supported by the data - i.e. where the model is extrapolating and if we can get more data in that region we can improve the model.\n\n\n\nCode\nplot(lmod0, 4)\n\n\n\n\n\n\n\n\n\nUsed to detect highly influential outliers, i.e. points that can shift the sample fit from the population fit. For large sample sizes, a rough guideline is to consider values above 4/(n-p), where n is the sample size and p is the number of predictors including the intercept, to indicate highly influential points.\nsee Williams (1987)\n\n\n25.3.5 Residuals vs Leverage\n\n\nCode\nplot(lmod0, 5)\n\n\n\n\n\n\n\n\n\nCode\n#plot(mod3, 5)\n\n\nThis plot helps us to sort through the outliers, if there are any. Not all outliers are influential in linear regression analysis. Even though data have extreme values, they might not be influential to determine a regression line. i.e. the fit wouldn’t be much different if we choose to omit them from the analysis. If a point is able to exert a influence on the regression line we call it a high leverage point. Even in this case it might not alter the trend. So we want to identify high leverage points that are at a large distance from their predictor’s mean.\nUnlike the other plots, this time patterns are not relevant. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of the dashed lines. When cases are outside of the dashed lines (meaning they have high “Cook’s distance” scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases.\nIn this case we see that the pints are within the cook’s distance contours so our outliers are not high leverage points.\n\n\n25.3.6 Cook’s Distance vs Leverage\n\n\nCode\nplot(lmod0, 6)\n\n\n\n\n\n\n\n\nFigure 25.5: Cooks distance v.s. Leverage\n\n\n\n\n\nCook’s distance and leverage are used to detect highly leverage points, i.e. data points that can shift the sample fit from the population fit.\nFor large sample sizes, a rough guideline is to consider Cook’s distance values above 1 to indicate highly influential points and leverage values greater than 2 times the number of predictors divided by the sample size to indicate high leverage observations. High leverage observations are ones which have predictor values very far from their averages, which can greatly influence the fitted model.\nThe contours in the scatterplot are standardized residuals labelled with their magnitudes\nsee Williams (1987)\n\n\n25.3.7 Python\n\nhttps://emredjan.xyz/blog/2017/07/11/emulating-r-plots-in-python/\nhttps://towardsdatascience.com/going-from-r-to-python-linear-regression-diagnostic-plots-144d1c4aa5a\nhttps://modernstatisticswithr.com/regression.html\n\n\n\n\n\n\n\nCook, R. Dennis. 1977b. “Detection of Influential Observation in Linear Regression.” Technometrics 19 (1): 15. https://doi.org/10.2307/1268249.\n\n\n———. 1977a. “Detection of Influential Observation in Linear Regression.” Technometrics 19 (1): 15. https://doi.org/10.2307/1268249.\n\n\nWilliams, D. A. 1987. “Generalized Linear Model Diagnostics Using the Deviance and Single Case Deletions.” Applied Statistics 36 (2): 181. https://doi.org/10.2307/2347550.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Notes - Linear regression</span>"
    ]
  },
  {
    "objectID": "C2-L08.html",
    "href": "C2-L08.html",
    "title": "22  ANOVA",
    "section": "",
    "text": "22.1 Introduction to ANOVA",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "C2-L08.html#introduction-to-anova",
    "href": "C2-L08.html#introduction-to-anova",
    "title": "22  ANOVA",
    "section": "",
    "text": "Introduction to ANOVA",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "C2-L08.html#one-way-anova-model-using-jags",
    "href": "C2-L08.html#one-way-anova-model-using-jags",
    "title": "22  ANOVA",
    "section": "22.2 One way ANOVA model using JAGS",
    "text": "22.2 One way ANOVA model using JAGS\n\n22.2.1 Data & EDA\nAs an example of a one-way ANOVA, we’ll look at the Plant Growth data in R.\n\n\n\n\nListing 22.1: Plant Growth Query\n\n\n\nCode\ndata(\"PlantGrowth\")\n#?PlantGrowth\nhead(PlantGrowth)\n\n\n\n\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n\nWe first load the dataset (Listing 22.1)\nBecause the explanatory variable group is a factor and not continuous, we choose to visualize the data with box plots rather than scatter plots.\n\n\nCode\nboxplot(weight ~ group, data=PlantGrowth)\n\n\n\n\n\n\n\n\nFigure 22.1: PlantGrowth boxplot\n\n\n\n\n\nThe box plots summarize the distribution of the data for each of the three groups. It appears that treatment 2 has the highest mean yield. It might be questionable whether each group has the same variance, but we’ll assume that is the case.\n\n\n22.2.2 Modeling\nAgain, we can start with the reference analysis (with a noninformative prior) with a linear model in R.\n\n\nCode\nlmod = lm(weight ~ group, data=PlantGrowth)\nsummary(lmod)\n\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\n\n\nCode\nanova(lmod)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\nplot(lmod) # for graphical residual analysis\n\n\n\n\n\n\n\n\nFigure 22.2: Graphical residual analysis\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.3: Graphical residual analysis\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.4: Graphical residual analysis\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.5: Graphical residual analysis\n\n\n\n\n\nThe default model structure in R is the linear model with dummy indicator variables. Hence, the “intercept” in this model is the mean yield for the control group. The two other parameters are the estimated effects of treatments 1 and 2. To recover the mean yield in treatment group 1, you would add the intercept term and the treatment 1 effect. To see how R sets the model up, use the model.matrix(lmod) function to extract the X matrix.\nThe anova() function in R compares variability of observations between the treatment groups to variability within the treatment groups to test whether all means are equal or whether at least one is different. The small p-value here suggests that the means are not all equal.\nLet’s fit the cell means model in JAGS.\n\n\nCode\nlibrary(\"rjags\")\n\n\n\n\nCode\nmod_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dnorm(mu[grp[i]], prec)\n    }\n    \n    for (j in 1:3) {\n        mu[j] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n    prec ~ dgamma(5/2.0, 5*1.0/2.0)\n    sig = sqrt( 1.0 / prec )\n} \"\n\nset.seed(82)\nstr(PlantGrowth)\n\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\ndata_jags = list(y=PlantGrowth$weight, \n              grp=as.numeric(PlantGrowth$group))\n\nparams = c(\"mu\", \"sig\")\n\ninits = function() {\n    inits = list(\"mu\"=rnorm(3,0.0,100.0), \"prec\"=rgamma(1,1.0,1.0))\n}\n\nmod = jags.model(textConnection(mod_string), data=data_jags, inits=inits, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 30\n   Unobserved stochastic nodes: 4\n   Total graph size: 74\n\nInitializing model\n\n\nCode\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,\n                        variable.names=params,\n                        n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim)) # combined chains\n\n\n\n\n22.2.3 Model checking\nAs usual, we check for convergence of our MCMC.\n\n\nCode\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim)\n\ngelman.diag(mod_sim)\n\n\nPotential scale reduction factors:\n\n      Point est. Upper C.I.\nmu[1]          1          1\nmu[2]          1          1\nmu[3]          1          1\nsig            1          1\n\nMultivariate psrf\n\n1\n\n\nCode\nautocorr.diag(mod_sim)\n\n\n              mu[1]        mu[2]        mu[3]          sig\nLag 0   1.000000000  1.000000000  1.000000000  1.000000000\nLag 1  -0.002470573 -0.013124347  0.008955846  0.073525419\nLag 5  -0.011092755  0.001302449  0.002623314 -0.004339828\nLag 10 -0.007632405  0.007061092 -0.003239811 -0.003247443\nLag 50  0.002944126 -0.001364664 -0.000619973 -0.006192542\n\n\nCode\neffectiveSize(mod_sim)\n\n\n   mu[1]    mu[2]    mu[3]      sig \n15000.00 15436.55 14673.76 12967.94 \n\n\n\n\n\n\n\n\nFigure 22.6: MCMC convergence diagnostics\n\n\n\n\n\nWe can also look at the residuals to see if there are any obvious problems with our model choice.\n\n\nCode\n(pm_params = colMeans(mod_csim))\n\n\n    mu[1]     mu[2]     mu[3]       sig \n5.0302827 4.6605291 5.5276798 0.7129141 \n\n\n\n\nCode\nyhat = pm_params[1:3][data_jags$grp]\nresid = data_jags$y - yhat\nplot(resid)\n\n\n\n\n\n\n\n\nFigure 22.7: Residuals vs Index\n\n\n\n\n\n\n\nCode\nplot(yhat, resid)\n\n\n\n\n\n\n\n\nFigure 22.8: Residuals vs Fitted values for PlantGrowth model\n\n\n\n\n\nAgain, it might be appropriate to have a separate variance for each group. We will have you do that as an exercise.\n\n\n22.2.4 Results\nLet’s look at the posterior summary of the parameters.\n\n\nCode\nsummary(mod_sim)\n\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean     SD  Naive SE Time-series SE\nmu[1] 5.0303 0.2246 0.0018337      0.0018337\nmu[2] 4.6605 0.2262 0.0018466      0.0018222\nmu[3] 5.5277 0.2282 0.0018631      0.0018842\nsig   0.7129 0.0922 0.0007528      0.0008118\n\n2. Quantiles for each variable:\n\n        2.5%    25%    50%    75% 97.5%\nmu[1] 4.5826 4.8835 5.0327 5.1786 5.468\nmu[2] 4.2126 4.5106 4.6622 4.8097 5.099\nmu[3] 5.0825 5.3773 5.5279 5.6784 5.980\nsig   0.5627 0.6483 0.7026 0.7669 0.922\n\n\n\n\nCode\nHPDinterval(mod_csim)\n\n\n          lower     upper\nmu[1] 4.5926785 5.4763714\nmu[2] 4.2107388 5.0966888\nmu[3] 5.0860902 5.9819091\nsig   0.5490977 0.8967264\nattr(,\"Probability\")\n[1] 0.95\n\n\nThe HPDinterval() function in the coda package calculates intervals of highest posterior density for each parameter.\nWe are interested to know if one of the treatments increases mean yield. It is clear that treatment 1 does not. What about treatment 2?\n\n\nCode\nmean(mod_csim[,3] &gt; mod_csim[,1])\n\n\n[1] 0.9418\n\n\nThere is a high posterior probability that the mean yield for treatment 2 is greater than the mean yield for the control group.\nIt may be the case that treatment 2 would be costly to put into production. Suppose that to be worthwhile, this treatment must increase mean yield by 10%. What is the posterior probability that the increase is at least that?\n\n\nCode\nmean(mod_csim[,3] &gt; 1.1*mod_csim[,1])\n\n\n[1] 0.4906\n\n\nWe have about 50/50 odds that adopting treatment 2 would increase mean yield by at least 10%.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "C2-L08.html#two-factor-anova",
    "href": "C2-L08.html#two-factor-anova",
    "title": "22  ANOVA",
    "section": "22.3 Two Factor ANOVA",
    "text": "22.3 Two Factor ANOVA\n\n22.3.1 Data\nLet’s explore an example with two factors. We’ll use the Warpbreaks data set in R. Check the documentation for a description of the data by typing ?warpbreaks.\n\n\nCode\ndata(\"warpbreaks\")\n#?warpbreaks\nhead(warpbreaks)\n\n\n  breaks wool tension\n1     26    A       L\n2     30    A       L\n3     54    A       L\n4     25    A       L\n5     70    A       L\n6     52    A       L\n\n\n\nCode\n# This chunk is for displaying the output that was previously static.\n# If the static output below is preferred, this chunk can be removed \n# and the static output remains unlabelled as it's not a code cell.\n# For a labeled table, this chunk should generate it.\n# The original file had static output here:\n##   breaks wool tension\n## 1     26    A       L\n## 2     30    A       L\n## 3     54    A       L\n## 4     25    A       L\n## 5     70    A       L\n## 6     52    A       L\n# To make this a labeled table from code:\nhead(warpbreaks)\n\n\n\n\nTable 22.1: Preview of first few rows of warpbreaks data\n\n\n\n  breaks wool tension\n1     26    A       L\n2     30    A       L\n3     54    A       L\n4     25    A       L\n5     70    A       L\n6     52    A       L\n\n\n\n\n\nCode\ntable(warpbreaks$wool, warpbreaks$tension)\n\n\n\n\nTable 22.2: Contingency table of wool type vs tension level\n\n\n\n   \n    L M H\n  A 9 9 9\n  B 9 9 9\n\n\n\n\nAgain, we visualize the data with box plots.\n\n\nCode\nboxplot(breaks ~ wool + tension, data=warpbreaks)\n\n\n\n\n\n\n\n\nFigure 22.9: Warpbreaks boxplot\n\n\n\n\n\n\n\nCode\nboxplot(log(breaks) ~ wool + tension, data=warpbreaks)\n\n\n\n\n\n\n\n\nFigure 22.10: Warpbreaks boxplot with log-transformed breaks\n\n\n\n\n\nThe different groups have more similar variance if we use the logarithm of breaks. From this visualization, it looks like both factors may play a role in the number of breaks. It appears that there is a general decrease in breaks as we move from low to medium to high tension. Let’s start with a one-way model using tension only.\n\n\n22.3.2 One-way model\n\n\nCode\nmod1_string = \" model {\n    for( i in 1:length(y)) {\n        y[i] ~ dnorm(mu[tensGrp[i]], prec)\n    }\n    \n    for (j in 1:3) {\n        mu[j] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n    prec ~ dgamma(5/2.0, 5*2.0/2.0)\n    sig = sqrt(1.0 / prec)\n} \"\n\nset.seed(83)\nstr(warpbreaks)\n\n\n'data.frame':   54 obs. of  3 variables:\n $ breaks : num  26 30 54 25 70 52 51 26 67 18 ...\n $ wool   : Factor w/ 2 levels \"A\",\"B\": 1 1 1 1 1 1 1 1 1 1 ...\n $ tension: Factor w/ 3 levels \"L\",\"M\",\"H\": 1 1 1 1 1 1 1 1 1 2 ...\n\n\nCode\ndata1_jags = list(y=log(warpbreaks$breaks), tensGrp=as.numeric(warpbreaks$tension))\n\nparams1 = c(\"mu\", \"sig\")\n\nmod1 = jags.model(textConnection(mod1_string), data=data1_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 54\n   Unobserved stochastic nodes: 4\n   Total graph size: 123\n\nInitializing model\n\n\nCode\nupdate(mod1, 1e3)\n\nmod1_sim = coda.samples(model=mod1,\n                        variable.names=params1,\n                        n.iter=5e3)\n\n\n\n\nCode\n## convergence diagnostics\nplot(mod1_sim)\n\n\n\n\n\n\n\n\nFigure 22.11: MCMC convergence diagnostics for one-way tension model\n\n\n\n\n\n\n\nCode\ngelman.diag(mod1_sim)\n\n\nPotential scale reduction factors:\n\n      Point est. Upper C.I.\nmu[1]          1          1\nmu[2]          1          1\nmu[3]          1          1\nsig            1          1\n\nMultivariate psrf\n\n1\n\n\nCode\nautocorr.diag(mod1_sim)\n\n\n               mu[1]        mu[2]        mu[3]          sig\nLag 0   1.0000000000  1.000000000  1.000000000  1.000000000\nLag 1   0.0016950787 -0.006586093 -0.007259027  0.048217920\nLag 5   0.0057841409 -0.009841853  0.010191705  0.015061091\nLag 10 -0.0098245436 -0.004672695  0.004590615 -0.008874268\nLag 50 -0.0002831298 -0.000821222 -0.001508349  0.007998712\n\n\nCode\neffectiveSize(mod1_sim)\n\n\n   mu[1]    mu[2]    mu[3]      sig \n15297.81 15000.00 15058.27 14002.72 \n\n\nThe 95% posterior interval for the mean of group 2 (medium tension) overlaps with both the low and high groups, but the intervals for low and high group only slightly overlap. That is a pretty strong indication that the means for low and high tension are different. Let’s collect the DIC for this model and move on to the two-way model.\n\n\nCode\ndic1 = dic.samples(mod1, n.iter=1e3)\n\n\n\n\n22.3.3 Two-way additive model\nWith two factors, one with two levels and the other with three, we have six treatment groups, which is the same situation we discussed when introducing multiple factor ANOVA. We will first fit the additive model which treats the two factors separately with no interaction. To get the X matrix (or design matrix) for this model, we can create it in R.\n\nCode\nX = model.matrix( ~ wool + tension, data=warpbreaks)\nhead(X)\n\n\n\n\nTable 22.3: Head of the design matrix for the additive model\n\n\n\n  (Intercept) woolB tensionM tensionH\n1           1     0        0        0\n2           1     0        0        0\n3           1     0        0        0\n4           1     0        0        0\n5           1     0        0        0\n6           1     0        0        0\n\n\n\n\n\nCode\ntail(X)\n\n\n\n\nTable 22.4: Tail of the design matrix for the additive model\n\n\n\n   (Intercept) woolB tensionM tensionH\n49           1     1        0        1\n50           1     1        0        1\n51           1     1        0        1\n52           1     1        0        1\n53           1     1        0        1\n54           1     1        0        1\n\n\n\n\nBy default, R has chosen the mean for wool A and low tension to be the intercept. Then, there is an effect for wool B, and effects for medium tension and high tension, each associated with dummy indicator variables.\n\n\nCode\nmod2_string = \" model {\n    for( i in 1:length(y)) {\n        y[i] ~ dnorm(mu[i], prec)\n        mu[i] = int + alpha*isWoolB[i] + beta[1]*isTensionM[i] + beta[2]*isTensionH[i]\n    }\n    \n    int ~ dnorm(0.0, 1.0/1.0e6)\n    alpha ~ dnorm(0.0, 1.0/1.0e6)\n    for (j in 1:2) {\n        beta[j] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n    prec ~ dgamma(3/2.0, 3*1.0/2.0)\n    sig = sqrt(1.0 / prec)\n} \"\n\ndata2_jags = list(y=log(warpbreaks$breaks), isWoolB=X[,\"woolB\"], isTensionM=X[,\"tensionM\"], isTensionH=X[,\"tensionH\"])\n\nparams2 = c(\"int\", \"alpha\", \"beta\", \"sig\")\n\nmod2 = jags.model(textConnection(mod2_string), data=data2_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 54\n   Unobserved stochastic nodes: 5\n   Total graph size: 243\n\nInitializing model\n\n\nCode\nupdate(mod2, 1e3)\n\nmod2_sim = coda.samples(model=mod2,\n                        variable.names=params2,\n                        n.iter=5e3)\n\n\n\n\nCode\n## convergence diagnostics\nplot(mod2_sim)\n\ngelman.diag(mod2_sim)    # Corrected from mod1_sim\n\n\nPotential scale reduction factors:\n\n        Point est. Upper C.I.\nalpha            1       1.00\nbeta[1]          1       1.01\nbeta[2]          1       1.00\nint              1       1.01\nsig              1       1.00\n\nMultivariate psrf\n\n1\n\n\nCode\nautocorr.diag(mod2_sim)  # Corrected from mod1_sim\n\n\n              alpha     beta[1]      beta[2]          int          sig\nLag 0   1.000000000 1.000000000  1.000000000  1.000000000 1.0000000000\nLag 1   0.488247341 0.504614326  0.514299141  0.752576703 0.0724634014\nLag 5   0.026179046 0.108567984  0.122712326  0.206413998 0.0045116200\nLag 10  0.001578138 0.023058972  0.002245149  0.016060732 0.0081128479\nLag 50 -0.006603747 0.005023901 -0.009661676 -0.001698174 0.0009822111\n\n\nCode\neffectiveSize(mod2_sim) # Corrected from mod1_sim\n\n\n    alpha   beta[1]   beta[2]       int       sig \n 5248.894  3578.852  3529.752  2382.620 12352.381 \n\n\n\n\n\n\n\n\nFigure 22.12: Convergence and diagnostics for the additive two-way ANOVA model\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.13: Convergence and diagnostics for the additive two-way ANOVA model\n\n\n\n\n\nLet’s summarize the results, collect the DIC for this model, and compare it to the first one-way model.\n\n\nCode\nsummary(mod2_sim)\n\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean      SD  Naive SE Time-series SE\nalpha   -0.1512 0.12389 0.0010115      0.0017132\nbeta[1] -0.2869 0.15262 0.0012461      0.0025518\nbeta[2] -0.4904 0.15393 0.0012568      0.0026011\nint      3.5764 0.12583 0.0010274      0.0025813\nsig      0.4548 0.04517 0.0003688      0.0004066\n\n2. Quantiles for each variable:\n\n           2.5%     25%     50%      75%    97.5%\nalpha   -0.3930 -0.2337 -0.1516 -0.07003  0.09298\nbeta[1] -0.5871 -0.3882 -0.2870 -0.18639  0.01674\nbeta[2] -0.7958 -0.5924 -0.4891 -0.38903 -0.18686\nint      3.3278  3.4938  3.5768  3.65986  3.82212\nsig      0.3767  0.4227  0.4514  0.48250  0.55330\n\n\n\n\nCode\n(dic2 = dic.samples(mod2, n.iter=1e3))\n\n\nMean deviance:  55.55 \npenalty 5.119 \nPenalized deviance: 60.66 \n\n\nCode\ndic1\n\n\nMean deviance:  66.52 \npenalty 4.025 \nPenalized deviance: 70.55 \n\n\nThis suggests there is much to be gained adding the wool factor to the model. Before we settle on this model however, we should consider whether there is an interaction. Let’s look again at the box plot with all six treatment groups.\n\n\nCode\nboxplot(log(breaks) ~ wool + tension, data=warpbreaks)\n\n\n\n\n\n\n\n\nFigure 22.14: Re-examining boxplot of log(breaks) by wool and tension for interaction effects\n\n\n\n\n\nOur two-way model has a single effect for wool B and the estimate is negative. If this is true, then we would expect wool B to be associated with fewer breaks than its wool A counterpart on average. This is true for low and high tension, but it appears that breaks are higher for wool B when there is medium tension. That is, the effect for wool B is not consistent across tension levels, so it may appropriate to add an interaction term. In R, this would look like:\n\n\nCode\nlmod2 = lm(log(breaks) ~ .^2, data=warpbreaks)\nsummary(lmod2)\n\n\n\nCall:\nlm(formula = log(breaks) ~ .^2, data = warpbreaks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81504 -0.27885  0.04042  0.27319  0.64358 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.7179     0.1247  29.824  &lt; 2e-16 ***\nwoolB           -0.4356     0.1763  -2.471  0.01709 *  \ntensionM        -0.6012     0.1763  -3.410  0.00133 ** \ntensionH        -0.6003     0.1763  -3.405  0.00134 ** \nwoolB:tensionM   0.6281     0.2493   2.519  0.01514 *  \nwoolB:tensionH   0.2221     0.2493   0.891  0.37749    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.374 on 48 degrees of freedom\nMultiple R-squared:  0.3363,    Adjusted R-squared:  0.2672 \nF-statistic: 4.864 on 5 and 48 DF,  p-value: 0.001116\n\n\nAdding the interaction, we get an effect for being in wool B and medium tension, as well as for being in wool B and high tension. There are now six parameters for the mean, one for each treatment group, so this model is equivalent to the full cell means model. Let’s use that.\n\n\n22.3.4 Two-way cell means model\nIn this new model, \\mu will be a matrix with six entries, each corresponding to a treatment group.\n\n\nCode\nmod3_string = \" model {\n    for( i in 1:length(y)) {\n        y[i] ~ dnorm(mu[woolGrp[i], tensGrp[i]], prec)\n    }\n    \n    for (j in 1:max(woolGrp)) {\n        for (k in 1:max(tensGrp)) {\n            mu[j,k] ~ dnorm(0.0, 1.0/1.0e6)\n        }\n    }\n    \n    prec ~ dgamma(3/2.0, 3*1.0/2.0)\n    sig = sqrt(1.0 / prec)\n} \"\n\nstr(warpbreaks)\n\n\n'data.frame':   54 obs. of  3 variables:\n $ breaks : num  26 30 54 25 70 52 51 26 67 18 ...\n $ wool   : Factor w/ 2 levels \"A\",\"B\": 1 1 1 1 1 1 1 1 1 1 ...\n $ tension: Factor w/ 3 levels \"L\",\"M\",\"H\": 1 1 1 1 1 1 1 1 1 2 ...\n\n\nCode\ndata3_jags = list(y=log(warpbreaks$breaks), woolGrp=as.numeric(warpbreaks$wool), tensGrp=as.numeric(warpbreaks$tension))\n\nparams3 = c(\"mu\", \"sig\")\n\nmod3 = jags.model(textConnection(mod3_string), data=data3_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 54\n   Unobserved stochastic nodes: 7\n   Total graph size: 179\n\nInitializing model\n\n\nCode\nupdate(mod3, 1e3)\n\nmod3_sim = coda.samples(model=mod3,\n                        variable.names=params3,\n                        n.iter=5e3)\nmod3_csim = as.mcmc(do.call(rbind, mod3_sim))\n\n\n\n\nCode\nplot(mod3_sim)\n\n\n\n\n\n\n\n\nFigure 22.15: Traceplots for the cell means model\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.16: Traceplots for the cell means model\n\n\n\n\n\n\n\nCode\n## convergence diagnostics\ngelman.diag(mod3_sim)\n\n\nPotential scale reduction factors:\n\n        Point est. Upper C.I.\nmu[1,1]          1          1\nmu[2,1]          1          1\nmu[1,2]          1          1\nmu[2,2]          1          1\nmu[1,3]          1          1\nmu[2,3]          1          1\nsig              1          1\n\nMultivariate psrf\n\n1\n\n\nCode\nautocorr.diag(mod3_sim)\n\n\n            mu[1,1]      mu[2,1]       mu[1,2]     mu[2,2]      mu[1,3]\nLag 0   1.000000000  1.000000000  1.0000000000 1.000000000  1.000000000\nLag 1  -0.006653917 -0.010920738 -0.0125676178 0.006505505  0.001294185\nLag 5   0.005490295 -0.010375069 -0.0043588084 0.003749865 -0.005015524\nLag 10 -0.004782757 -0.003077916  0.0029383176 0.012830404  0.013644418\nLag 50  0.006189806  0.006165720  0.0009120387 0.014439615  0.002772531\n            mu[2,3]          sig\nLag 0   1.000000000  1.000000000\nLag 1  -0.012860707  0.119500761\nLag 5   0.021854384  0.016412412\nLag 10 -0.001376816 -0.005501016\nLag 50  0.003631993 -0.008607118\n\n\nCode\neffectiveSize(mod3_sim)\n\n\n mu[1,1]  mu[2,1]  mu[1,2]  mu[2,2]  mu[1,3]  mu[2,3]      sig \n14896.09 15292.79 14614.21 15000.00 16124.29 14349.97 11798.19 \n\n\nCode\nraftery.diag(mod3_sim)\n\n\n[[1]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                               \n         Burn-in  Total Lower bound  Dependence\n         (M)      (N)   (Nmin)       factor (I)\n mu[1,1] 2        3741  3746         0.999     \n mu[2,1] 2        3866  3746         1.030     \n mu[1,2] 2        3680  3746         0.982     \n mu[2,2] 2        3741  3746         0.999     \n mu[1,3] 2        3774  3746         1.010     \n mu[2,3] 2        3866  3746         1.030     \n sig     2        3680  3746         0.982     \n\n\n[[2]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                               \n         Burn-in  Total Lower bound  Dependence\n         (M)      (N)   (Nmin)       factor (I)\n mu[1,1] 2        3741  3746         0.999     \n mu[2,1] 2        3741  3746         0.999     \n mu[1,2] 2        3741  3746         0.999     \n mu[2,2] 2        3995  3746         1.070     \n mu[1,3] 2        3680  3746         0.982     \n mu[2,3] 2        3803  3746         1.020     \n sig     2        3680  3746         0.982     \n\n\n[[3]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                               \n         Burn-in  Total Lower bound  Dependence\n         (M)      (N)   (Nmin)       factor (I)\n mu[1,1] 2        3680  3746         0.982     \n mu[2,1] 2        3930  3746         1.050     \n mu[1,2] 2        3741  3746         0.999     \n mu[2,2] 2        3620  3746         0.966     \n mu[1,3] 2        3774  3746         1.010     \n mu[2,3] 2        3620  3746         0.966     \n sig     2        3803  3746         1.020     \n\n\nLet’s compute the DIC and compare with our previous models.\n\n\nCode\n(dic3 = dic.samples(mod3, n.iter=1e3))\n\n\nMean deviance:  52.11 \npenalty 7.27 \nPenalized deviance: 59.38 \n\n\nCode\ndic2\n\n\nMean deviance:  55.55 \npenalty 5.119 \nPenalized deviance: 60.66 \n\n\nCode\ndic1\n\n\nMean deviance:  66.52 \npenalty 4.025 \nPenalized deviance: 70.55 \n\n\nThis suggests that the full model with interaction between wool and tension (which is equivalent to the cell means model) is the best for explaining/predicting warp breaks.\n\n\n22.3.5 Results\n\n\nCode\nsummary(mod3_sim)\n\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean      SD  Naive SE Time-series SE\nmu[1,1] 3.7156 0.14777 0.0012065      0.0012107\nmu[2,1] 3.2824 0.15017 0.0012262      0.0012148\nmu[1,2] 3.1188 0.14846 0.0012122      0.0012289\nmu[2,2] 3.3093 0.14933 0.0012193      0.0012193\nmu[1,3] 3.1157 0.14967 0.0012220      0.0011849\nmu[2,3] 2.9030 0.14675 0.0011982      0.0012314\nsig     0.4436 0.04513 0.0003685      0.0004156\n\n2. Quantiles for each variable:\n\n          2.5%    25%    50%    75%  97.5%\nmu[1,1] 3.4245 3.6174 3.7150 3.8148 4.0079\nmu[2,1] 2.9882 3.1804 3.2826 3.3840 3.5754\nmu[1,2] 2.8272 3.0189 3.1195 3.2176 3.4128\nmu[2,2] 3.0156 3.2099 3.3095 3.4063 3.6027\nmu[1,3] 2.8195 3.0157 3.1144 3.2160 3.4092\nmu[2,3] 2.6100 2.8050 2.9016 3.0020 3.1905\nsig     0.3657 0.4119 0.4398 0.4714 0.5436\n\n\n\n\nCode\nHPDinterval(mod3_csim)\n\n\n            lower     upper\nmu[1,1] 3.4332568 4.0147137\nmu[2,1] 2.9870981 3.5733563\nmu[1,2] 2.8226930 3.4062134\nmu[2,2] 3.0149377 3.6008799\nmu[1,3] 2.8317257 3.4183505\nmu[2,3] 2.6225885 3.2014416\nsig     0.3604639 0.5357582\nattr(,\"Probability\")\n[1] 0.95\n\n\n\n\nCode\npar(mfrow=c(3,2)) # arrange frame for plots\ndensplot(mod3_csim[,1:6], xlim=c(2.0, 4.5))\n\n\n\n\n\n\n\n\nFigure 22.17: Posterior densities for cell means\n\n\n\n\n\nIt might be tempting to look at comparisons between each combination of treatments, but we warn that this could yield spurious results. When we discussed the statistical modeling cycle, we said it is best not to search your results for interesting hypotheses, because if there are many hypotheses, some will appear to show “effects” or “associations” simply due to chance. Results are most reliable when we determine a relatively small number of hypotheses we are interested in beforehand, collect the data, and statistically evaluate the evidence for them.\nOne question we might be interested in with these data is finding the treatment combination that produces the fewest breaks. To calculate this, we can go through our posterior samples and for each sample, find out which group has the smallest mean. These counts help us determine the posterior probability that each of the treatment groups has the smallest mean.\n\nCode\nprop.table( table( apply(mod3_csim[,1:6], 1, which.min) ) )\n\n\n\n\nTable 22.5: Posterior probabilities of each treatment group having the smallest mean break rate\n\n\n\n\n         2          3          4          5          6 \n0.01553333 0.11493333 0.01026667 0.11486667 0.74440000 \n\n\n\n\nThe evidence supports wool B with high tension as the treatment that produces the fewest breaks.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "C2-L09.html",
    "href": "C2-L09.html",
    "title": "23  Logistic regression",
    "section": "",
    "text": "23.1 Introduction to Logistic Regression\nLogistic regression is the preferred model when modelling a problem where the response variable is binary such as a classification or the outcome of a Bernoulli trial. In such the traditional least square fit suffers from a number of shortcomings. The main idea here is a log transform. However a naive approach this transform imposes issues with 0 valued inputs since log(0)=-\\infty",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "C2-L09.html#introduction-to-logistic-regression",
    "href": "C2-L09.html#introduction-to-logistic-regression",
    "title": "23  Logistic regression",
    "section": "",
    "text": "Figure 23.1: Introduction to logistic regression\n\n\n\n23.1.1 Data\nFor an example of logistic regression  , we’ll use the urine data set from the boot package in R. The response variable is r, which takes on values of 0 or 1. We will remove some rows from the data set which contain missing values.logistic regression\n\n\nCode\nlibrary(\"boot\")\ndata(\"urine\")\n?urine\nhead(urine)\n\n\n  r gravity   ph osmo cond urea calc\n1 0   1.021 4.91  725   NA  443 2.45\n2 0   1.017 5.74  577 20.0  296 4.49\n3 0   1.008 7.20  321 14.9  101 2.36\n4 0   1.011 5.51  408 12.6  224 2.15\n5 0   1.005 6.52  187  7.5   91 1.16\n6 0   1.020 5.27  668 25.3  252 3.34\n\n\n\n\nCode\n1dat = na.omit(urine)\n\n\n\n1\n\ndrop missing values\n\n\n\n\nLet’s look at pairwise scatter plots of the seven variables.\n\n\nCode\npairs(dat)\n\n\n\n\n\n\n\n\n\nOne thing that stands out is that several of these variables are strongly correlated with one another. For example gravity and osmo appear to have a very close linear relationship. Collinearity between x variables in linear regression models can cause trouble for statistical inference. Two correlated variables will compete for the ability to predict the response variable, leading to unstable estimates. This is not a problem for prediction of the response, if prediction is the end goal of the model. But if our objective is to discover how the variables relate to the response, we should avoid collinearity.\n\n\n\n\n\n\nImportantCollinearity and Multicollinearity\n\n\n\n When two covariates are highly correlated we call this relation collinearity. When one covariate in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy we call this relation multicollinearity. It is possible that no two pairs of a such a group of covariates are correlated.\nIn both cases this will lead to the design matrix being almost singular. Near singular matrices are a strong cause of instability in numerical calculations. Statistical this tends to lead to a model with inflated standard errors compared to models where we only keep the a subset where variables are neither collinear nor multicollinear. A consequence of this is that we will see a drop in statistical significance for these variables, which will make interpreting the model harder.\nWe have seen a few strategies ways to deal with these issues:\n\ninclude pair plot in the exploratory data analysis phase.\npicking subsets and checking DIC or,\nvariable selection using double exponential priors.\nPCA creates independent covariates with a lower dimension with a trade of losing interpretability. See (Johnson and Wichern 2001, 386) (Belsley, Kuh, and Welsch 1980, 85–191) (Härdle and Simar 2019)\nFeature elimination based on combination of Variance inflation factors (VIF) (Sheather 2009, 203)\n\n\n\nWe can more formally estimate the correlation among these variables using the corrplot package.\n\n\nCode\nlibrary(\"corrplot\")\nCor = cor(dat)\ncorrplot(Cor, type=\"upper\", method=\"ellipse\", tl.pos=\"d\")\ncorrplot(Cor, type=\"lower\", method=\"number\", col=\"black\", \n         add=TRUE, diag=FALSE, tl.pos=\"n\", cl.pos=\"n\")\n\n\n\n\n\n\n\n\n\n\n\n23.1.2 Variable selection\nOne primary goal of this analysis is to find out which variables are related to the presence of calcium oxalate crystals. This objective is often called “variable selection.” We have already seen one way to do this: fit several models that include different sets of variables and see which one has the best DIC. Another way to do this is to use a linear model where the priors for the \\beta coefficients favor values near 0 (indicating a weak relationship). This way, the burden of establishing association lies with the data. If there is not a strong signal, we assume it doesn’t exist.\nRather than tailoring a prior for each individual \\beta based on the scale its covariate takes values on, it is customary to subtract the mean and divide by the standard deviation for each variable.\n\n\nCode\nX = scale(dat[,-1], center=TRUE, scale=TRUE)\nhead(X[,\"gravity\"])\n\n\n         2          3          4          5          6          7 \n-0.1403037 -1.3710690 -0.9608139 -1.7813240  0.2699514 -0.8240622 \n\n\n\n\nCode\ncolMeans(X)\n\n\n      gravity            ph          osmo          cond          urea \n-9.861143e-15  8.511409e-17  1.515743e-16 -1.829852e-16  7.335402e-17 \n         calc \n-1.689666e-18 \n\n\n\n\nCode\napply(X, 2, sd)\n\n\ngravity      ph    osmo    cond    urea    calc \n      1       1       1       1       1       1 \n\n\n\n\n23.1.3 Model\nOur prior for the \\beta (which we’ll call b in the model) coefficients will be the double exponential (or Laplace) distribution, which as the name implies, is the exponential distribution with tails extending in the positive direction as well as the negative direction, with a sharp peak at 0. We can read more about it in the JAGS manual. The distribution looks like:\n\n\nCode\nddexp = function(x, mu, tau) {\n  0.5*tau*exp(-tau*abs(x-mu)) \n}\ncurve(ddexp(x, mu=0.0, tau=1.0), from=-5.0, to=5.0, ylab=\"density\", main=\"Double exponential\\ndistribution\") # double exponential distribution\ncurve(dnorm(x, mean=0.0, sd=1.0), from=-5.0, to=5.0, lty=2, add=TRUE) # normal distribution\nlegend(\"topright\", legend=c(\"double exponential\", \"normal\"), lty=c(1,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(\"rjags\")\n\n\n\n\nCode\nmod1_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dbern(p[i])\n        logit(p[i]) = int + b[1]*gravity[i] + b[2]*ph[i] + b[3]*osmo[i] + b[4]*cond[i] + b[5]*urea[i] + b[6]*calc[i]\n    }\n    int ~ dnorm(0.0, 1.0/25.0)\n    for (j in 1:6) {\n        b[j] ~ ddexp(0.0, sqrt(2.0)) # has variance 1.0\n    }\n} \"\n\nset.seed(92)\nhead(X)\n\n\n     gravity         ph       osmo       cond        urea        calc\n2 -0.1403037 -0.4163725 -0.1528785 -0.1130908  0.25747827  0.09997564\n3 -1.3710690  1.6055972 -1.2218894 -0.7502609 -1.23693077 -0.54608444\n4 -0.9608139 -0.7349020 -0.8585927 -1.0376121 -0.29430353 -0.60978050\n5 -1.7813240  0.6638579 -1.7814497 -1.6747822 -1.31356713 -0.91006194\n6  0.2699514 -1.0672806  0.2271214  0.5490664 -0.07972172 -0.24883614\n7 -0.8240622 -0.5825618 -0.6372741 -0.4379226 -0.51654898 -0.83726644\n\n\nCode\ndata_jags = list(y=dat$r, gravity=X[,\"gravity\"], ph=X[,\"ph\"], osmo=X[,\"osmo\"], cond=X[,\"cond\"], urea=X[,\"urea\"], calc=X[,\"calc\"])\n\nparams = c(\"int\", \"b\")\n\nmod1 = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 77\n   Unobserved stochastic nodes: 7\n   Total graph size: 1085\n\nInitializing model\n\n\nCode\nupdate(mod1, 1e3)\n\nmod1_sim = coda.samples(model=mod1,\n                        variable.names=params,\n                        n.iter=5e3)\nmod1_csim = as.mcmc(do.call(rbind, mod1_sim))\n\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod1_sim, ask=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngelman.diag(mod1_sim)\n\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\nb[1]          1       1.00\nb[2]          1       1.00\nb[3]          1       1.00\nb[4]          1       1.01\nb[5]          1       1.00\nb[6]          1       1.00\nint           1       1.00\n\nMultivariate psrf\n\n1\n\n\nCode\nautocorr.diag(mod1_sim)\n\n\n             b[1]         b[2]       b[3]       b[4]       b[5]         b[6]\nLag 0  1.00000000  1.000000000 1.00000000 1.00000000 1.00000000 1.0000000000\nLag 1  0.82455145  0.275126853 0.88415792 0.73813220 0.79047975 0.4900889290\nLag 5  0.38282625  0.024783761 0.54186932 0.30458860 0.34063895 0.0289921484\nLag 10 0.15623277  0.007830358 0.27598994 0.13981694 0.13659864 0.0006661123\nLag 50 0.01745844 -0.003605210 0.02771368 0.02391942 0.03779496 0.0191733095\n               int\nLag 0  1.000000000\nLag 1  0.280587632\nLag 5  0.019727112\nLag 10 0.024918124\nLag 50 0.006355973\n\n\nCode\nautocorr.plot(mod1_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\neffectiveSize(mod1_sim)\n\n\n     b[1]      b[2]      b[3]      b[4]      b[5]      b[6]       int \n1405.2325 8271.9006  922.2363 1717.9002 1553.5772 5140.3197 8106.0347 \n\n\nCode\n## calculate DIC\ndic1 = dic.samples(mod1, n.iter=1e3)\n\n\nLet’s look at the results.\n\n\nCode\nsummary(mod1_sim)\n\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean     SD Naive SE Time-series SE\nb[1]  1.6750 0.7355 0.006005       0.019654\nb[2] -0.1413 0.2859 0.002335       0.003146\nb[3] -0.3351 0.7609 0.006213       0.025145\nb[4] -0.7387 0.4898 0.003999       0.012074\nb[5] -0.6002 0.5796 0.004732       0.014837\nb[6]  1.6130 0.4833 0.003947       0.006771\nint  -0.1758 0.3015 0.002461       0.003351\n\n2. Quantiles for each variable:\n\n        2.5%     25%     50%      75%  97.5%\nb[1]  0.3903  1.1447  1.6251  2.14935 3.2313\nb[2] -0.7313 -0.3223 -0.1287  0.04632 0.4062\nb[3] -2.0128 -0.7892 -0.2416  0.14213 1.0840\nb[4] -1.7470 -1.0615 -0.7238 -0.39099 0.1457\nb[5] -1.8615 -0.9675 -0.5393 -0.18216 0.3898\nb[6]  0.7474  1.2766  1.5842  1.92495 2.6181\nint  -0.7665 -0.3789 -0.1777  0.02473 0.4293\n\n\n\n\nCode\n#par(mfrow=c(3,2))\npar(mar = c(2.5, 1, 2.5, 1))\n\ndensplot(mod1_csim[,1:6], xlim=c(-3.0, 3.0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncolnames(X) # variable names\n\n\n[1] \"gravity\" \"ph\"      \"osmo\"    \"cond\"    \"urea\"    \"calc\"   \n\n\nIt is clear that the coefficients for variables gravity, cond (conductivity), and calc (calcium concentration) are not 0. The posterior distribution for the coefficient of osmo (osmolarity) looks like the prior, and is almost centered on 0 still, so we’ll conclude that osmo is not a strong predictor of calcium oxalate crystals. The same goes for ph.\nurea (urea concentration) appears to be a borderline case. However, if we refer back to our correlations among the variables, we see that urea is highly correlated with gravity, so we opt to remove it.\nOur second model looks like this:\n\n\nCode\nmod2_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dbern(p[i])\n        logit(p[i]) = int + b[1]*gravity[i] + b[2]*cond[i] + b[3]*calc[i]\n    }\n    int ~ dnorm(0.0, 1.0/25.0)\n    for (j in 1:3) {\n        b[j] ~ dnorm(0.0, 1.0/25.0) # noninformative for logistic regression\n    }\n} \"\n\nmod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)\n\n\nWarning in jags.model(textConnection(mod2_string), data = data_jags, n.chains =\n3): Unused variable \"ph\" in data\n\n\nWarning in jags.model(textConnection(mod2_string), data = data_jags, n.chains =\n3): Unused variable \"osmo\" in data\n\n\nWarning in jags.model(textConnection(mod2_string), data = data_jags, n.chains =\n3): Unused variable \"urea\" in data\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 77\n   Unobserved stochastic nodes: 4\n   Total graph size: 635\n\nInitializing model\n\n\n\n\nCode\nupdate(mod2, 1e3)\n\nmod2_sim = coda.samples(model=mod2,\n                        variable.names=params,\n                        n.iter=5e3)\nmod2_csim = as.mcmc(do.call(rbind, mod2_sim))\n\npar(mar = c(2.5, 1, 2.5, 1))\n#plot(mod2_sim, ask=TRUE)\nplot(mod2_sim)\n\n\n\n\n\n\n\n\n\nCode\ngelman.diag(mod2_sim)\n\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\nb[1]          1          1\nb[2]          1          1\nb[3]          1          1\nint           1          1\n\nMultivariate psrf\n\n1\n\n\nCode\nautocorr.diag(mod2_sim)\n\n\n             b[1]       b[2]       b[3]           int\nLag 0  1.00000000 1.00000000 1.00000000  1.0000000000\nLag 1  0.58128433 0.67350376 0.50743018  0.2872249841\nLag 5  0.11242736 0.16403208 0.05713961  0.0101213096\nLag 10 0.04561829 0.04332871 0.02999190  0.0007838081\nLag 50 0.01595758 0.02919047 0.01100776 -0.0031651856\n\n\nCode\nautocorr.plot(mod2_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\neffectiveSize(mod2_sim)\n\n\n    b[1]     b[2]     b[3]      int \n3443.873 2651.275 4476.259 8073.307 \n\n\nCode\ndic2 = dic.samples(mod2, n.iter=1e3)\n\n\n\n\n23.1.4 Results\n\n\nCode\ndic1\n\n\nMean deviance:  68.65 \npenalty 5.448 \nPenalized deviance: 74.1 \n\n\n\n\nCode\ndic2\n\n\nMean deviance:  70.99 \npenalty 3.816 \nPenalized deviance: 74.81 \n\n\n\n\nCode\nsummary(mod2_sim)\n\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean     SD Naive SE Time-series SE\nb[1]  1.4223 0.5125 0.004184       0.008743\nb[2] -1.3550 0.4761 0.003888       0.009254\nb[3]  1.8755 0.5600 0.004573       0.008377\nint  -0.1528 0.3230 0.002637       0.003610\n\n2. Quantiles for each variable:\n\n        2.5%     25%    50%      75%   97.5%\nb[1]  0.4929  1.0686  1.401  1.74706  2.5016\nb[2] -2.3444 -1.6670 -1.337 -1.02664 -0.4878\nb[3]  0.8940  1.4857  1.837  2.22553  3.0748\nint  -0.7685 -0.3713 -0.159  0.05905  0.4990\n\n\n\n\nCode\nHPDinterval(mod2_csim)\n\n\n          lower      upper\nb[1]  0.4667769  2.4602958\nb[2] -2.2950147 -0.4479398\nb[3]  0.8596236  3.0278554\nint  -0.7707860  0.4938714\nattr(,\"Probability\")\n[1] 0.95\n\n\n\n\nCode\n#par(mfrow=c(3,1))\npar(mar = c(2.5, 1, 2.5, 1))\ndensplot(mod2_csim[,1:3], xlim=c(-3.0, 3.0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncolnames(X)[c(1,4,6)] # variable names\n\n\n[1] \"gravity\" \"cond\"    \"calc\"   \n\n\nThe DIC is actually better for the first model. Note that we did change the prior between models, and generally we should not use the DIC to choose between priors. Hence comparing DIC between these two models may not be a fair comparison. Nevertheless, they both yield essentially the same conclusions. Higher values of gravity and calc (calcium concentration) are associated with higher probabilities of calcium oxalate crystals, while higher values of cond (conductivity) are associated with lower probabilities of calcium oxalate crystals.\nThere are more modeling options in this scenario, perhaps including transformations of variables, different priors, and interactions between the predictors, but we’ll leave it to you to see if you can improve the model.\n\n\n\n\n\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. 1980. Regression Diagnostics. John Wiley & Sons, Inc. https://doi.org/10.1002/0471725153.\n\n\nHärdle, Wolfgang Karl, and Léopold Simar. 2019. Applied Multivariate Statistical Analysis. Springer International Publishing. https://doi.org/10.1007/978-3-030-26006-4.\n\n\nJohnson, R. A., and D. W. Wichern. 2001. Applied Multivariate Statistical Analysis. Pearson Modern Classics for Advanced Statistics Series. Prentice Hall. https://books.google.co.il/books?id=QBqlswEACAAJ.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with r. Springer New York. https://doi.org/10.1007/978-0-387-09608-7.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "C2-L10.html",
    "href": "C2-L10.html",
    "title": "24  Poisson regression",
    "section": "",
    "text": "24.1 Introduction to Poisson regression\nPoisson regression is the preferred method to handle count data where the response is positive values but includes zeroes. The gist of this method is that We use a log transform link function on the regressors but not on the response which allows the response to be zero valued which correspond to zero counts. One limit of this approach mentioned below is that the Poisson takes one parameter \\lambda for both its expected value and its variance. We look give a deeper solution to this problem in the next course on mixture models, however in this course we will consider more restricted cases where we extend the Poisson regression with the Negative Binomial Distribution which allows us to model over-dispersed data.\nWe now have experience fitting regression models when the response is continuous, and when it is binary. What about when we have count data? We could fit a linear normal regression, but here we have a couple of drawbacks. First of all, counts usually aren’t negative. And the variances might not be constant. The Poisson distribution provides a natural likelihood for count data.\ny_i\\mid \\lambda+i \\stackrel {iid} \\sim \\mathrm{Pois}(\\lambda_i) \\qquad i=1, \\ldots, n\nHere, \\lambda conveniently represents the expected value of y \\mathbb{E}[y]. It turns out that \\lambda is also the variance of y \\mathbb{V}ar[y]. So if we expect a count to be higher, we also expect the variability in counts to go up.\nWe saw this earlier with the warp breaks data.\nIf we model the mean directly, like we did with linear regression. That is, we had the expected value yi was directly modeled with this linear form.\n\\mathbb{E}[y]=\\beta_0 + \\beta_1x_i \\qquad \\text{(linear regression)}\nWe would run into the same problem we did with logistic regression. The expected value has to be greater than zero in the Poisson distribution. To naturally deal with that restriction, we’re going to use the logarithmic link function.\nSo, the log link. That is, that the log of \\lambda_i is equal to this linear piece.\nlog link:\nlog(\\lambda_i) = \\beta_0+\\beta_1x_i \\qquad \\text{(log link)}\n\\tag{24.1}\n\\mathbb{E}[y]=\\beta_0+\\beta_1x_i \\qquad \\text{(linear regression)}\nFrom this, we can easily recover the expression for the mean itself. That is, we can invert this link function to get the expected value of y_i,\n\\implies \\mathbb{E}[y] = \\lambda_i = e^{\\left(\\beta_0+\\beta_1x_i \\right)}\n\\tag{24.2}\nIt might seem like this model is equivalent to fitting a normal linear regression to the log of y. But there are a few key differences. In the normal regression, we’re modeling the mean of the response directly. So we would be fitting a model to the \\log(y). Where we’re modeling the expected value of the \\log(y). This is different from what we’re modeling here, here we’re doing the log of the expected value of y.\n\\mathbb{E}[log(y)]\\ne log(\\mathbb{E}[y])\nThese are not equal, they’re usually similar, but they’re not the same. Another difference is that we have a separate independent parameter for the variants in a normal regression. In Poisson regression, the variance is automatically the same as \\lambda,which may not always be appropriate, as we’ll see in an upcoming example.\nAs usual, we can add more explanatory x variables to the Poisson regression framework. They can be continuous, categorical, or they could be counts themselves.\nIf we have three predictor variables x_i = ( x_{1,i}, x_{2,i}, x_{3,i} ), what would the likelihood part of the hierarchical representation of a Poisson regression with logarithmic link look like?\nHere we incorporated the (inverse) link function directly into the likelihood rather than writing it with two lines.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Poisson regression</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#sec-introduction-to-poisson-regression",
    "href": "C2-L10.html#sec-introduction-to-poisson-regression",
    "title": "24  Poisson regression",
    "section": "",
    "text": "Figure 24.1: Introduction to Poisson regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny_i \\mid x_i, \\beta \\overset{\\text{ind}}{\\sim} \\mathrm{Pois} \\left( e^{-(\\beta_0+\\beta_1 x_{1,i}+\\beta_2 x_{2,i}+\\beta_3 x_{3,i})}\\right)\ny_i \\mid x_i, \\beta \\overset{\\text{ind}}{\\sim} \\mathrm{Pois} \\left(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} \\right)\ny_i \\mid x_i, \\beta \\overset{\\text{ind}}{\\sim} \\mathrm{Pois} \\left(e^{\\beta_0 + \\beta_1 x_{1,i}+\\beta_2 x_{2,i}+\\beta_3 x_{3,i}}\\right)\ny_i \\mid x_i, \\beta \\overset{\\text{ind}}{\\sim} \\mathrm{Pois} \\left( \\log[ \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} ] \\right)",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Poisson regression</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#poisson-regression---jags-model",
    "href": "C2-L10.html#poisson-regression---jags-model",
    "title": "24  Poisson regression",
    "section": "24.2 Poisson regression - JAGS model",
    "text": "24.2 Poisson regression - JAGS model\n For an example of Poisson regression, we’ll use the badhealth data set from the COUNT package in R.doctor visits\n\n\nCode\nlibrary(\"COUNT\")\n\n\nLoading required package: msme\n\n\nLoading required package: MASS\n\n\nLoading required package: lattice\n\n\nLoading required package: sandwich\n\n\nCode\ndata(\"badhealth\")\n#?badhealth\nhead(badhealth)\n\n\n  numvisit badh age\n1       30    0  58\n2       20    0  54\n3       16    0  44\n4       20    0  57\n5       15    0  33\n6       15    0  28\n\n\naccording to the description:\n\n\n\n\n\n\nNoteData Card for badhealth\n\n\n\n1,127 observations from a 1998 German survey with 3 variables:\n\nnumvisit - number of visits to the doctor in 1998 (response)\nbadh - \\begin{cases} 1 \\qquad \\text{ patient claims to be in bad health} \\\\ 0 \\qquad \\text{ patient does not claim to be in bad health} \\end{cases}\nage - age of patient\n\n\n\n\n\nCode\nany(is.na(badhealth))\n\n\n[1] FALSE\n\n\n\nremove na\n\nAs usual, let’s visualize these data.\n\n\nCode\nhist(badhealth$numvisit, breaks=20)\n\n\n\n\n\n\n\n\nFigure 24.2: Histogram of number of doctor visits\n\n\n\n\n\n\n\nCode\nplot(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==0, xlab=\"age\", ylab=\"log(visits)\")\npoints(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==1, col=\"red\")\n\n\n\n\n\n\n\n\nFigure 24.3\n\n\n\n\n\n\n24.2.1 Doctor Visits Model\n It appears that both age and bad health are related to the number of doctor visits. We should include model terms for both variables. If we believe the age/visits relationship is different between healthy and non-healthy populations, we should also include an interaction term. We will fit the full model here and leave it to you to compare it with the simpler additive model.doctor visits\n\n\nCode\nlibrary(\"rjags\")\n\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\n\n\n\nCode\nmod_string = \" model {\n    for (i in 1:length(numvisit)) {\n        numvisit[i] ~ dpois(lam[i])\n        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]\n    }\n    \n    int ~ dnorm(0.0, 1.0/1e6)\n    b_badh ~ dnorm(0.0, 1.0/1e4)\n    b_age ~ dnorm(0.0, 1.0/1e4)\n    b_intx ~ dnorm(0.0, 1.0/1e4)\n} \"\n\nset.seed(102)\n\ndata_jags = as.list(badhealth)\n\nparams = c(\"int\", \"b_badh\", \"b_age\", \"b_intx\")\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1127\n   Unobserved stochastic nodes: 4\n   Total graph size: 3665\n\nInitializing model\n\n\nCode\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,  variable.names=params, n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim))\n\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim)\n\n\n\n\n\n\n\n\n\nCode\ngelman.diag(mod_sim)\n\n\nPotential scale reduction factors:\n\n       Point est. Upper C.I.\nb_age        1.00       1.01\nb_badh       1.01       1.04\nb_intx       1.01       1.04\nint          1.00       1.01\n\nMultivariate psrf\n\n1.01\n\n\nCode\nautocorr.diag(mod_sim)\n\n\n           b_age    b_badh    b_intx       int\nLag 0  1.0000000 1.0000000 1.0000000 1.0000000\nLag 1  0.9525175 0.9652643 0.9678261 0.9484847\nLag 5  0.8199741 0.8674916 0.8739739 0.8153412\nLag 10 0.6852371 0.7603195 0.7721200 0.6842460\nLag 50 0.2062104 0.2349866 0.2474145 0.1989648\n\n\nCode\nautocorr.plot(mod_csim)\n\n\n\n\n\n\n\n\n\nCode\neffectiveSize(mod_sim)\n\n\n   b_age   b_badh   b_intx      int \n281.1309 198.7581 186.1054 270.6465 \n\n\nCode\n## compute DIC\ndic = dic.samples(mod, n.iter=1e3)\n\n\n\n\n24.2.2 Model checking - Residuals\n\n“While inexact models may mislead, attempting to allow for every contingency a priori is impractical. Thus models must be built by an iterative feedback process in which an initial parsimonious model may be modified when diagnostic checks applied to residuals indicate the need.” —G. E. P. Box\n\nTo get a general idea of the model’s performance, we can look at predicted values and residuals as usual. Don’t forget that we must apply the inverse of the link function to get predictions for \\lambda .\n\n\nCode\n1X = as.matrix(badhealth[,-1])\n2X = cbind(X, with(badhealth, badh*age))\nhead(X)\n\n\n\n1\n\nwe drop the first column since it is the column for our y.\n\n2\n\nwe add a third column with \\mathbb{I}_{badh}\\times age\n\n\n\n\n     badh age  \n[1,]    0  58 0\n[2,]    0  54 0\n[3,]    0  44 0\n[4,]    0  57 0\n[5,]    0  33 0\n[6,]    0  28 0\n\n\n\n\nCode\n1(pmed_coef = apply(mod_csim, 2, median))\n\n\n\n1\n\nthis are the column medians of the coefficients.\n\n\n\n\n       b_age       b_badh       b_intx          int \n 0.008321089  1.557435148 -0.010663658  0.354097817 \n\n\n\n\nCode\n1llam_hat = pmed_coef[\"int\"] + X %*% pmed_coef[c(\"b_badh\", \"b_age\", \"b_intx\")]\n2lam_hat = exp(llam_hat)\n\nhist(lam_hat)\n\n\n\n1\n\nX \\cdot \\vec b_i gives the linear part.\n\n2\n\n\\hat\\lambda_i=e^{X \\cdot \\vec b_i} we need to apply the inverse link function\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nresid = badhealth$numvisit - lam_hat\nplot(resid) # the data were ordered\n\n\n\n\n\n\n\n\n\nthis plot looks bad, it might not be iid but we can ignore the issue since the data is presorted/\n\n\nCode\nplot(lam_hat, badhealth$numvisit)\nabline(0.0, 1.0)\n\n\n\n\n\n\n\n\nFigure 24.4\n\n\n\n\n\n\n\nCode\nplot(lam_hat[which(badhealth$badh==0)], resid[which(badhealth$badh==0)], xlim=c(0, 8), ylab=\"residuals\", xlab=expression(hat(lambda)), ylim=range(resid))\npoints(lam_hat[which(badhealth$badh==1)], resid[which(badhealth$badh==1)], col=\"red\")\n\n\n\n\n\n\n\n\nFigure 24.5\n\n\n\n\n\nIt is not surprising that the variability increases for values predicted at higher values since the mean is also the variance in the Poisson distribution. However, observations predicted to have about two visits should have variance about two, and observations predicted to have about six visits should have variance about six.\n\n\nCode\nvar(resid[which(badhealth$badh==0)])\n\n\n[1] 7.022625\n\n\n\n\nCode\nvar(resid[which(badhealth$badh==1)])\n\n\n[1] 41.19614\n\n\nFor this data the variance is much bigger this is not the case with these data. This indicates that either the model fits poorly (meaning the covariates don’t explain enough of the variability in the data), or the data are “overdispersed” for the Poisson likelihood we have chosen. This is a common issue with count data. If the data are more variable than the Poisson likelihood would suggest, a good alternative is the negative binomial distribution, which we will not pursue here.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Poisson regression</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#sec-predictive-distributions",
    "href": "C2-L10.html#sec-predictive-distributions",
    "title": "24  Poisson regression",
    "section": "24.3 Predictive distributions",
    "text": "24.3 Predictive distributions\nAssuming the model fit is adequate, we can interpret the results.\n\n\nCode\nsummary(mod_sim)\n\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean       SD  Naive SE Time-series SE\nb_age   0.008293 0.001965 1.605e-05      0.0001175\nb_badh  1.554980 0.181697 1.484e-03      0.0129844\nb_intx -0.010596 0.004184 3.416e-05      0.0003086\nint     0.354297 0.076815 6.272e-04      0.0046752\n\n2. Quantiles for each variable:\n\n            2.5%       25%       50%       75%     97.5%\nb_age   0.004454  0.006946  0.008321  0.009599  0.012136\nb_badh  1.187085  1.435767  1.557435  1.679406  1.903495\nb_intx -0.018766 -0.013363 -0.010664 -0.007846 -0.002256\nint     0.203306  0.303189  0.354098  0.407729  0.502542\n\n\nThe intercept is not necessarily interpretable here because it corresponds to the number of doctor visits for a healthy 0-year-old. While the number of visits for a newborn baby sounds like interesting information, the youngest person in the data set is 20 years old. In such cases we should avoid making such projections and say that the intercept is an artifact of the model.\n\nFor healthy individuals, it appears that age is associated with an increase in the Expected number of doctor visits.\nBad health is associated with an increase in expected number of visits.\nThe interaction coefficient is interpreted as an adjustment to the age coefficient for people in bad health. Hence, for people with bad health, age is essentially unassociated with number of visits.\n\n\n24.3.1 Predictive distributions\nLet’s say we have two people aged 35, one in good health and the other in poor health. Q. What is the posterior probability that the individual with poor health will have more doctor visits?\nThis goes beyond the posterior probabilities we have calculated comparing expected responses in previous lessons. Here we will create Monte Carlo samples for the responses themselves. This is done by taking the Monte Carlo samples of the model parameters, and for each of those, drawing a sample from the likelihood.\nLet’s walk through this.\nFirst, we need the x values for each individual. We’ll say the healthy one is Person 1 and the unhealthy one is Person 2. Their x values are:\n\n\nCode\n1x1 = c(0, 35, 0)\n2x2 = c(1, 35, 35)\n\n\n\n1\n\ngood health person’s data (bad_health_indicator=0,age=35,age*indicator=0)\n\n2\n\nbad health person’s (bad_health_indicator=1,age=35,age*indicator=35)\n\n\n\n\nThe posterior samples of the model parameters are stored in mod_csim:\n\n\nCode\nhead(mod_csim)\n\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 1 \nEnd = 7 \nThinning interval = 1 \n          b_age   b_badh       b_intx       int\n[1,] 0.01166850 1.671028 -0.013322077 0.2462753\n[2,] 0.01163804 1.669535 -0.013072131 0.2102952\n[3,] 0.01135825 1.642998 -0.012974285 0.2193505\n[4,] 0.01051171 1.622337 -0.010933384 0.2486904\n[5,] 0.01040266 1.526558 -0.008899249 0.2582633\n[6,] 0.01036078 1.445239 -0.008552249 0.2676151\n[7,] 0.01038651 1.443713 -0.007332383 0.2707102\n\n\nFirst, we’ll compute the linear part of the predictor:\n\n\nCode\nloglam1 = mod_csim[,\"int\"] + mod_csim[,c(2,1,3)] %*% x1\nloglam2 = mod_csim[,\"int\"] + mod_csim[,c(2,1,3)] %*% x2\n\n\nNext we’ll apply the inverse link:\n\n\nCode\nlam1 = exp(loglam1)\nlam2 = exp(loglam2)\n\n\nThe final step is to use these samples for the \\lambda parameter for each individual and simulate actual number of doctor visits using the likelihood:\n\n\nCode\n(n_sim = length(lam1))\n\n\n[1] 15000\n\n\nwe have distribution of 15000 samples of \\lambda for each person.\n\n\nCode\nplot(table(factor(y1, levels=0:18))/n_sim, pch=2, ylab=\"posterior prob.\", xlab=\"visits\")\npoints(table(y2+0.1)/n_sim, col=\"red\")\n\n\n\n\n\n\n\n\nFigure 24.6\n\n\n\n\n\n\n\nCode\ny1 = rpois(n=n_sim, lambda=lam1)\ny2 = rpois(n=n_sim, lambda=lam2)\n\nplot(table(factor(y1, levels=0:18))/n_sim, pch=2, ylab=\"posterior prob.\", xlab=\"visits\")\npoints(table(y2+0.1)/n_sim, col=\"red\")\n\n\n\n\n\n\n\n\n\nFinally, we can answer the original question: What is the probability that the person with poor health will have more doctor visits than the person with good health?\n\n\nCode\nmean(y2 &gt; y1)\n\n\n[1] 0.9202667\n\n\nBecause we used our posterior samples for the model parameters in our simulation (the loglam1 and loglam2 step above), this posterior predictive distribution on the number of visits for these two new individuals naturally account for our uncertainty in the model estimates. This is a more honest/realistic distribution than we would get if we had fixed the model parameters at their MLE or posterior means and simulated data for the new individuals.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Poisson regression</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#sec-prior-sensitivity-analysis",
    "href": "C2-L10.html#sec-prior-sensitivity-analysis",
    "title": "24  Poisson regression",
    "section": "24.4 Prior sensitivity analysis",
    "text": "24.4 Prior sensitivity analysis\n  When communicating results from any analysis, a responsible statistician will report and justify modeling decisions, especially assumptions. In a Bayesian analysis, there is an additional assumption that is open to scrutiny: the choices of prior distributions. In the models considered so far in this course, there are an infinite number of prior distributions we could have chosen from. When communicating results from any analysis, a responsible statistician will report and justify modeling decisions, especially assumptions. In a Bayesian analysis, there is another assumption that is open to scrutiny: the choices of prior distributions. In the models considered so far in this course, there are an infinite number of prior distributions we could have chosen from.Q. How do you justify the model you choose?\n If they truly represent your beliefs about the parameters before analysis and the model is appropriate, then the posterior distribution truly represents your updated beliefs. If you don’t have any strong beliefs beforehand, there are often default, reference, or non-informative prior options, and you will have to select one. However, a collaborator or a boss (indeed, somebody somewhere) may not agree with your choice of prior. One way to increase the credibility of your results is to repeat the analysis under a variety of priors, and report how the results differ as a result. This process is called prior sensitivity analysis. Q. How do you justify the priors you choose?\nAt a minimum you should always report your choice of model and prior. If you include a sensitivity analysis, select one or more alternative priors and describe how the results of the analysis change. If they are sensitive to the choice of prior, you will likely have to explain both sets of results, or at least explain why you favor one prior over another. If the results are not sensitive to the choice of prior, this is evidence that the data are strongly driving the results. It suggests that different investigators coming from different backgrounds should come to the same conclusions.\nIf the purpose of your analysis is to establish a hypothesis, it is often prudent to include a “skeptical” prior which does not favor the hypothesis. Then, if the posterior distribution still favors the hypothesis despite the unfavorable prior, you will be able to say that the data substantially favor the hypothesis. This is the approach we will take in the following example, continued from the previous lesson.\n\n24.4.1 Poisson regression example\n Let’s return to the example of number of doctor visits. We concluded from our previous analysis of these data that both bad health and increased age are associated with more visits. Suppose the burden of proof that bad health is actually associated with more visits rests with us, and we need to convince a skeptic.doctor visits\nFirst, let’s re-run the original analysis and remind ourselves of the posterior distribution for the badh (bad health) indicator.\n\n\nCode\nlibrary(\"COUNT\")\nlibrary(\"rjags\")\n\ndata(\"badhealth\")\n\n\n\n\nCode\nmod_string = \" model {\n    for (i in 1:length(numvisit)) {\n        numvisit[i] ~ dpois(lam[i])\n        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]\n    }\n    \n    int ~ dnorm(0.0, 1.0/1e6)\n    b_badh ~ dnorm(0.0, 1.0/1e4)\n    b_age ~ dnorm(0.0, 1.0/1e4)\n    b_intx ~ dnorm(0.0, 1.0/1e4)\n} \"\n\nset.seed(102)\n\ndata_jags = as.list(badhealth)\n\nparams = c(\"int\", \"b_badh\", \"b_age\", \"b_intx\")\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1127\n   Unobserved stochastic nodes: 4\n   Total graph size: 3665\n\nInitializing model\n\n\nCode\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,\n                        variable.names=params,\n                        n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim))\n\n\n\n\nCode\nplot(density(mod_csim[,\"b_badh\"]))\n\n\n\n\n\n\n\n\nFigure 24.7\n\n\n\n\n\nEssentially all of the posterior probability mass is above 0, suggesting that this coefficient is positive (and consequently that bad health is associated with more visits). We obtained this result using a relatively noninformative prior. What if we use a prior that strongly favors values near 0? Let’s repeat the analysis with a normal prior on the badh coefficient that has mean 0 and standard deviation 0.2, so that the prior probability that the coefficient is less than 0.6 is &gt;0.998 . We’ll also use a small variance on the prior for the interaction term involving badh (standard deviation 0.01 because this coefficient is on a much smaller scale).\n\n\nCode\nmod2_string = \" model {\n    for (i in 1:length(numvisit)) {\n        numvisit[i] ~ dpois(lam[i])\n        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]\n    }\n    \n    int ~ dnorm(0.0, 1.0/1e6)\n    b_badh ~ dnorm(0.0, 1.0/0.2^2)\n    b_age ~ dnorm(0.0, 1.0/1e4)\n    b_intx ~ dnorm(0.0, 1.0/0.01^2)\n} \"\n\nmod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1127\n   Unobserved stochastic nodes: 4\n   Total graph size: 3672\n\nInitializing model\n\n\nCode\nupdate(mod2, 1e3)\n\nmod2_sim = coda.samples(model=mod2,\n                        variable.names=params,\n                        n.iter=5e3)\nmod2_csim = as.mcmc(do.call(rbind, mod2_sim))\n\n\nHow did the posterior distribution for the coefficient of badh change?\n\n\nCode\ncurve(dnorm(x, mean=0.0, sd=sqrt(1e4)), from=-3.0, to=3.0, ylim=c(0.0, 3.0), lty=2,\n      main=\"b_badh\", ylab=\"density\", xlab=\"b_badh\")\ncurve(dnorm(x, mean=0.0, sd=0.2), from=-3.0, to=3.0, col=\"red\", lty=2, add=TRUE)\nlines(density(mod_csim[,\"b_badh\"]))\nlines(density(mod2_csim[,\"b_badh\"]), col=\"red\")\nlegend(\"topleft\", legend=c(\"noninformative prior\", \"posterior\", \"skeptical prior\", \"posterior\"),\n       lty=c(2,1,2,1), col=rep(c(\"black\", \"red\"), each=2), bty=\"n\")\n\n\n\n\n\n\n\n\nFigure 24.8\n\n\n\n\n\nUnder the skeptical prior, our posterior distribution for b_badh has significantly dropped to between about 0.6 and 1.1. Although the strong prior influenced our inference on the magnitude of the bad health effect on visits, it did not change the fact that the coefficient is significantly above 0. In other words: even under the skeptical prior, bad health is associated with more visits, with posterior probability near 1.\nWe should also check the effect of our skeptical prior on the interaction term involving both age and health.\n\n\nCode\ncurve(dnorm(x, mean=0.0, sd=sqrt(1e4)), from=-0.05, to=0.05, ylim=c(0.0, 140.0), lty=2,\n      main=\"b_intx\", ylab=\"density\", xlab=\"b_intx\")\ncurve(dnorm(x, mean=0.0, sd=0.01), from=-0.05, to=0.05, col=\"red\", lty=2, add=TRUE)\nlines(density(mod_csim[,\"b_intx\"]))\nlines(density(mod2_csim[,\"b_intx\"]), col=\"red\")\nlegend(\"topleft\", legend=c(\"noninformative prior\", \"posterior\", \"skeptical prior\", \"posterior\"),\n       lty=c(2,1,2,1), col=rep(c(\"black\", \"red\"), each=2), bty=\"n\")\n\n\n\n\n\n\n\n\nFigure 24.9\n\n\n\n\n\n\n\nCode\nmean(mod2_csim[,\"b_intx\"] &gt; 0) # posterior probability that b_intx is positive\n\n\n[1] 0.9510667\n\n\nThe result here is interesting. Our estimate for the interaction coefficient has gone from negative under the non-informative prior to positive under the skeptical prior, so the result is sensitive. In this case, because the skeptical prior shrinks away much of the bad health main effect, it is likely that this interaction effect attempts to restore some of the positive effect of bad health on visits. Thus, despite some observed prior sensitivity, our conclusion that bad health positively associates with more visits remains unchanged.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Poisson regression</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#overdispersed-model",
    "href": "C2-L10.html#overdispersed-model",
    "title": "24  Poisson regression",
    "section": "24.5 Overdispersed model",
    "text": "24.5 Overdispersed model\nRecall that the Negative Binomial can be used to model overdispersed count data.\nstan has three parameterizations for the Negative Binomial.\nThe first looks like similar to a binomial parameterization:\n\n\\text{NegBinomial}(y~|~\\alpha,\\beta)  = \\binom{y +\n\\alpha - 1}{\\alpha - 1} \\, \\left( \\frac{\\beta}{\\beta+1}\n\\right)^{\\!\\alpha} \\, \\left( \\frac{1}{\\beta + 1} \\right)^{\\!y} \\!.\n\n\n\\mathbb{E}[y] = \\frac{\\alpha}{\\beta} \\ \\ \\text{ and } \\ \\ \\text{Var}[Y] = \\frac{\\alpha}{\\beta^2} (\\beta + 1).\n\nwe can sample from this using the following statement\nn ~ neg_binomial(alpha, beta)\nBut this parameterization if not a match to the Poisson model, so we move on\nThe second parametrization \\mu \\in \\mathbb{R}^+ and \\phi \\in \\mathbb{R}^+:\n\n\\mathrm{NegBinomial2}(n \\mid \\mu, \\phi)  = \\binom{n + \\phi - 1}{n} \\,\n\\left( \\frac{\\mu}{\\mu+\\phi} \\right)^{\\!n} \\, \\left(\n\\frac{\\phi}{\\mu+\\phi} \\right)^{\\!\\phi}\n\n\n\\mathbb{E}[n] = \\mu \\ \\ \\text{ and } \\ \\ \\ \\mathbb{V}\\text{ar}[n] = \\mu + \\frac{\\mu^2}{\\phi}\n\nwe can sample from this using the following statement\nn ~ neg_binomial_2(mu, phi)\nAnd there is a third parametrization\n\nNegBinomial2Log(y\\mid\\mu,\\phi) = NegBinomial2(y\\mid exp(\\eta),\\phi).\n\nwe can sample from this using the following statement:\ny \\~ **neg_binomial_2\\_log**(y\\|mu, phi)`\njags has just one parameterization:\n\nf(y \\mid r, p) = \\frac{\\Gamma(y+r)}{\\Gamma(r)\\Gamma(y+1)}p^r(1-p)^y\n\nWe think of the Negative Binomial Distribution as the probability of completing y successful trials allowing for r failures in a sequence of (y+r) Bernoulli trials where success is defined as drawing (with replacement) a white ball from an urn of white and black balls with a probability p of success.\n\n\\mathbb{E}[Y] = \\mu = { r(1-p) \\over p } \\qquad \\text{ and } \\qquad \\mathbb{V}\\text{ar}[Y] = \\mu + \\frac{\\mu^2}{r}\n\n\n24.5.1 Transformations:\nSince we want to have a model corresponding to a poisson regression we will transform the model as follows:\nIf we set p = {\\text{r} \\over {r} + \\lambda } then the mean becomes : \\lambda\nand if we also set r= {\\lambda^2 \\over \\omega} then the variance becomes a sum of \\lambda + \\omega where \\omega is our over dispersion term.\n\n\\omega = \\lambda^2 / r\n\n\n\\begin{aligned}\n\\mathbb{E}[Y] &= { r(1-p) \\over p }\n\\\\ &= rp^{-1} -r\n\\\\ & \\stackrel {sub\\ p} =  {\\cancel{r}(\\bcancel{r}+\\lambda) \\over \\cancel{r}} - \\bcancel{r}\n\\\\ &= \\lambda \\mathbb{V}\\text{ar}[Y]\n\\\\ &= { (1-p) r \\over p^2 }\n\\\\ & \\stackrel {sub \\ \\lambda } = {1 \\over p} \\lambda\n\\\\ & \\stackrel {sub p} = \\lambda { (r+ \\lambda) \\over r}\n\\\\ &=  {\\lambda r +  \\lambda^2 \\over r }\n\\\\ &= 1 \\lambda + {\\lambda^2 \\over r}\n\\\\ &\\stackrel { sub \\ \\omega}= \\lambda + \\omega\n\\end{aligned}\n\nWhere we interpret \\lambda as the mean and \\omega as the overdispersion \n\n\nCode\nlibrary(\"rjags\")\nlibrary(\"COUNT\")\ndata(\"badhealth\")\n\n\n\n\nCode\nmod3_string = \"\nmodel {\n    for (i in 1:length(numvisit)) { \n1        mu[i]       = b0 + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]\n2        lambda[i]   = exp(mu[i])\n3        p[i]        = r / (r + lambda[i])\n4        numvisit[i] ~ dnegbin(p[i], r)\n5        resid[i]       = numvisit[i] - p[i]\n    }\n    ## Priors\n6    b0        ~ dnorm(0.0, 1.0/1e6)\n7    b_badh    ~ dnorm(0.0, 1.0/0.2^2)\n8    b_age     ~ dnorm(0.0, 1.0/1e4)\n9    b_intx    ~ dnorm(0.0, 1.0/0.01^2)\n10    r ~ dunif(0,50)\n\n    ## extra deterministic parameters\n    omega      &lt;-  pow(mean(lambda),2)/2\n11    #theta      &lt;- pow(1/mean(p),2)\n12    #scale      &lt;- mean((1-p)/p)\n}\"\ndata3_jags = as.list(badhealth)\nmod3 = jags.model(textConnection(mod3_string), data=data3_jags, n.chains=3)\n13update(mod3, 1e3)\nparams3 = c(\"b_intx\", \"b_badh\", \"b_age\", 'over_disp', 'b0','omega','r')\n14mod3_sim = coda.samples(model=mod3,  variable.names=params3, n.iter=5e3)\n15mod3_csim = as.mcmc(do.call(rbind, mod3_sim))\n16(dic3 = dic.samples(mod3, n.iter=1e3))\n\n\n\n1\n\nthe linear part\n\n2\n\nlambda corresponds to the parameter used in the Poisson regression\n\n3\n\np is the success parameter\n\n4\n\nwe draw from the negative binomial distribution\n\n5\n\nsampling using the parametrization of the Negative Binomial distribution.\n\n6\n\nnormal prior for intercept b0\n\n7\n\nnormal prior for b_badh\n\n8\n\nnormal prior for b_age\n\n9\n\nnormal prior for b_intx\n\n10\n\nuniform prior for over_disp - at the upper limit of 50 NegBin converges to Poisson see (Jackman 2009, 280)\n\n11\n\ntheta param\n\n12\n\nscale param\n\n13\n\nburn in\n\n14\n\nsample\n\n15\n\nstack samples from the chains\n\n16\n\nestimate the DIC\n\n\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1127\n   Unobserved stochastic nodes: 5\n   Total graph size: 4204\n\nInitializing model\n\nMean deviance:  4478 \npenalty 4.072 \nPenalized deviance: 4482 \n\n\n\n\nCode\ngelman.diag(mod3_sim )\n\n\nPotential scale reduction factors:\n\n       Point est. Upper C.I.\nb0           1.01       1.03\nb_age        1.01       1.03\nb_badh       1.01       1.02\nb_intx       1.00       1.01\nomega        1.00       1.00\nr            1.00       1.00\n\nMultivariate psrf\n\n1.01\n\n\n\n\nCode\nraftery.diag(mod3_sim)\n\n\n[[1]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                              \n        Burn-in  Total Lower bound  Dependence\n        (M)      (N)   (Nmin)       factor (I)\n b0     48       48032 3746         12.800    \n b_age  27       29031 3746          7.750    \n b_badh 9        9308  3746          2.480    \n b_intx 8        8602  3746          2.300    \n omega  2        3741  3746          0.999    \n r      5        6185  3746          1.650    \n\n\n[[2]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                              \n        Burn-in  Total Lower bound  Dependence\n        (M)      (N)   (Nmin)       factor (I)\n b0     40       38235 3746         10.20     \n b_age  21       22956 3746          6.13     \n b_badh 16       14964 3746          3.99     \n b_intx 7        7675  3746          2.05     \n omega  3        4062  3746          1.08     \n r      5        6078  3746          1.62     \n\n\n[[3]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                              \n        Burn-in  Total Lower bound  Dependence\n        (M)      (N)   (Nmin)       factor (I)\n b0     33       36540 3746         9.75      \n b_age  28       27970 3746         7.47      \n b_badh 9        9497  3746         2.54      \n b_intx 10       11234 3746         3.00      \n omega  2        3803  3746         1.02      \n r      4        4955  3746         1.32      \n\n\n\n\nCode\nautocorr.diag(mod3_sim)\n\n\n              b0     b_age      b_badh      b_intx        omega           r\nLag 0  1.0000000 1.0000000  1.00000000  1.00000000  1.000000000 1.000000000\nLag 1  0.9353475 0.9388936  0.77623048  0.78475391  0.008166432 0.253348873\nLag 5  0.7525400 0.7549795  0.34558544  0.35362456 -0.003371287 0.003942014\nLag 10 0.5772155 0.5768298  0.12879323  0.13563268  0.010477006 0.001784702\nLag 50 0.0500231 0.0559672 -0.01879399 -0.01592171 -0.002914743 0.020807598\n\n\n\n\nCode\neffectiveSize(mod3_sim)\n\n\n        b0      b_age     b_badh     b_intx      omega          r \n  416.6925   415.6010  1564.8925  1556.5188 16290.4982  8809.7537 \n\n\n\n\nCode\nsummary(mod3_sim)\n\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean       SD  Naive SE Time-series SE\nb0     0.477486 0.127084 1.038e-03      0.0062531\nb_age  0.005385 0.003300 2.695e-05      0.0001621\nb_badh 0.380941 0.166621 1.360e-03      0.0042089\nb_intx 0.014844 0.004324 3.531e-05      0.0001098\nomega  2.777912 0.217878 1.779e-03      0.0017088\nr      0.989728 0.069628 5.685e-04      0.0007429\n\n2. Quantiles for each variable:\n\n            2.5%      25%      50%      75%   97.5%\nb0      0.229533 0.390556 0.475626 0.564281 0.72471\nb_age  -0.001077 0.003151 0.005424 0.007588 0.01187\nb_badh  0.054162 0.268074 0.383434 0.495012 0.69961\nb_intx  0.006663 0.011850 0.014782 0.017772 0.02347\nomega   2.379396 2.627271 2.767154 2.918075 3.23263\nr       0.861347 0.942159 0.987243 1.034223 1.13210\n\n\n\nCode\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod3_sim, auto.layout = FALSE)\n\n\n\n\n\n\n\n\n\n\nFigure 24.10\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.11\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.12\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.13\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.14\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.15\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.16\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.18\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.19\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.20\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.21\n\n\n\n\n\n\n\nCode\nautocorr.plot(mod3_csim,auto.layout = FALSE)\n\n\n\n\n\n\n\n\n\n\nFigure 24.22\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.23\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.24\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.25\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.26\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 24.27\n\n\n\n\n\n\n\n\nCode\nX = as.matrix(badhealth[,-1])\nX = cbind(X, with(badhealth, badh*age))\n(pmed_coef = apply(mod3_csim, 2, median))\n\n\n         b0       b_age      b_badh      b_intx       omega           r \n0.475626057 0.005424455 0.383434113 0.014782455 2.767154337 0.987242999 \n\n\nCode\n(r = pmed_coef[\"r\"] )\n\n\n       r \n0.987243 \n\n\nCode\nmu_hat = pmed_coef[\"b0\"] + X %*% pmed_coef[c(\"b_badh\", \"b_age\", \"b_intx\")]\nlambda_hat = exp(mu_hat)\np_hat = r / (r + lambda_hat)\nhist(lambda_hat)\n\n\n\n\n\n\n\n\n\nCode\nhist(p_hat)\n\n\n\n\n\n\n\n\n\nresiduals\n\n\nCode\nresid = badhealth$numvisit - p_hat\nhead(resid)\n\n\n         [,1]\n[1,] 29.69063\n[2,] 19.68598\n[3,] 15.67418\n[4,] 19.68947\n[5,] 14.66094\n[6,] 14.65483\n\n\n\n\nCode\nplot(resid) # the data were ordered\n\n\n\n\n\n\n\n\nFigure 24.28: Plot of residuals\n\n\n\n\n\n\nCode\nhead(mod3_csim)\n\n\n\n\nTable 24.1: First few rows of mod3_csim\n\n\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 1 \nEnd = 7 \nThinning interval = 1 \n            b0       b_age     b_badh     b_intx    omega         r\n[1,] 0.4750218 0.005754781 0.11911632 0.01899195 2.731221 0.9416937\n[2,] 0.4361648 0.005685571 0.24015770 0.01766072 2.586115 1.0204129\n[3,] 0.5495783 0.006180571 0.16148238 0.01898878 3.345023 0.9823598\n[4,] 0.5348564 0.003527329 0.06008378 0.02212811 2.693255 1.0079310\n[5,] 0.5629573 0.001806209 0.04938176 0.02633532 2.740525 1.0413283\n[6,] 0.5571392 0.003235151 0.10177277 0.02502712 3.018429 1.0583014\n[7,] 0.5398431 0.003661378 0.11241380 0.02423512 2.972390 1.0355894\n\n\n\n\n\n\nCode\nplot(p_hat, badhealth$numvisit)\nabline(0.0, 1.0)\n\n\n\n\n\n\n\n\nFigure 24.29: Plot of p_hat vs numvisit\n\n\n\n\n\n\n\nCode\nplot(p_hat[which(badhealth$badh==0)], resid[which(badhealth$badh==0)], xlim=range(p_hat), ylab=\"residuals\", xlab=expression(hat(p)), ylim=range(resid))\npoints(p_hat[which(badhealth$badh==1)], resid[which(badhealth$badh==1)], col=\"red\")\n\n\n\n\n\n\n\n\nFigure 24.30: Plot of p_hat vs residuals, colored by health status\n\n\n\n\n\n\n\nCode\nvar(resid[which(badhealth$badh==0)])\n\n\n[1] 7.061\n\n\nCode\nvar(resid[which(badhealth$badh==1)])\n\n\n[1] 41.21256\n\n\n\n\n\n\n\n\nJackman, Simon. 2009. “Bayesian Analysis for the Social Sciences.” Wiley Series in Probability and Statistics, October. https://doi.org/10.1002/9780470686621.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Poisson regression</span>"
    ]
  },
  {
    "objectID": "C2-L11.html",
    "href": "C2-L11.html",
    "title": "25  Hierarchical modeling",
    "section": "",
    "text": "25.1 Introduction to Hierarchical modeling",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#introduction-to-hierarchical-modeling",
    "href": "C2-L11.html#introduction-to-hierarchical-modeling",
    "title": "25  Hierarchical modeling",
    "section": "",
    "text": "Introduction to Hierarchical modeling",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#normal-hierarchical-model",
    "href": "C2-L11.html#normal-hierarchical-model",
    "title": "25  Hierarchical modeling",
    "section": "25.2 Normal hierarchical model",
    "text": "25.2 Normal hierarchical model\nHandout: Normal hierarchical model",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#applications-of-hierarchical-modeling",
    "href": "C2-L11.html#applications-of-hierarchical-modeling",
    "title": "25  Hierarchical modeling",
    "section": "25.3 Applications of hierarchical modeling",
    "text": "25.3 Applications of hierarchical modeling\nHandout: Common applications of Bayesian hierarchical models",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#prior-predictive-simulation",
    "href": "C2-L11.html#prior-predictive-simulation",
    "title": "25  Hierarchical modeling",
    "section": "25.4 Prior predictive simulation",
    "text": "25.4 Prior predictive simulation\n\n25.4.1 Data\nLet’s fit our hierarchical model for counts of chocolate chips. The data can be found in cookies.dat.\n\n\nCode\ndat = read.table(file=\"data/cookies.dat\", header=TRUE)\nhead(dat)\n\n\n  chips location\n1    12        1\n2    12        1\n3     6        1\n4    13        1\n5    12        1\n6    12        1\n\n\n\n\nCode\ntable(dat$location)\n\n\n\n 1  2  3  4  5 \n30 30 30 30 30 \n\n\nWe can also visualize the distribution of chips by location.\n\n\nCode\nhist(dat$chips)\n\n\n\n\n\n\n\n\n\n\n\nCode\nboxplot(chips ~ location, data=dat)\n\n\n\n\n\n\n\n\n\n\n\n25.4.2 Prior predictive checks\nBefore implementing the model, we need to select prior distributions for \\alpha and \\beta, the hyperparameters governing the gamma distribution for the \\lambda parameters. First, think about what the \\lambda’s represent. For location j, \\lambda_j is the expected number of chocolate chips per cookie. Hence, \\alpha and \\beta control the distribution of these means between locations. The mean of this gamma distribution will represent the overall mean of number of chips for all cookies. The variance of this gamma distribution controls the variability between locations. If this is high, the mean number of chips will vary widely from location to location. If it is small, the mean number of chips will be nearly the same from location to location.\nTo see the effects of different priors on the distribution of \\lambda’s, we can simulate. Suppose we try independent exponential priors for \\alpha and \\beta.\n\n\nCode\nset.seed(112)\nn_sim = 500\nalpha_pri = rexp(n_sim, rate=1.0/2.0)\nbeta_pri = rexp(n_sim, rate=5.0)\nmu_pri = alpha_pri/beta_pri\nsig_pri = sqrt(alpha_pri/beta_pri^2)\n\nsummary(mu_pri)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.0213    2.9829    9.8522   61.1271   29.9801 4858.7861 \n\n\n\n\nCode\nsummary(sig_pri)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.1834    3.3663    8.5488   41.8137   22.2219 2865.6461 \n\n\nAfter simulating from the priors for \\alpha and \\beta, we can use those samples to simulate further down the hierarchy:\n\n\nCode\nlam_pri = rgamma(n=n_sim, shape=alpha_pri, rate=beta_pri)\nsummary(lam_pri)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n    0.000     1.171     7.668    83.062    28.621 11005.331 \n\n\nOr for a prior predictive reconstruction of the original data set:\n\n\nCode\n(lam_pri = rgamma(n=5, shape=alpha_pri[1:5], rate=beta_pri[1:5]))\n\n\n[1] 66.444084  9.946688  6.028319 15.922568 47.978587\n\n\n\n\nCode\n(y_pri = rpois(n=150, lambda=rep(lam_pri, each=30)))\n\n\n  [1] 63 58 64 63 70 62 61 48 71 73 70 77 66 60 72 77 69 62 66 71 49 80 66 75 74\n [26] 55 62 90 65 57 12  9  7 10 12 10 11  7 14 13  9  6  6 13  7 10 12  9  9 10\n [51]  7  8  6  9  7 10 13 13  8 12  6 10  3  6  7  4  6  7  5  5  4  3  6  2  8\n [76]  4  8  4  5  7  1  4  5  3  8  8  3  1  7  3 16 14 13 17 17 12 13 13 16 16\n[101] 15 14 11 10 13 17 16 19 16 17 15 16  7 17 21 16 12 15 14 13 52 44 51 46 39\n[126] 40 40 44 46 59 45 49 58 42 31 52 43 47 53 41 48 57 35 60 51 58 36 34 41 59\n\n\nBecause these priors have high variance and are somewhat noninformative, they produce unrealistic predictive distributions. Still, enough data would overwhelm the prior, resulting in useful posterior distributions. Alternatively, we could tweak and simulate from these prior distributions until they adequately represent our prior beliefs. Yet another approach would be to re-parameterize the gamma prior, which we’ll demonstrate as we fit the model.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#jags-model",
    "href": "C2-L11.html#jags-model",
    "title": "25  Hierarchical modeling",
    "section": "25.5 JAGS Model",
    "text": "25.5 JAGS Model\n\n\nCode\nlibrary(\"rjags\")\n\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\n\n\n\nCode\nmod_string = \" model {\nfor (i in 1:length(chips)) {\n  chips[i] ~ dpois(lam[location[i]])\n}\n\nfor (j in 1:max(location)) {\n  lam[j] ~ dgamma(alpha, beta)\n}\n\nalpha = mu^2 / sig^2\nbeta = mu / sig^2\n\nmu ~ dgamma(2.0, 1.0/5.0)\nsig ~ dexp(1.0)\n\n} \"\n\nset.seed(113)\n\ndata_jags = as.list(dat)\n\nparams = c(\"lam\", \"mu\", \"sig\")\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 150\n   Unobserved stochastic nodes: 7\n   Total graph size: 315\n\nInitializing model\n\n\nCode\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,\n                       variable.names=params,\n                       n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim))\n\n\n\n\nCode\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngelman.diag(mod_sim)\n\n\nPotential scale reduction factors:\n\n       Point est. Upper C.I.\nlam[1]          1       1.00\nlam[2]          1       1.00\nlam[3]          1       1.00\nlam[4]          1       1.00\nlam[5]          1       1.00\nmu              1       1.00\nsig             1       1.01\n\nMultivariate psrf\n\n1\n\n\nCode\nautocorr.diag(mod_sim)\n\n\n             lam[1]       lam[2]      lam[3]      lam[4]        lam[5]\nLag 0   1.000000000  1.000000000 1.000000000 1.000000000  1.000000e+00\nLag 1   0.017475326  0.103851615 0.020847955 0.016108879  6.895898e-02\nLag 5   0.010694008 -0.005831616 0.002908255 0.009488679  5.012006e-03\nLag 10  0.011432512 -0.011613005 0.005283840 0.006077495 -6.003798e-05\nLag 50 -0.005996549 -0.005425876 0.003709879 0.006672658 -6.543832e-03\n                mu        sig\nLag 0  1.000000000  1.0000000\nLag 1  0.392438974  0.5839446\nLag 5  0.019631588  0.1296995\nLag 10 0.005413882  0.0239028\nLag 50 0.002486821 -0.0021701\n\n\n\nCode\npar(mar = c(2.5, 1, 2.5, 1))\nautocorr.plot(mod_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\neffectiveSize(mod_sim)\n\n\n   lam[1]    lam[2]    lam[3]    lam[4]    lam[5]        mu       sig \n14166.123 11534.514 12999.514 14626.871 12867.839  6084.412  3180.606 \n\n\nCode\n## compute DIC\ndic = dic.samples(mod, n.iter=1e3)\n\n\n\n25.5.1 Model checking\nAfter assessing convergence, we can check the fit via residuals. With a hierarhcical model, there are now two levels of residuals: the observation level and the location mean level. To simplify, we’ll look at the residuals associated with the posterior means of the parameters.\nFirst, we have observation residuals, based on the estimates of location means.\n\n\nCode\n## observation level residuals\n(pm_params = colMeans(mod_csim))\n\n\n   lam[1]    lam[2]    lam[3]    lam[4]    lam[5]        mu       sig \n 9.280912  6.218495  9.522481  8.950237 11.767102  9.104528  2.091862 \n\n\n\n\nCode\nyhat = rep(pm_params[1:5], each=30)\nresid = dat$chips - yhat\nplot(resid)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(jitter(yhat), resid)\n\n\n\n\n\n\n\n\n\n\n\nCode\nvar(resid[yhat&lt;7])\n\n\n[1] 6.447126\n\n\n\n\nCode\nvar(resid[yhat&gt;11])\n\n\n[1] 13.72414\n\n\nAlso, we can look at how the location means differ from the overall mean \\mu.\n\n\nCode\n## location level residuals\nlam_resid = pm_params[1:5] - pm_params[\"mu\"]\nplot(lam_resid)\nabline(h=0, lty=2)\n\n\n\n\n\n\n\n\n\nWe don’t see any obvious violations of our model assumptions.\n\n\n25.5.2 Results\n\n\nCode\nsummary(mod_sim)\n\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n         Mean     SD Naive SE Time-series SE\nlam[1]  9.281 0.5302 0.004329       0.004460\nlam[2]  6.218 0.4615 0.003768       0.004323\nlam[3]  9.522 0.5491 0.004483       0.004840\nlam[4]  8.950 0.5291 0.004320       0.004378\nlam[5] 11.767 0.6209 0.005070       0.005475\nmu      9.105 0.9881 0.008068       0.012678\nsig     2.092 0.7175 0.005859       0.012743\n\n2. Quantiles for each variable:\n\n         2.5%    25%    50%    75%  97.5%\nlam[1]  8.270  8.910  9.271  9.637 10.350\nlam[2]  5.348  5.904  6.208  6.522  7.146\nlam[3]  8.477  9.144  9.514  9.885 10.633\nlam[4]  7.941  8.586  8.943  9.303 10.013\nlam[5] 10.576 11.342 11.751 12.183 13.014\nmu      7.228  8.480  9.067  9.684 11.212\nsig     1.101  1.591  1.952  2.444  3.855",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#posterior-predictive-simulation",
    "href": "C2-L11.html#posterior-predictive-simulation",
    "title": "25  Hierarchical modeling",
    "section": "25.6 Posterior predictive simulation",
    "text": "25.6 Posterior predictive simulation\nJust as we did with the prior distribution, we can use these posterior samples to get Monte Carlo estimates that interest us from the posterior predictive distribution.\nFor example, we can use draws from the posterior distribution of \\mu and \\sigma to simulate the posterior predictive distribution of the mean for a new location.\n\n\nCode\n(n_sim = nrow(mod_csim))\n\n\n[1] 15000\n\n\n\n\nCode\nlam_pred = rgamma(n=n_sim, shape=mod_csim[,\"mu\"]^2/mod_csim[,\"sig\"]^2, \n                  rate=mod_csim[,\"mu\"]/mod_csim[,\"sig\"]^2)\nhist(lam_pred)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmean(lam_pred &gt; 15)\n\n\n[1] 0.01806667\n\n\nUsing these \\lambda draws, we can go to the observation level and simulate the number of chips per cookie, which takes into account the uncertainty in \\lambda:\n\n\nCode\ny_pred = rpois(n=n_sim, lambda=lam_pred)\nhist(y_pred)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmean(y_pred &gt; 15)\n\n\n[1] 0.0602\n\n\n\n\nCode\nhist(dat$chips)\n\n\n\n\n\n\n\n\n\nFinally, we could answer questions like: what is the posterior probability that the next cookie produced in Location 1 will have fewer than seven chips?\n\n\nCode\ny_pred1 = rpois(n=n_sim, lambda=mod_csim[,\"lam[1]\"])\nhist(y_pred1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmean(y_pred1 &lt; 7)\n\n\n[1] 0.1883333",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#random-intercept-linear-model",
    "href": "C2-L11.html#random-intercept-linear-model",
    "title": "25  Hierarchical modeling",
    "section": "25.7 Random intercept linear model",
    "text": "25.7 Random intercept linear model\nWe can extend the linear model for the Leinhardt data on infant mortality by incorporating the region variable. We’ll do this with a hierarhcical model, where each region has its own intercept.\n\n\nCode\nlibrary(\"car\")\n\n\nLoading required package: carData\n\n\nCode\ndata(\"Leinhardt\")\n?Leinhardt\nstr(Leinhardt)\n\n\n'data.frame':   105 obs. of  4 variables:\n $ income: int  3426 3350 3346 4751 5029 3312 3403 5040 2009 2298 ...\n $ infant: num  26.7 23.7 17 16.8 13.5 10.1 12.9 20.4 17.8 25.7 ...\n $ region: Factor w/ 4 levels \"Africa\",\"Americas\",..: 3 4 4 2 4 4 4 4 4 4 ...\n $ oil   : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nCode\npairs(Leinhardt)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhead(Leinhardt)\n\n\n          income infant   region oil\nAustralia   3426   26.7     Asia  no\nAustria     3350   23.7   Europe  no\nBelgium     3346   17.0   Europe  no\nCanada      4751   16.8 Americas  no\nDenmark     5029   13.5   Europe  no\nFinland     3312   10.1   Europe  no\n\n\nPreviously, we worked with infant mortality and income on the logarithmic scale. Recall also that we had to remove some missing data.\n\n\nCode\ndat = na.omit(Leinhardt)\ndat$logincome = log(dat$income)\ndat$loginfant = log(dat$infant)\nstr(dat)\n\n\n'data.frame':   101 obs. of  6 variables:\n $ income   : int  3426 3350 3346 4751 5029 3312 3403 5040 2009 2298 ...\n $ infant   : num  26.7 23.7 17 16.8 13.5 10.1 12.9 20.4 17.8 25.7 ...\n $ region   : Factor w/ 4 levels \"Africa\",\"Americas\",..: 3 4 4 2 4 4 4 4 4 4 ...\n $ oil      : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ logincome: num  8.14 8.12 8.12 8.47 8.52 ...\n $ loginfant: num  3.28 3.17 2.83 2.82 2.6 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:4] 24 83 86 91\n  ..- attr(*, \"names\")= chr [1:4] \"Iran\" \"Haiti\" \"Laos\" \"Nepal\"\n\n\nNow we can fit the proposed model:\n\n\nCode\nlibrary(\"rjags\")\n\nmod_string = \" model {\n  for (i in 1:length(y)) {\n    y[i] ~ dnorm(mu[i], prec)\n    mu[i] = a[region[i]] + b[1]*log_income[i] + b[2]*is_oil[i]\n  }\n  \n  for (j in 1:max(region)) {\n    a[j] ~ dnorm(a0, prec_a)\n  }\n  \n  a0 ~ dnorm(0.0, 1.0/1.0e6)\n  prec_a ~ dgamma(1/2.0, 1*10.0/2.0)\n  tau = sqrt( 1.0 / prec_a )\n  \n  for (j in 1:2) {\n    b[j] ~ dnorm(0.0, 1.0/1.0e6)\n  }\n  \n  prec ~ dgamma(5/2.0, 5*10.0/2.0)\n  sig = sqrt( 1.0 / prec )\n} \"\n\nset.seed(116)\ndata_jags = list(y=dat$loginfant, log_income=dat$logincome,\n                  is_oil=as.numeric(dat$oil==\"yes\"), region=as.numeric(dat$region))\ndata_jags$is_oil\n\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nCode\ntable(data_jags$is_oil, data_jags$region)\n\n\n   \n     1  2  3  4\n  0 31 20 24 18\n  1  3  2  3  0\n\n\nCode\nparams = c(\"a0\", \"a\", \"b\", \"sig\", \"tau\")\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 101\n   Unobserved stochastic nodes: 9\n   Total graph size: 622\n\nInitializing model\n\n\nCode\nupdate(mod, 1e3) # burn-in\n\nmod_sim = coda.samples(model=mod,\n                       variable.names=params,\n                       n.iter=5e3)\n\nmod_csim = as.mcmc(do.call(rbind, mod_sim)) # combine multiple chains\n\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngelman.diag(mod_sim)\n\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\na[1]       1.04       1.09\na[2]       1.04       1.09\na[3]       1.03       1.07\na[4]       1.04       1.09\na0         1.01       1.02\nb[1]       1.04       1.09\nb[2]       1.00       1.00\nsig        1.00       1.00\ntau        1.00       1.00\n\nMultivariate psrf\n\n1.02\n\n\nCode\nautocorr.diag(mod_sim)\n\n\n            a[1]      a[2]      a[3]      a[4]         a0      b[1]       b[2]\nLag 0  1.0000000 1.0000000 1.0000000 1.0000000 1.00000000 1.0000000 1.00000000\nLag 1  0.9103546 0.9115277 0.9071705 0.9282080 0.23027347 0.9770293 0.13981898\nLag 5  0.8263392 0.8292656 0.8269382 0.8445628 0.20871927 0.8894706 0.04118506\nLag 10 0.7357214 0.7365382 0.7361989 0.7496506 0.18374203 0.7907846 0.02727384\nLag 50 0.2991356 0.3026399 0.2963184 0.3004492 0.07096728 0.3193234 0.02331449\n                 sig          tau\nLag 0   1.0000000000  1.000000000\nLag 1   0.0425530031  0.282941154\nLag 5   0.0036055385  0.002452563\nLag 10 -0.0002079002 -0.001994740\nLag 50  0.0099965893 -0.006020321\n\n\nCode\nautocorr.plot(mod_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\neffectiveSize(mod_sim)\n\n\n      a[1]       a[2]       a[3]       a[4]         a0       b[1]       b[2] \n  192.4847   206.2155   202.9092   196.2619   878.5352   174.3698  8457.9923 \n       sig        tau \n13020.1364  8505.6162 \n\n\n\n25.7.1 Results\nConvergence looks okay, so let’s compare this with the old model from Lesson 7 using DIC:\n\n\nCode\ndic.samples(mod, n.iter=1e3)\n\n\nMean deviance:  213.6 \npenalty 6.899 \nPenalized deviance: 220.5 \n\n\nCode\n### nonhierarchical model: 230.1\n\n\nIt appears that this model is an improvement over the non-hierarchical one we fit earlier. Notice that the penalty term, which can be interpreted as the “effective” number of parameters, is less than the actual number of parameters (nine). There are fewer “effective” parameters because they are “sharing” information or “borrowing strength” from each other in the hierarhical structure. If we had skipped the hierarchy and fit one intercept, there would have been four parameters. If we had fit separate, independent intercepts for each region, there would have been seven parameters (which is close to what we ended up with).\nFinally, let’s look at the posterior summary.\n\n\nCode\nsummary(mod_sim)\n\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean      SD  Naive SE Time-series SE\na[1]  6.5156 0.53680 0.0043830      0.0413942\na[2]  5.9640 0.67682 0.0055262      0.0507340\na[3]  5.8099 0.59925 0.0048928      0.0452623\na[4]  5.4833 0.82131 0.0067060      0.0629274\na0    5.9393 1.32152 0.0107902      0.0485766\nb[1] -0.3343 0.10168 0.0008302      0.0080968\nb[2]  0.6404 0.35125 0.0028679      0.0039938\nsig   0.9193 0.06552 0.0005350      0.0005758\ntau   2.0499 1.06591 0.0087031      0.0115791\n\n2. Quantiles for each variable:\n\n         2.5%     25%     50%     75%   97.5%\na[1]  5.48508  6.1544  6.5179  6.8661  7.5956\na[2]  4.65675  5.5078  5.9605  6.4015  7.3368\na[3]  4.65422  5.4017  5.8107  6.1985  7.0150\na[4]  3.89843  4.9272  5.4787  6.0236  7.1501\na0    3.34482  5.1757  5.9444  6.7026  8.4672\nb[1] -0.53954 -0.4002 -0.3343 -0.2654 -0.1364\nb[2] -0.05387  0.4028  0.6459  0.8762  1.3316\nsig   0.80235  0.8738  0.9152  0.9604  1.0589\ntau   0.97881  1.4119  1.7834  2.3568  4.7475\n\n\nIn this particular model, the intercepts do not have a real interpretation because they correspond to the mean response for a country that does not produce oil and has $0 log-income per capita (which is $1 income per capita). We can interpret a_0 as the overall mean intercept and \\tau as the standard deviation of intercepts across regions.\n\n\n25.7.2 Other models\nWe have not investigated adding interaction terms, which might be appropriate. We only considered adding hierarchy on the intercepts, but in reality nothing prevents us from doing the same for other terms in the model, such as the coefficients for income and oil. We could try any or all of these alternatives and see how the DIC changes for those models. This, together with other model checking techniques we have discussed could be used to identify your best model that you can use to make inferences and predictions.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#mixture-models",
    "href": "C2-L11.html#mixture-models",
    "title": "25  Hierarchical modeling",
    "section": "25.8 Mixture models",
    "text": "25.8 Mixture models\nHistograms of data often reveal that they do not follow any standard probability distribution. Sometimes we have explanatory variables (or covariates) to account for the different values, and normally distributed errors are adequate, as in normal regression. However, if we only have the data values themselves and no covariates, we might have to fit a non-standard distribution to the data. One way to do this is by mixing standard distributions.\nMixture distributions are just a weighted combination of probability distributions. For example, we could take an exponential distribution with mean 1 and normal distribution with mean 3 and variance 1 (although typically the two mixture components would have the same support; here the exponential component has to be non-negative and the normal component can be positive or negative). Suppose we give them weights: 0.4 for the exponential distribution and 0.6 for the normal distribution. We could write the PDF for this distribution as\n\n\\mathbb{P}r(y) = 0.4 \\cdot \\exp(-y) \\cdot \\mathbb{I}_{(y \\ge 0)} + 0.6 \\cdot \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(- \\frac{1}{2} (y - 3)^2\\right)\n\nThe PDF of this mixture distribution would look like this:\n\n\nCode\ncurve( 0.4*dexp(x, 1.0) + 0.6*dnorm(x, 3.0, 1.0), from=-2.0, to=7.0, ylab=\"density\", xlab=\"y\", main=\"40/60 mixture of exponential and normal distributions\", lwd=2)\n\n\n\n\n\n\n\n\n\nWe could think of these two distributions as governing two distinct populations, one following the exponential distribution and the other following the normal distribution.\nLet’s draw the weighted PDFs for each population.\n\n\nCode\ncurve( 0.4*dexp(x, 1.0) + 0.6*dnorm(x, 3.0, 1.0), from=-2.0, to=7.0, ylab=\"density\", xlab=\"y\", main=\"40/60 mixture of exponential and normal distributions\", lwd=2)\ncurve( 0.4*dexp(x, 1.0), from=-2.0, to=7.0, col=\"red\", lty=2, add=TRUE)\ncurve( 0.6*dnorm(x, 3.0, 1.0), from=-2.0, to=7.0, col=\"blue\", lty=2, add=TRUE)\n\n\n\n\n\n\n\n\n\nThe general form for a discrete mixture of distributions is as follows:\n\n\\mathbb{P}r(y) = \\sum_{j=1}^J \\omega_j \\cdot f_j (y)\n\nwhere the \\omega’s are positive weights that add up to 1 (they are probabilities) and each of the J f_j(y) functions is a PDF for some distribution. In the example above, the weights were 0.4 and 0.6, f_1 was an exponential PDF and f_2 was a normal PDF.\nOne way to simulate from a mixture distribution is with a hierarchical model. We first simulate an indicator for which “population” the next observation will come from using the weights \\omega. Let’s call this z_i. In the example above, z_i would take the value 1 (indicating the exponential distribution) with probability 0.4 and 2 (indicating the normal distribution) with probability 0.6. Next, simulate the observation y_i from the distribution corresponding to z_i.\nLet’s simulate from our example mixture distribution.\n\n\nCode\nset.seed(117)\nn = 1000\nz = numeric(n)\ny = numeric(n)\nfor (i in 1:n) {\n  z[i] = sample.int(2, 1, prob=c(0.4, 0.6)) # returns a 1 with probability 0.4, or a 2 with probability 0.6\n  if (z[i] == 1) {\n    y[i] = rexp(1, rate=1.0)\n  } else if (z[i] == 2) {\n    y[i] = rnorm(1, mean=3.0, sd=1.0)\n  }\n}\nhist(y, breaks=30)\n\n\n\n\n\n\n\n\n\nIf we keep only the y values and throw away the z values, we have a sample from the mixture model above. To see that they are equivalent, we can marginalize the joint distribution of y and z:\n\n\\mathbb{P}r(y) = \\sum_{j=1}^2 \\mathbb{P}r(y, z=j) = \\sum_{j=1}^2 \\mathbb{P}r(z=j) \\cdot \\mathbb{P}r(y \\mid z=j) = \\sum_{j=1}^2 \\omega_j \\cdot f_j(y)\n\n\n25.8.1 Bayesian inference for mixture models\nWhen we fit a mixture model to data, we usually only have the y values and do not know which “population” they belong to. Because the z variables are unobserved, they are called latent variables. We can treat them as parameters in a hierarchical model and perform Bayesian inference for them. The hierarchial model might look like this:\n\n\\begin{aligned}\ny_i \\mid z_i, \\theta & \\overset{\\text{ind}}{\\sim} f_{z_i}(y \\mid \\theta) \\, , \\quad i = 1, \\ldots, n \\\\\n\\text{Pr}(z_i = j \\mid \\omega) &= \\omega_j \\, , \\quad j=1, \\ldots, J \\\\\n\\omega &\\sim \\mathbb{P}r(\\omega) \\\\\n\\theta &\\sim  \\mathbb{P}r(\\theta)\n\\end{aligned}\n\nwhere we might use a Dirichlet prior (see the review of distributions in the supplementary material) for the weight vector \\omega and conjugate priors for the population-specific parameters in \\theta. With this model, we could obtain posterior distributions for z (population membership of the observations), \\omega (population weights), and \\theta (population-specific parameters in f_j). Next, we will look at how to fit a mixture of two normal distributions in JAGS.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#example-with-jags",
    "href": "C2-L11.html#example-with-jags",
    "title": "25  Hierarchical modeling",
    "section": "25.9 Example with JAGS",
    "text": "25.9 Example with JAGS\n\n25.9.1 Data\nFor this example, we will use the data in the attached file mixture.csv.\n\n\nCode\ndat = read.csv(\"data/mixture.csv\", header=FALSE)\ny = dat$V1\n(n = length(y))\n\n\n[1] 200\n\n\nLet’s visualize these data.\n\n\nCode\nhist(y, breaks=20)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(density(y))\n\n\n\n\n\n\n\n\n\nIt appears that we have two populations, but we do not know which population each observation belongs to. We can learn this, along with the mixture weights and population-specific parameters with a Bayesian hierarchical model.\nWe will use a mixture of two normal distributions with variance 1 and different (and unknown) means.\n\n\n25.9.2 Model\n\n\nCode\nlibrary(\"rjags\")\n\n\n\n\nCode\nmod_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dnorm(mu[z[i]], prec)\n      z[i] ~ dcat(omega)\n    }\n  \n  mu[1] ~ dnorm(-1.0, 1.0/100.0)\n    mu[2] ~ dnorm(1.0, 1.0/100.0) T(mu[1],) # ensures mu[1] &lt; mu[2]\n\n    prec ~ dgamma(1.0/2.0, 1.0*1.0/2.0)\n  sig = sqrt(1.0/prec)\n    \n    omega ~ ddirich(c(1.0, 1.0))\n} \"\n\nset.seed(11)\n\ndata_jags = list(y=y)\n\nparams = c(\"mu\", \"sig\", \"omega\", \"z[1]\", \"z[31]\", \"z[49]\", \"z[6]\") # Select some z's to monitor\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 204\n   Total graph size: 614\n\nInitializing model\n\n\nCode\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,\n                        variable.names=params,\n                        n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim))\n\n\n\n\nCode\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim, ask=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nautocorr.diag(mod_sim)\n\n\n             mu[1]       mu[2]     omega[1]     omega[2]         sig\nLag 0  1.000000000 1.000000000  1.000000000  1.000000000 1.000000000\nLag 1  0.516771924 0.329430049  0.274134661  0.274134661 0.405725259\nLag 5  0.068885924 0.031571189  0.040459916  0.040459916 0.020278554\nLag 10 0.010540598 0.006660658  0.001231882  0.001231882 0.005875609\nLag 50 0.009937585 0.008550528 -0.001961038 -0.001961038 0.008247078\n               z[1]        z[31]        z[49] z[6]\nLag 0   1.000000000 1.0000000000  1.000000000  NaN\nLag 1   0.012094675 0.0441982214  0.030104201  NaN\nLag 5   0.007019988 0.0055734185  0.009140674  NaN\nLag 10 -0.002298574 0.0001659152 -0.001778667  NaN\nLag 50  0.007207918 0.0038720247 -0.007504052  NaN\n\n\nCode\neffectiveSize(mod_sim)\n\n\n    mu[1]     mu[2]  omega[1]  omega[2]       sig      z[1]     z[31]     z[49] \n 4158.705  5930.632  6363.539  6363.539  5927.760 14704.960 13142.679 13400.115 \n     z[6] \n 5000.000 \n\n\n\n\n25.9.3 Results\n\n\nCode\nsummary(mod_sim)\n\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean       SD  Naive SE Time-series SE\nmu[1]    -2.1304 0.164674 1.345e-03      2.555e-03\nmu[2]     1.4849 0.124798 1.019e-03      1.625e-03\nomega[1]  0.3866 0.040262 3.287e-04      5.052e-04\nomega[2]  0.6134 0.040262 3.287e-04      5.052e-04\nsig       1.1359 0.074225 6.060e-04      9.642e-04\nz[1]      1.0089 0.094096 7.683e-04      7.772e-04\nz[31]     1.5825 0.493158 4.027e-03      4.304e-03\nz[49]     1.8042 0.396828 3.240e-03      3.438e-03\nz[6]      1.9999 0.008165 6.667e-05      6.667e-05\n\n2. Quantiles for each variable:\n\n            2.5%     25%     50%     75%   97.5%\nmu[1]    -2.4548 -2.2396 -2.1318 -2.0195 -1.8055\nmu[2]     1.2402  1.4015  1.4868  1.5690  1.7269\nomega[1]  0.3091  0.3590  0.3863  0.4138  0.4670\nomega[2]  0.5330  0.5862  0.6137  0.6410  0.6909\nsig       1.0061  1.0832  1.1304  1.1833  1.2942\nz[1]      1.0000  1.0000  1.0000  1.0000  1.0000\nz[31]     1.0000  1.0000  2.0000  2.0000  2.0000\nz[49]     1.0000  2.0000  2.0000  2.0000  2.0000\nz[6]      2.0000  2.0000  2.0000  2.0000  2.0000\n\n\n\n\nCode\n## for the population parameters and the mixing weights\n\npar(mar = c(2.5, 1, 2.5, 1))\npar(mfrow=c(3,2))\ndensplot(mod_csim[,c(\"mu[1]\", \"mu[2]\", \"omega[1]\", \"omega[2]\", \"sig\")])\n\n\n\n\n\n\n\n\n\n\n\nCode\n## for the z's\npar(mfrow=c(2,2))\npar(mar = c(2.5, 1, 2.5, 1))\ndensplot(mod_csim[,c(\"z[1]\", \"z[31]\", \"z[49]\", \"z[6]\")])\n\n\n\n\n\n\n\n\n\n\n\nCode\ntable(mod_csim[,\"z[1]\"]) / nrow(mod_csim) ## posterior probabilities for z[1], the membership of y[1]\n\n\n\n          1           2 \n0.991066667 0.008933333 \n\n\n\n\nCode\ntable(mod_csim[,\"z[31]\"]) / nrow(mod_csim) ## posterior probabilities for z[31], the membership of y[31]\n\n\n\n        1         2 \n0.4174667 0.5825333 \n\n\n\n\nCode\ntable(mod_csim[,\"z[49]\"]) / nrow(mod_csim) ## posterior probabilities for z[49], the membership of y[49]\n\n\n\n     1      2 \n0.1958 0.8042 \n\n\n\n\nCode\ntable(mod_csim[,\"z[6]\"]) / nrow(mod_csim) ## posterior probabilities for z[6], the membership of y[6]\n\n\n\n           1            2 \n6.666667e-05 9.999333e-01 \n\n\n\n\nCode\ny[c(1, 31, 49, 6)]\n\n\n[1] -2.2661749 -0.3702666  0.0365564  3.7548080\n\n\nIf we look back to the y values associated with these z variables we monitored, we see that y_1 is clearly in Population 1’s territory, y_{31} is ambiguous, y_{49} is ambiguous but is closer to Population 2’s territory, and y_6 is clearly in Population 2’s territory. The posterior distributions for the z variables closely reflect our assessment.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Hierarchical modeling</span>"
    ]
  },
  {
    "objectID": "A01.html",
    "href": "A01.html",
    "title": "27  Appendix: Notation",
    "section": "",
    "text": "27.1 The argmax function\nparameters are with Greek letters\n\\theta\nestimate\n\\hat{p}\nthe Certain event\n\\Omega\nprobability of RV x taking value x\n\\mathbb{P}r(X=x)\nodds\n\\mathcal{O}(X)\nrandom variables \nX\nX is distributed as\nX\n\\sim N(\\mu, \\sigma^2)\nX is proportional to\nX\\propto N(\\mu, \\sigma^2)\nProbability of A and B\n\\mathbb{P}r(X \\cap Y)\nConditional probability\n\\mathbb{P}r(X \\mid Y)\nJoint probability\n\\mathbb{P}r(X,Y)\nY_i \\stackrel{iid}\\sim N(\\mu, \\sigma^2)\nApproximately distributed as (say using the CLT)\nY_i \\stackrel{.}\\sim N(\\mu, \\sigma^2)\n\\mathbb{E}[X_i]\nThe expected value of an RV X set to 0 (A.K.A. a fair bet)\n\\mathbb{E}[X_i]  \\stackrel{set} = 0\nThe variance of an RV\n\\mathbb{V}ar[X_i]\nlogical implication\n\\implies\nif and only if\n\\iff\ntherefore\n\\therefore\nindependence\nA \\perp \\!\\!\\! \\perp B \\iff \\mathbb{P}r(A \\mid B) = \\mathbb{P}r(A)\nIndicator function \n\\mathbb{I}_{\\{A\\}} = \\begin{cases}\n1 & \\text{if } A \\text{ is true} \\\\ 0 & \\text{otherwise} \\end{cases}\nDirichlet function\nThis is a continuous version of the indicator function, defined as a limit.\n\\delta(x) = \\lim_{\\epsilon \\to 0} \\frac{1}{2\\epsilon} \\mathbb{I}_{\\{|x| &lt; \\epsilon\\}}\nThe Dirichlet function is used to represent a point mass at a point, often used as a component for zero inflated mixtures.\nThe following are from course 4\nWhen we want to maximize a function f(x), there are two things we may be interested in:",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A01.html#sec-argmax-function",
    "href": "A01.html#sec-argmax-function",
    "title": "27  Appendix: Notation",
    "section": "",
    "text": "The value f(x) achieves when it is maximized, which we denote \\max_x f(x).\nThe x-value that results in maximizing f(x), which we denote \\hat x = \\arg \\max_x f(x). Thus \\max_x f(x) = f(\\hat x).",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A01.html#sec-indicator-functions",
    "href": "A01.html#sec-indicator-functions",
    "title": "27  Appendix: Notation",
    "section": "27.2 Indicator Functions",
    "text": "27.2 Indicator Functions\nThe concept of an indicator function is a really useful one. This is a function that takes the value one if its argument is true, and the value zero if its argument is false. Sometimes these functions are called Heaviside functions or unit step functions. I write an indicator function as \\mathbb{I}_{A}(x), although sometimes they are written \\mathbb{1}_{A}(x). If the context is obvious, we can also simply write I{A}.\nExample:\n\n    \\mathbb{I}_{x&gt;3}(x)=\n    \\begin{cases}\n      0, & \\text{if}\\ x \\le 3 \\\\\n      1, & \\text{otherwise}\n    \\end{cases}\n\nNote: Indicator functions are easy to implement in code using a lambda function. They can be combined using a dictionary.\nBecause 0 · 1 = 0, the product of indicator functions can be combined into a single indicator function with a modified condition.\n\n27.2.1 Products of Indicator Functions:\n\n    \\mathbb{I}{x&lt;5} \\cdot \\mathbb{I}{x≥0} = \\mathbb{I}{0≤x&lt;5}\n\n\n    \\prod_{i=1}^N \\mathbb{I}_{(x_i&lt;2)} = \\mathbb{I}_{(x_i&lt;2) \\forall i} = \\mathbb{I}_{\\max_i x_i&lt;2}",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A02.html",
    "href": "A02.html",
    "title": "28  Appendix: Discrete Distributions",
    "section": "",
    "text": "28.1 Discrete Uniform",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-discrete-uniform",
    "href": "A02.html#sec-discrete-uniform",
    "title": "28  Appendix: Discrete Distributions",
    "section": "",
    "text": "28.1.1 Stories\n\n\n\n\n\n\nNoteDiscrete Uniform Finite set Parametrization\n\n\n\nLet C be a finite, nonempty set of numbers and X random variable associated with the event of choosing one of these numbers uniformly at random, that is all values being equally likely X(x=c)\nThen X is said to have the Discrete Uniform distribution with parameter C.\nWe denote this by X ∼ DUnif(C).\n\n\n\n\n\n\n\n\nNoteDiscrete Uniform with Lower and Upper bound Parametrization\n\n\n\nWhen the set C above is C=\\{c \\in \\mathbb{Z} \\mid a \\le c \\le b\\ \\}.\nThen X is said to have the Discrete Uniform distribution with lower bound parameter a and upper bound parameter b.\nWe denote this by X ∼ DUnif(a,b).\n\n\n\n\n\n\n\n\nNoteUrn Model\n\n\n\nSuppose we have an urn with n balls labeled with the numbers a 1, \\dots, a_n . One drawing from the urn produces a discrete uniform random variable on the set \\{a_1, \\dots, a_n \\}.\n\n\n\n\n28.1.2 Moments\n\n\\begin{aligned}\n    \\phi_X(t)&={\\displaystyle {\\frac {e^{at}-e^{(b+1)t}}{n(1-e^{t})}}}  && \\text{(MGF)}\n\\\\  \\mathbb{E}[X] &= \\frac{a + b}{2} && \\text{(Expectation)}\n\\\\  \\mathbb{V}ar[X] &= \\frac{(b - a + 1)^2 - 1}{12} && \\text{(Variance)}\n\\end{aligned}\n\\tag{28.1}\n\n\n28.1.3 Probability mass function (PMF)\n\nf(x \\mid a, b) = \\frac{1}{b - a + 1}\n\n\n\n28.1.4 Cumulative distribution function (CDF)\n\nF(x \\mid a, b) = \\frac{\\lfloor x \\rfloor - a - 1}{b - a + 1} \\\\\\text{where} \\lfloor x \\rfloor \\text{ is the floor function (rounds down reals to nearest smaller integer)}\n\n\n\n28.1.5 Prior\nSince a number of families have the uniform as a special case we can use them as priors when we want a uniform prior:",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-bernoulli-distribution",
    "href": "A02.html#sec-bernoulli-distribution",
    "title": "28  Appendix: Discrete Distributions",
    "section": "28.2 Bernoulli Distribution",
    "text": "28.2 Bernoulli Distribution\n\n28.2.1 Stories\n\n\n\n\n\n\nNote\n\n\n\nThe Bernoulli distribution arises when modeling the outcome of a binary event called a Bernoulli trial. .\nLet X be the indicator variable corresponding to the success of getting “heads” in a “coin toss”, with a coin that has probability of success p for getting “heads”.\nThen X has a Bernoulli Distribution with parameter p\nWe denote this as X \\sim Bern(p)\n\n\n\n\n28.2.2 Parameters\nBecause of this story, the parameter p is often called the success probability of the Bern(p) distribution.\n\n\n28.2.3 Examples\n\n\n\n\n\n\nNote\n\n\n\n\nfair coin toss\nunfair coin toss\nad click\nweb site conversion\ndeath or survival of a patient in a medical trial\nindicator random variable\n\n\n\n\n\n28.2.4 Checklist\n\n\n\n\n\n\nNote\n\n\n\n\nDiscrete data\nA single trial\nOnly two trial outcomes: success and failure (These do not need to literally represent successes and failures, but this shorthand is typically used.)\n\n\n\n\n\\begin{aligned}\nX &\\sim Bernoulli(p)\\\\ & \\sim Bern(p)\\\\ & \\sim B(p)  \\end{aligned}\n\\tag{28.2}\n\n\n28.2.5 Moments\n\nM_X(t)=q+pe^{t} \\qquad \\text{(MGF)}\n\\tag{28.3}\n\n\\mathbb{E}[X]= p \\qquad \\text{(Expectation)}\n\\tag{28.4}\n\n\\mathbb{V}ar[x]= \\mathbb{P}r(1-p) \\qquad \\text{(Variance)}\n\\tag{28.5}\n\n\n28.2.6 PMF\nWhere parameter p is the probability of getting heads.\nThe probability for the two events is:\n\n\\mathbb{P}r(X=1) = p \\qquad \\mathbb{P}r(X=0)=1-p\n\n\n{\\displaystyle {\\begin{cases}1-p&{\\text{if }}k=0\\\\p&{\\text{if }}k=1\\end{cases}}}  \\qquad \\text{(PMF)}\n\\tag{28.6}\n\n\n28.2.7 CDF\n\n{\\displaystyle {\\begin{cases}0&{\\text{if }}k&lt;0\\\\1-p&{\\text{if }}0\\leq k&lt;1\\\\1&{\\text{if }}k\\geq 1\\end{cases}}} \\qquad \\text{(CDF)}\n\\tag{28.7}\n\n\n28.2.8 Likelihood\n\nL(\\theta) = \\prod p^x(1-p)^{1-x} \\mathbb{I}_{[0,1]}(x)  \\qquad \\text{(Likelihood)}\n\\tag{28.8}\n\n\\mathcal{L}(\\theta) =log(p) \\sum x + log(1-p)\\sum (1-x)  \\qquad \\text{(Log Likelihood)}\n\\tag{28.9}\n\n\n28.2.9 Entropy and Information\n\n\\mathbb{H}(x)= -q \\ln(q)- p \\ln(p) \\qquad \\text{(Entropy)}\n\\tag{28.10}\n\n\\mathcal{I}[X]\\frac{1}{\\mathbb{P}r(1-p)} \\qquad \\text{(Fisher Information)}\n\\tag{28.11}\n\nBeta(x) \\qquad \\text{(Conjugate Prior)}\n\\tag{28.12}\n\n\n28.2.10 Usage\n\n\n\nTable 28.1: Usage of Bernoulli\n\n\n\n\n\nPackage\nSyntax\n\n\n\n\nNumPy\nrg.choice([0, 1], p=[1-theta, theta])\n\n\nSciPy\nscipy.stats.bernoulli(theta)\n\n\nStan\nbernoulli(theta)\n\n\n\n\n\n\n\n\n28.2.11 Plots\n\n\nCode\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=0.30, var=0.21, skew=0.87, kurt=-1.24\n\n\nCode\nx = np.arange(bernoulli.ppf(0.01, p),\n              bernoulli.ppf(0.99, p))\nax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\n\nrv = bernoulli(p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n## Generate random numbers\nr = bernoulli.rvs(p, size=10)\nr\n\n\narray([0, 0, 0, 0, 1, 1, 0, 0, 0, 0])\n\n\n\n\n\n\nA Swiss stamp issueed in 1994 depicting Mathematician Jakob Bernouilli and the formula and diagram of the law of large numbers\n\n\n\n\n\n\n\nTipBiographical note on Jacob Bernoulli\n\n\n\n\nIt seems that to make a correct conjecture about any event whatever, it is necessary to calculate exactly the number of possible cases and then to determine how much more likely it is that one case will occur than another. (Bernoulli 1713)\n\nThe Bernoulli distribution as well as The Binomial distribution are due to Jacob Bernoulli (1655-1705) who was a prominent mathematician in the Bernoulli family. He discovered the fundamental mathematical constant e. With his brother Johann, he was among the first to develop Leibniz’s calculus, introducing the word integral and applying it to polar coordinates and the study of curves such as the catenary, the logarithmic spiral and the cycloid\nHis most important contribution was in the field of probability, where he derived the first version of the law of large numbers (LLN). The LLN is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and will tend to become closer to the expected value as more trials are performed. A special form of the LLN (for a binary random variable) was first proved by Jacob Bernoulli. It took him over 20 years to develop sufficiently rigorous mathematical proof.\n\nFor a more extensive biography visit the following link\n\n\nThe Bernoulli distribution is built on a trial of a coin toss (possibly biased).\n\nWe use the Bernoulli distribution to model a random variable for the probability of such a coin toss trial.\nWe use the Binomial distribution to model a random variable for the probability of getting k heads in N independent trails.",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-binomial-distribution",
    "href": "A02.html#sec-binomial-distribution",
    "title": "28  Appendix: Discrete Distributions",
    "section": "28.3 Binomial distribution",
    "text": "28.3 Binomial distribution\n\n28.3.1 Stories\n\n\n\n\n\n\nNote\n\n\n\n\n\\overbrace{\\underbrace{\\fbox{0}\\ \\ldots \\fbox{0}}_{N_0}\\ \\underbrace{\\fbox{1}\\ \\ldots \\fbox{1}}_{N_1}}^N\n\\tag{28.13}\nThe Binomial distribution arises when we conduct multiple independent Bernoulli trials and wish to model X the number of successes in Y_i\\mid \\theta identically distributed Bernoulli trials with the same probability of success \\theta. If n independent Bernoulli trials are performed, each with the same success probability p. The distribution of X is called the Binomial distribution with parameters n and p. We write X \\sim \\text{Bin}(n, p) to mean that X has the Binomial distribution with parameters n and p, where n is a positive integer and 0 &lt; p &lt; 1.\n\n\n\n\n28.3.2 Parameters\n\n\\theta - the probability of success in the Bernoulli trials\nN - the total number of trials being conducted\n\n\n\n28.3.3 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nDiscrete data\nTwo possible outcomes for each trial\nEach trial is independent and\nThe probability of success/failure is the same in each trial\nThe outcome is the aggregate number of successes\n\n\n\n\n\n28.3.4 Examples\n\nto model the aggregate outcome of clinical drug trials,\nto estimate the proportion of the population voting for each political party using exit poll data (where there are only two political parties).\n\n\nX \\sim Bin[n,p]\n\\tag{28.14}\n\nf(X=x \\mid \\theta) = {n \\choose x} \\theta^x(1-\\theta)^{n-x}\n\\tag{28.15}\n\nL(\\theta)=\\prod_{i=1}^{n} {n\\choose x_i}  \\theta ^ {x_i} (1− \\theta) ^ {(n−x_i)}\n\\tag{28.16}\n\n\\begin{aligned}\\ell( \\theta) &= \\log \\mathcal{L}( \\theta) \\\\&= \\sum_{i=1}^n \\left[\\log {n\\choose x_i} + x_i \\log  \\theta + (n-x_i)\\log (1- \\theta) \\right].\\end{aligned}\n\\tag{28.17}\n\n\\mathbb{E}[X]= N \\times  \\theta\n\\tag{28.18}\n\n\\mathbb{V}ar[X]=N \\cdot \\theta \\cdot (1-\\theta)\n\\tag{28.19}\n\n\\mathbb{H}(X) = \\frac{1}{2}\\log_2 \\left (2\\pi n \\theta(1 - \\theta)\\right) + O(\\frac{1}{n})\n\\tag{28.20}\n\n\\mathcal{I}(\\theta)=\\frac{n}{ \\theta \\cdot (1- \\theta)}\n\\tag{28.21}\n\n\n28.3.5 Usage\n\n\n\nTable 28.2: Usage of Binomial\n\n\n\n\n\nPackage\nSyntax\n\n\n\n\nNumPy\nrg.binomial(N, theta)\n\n\nSciPy\nscipy.stats.binom(N, theta)\n\n\nStan\nbinomial(N, theta)\n\n\n\n\n\n\n\n\n28.3.6 Relationships\n\n\n\nbinomial distribution relations\n\n\nThe Binomial Distribution is related to\n\nThe Binomial is a special case of the Multinomial distribution with K =2 (two categories).\nthe Poisson distribution distribution. If X \\sim Binomial(n, p) rv and Y \\sim Poisson(np) distribution then \\mathbb{P}r(X = n) ≈ \\mathbb{P}r(Y = n) for large n and small np.\nThe Bernoulli distribution is a special case of the the Binomial distribution  X \\sim Binomial(n=1, p) \\\\ \\implies X \\sim Bernoulli(p)\nthe Normal distribution If X \\sim Binomial(n, p) RV and Y \\sim Normal(\\mu=np,\\sigma=n\\mathbb{P}r(1-p)) then for integers j and k, \\mathbb{P}r(j ≤ X ≤ k) ≈ \\mathbb{P}r(j – 1/2 ≤ Y ≤ k + 1/2). The approximation is better when p ≈ 0.5 and when n is large. For more information, see normal approximation to the Binomial\nThe Binomial is a limit of the Hypergeometric. The difference between a binomial distribution and a hypergeometric distribution is the difference between sampling with replacement and sampling without replacement. As the population size increases relative to the sample size, the difference becomes negligible. So If X \\sim Binomial(n, p) RV and Y \\sim HyperGeometric(N,a,b) then\n\n\\lim_{n\\to \\infty} X = Y\n\n\n\n28.3.7 Plots\n\n\nCode\nimport numpy as np\nfrom scipy.stats import binom\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\nn, p = 5, 0.4\nmean, var, skew, kurt = binom.stats(n, p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=2.00, var=1.20, skew=0.18, kurt=-0.37\n\n\nCode\nx = np.arange(binom.ppf(0.01, n, p),\n              binom.ppf(0.99, n, p))\nax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\nax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\nrv = binom(n, p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n## generate random numbers\nr = binom.rvs(n, p, size=10)\nr\n\n\narray([2, 2, 3, 2, 2, 3, 4, 3, 2, 2])\n\n\n``` {{python}}\nfrom __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport numpy as np\nimport scipy\nfrom scipy.special import gamma, factorial, comb\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\n#pyo.init_notebook_mode()\nINTERACT_FLAG=False\ndef binomial_vector_over_y(theta, n):\n    total_events = n\n    y =  np.linspace(0, total_events , total_events + 1)\n    p_y = [comb(int(total_events), int(yelem)) * theta** yelem * (1 - theta)**(total_events - yelem) for yelem in y]\n\n    fig = px.line(x=y, y=p_y, color_discrete_sequence=[\"steelblue\"], \n                  height=600, width=800, title=\" Binomial distribution for theta = %lf, n = %d\" %(theta, n))\n    fig.data[0].line['width'] = 4\n    fig.layout.xaxis.title.text = \"y\"\n    fig.layout.yaxis.title.text = \"P(y)\"\n    fig.show()\n    \nif(INTERACT_FLAG):    \n    interact(binomial_vector_over_y, theta=0.5, n=15)\nelse:\n    binomial_vector_over_y(theta=0.5, n=10)\n```",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-hypergeometric-distribution",
    "href": "A02.html#sec-hypergeometric-distribution",
    "title": "28  Appendix: Discrete Distributions",
    "section": "28.4 Hypergeometric distribution",
    "text": "28.4 Hypergeometric distribution\n\n28.4.1 story 1 - Urn Model\nThe beta-binomial distribution with parameters \\alpha success rate and \\beta failure and n the number of trials can be motivated by an Pólya urn model.\nImagine a trial in which a ball is drawn without replacement from urn containing \\alpha white balls and \\beta black balls. If this is repeated n times, then the probability of observing x white balls follows a hypergeometric distribution with parameters n, \\alpha and \\beta.\nNote: is we used a\nIf the random draws are with simple replacement (no balls over and above the observed ball are added to the urn), then the distribution follows a binomial distribution and if the random draws are made without replacement, the distribution follows a hypergeometric distribution.\n\n\n\n28.4.2 Examples\n\nk white balls from an in Urn without replacement\ncapture-recapture\nAces in a poker hand\n\n\n\n28.4.3 Story\nConsider an urn with w white balls and b black balls. We draw n balls out of the urn at random without replacement, such that all w+b samples are equally likely. Let X be the number of white balls in n the sample. Then X is said to have the Hypergeometric distribution with parameters w, b, and n; we denote this by X ∼ HGeom(w, b, n)",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-poisson-distribution",
    "href": "A02.html#sec-poisson-distribution",
    "title": "28  Appendix: Discrete Distributions",
    "section": "28.5 Poisson distribution",
    "text": "28.5 Poisson distribution\n\n28.5.1 Stories\n\n\n\n\n\n\nNotePoisson Parametrization\n\n\n\nThe Poisson distribution arises when modeling the number of successes of independent and identically distributed (IID) events in a fixed interval of time or space, occurring at a constant rate \\lambda. Let X represent the count of the number of phone calls received at a call center in a given interval, such as an hour, with the parameter \\lambda corresponding to the average rate at which events occur in that interval. Then X is said to have the Poisson distribution with parameter \\lambda, and we denote this as X \\sim \\text{Pois}(\\lambda).\n\nX \\sim Pois(\\lambda)\n\\tag{28.22}\n\n\n\n\n\n28.5.2 Checklist\n\nCount of discrete events\nIndividual events occur at a given rate and independently of other events\nFixed amount of time or space in which the events can occur\n\n\n\n28.5.3 Examples\n\nThe number of emails you receive in an hour. There are a lot of people who could potentially email you at that hour, but it is unlikely that any specific person will actually email you at that hour. Alternatively, imagine subdividing the hour into milliseconds. There are 3.6×106 seconds in an hour, but in any specific millisecond, it is unlikely that you will get an email.\nThe number of chips in a chocolate chip cookie. Imagine subdividing the cookie into small cubes; the probability of getting a chocolate chip in a single cube is small, but the number of cubes is large.\nThe number of earthquakes in a year in some regions of the world. At any given time and location, the probability of an earthquake is small, but there are a large number of possible times and locations for earthquakes to occur over the course of the year.\nCount of component failures per week\nestimating the failure rate of artificial heart valves,\nestimating the prevalence of violent crimes in different districts,\napproximating the binomial which is, itself, being used to explain the prevalence of autism in the UK.\n\n\n\n28.5.4 Moments\n\n\\mathrm{E}(X) = \\lambda\n\n\n\\mathrm{V}ar(X) = \\lambda\n\n\n\n28.5.5 Probability mass function (PMF)\n\nf(x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\tag{28.23}\n\n\n28.5.6 Cumulative distribution function (CDF)\n\nF(x \\mid \\lambda) = \\frac{\\Gamma(\\lfloor x+1\\rfloor,\\lambda)}{\\lfloor x \\rfloor !} \\qquad \\text{CDF}\n\\tag{28.24}\n\n\\text{where }\\Gamma(u,v)=\\int_{v}^{\\infty}t^{u-1}e^{-t} \\mathrm{d}t \\text{ is the upper incomplete gamma function}\n\\tag{28.25}\n\n\\text{and } \\lfloor x \\rfloor \\text{ is the floor function (rounds down reals to nearest smaller integer)}\n\\tag{28.26}",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-geometric-distribution",
    "href": "A02.html#sec-geometric-distribution",
    "title": "28  Appendix: Discrete Distributions",
    "section": "28.6 Geometric distribution",
    "text": "28.6 Geometric distribution\n\n28.6.1 Stories\n\n\n\n\n\n\nNoteGeometric Distribution Failures before success\n\n\n\nConsider a sequence of independent Bernoulli trials, each with the same success probability p \\in (0, 1), with trials performed until a success occurs. Let X be the number of failures before the first successful trial. Then X has the Geometric distribution with parameter p; we denote this by X \\sim Geom(p).\nFor example, if we flip a fair coin until it lands Heads for the first time, then the number of Tails before the first occurrence of Heads is distributed as Geom(1/2).\nTo get the Geometric PMF from the story, imagine the Bernoulli trials as a string of 0’s (failures) ending in a single 1 (success). Each 0 has probability q = 1 − p and the final 1 has probability p, so a string of k failures followed by one success has probability q^kp.\n\n\n\n\n\n\n\n\nNoteGeometric distribution Failures and success\n\n\n\nConsider a sequence of independent Bernoulli trials, each with the same success probability p \\in (0, 1), with trials performed until a success occurs. Let X be the number of failures before the first successful trial. Then X has the Geometric distribution with parameter p; we denote this by X \\sim Geom(p).\nFor example, if we flip a fair coin until it lands Heads for the first time, then the number of Tails before the first occurrence of Heads is distributed as Geom(1/2).\nTo get the Geometric PMF from the story, imagine the Bernoulli trials as a string of 0’s (failures) ending in a single 1 (success). Each 0 has probability q = 1 − p and the final 1 has probability p, so a string of k failures followed by one success has probability q^kp.\n\n\n\n\n\n28.6.2 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nDiscrete data\nTwo possible outcomes for each trial\nEach trial is independent and\nThe probability of success/failure is the same in each trial\nThe outcome is the count of failures before the first success\n\n\n\n\n\n28.6.3 Examples\n\nConsider polymerization of an actin filament. At each time step, an actin monomer may add to the end of the filament (“failure”), or an actin monomer may fall off the end (“success”) with (usually very low) probability θ. The length of actin filaments, measured in a number of constitutive monomers, is Geometrically distributed.\n\nThe Geometric distribution arises when we want to know “What is the number of Bernoulli trials required to get the first success?”, i.e., the number of Bernoulli events until a success is observed, such as the probability of getting the first head when flipping a coin. It takes values on the positive integers starting with one (since at least one trial is needed to observe a success).\n\n\nX \\sim Geo(p)\n\\tag{28.27}\n\n\n28.6.4 Moments\n\n\\mathbb{M}_X[t] = \\frac{pe^t}{1-(1-p)e^t} \\qquad t&lt;-ln(1-p)\n\\tag{28.28}\n\n\\mathbb{E}[X] = \\frac{1}{p}\n\\tag{28.29}\n\n\\mathbb{V}ar[X]=\\frac{1-p}{p^2}\n\\tag{28.30}\n\n\n28.6.5 PMF\n\n\\mathbb{P}r(X = x \\mid p) = \\mathbb{P}r(1-p)^{x-1} \\qquad \\forall x \\in N;\\quad 0\\le p \\le 1\n\\tag{28.31}\n\n\n28.6.6 CDF\n\n1-(1-p)^{\\lfloor x\\rfloor } \\qquad x&lt;1\n\\tag{28.32}\n\n\n28.6.7 Memoryless property\n\nThe geometric distribution is based on geometric series.\nThe geometric distribution has the memoryless property:\n\nP (X &gt; s \\mid X &gt;  t) = P (X &gt; s − t)\n\nOne can say that the distribution “forgets” what has occurred, so that The probability of getting an additional s − t failures, having already observed t failures, is the same as the probability of observing s − t failures at the start of the sequence. In other words, the probability of getting a run of failures depends only on the length of the run, not on its position.\nY=X-1 is the \\text{negative binomial}(1,p)\n\n\n28.6.8 Worked out Examples\n\nExample 28.1 (Geometric Distribution) The Geometric distribution arises when we consider how long we will have to “wait for a success” during repeated Bernoulli trials.\nWhat is the probability that we flip a fair coin four times and don’t see any heads?\nThis is the same as asking what is \\mathbb{P}r(X &gt; 4) where X ∼ Geo(1/2).\n\n  \\begin{aligned}\n    \\mathbb{P}r(X &gt; 4) &= 1 − \\mathbb{P}r(X =1)−\\mathbb{P}r(X = 2)−\\mathbb{P}r(X = 3)−\\mathbb{P}r(X = 4) \\\\\n    &= 1−(\\frac{1}{2})−(\\frac{1}{2})(\\frac{1}{2})−(\\frac{1}{2})(\\frac{1}{2})^2−(\\frac{1}{2})(\\frac{1}{2})^3  \\\\\n   &= \\frac{1}{16}\n    \\end{aligned}\n\nOf course, we could also have just computed it directly, but here we see an example of using the geometric distribution and we can also see that we got the right answer.\n\n\n\n28.6.9 Plots\n\n\nCode\nimport numpy as np\nfrom scipy.stats import geom\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\np = 0.5\nmean, var, skew, kurt = geom.stats(p,moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=2.00, var=2.00, skew=2.12, kurt=6.50\n\n\nCode\nx = np.arange(geom.ppf(0.01, p),\n              geom.ppf(0.99, p))\nax.plot(x, geom.pmf(x, p), 'bo', ms=8, label='geom pmf')\nax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=5, alpha=0.5)\n\nrv = geom(p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\nr = geom.rvs(p,size=10)\nr\n\n\narray([2, 1, 5, 2, 1, 1, 2, 1, 1, 2])",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-negative-binomial-distribution",
    "href": "A02.html#sec-negative-binomial-distribution",
    "title": "28  Appendix: Discrete Distributions",
    "section": "28.7 Negative Binomial Distribution",
    "text": "28.7 Negative Binomial Distribution\n\n28.7.1 Story\n\n\n\n\n\n\nNote\n\n\n\nIn a sequence of independent Bernoulli trials with success probability p, if X is the number of failures before the rth success, then X is said to have the Negative Binomial distribution with parameters r and p, denoted X \\sim NBin(r, p).\n\n\nBoth the Binomial and the Negative Binomial distributions are based on independent Bernoulli trials; they differ in the stopping rule and in what they are counting.\nThe Binomial counts the number of successes in a fixed number of trials; the Negative Binomial counts the number of failures until a fixed number of successes.\nIn light of these similarities, it comes as no surprise that the derivation of the Negative Binomial PMF bears a resemblance to the corresponding derivation for the Binomial.\n\n\n28.7.2 Parameters\n\nr the number of successes.\np the probability of the Bernoulli trial.\n\n\n\n28.7.3 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nCount of discrete events\nNon-independent events; it is sometimes said that the events can exhibit contagion, meaning that if one event occurs, it is more likely that another will also occur\nCan model a data-generating process where the variance exceeds the mean\nFixed amount of time or space in which the events can occur\n\n\n\n\n\n28.7.4 Examples\n\nStamp collection - Suppose there are n types of stamps, which you are collecting one by one, with the goal of getting a complete set. When collecting stamps, the stamp types are random. Assume that each time you collect a stamp, it is equally likely to be any of the n types. What is the expected number of toys needed until you have a complete set?\neverything the Poisson can do and more,\nto model the number of measles cases that occur on an island,\nthe number of banks that collapse in a financial crisis.\nthe length of a hospital stay\nthe probability you will have to visit Y houses if you must sell r cookies before returning home\n\n\n\n28.7.5 Moments\n\n\\mathrm{E}(X) = \\lambda\n\n\nvar(X) = \\lambda + \\frac{\\lambda^2}{\\kappa}\n\n\n\n28.7.6 Probability mass function (PMF)\n\nf(x \\mid \\lambda,\\kappa) = \\frac{\\Gamma(x+\\kappa)}{x!\\Gamma(\\kappa+1)}\\left(\\frac{\\lambda}{\\lambda+\\kappa}\\right)^x \\left(\\frac{\\kappa}{\\lambda+\\kappa}\\right)^\\kappa\n\n\n\n28.7.7 Cumulative distribution function (CDF)\n\nF(x \\mid \\lambda,\\kappa) =\n\\begin{cases}\n  I_{\\frac{\\kappa}{\\kappa+\\lambda}}(\\kappa,1+\\lfloor x \\rfloor), & x \\ge q 0 \\\\\n  0,                                                             & \\text{Otherwise}\n\\end{cases}\n\n\n\\text{where } I_w(u,v) \\text{ is the regularised incomplete beta function: }\nI_w(u,v) = \\frac{B(w; u, v)}{B(u,v)}\n\n\n\\text{where } B(w; u,v)=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the incomplete beta function and }\\\\ B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the complete beta function}",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-multinomial-distribution",
    "href": "A02.html#sec-multinomial-distribution",
    "title": "28  Appendix: Discrete Distributions",
    "section": "28.8 Multinomial Distribution",
    "text": "28.8 Multinomial Distribution\nThe Multinomial distribution is a generalization of the Binomial. Whereas the Binomial distribution counts the successes in a fixed number of trials that can only be categorized as success or failure, the Multinomial distribution keeps track of trials whose outcomes can fall into multiple categories, such as excellent, adequate, poor; or red, yellow, green, blue.\n\n28.8.1 Story\nMultinomial distribution. Each of N objects is independently placed into one of k categories. An object is placed into category j with probability p_j ,P where the p_j are non-negative and \\sum^k_{j=1} p_j = 1. Let X_1 be the number of objects in category 1, X_2 the number of objects in category 2, etc., so that X_1 + \\dots + X_k = n. Then X = (X_1 , \\dots , X_k ) is said to have the Multinomial distribution with parameters n and p = (p_1 , \\dots , p_k ). We write this as X \\sim Mult_k(n, p).\nWe call X a random vector because it is a vector of random variables. The joint PMF of X can be derived from the story.\n\n\n28.8.2 Examples\n\nBlood type counts across n individuals\nNumbers of people voting for each party in a sample\n\n\n\n28.8.3 Moments\n\n\\mathrm{E}(X_i) = n p_i \\text{, }\\forall i\n\n\nvar(X_i) = n p_i (1-p_i) \\text{, }\\forall i\n\n\ncov(X_i,X_j) = -n p_i p_j \\text{, }\\forall i\\neq j\n\n\n\n28.8.4 Probability Mass Function (PMF)\n\nf(x_1,x_2,\\dots,x_d \\mid n,p_1,p_2,\\dots,p_d) = \\frac{n!}{x_1 ! x_2 ! \\dots x_d !} p_1^{x_1} p_2^{x_2}\\dots p_d^{x_d}",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#beta-binomial",
    "href": "A02.html#beta-binomial",
    "title": "28  Appendix: Discrete Distributions",
    "section": "28.9 Beta Binomial",
    "text": "28.9 Beta Binomial\n\n28.9.1 Story 1 - Polya Urn Model\nThe beta-binomial distribution with parameters \\alpha success rate and \\beta failure and n the number of trials can be motivated by an Pólya urn model.\nImagine an urn containing \\alpha red balls and \\beta black balls, where random draws are made. If a red ball is observed, then two red balls are returned to the urn. Likewise, if a black ball is drawn, then two black balls are returned to the urn. If this is repeated n times, then the probability of observing x red balls follows a beta-binomial distribution with parameters n, \\alpha and \\beta.\nIf the random draws are with simple replacement (no balls over and above the observed ball are added to the urn), then the distribution follows a binomial distribution and if the random draws are made without replacement, the distribution follows a hypergeometric distribution.\n\n\n28.9.2 Story 2 compound distribution\nThe Beta distribution is a conjugate distribution of the binomial distribution. This fact leads to an analytically tractable compound distribution constructed in a hierarchical fashion where one can think of the p parameter in the binomial distribution as being randomly drawn from a beta distribution.\nSuppose we were interested in predicting the number of heads, x in n future trials. This is given by\n\n{\\displaystyle {\\begin{aligned}f(x\\mid n,\\alpha ,\\beta )&=\\int _{0}^{1}\\mathrm {Bin} (x \\mid n,p)\\mathrm {Beta} (p\\mid \\alpha ,\\beta )\\,dp\\\\[6pt]&={n \\choose x}{\\frac {1}{\\mathrm {B} (\\alpha ,\\beta )}}\\int _{0}^{1}p^{x+\\alpha -1}(1-p)^{n-x+\\beta -1}\\,dp\\\\[6pt]&={n \\choose x}{\\frac {\\mathrm {B} (x+\\alpha ,n-x+\\beta )}{\\mathrm {B} (\\alpha ,\\beta )}}.\\end{aligned}}}\n\n\n{\\displaystyle f(x\\mid n,\\alpha ,\\beta )={\\frac {\\Gamma (n+1)}{\\Gamma (x+1)\\Gamma (n-x+1)}}{\\frac {\\Gamma (x+\\alpha )\\Gamma (n-x+\\beta )}{\\Gamma (n+\\alpha +\\beta )}}{\\frac {\\Gamma (\\alpha +\\beta )}{\\Gamma (\\alpha )\\Gamma (\\beta )}}.}\n\n\n\n28.9.3 Moments\n\n\\mathrm{E}(X) = \\frac{n\\alpha}{\\alpha+\\beta}\n \nvar(X) = \\frac{n\\alpha\\beta(\\alpha+\\beta+n)}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\n\n\n\n28.9.4 Probability mass function (PMF)\n\nf(x \\mid n,\\alpha,\\beta) = \\binom{n}{x}\\frac{B(x+\\alpha,n-x+\\beta)}{B(\\alpha,\\beta)}\n\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the (complete) beta function }\n\n\n\n28.9.5 Cumulative distribution function (CDF)\n\nF(x\\mid n,\\alpha,\\beta) = \\begin{cases}\n0, & x&lt;0 \\\\\n\\binom{n}{x}\\frac{B(x+\\alpha,n-x+\\beta)}{B(\\alpha,\\beta)} {}_{3}F_2(1,-x,n-x+\\beta;n-x-1,1-x-\\alpha;1), & 0\\leq x \\leq n \\\\\n1, & x&gt;n \\end{cases}\n\n\n\\text{where } {}_{3}F_2(a,b,x) \\text{ is the generalised hypergeometric function}\n\n\n\n28.9.6 Relations\n\nThe Pascal distribution (after Blaise Pascal) is special cases of the negative binomial distribution. Used with an integer-valued stopping-time parameter r\nThe Pólya distribution (for George Pólya) is special cases of the negative binomial distribution. Used with a real-valued-valued stopping-time parameter r\n\n\n\n\n\nA photo of Hungarian Mathematician George Pólya\n\n\n\n\n\n\n\nTipBiographical note on George Pólya\n\n\n\n\nThe cookbook gives a detailed description of ingredients and procedures but no proofs for its prescriptions or reasons for its recipes; the proof of the pudding is in the eating … Mathematics cannot be tested in exactly the same manner as a pudding; if all sorts of reasoning are debarred, a course of calculus may easily become an incoherent inventory of indigestible information. (Polya 1945)\n\nPólya was arguably the most influential mathematician of the 20th century. His basic research contributions span complex analysis, mathematical physics, probability theory, geometry, and combinatorics. He was a teacher par excellence who maintained a strong interest in pedagogical matters throughout his long career.\nHe was awarded a doctorate in mathematics having studied, essentially without supervision, a problem in the theory of geometric probability. Later Pólya looked at the Fourier transform of a probability measure, showing in 1923 that it was a characteristic function. He wrote on the normal distribution and coined the term “central limit theorem” in 1920 which is now standard usage.\nIn 1921 he proved his famous theorem on random walks on an integer lattice. He considered a d-dimensional array of lattice points where a point moves to any of its neighbors with equal probability. He asked whether given an arbitrary point A in the lattice, a point executing a random walk starting from the origin would reach A with probability 1. Pólya’s surprising answer was that it would for d=1 and for d=2, but it would not for d\\ge 3. In later work he looked at two points executing independent random walks and also at random walks satisfying the condition that the moving point never passed through the same lattice point twice.\nOne of Pólya’s notable achievements was his collaboration with the economist Abraham Wald during World War II. They developed statistical techniques to solve military problems, including estimating enemy troop movements and predicting the effectiveness of bombing missions. These contributions played a vital role in aiding the Allies during the war.\nHis book “How to Solve It,” published in 1945, presented problem-solving heuristics applicable to various mathematical domains, including probability and statistics. This influential work emphasized the importance of understanding the problem, devising a plan, executing the plan, and reflecting on the results. Pólya’s problem-solving strategies continue to be widely taught and practiced.\n\nFor a more extensive biography visit the following link\n\n\n\n\n\n\n\n\nBernoulli, J. 1713. Ars Conjectandi [the Art of Conjecturing]. Impensis Thurnisiorum. https://books.google.co.il/books?id=Ba5DAAAAcAAJ.\n\n\nPolya, G. 1945. How to Solve It. Princeton University Press. https://doi.org/10.1515/9781400828678.",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html",
    "href": "A03.html",
    "title": "29  Appendix: Continuous Distributions",
    "section": "",
    "text": "29.1 The Continuous Uniform\nFollowing a subjective view of distribution, which is more amenable to reinterpretation I use an indicator function to place restrictions on the range of parameter of the PDF.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-continuous-uniform",
    "href": "A03.html#sec-continuous-uniform",
    "title": "29  Appendix: Continuous Distributions",
    "section": "",
    "text": "29.1.1 Stories\n\n\n\n\n\n\nNoteDiscrete Uniform Finite set Parametrization\n\n\n\n\n\n\n\nX \\sim U[\\alpha,\\beta]\n\\tag{29.1}\n\n\n29.1.2 Moments\n\n\\mathbb{E}[X]=\\frac{(\\alpha+\\beta)}{2}\n\\tag{29.2}\n\n\\mathbb{V}ar[X]=\\frac{(\\beta-\\alpha)^2}{12}\n\\tag{29.3}\n\n\n29.1.3 Probability mass function (PDF)\n\nf(x)= \\frac{1}{\\alpha-\\beta} \\mathbb{I}_{\\{\\alpha \\le x \\le \\beta\\}}(x)\n\\tag{29.4}\n\n\n29.1.4 Cumulative distribution function (CDF)\n\nF(x\\mid \\alpha,\\beta)=\\begin{cases}\n  0,  & \\text{if }x &lt; \\alpha \\\\\n  \\frac{x-\\alpha}{\\beta-\\alpha}, & \\text{if } x\\in [\\alpha,\\beta]\\\\\n  1, & \\text{if } x &gt; \\beta\n  \\end{cases}\n\\tag{29.5}\n\n\n29.1.5 Prior\nSince a number of families have the uniform as a special case we can use them as priors when we want a uniform prior:\nNormal(0,1)= Beta(1,1)",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-beta-distribution",
    "href": "A03.html#sec-the-beta-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.2 The Beta Distribution",
    "text": "29.2 The Beta Distribution\n\n\n29.2.1 Story\nThe Beta distribution is used for random variables which take on values between 0 and 1. For this reason (and other reasons we will see later in the course), the Beta distribution is commonly used to model probabilities.\n\nX \\sim Beta(\\alpha, \\beta)\n\\tag{29.6}\n\n\n29.2.2 PDF & CDF\n\nf(x \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha−1}(1 − x)^{\\beta−1}\\mathbb{I}_{x\\in(0,1)}\\mathbb{I}_{\\alpha\\in\\mathbb{R}^+}\\mathbb{I}_{\\beta\\in\\mathbb{R}^+} \\qquad \\text{(PDF)}\n\\tag{29.7}\n\n\\begin{aligned}\n                 & F(x \\mid \\alpha,\\beta) &= I_x(\\alpha,\\beta) && \\text{(CDF)}\n\\\\ \\text{where } & I_w(u,v) & &&\\text{ is the regularized beta function: }\n\\\\               & I_w(u,v) &= \\frac{B(w; u, v)}{B(u,v)}\n\\\\ \\text{where } & B(w; u,v) &=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t  && \\text{ is the incomplete beta function  }\n\\\\ \\text{and }   & B(u,v)& && \\text{ is the (complete) beta function}\n\\end{aligned}\n\\tag{29.8}\n\n\n29.2.3 Moments\n\n\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha + \\beta} \\qquad (\\text{expectation})\n\\tag{29.9}\n\n\\mathbb{V}ar[X] = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)} \\qquad (\\text{variance})\n\\tag{29.10}\n\n\\mathbb{M}_X(t) = 1+ \\sum^\\infty_{i=1} \\left ( {\\prod^\\infty_{j=0} \\frac{\\alpha+j}{\\alpha + \\beta + j} } \\right ) \\frac{t^i}{i!}\n\\tag{29.11}\nwhere \\Gamma(·) is the Gamma function introduced with the gamma distribution.\nNote also that \\alpha &gt; 0 and \\beta &gt; 0.\n\n\n29.2.4 Relations\n\n\n\nRelations of the Beta distribution\n\n\nThe standard Uniform(0, 1) distribution is a special case of the beta distribution with \\alpha = \\beta = 1.\n\nUniform(0, 1) = Beta(1,1)\n\\tag{29.12}\n\n\n29.2.5 As a prior\nThe Beta distribution is often used as a prior for parameters that are probabilities,since it takes values from 0 and 1.\nDuring prior elicitation the parameters can be set using\n\nthe mean: \\alpha \\over \\alpha +\\beta which I would interpret here as count of successes over trials prior to seeing the data.\nvariance: Equation 29.10 or\nThe effective sample size which is \\alpha+\\beta (see course 1 lesson 7.3 for the derivation).",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-cauchy",
    "href": "A03.html#sec-cauchy",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.3 The Cauchy Distribution",
    "text": "29.3 The Cauchy Distribution\n\n\n29.3.1 PDF\n\n\\text{Cauchy}(y\\mid\\mu,\\sigma) = \\frac{1}{\\pi \\sigma} \\\n\\frac{1}{1 + \\left((y - \\mu)/\\sigma\\right)^2} \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{29.13}\n\n\n29.3.2 CDF\n\nF(x \\mid \\mu, \\sigma) = \\frac{1}{2} + \\frac{1}{\\pi}\\text{arctan}\\left(\\frac{x-\\mu}{\\sigma}\\right) \\qquad \\text{(CDF)}\n\\tag{29.14}\n\n\\mathbb{E}(X) = \\text{ undefined}\n\n\n\\mathbb{V}ar[X] = \\text{ undefined}\n\n\n\n29.3.3 As a prior\nThe Cauchy despite having no mean or variance is recommended as a prior for regression coefficients in Logistic regression. see (Gelman et al. 2008) this is analyzed and discussed in (Ghosh, Li, and Mitra 2018)",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-double-exponential",
    "href": "A03.html#sec-double-exponential",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.4 Double Exponential Distribution (Laplace)",
    "text": "29.4 Double Exponential Distribution (Laplace)\n \n\n\\text{DoubleExponential}(y \\mid \\mu,\\sigma) =\n\\frac{1}{2\\sigma} \\exp \\left( - \\, \\frac{|y - \\mu|}{\\sigma} \\right)\n\\qquad \\text (PDF)\n\\tag{29.15}",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-gamma-distribution",
    "href": "A03.html#sec-the-gamma-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.5 The Gamma Distribution",
    "text": "29.5 The Gamma Distribution\n\n\n29.5.1 Story\nIf X_1, X_2, ..., X_n are independent (and identically distributed Exp(\\lambda)) waiting times between successive events, then the total waiting time for all n events to occur Y = \\sum X_i will follow a gamma distribution with shape parameter \\alpha = n and rate parameter \\beta = \\lambda:\nWe denote this as:\n\nY =\\sum^N_{i=0} \\mathrm{Exp}_i(\\lambda) \\sim \\mathrm{Gamma}(\\alpha = N, \\beta = \\lambda)\n\\tag{29.16}\n\n\n29.5.2 PDF\n\nf(y \\mid \\alpha , \\beta) = \\frac{\\beta^\\alpha} {\\Gamma(\\alpha)} y^{\\alpha−1} e^{− \\beta y} \\mathbb{I}_{y \\ge \\theta }(y)\n\\tag{29.17}\n\n\n29.5.3 Moments\n\n\\mathbb{E}[Y] = \\frac{\\alpha}{ \\beta}\n\\tag{29.18}\n\n\\mathbb{V}ar[Y] = \\frac{\\alpha}{ \\beta^2}\n\\tag{29.19}\nwhere \\Gamma(·) is the gamma function, a generalization of the factorial function which can accept non-integer arguments. If n is a positive integer, then \\Gamma(n) = (n − 1)!.\nNote also that \\alpha &gt; 0 and $ &gt; 0$.\n\n\n29.5.4 Relations\n\n\n\nRelations of the Gamma Distribution\n\n\nThe exponential distribution is a special case of the Gamma distribution with \\alpha = 1. The gamma distribution commonly appears in statistical problems, as we will see in this course. It is used to model positive-valued, continuous quantities whose distribution is right-skewed. As \\alpha increases, the gamma distribution more closely resembles the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#inverse-gamma-distribution",
    "href": "A03.html#inverse-gamma-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.6 Inverse Gamma Distribution",
    "text": "29.6 Inverse Gamma Distribution\n\n\n29.6.1 PDF\n\n\\text{InvGamma}(y|\\alpha,\\beta) =\n\\frac{1} {\\Gamma(\\alpha)}\\frac{\\beta^{\\alpha}}{y^{\\alpha + 1}}  e^{- \\frac{ \\beta}{y}}\n   \\ \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{\\beta \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)\n\\tag{29.20}\n\n\n29.6.2 Moments\n\n\\mathbb{E}[X]=\\frac{\\beta}{\\alpha - 1} \\qquad \\text{Expectation}\n\\tag{29.21}\n\n\\mathbb{V}ar[X]=\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}\\qquad \\text{Variance}\n\\tag{29.22}",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#the-z-or-standard-normal-distribution",
    "href": "A03.html#the-z-or-standard-normal-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.7 The Z or Standard normal distribution",
    "text": "29.7 The Z or Standard normal distribution\n· The Standard normal distribution is given by:\n\nZ \\sim \\mathcal{N}[1,0]\n\\tag{29.23}\n\nf(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}}\n\\tag{29.24}\n\n\\mathcal{L}(\\mu,\\sigma)=\\prod_{i=1}^{n}{1 \\over 2 \\pi \\sigma}e^{−(x_i−\\mu)^2 \\over 2 \\sigma^2}\n\\tag{29.25}\n\n\\begin{aligned}\n\\ell(\\mu, \\sigma) &= \\log \\mathcal{L}(\\mu, \\sigma) \\\\&= -\\frac{n}{2}\\log(2\\pi) - n\\log\\sigma - \\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\n\\end{aligned}\\sigma^2\n\\tag{29.26}\n\n\\begin{aligned}\n  \\mathbb{E}(Z)&= 0 \\quad \\text{(Expectation)} \\qquad  \\mathbb{V}ar(Z)&= 1 \\quad \\text{(Variance)}\n\\end{aligned}\n\\tag{29.27}",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-normal-distribution",
    "href": "A03.html#sec-the-normal-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.8 The Normal Distribution",
    "text": "29.8 The Normal Distribution\n The normal, or Gaussian distribution is one of the most important distributions in statistics.\nIt arises as the limiting distribution of sums (and averages) of random variables. This is due to the Section 32.1. Because of this property, the normal distribution is often used to model the “errors,” or unexplained variations of individual observations in regression models.\nNow consider X = \\sigma Z+\\mu where \\sigma &gt; 0 and \\mu is any real constant. Then E(X) = E(\\sigma Z+\\mu) = \\sigma E(Z) + \\mu = \\sigma_0 + \\mu = \\mu and $Var(X) = Var(^2 Z + ) = ^2 Var(Z) + 0 = ^2 = ^2 $\nThen, X follows a normal distribution with mean \\mu and variance \\sigma^2 (standard deviation \\sigma) denoted as\n\nX \\sim N[\\mu,\\sigma^2]\n\\tag{29.28}\n\n29.8.1 PDF\n\nf(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}  e^{-\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}(x-\\mu)^2}\n\\tag{29.29}\n\n\n29.8.2 Moments\n\n\\mathbb{E}(x)= \\mu\n\\tag{29.30}\n\nVar(x)= \\sigma^2\n\\tag{29.31}\n\nThe normal distribution is symmetric about the mean \\mu and is often described as a bell-shaped curve.\nAlthough X can take on any real value (positive or negative), more than 99% of the probability mass is concentrated within three standard deviations of the mean.\n\nThe normal distribution has several desirable properties.\nOne is that if X_1 \\sim N(\\mu_1, \\sigma^2_1) and X_2 ∼ N(\\mu_2, \\sigma^2_2) are independent, then X_1+X_2 \\sim N(\\mu_1+\\mu_2, \\sigma^2_1+\\sigma^2_2).\nConsequently, if we take the average of n Independent and Identically Distributed (IID) Normal random variables we have:\n\n\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim N(\\mu, \\frac{\\sigma^2}{n})\n\\tag{29.32}\n\n\nCode\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = norm.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=0.00, var=1.00, skew=0.00, kurt=0.00\n\n\nCode\nx = np.linspace(norm.ppf(0.01),\n                norm.ppf(0.99), 100)\nax.plot(x, norm.pdf(x),\n       'r-', lw=5, alpha=0.6, label='norm pdf')\n\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nr = norm.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n\n(array([0.00377651, 0.        , 0.        , 0.01510605, 0.01888256,\n       0.08308327, 0.05287117, 0.09441281, 0.15861352, 0.26057936,\n       0.32100356, 0.36632171, 0.40786334, 0.41919288, 0.33988612,\n       0.31722704, 0.28701495, 0.22659075, 0.1510605 , 0.10196584,\n       0.06797722, 0.04909466, 0.02265907, 0.00755302, 0.        ,\n       0.00377651]), array([-3.52647772, -3.26168314, -2.99688857, -2.732094  , -2.46729942,\n       -2.20250485, -1.93771028, -1.6729157 , -1.40812113, -1.14332656,\n       -0.87853198, -0.61373741, -0.34894284, -0.08414826,  0.18064631,\n        0.44544088,  0.71023545,  0.97503003,  1.2398246 ,  1.50461917,\n        1.76941375,  2.03420832,  2.29900289,  2.56379747,  2.82859204,\n        3.09338661,  3.35818119]), [&lt;matplotlib.patches.Polygon object at 0x779a790b29e0&gt;])\n\n\nCode\nax.set_xlim([x[0], x[-1]])\n\n\n(-2.3263478740408408, 2.3263478740408408)\n\n\nCode\nax.legend(loc='best', frameon=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-t-distribution",
    "href": "A03.html#sec-the-t-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.9 The t-Distribution",
    "text": "29.9 The t-Distribution\n If we have normal data, we can use (Equation 29.32) to help us estimate the mean \\mu. Reversing the transformation from the previous section, we get:\n\n\\frac {\\hat X - \\mu}{\\sigma / \\sqrt(n)} \\sim N(0, 1)\n\\tag{29.33}\nHowever, we may not know the value of \\sigma. If we estimate it from data, we can replace it with S = \\sqrt{\\sum_i \\frac{(X_i-\\hat X)^2}{n-1}}, the sample standard deviation. This causes the expression (Equation 29.33) to no longer be distributed as a Standard Normal; but as a standard t-distribution with ν = n − 1 degrees of freedom\n\nX \\sim t[\\nu]\n\\tag{29.34}\nf(t\\mid\\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}}\\left (1 + \\frac{t^2}{\\nu}\\right)^{-(\\frac{\\nu+1}{2})}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{29.35}\n\n\\text{where }\\Gamma(w)=\\int_{0}^{\\infty}t^{w-1}e^{-t}\\mathrm{d}t \\text{ is the gamma function}\n\nf(t\\mid\\nu)={\\frac {1}{{\\sqrt {\\nu }}\\,\\mathrm {B} ({\\frac {1}{2}},{\\frac {\\nu }{2}})}}\\left(1+{\\frac {t^{2}}{\\nu }}\\right)^{-(\\nu +1)/2}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{29.36}\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the beta function}\n\n\\begin{aligned} && F(t)&=\\int _{-\\infty }^{t}f(u)\\,du=1-{\\tfrac {1}{2}}I_{x(t)}\\left({\\tfrac {\\nu }{2}},{\\tfrac {1}{2}}\\right) &&\\text{(CDF)}\n\\\\ \\text{where } && I_{x(t)}&= \\frac{B(x; u, v)}{B(u,v)} &&\\text{is the regularized Beta function}\n\\\\ \\text{where } &&B(w; u,v)&=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t &&  \\text{ is the incomplete Beta function }\n\\\\ \\text {and }&& B(u,v)&=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t && \\text{ is the (complete) beta function} \\end{aligned}\n\\tag{29.37}\n\\int _{-\\infty }^{t}f(u)\\,du={\\tfrac {1}{2}}+t{\\frac {\\Gamma \\left({\\tfrac {1}{2}}(\\nu +1)\\right)}{{\\sqrt {\\pi \\nu }}\\,\\Gamma \\left({\\tfrac {\\nu }{2}}\\right)}}\\,{}_{2}F_{1}\\left({\\tfrac {1}{2}},{\\tfrac {1}{2}}(\\nu +1);{\\tfrac {3}{2}};-{\\tfrac {t^{2}}{\\nu }}\\right)\n\n\n\\mathcal{L}(\\mu, \\sigma, \\nu) = \\prod_{i=1}^n \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right)^{-\\frac{\\nu+1}{2}}\n\\tag{29.38}\n\n\\begin{aligned}\n\\ell(\\mu, \\sigma, \\nu) &= \\log \\mathcal{L}(\\mu, \\sigma, \\nu) \\\\&= \\sum_{i=1}^n \\left[\\log\\Gamma\\left(\\frac{\\nu+1}{2}\\right) - \\log\\sqrt{\\nu\\pi} - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\frac{\\nu+1}{2}\\log\\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right)\\right] \\\\ &= n\\log\\Gamma\\left(\\frac{\\nu+1}{2}\\right) - n\\log\\sqrt{\\nu\\pi} - n\\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\frac{\\nu+1}{2}\\sum_{i=1}^n\\log\\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right).\n\\end{aligned}\n\\tag{29.39}\n\n\\mathbb{E}[Y] = 0 \\qquad \\text{ if } \\nu &gt; 1\n\\tag{29.40}\n\n\\mathbb{V}ar[Y] = \\frac{\\nu}{\\nu - 2} \\qquad \\text{ if } \\nu &gt; 2\n\\tag{29.41}",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#location-scale-parametrization-t-distribution",
    "href": "A03.html#location-scale-parametrization-t-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.10 Location Scale Parametrization t-distribution",
    "text": "29.10 Location Scale Parametrization t-distribution\n\nX=\\mu+\\sigma T\n\nThe resulting distribution is also called the non-standardized Student’s t-distribution.\nthis is another parameterization of the student-t with:\n\nlocation \\mu \\in \\mathbb{R}^+\nscale \\sigma \\in \\mathbb{R}^+\ndegrees of freedom \\nu \\in \\mathbb{R}^+\n\n\nf(x \\mid \\mu, \\sigma, \\nu) = \\frac{\\left(\\frac{\\nu }{\\nu +\\frac{(x-\\mu )^2}{\\sigma ^2}}\\right)^{\\frac{\\nu+1}{2}}}{\\sqrt{\\nu } \\sigma  B\\left(\\frac{\\nu }{2},\\frac{1}{2} \\right)}\n\\tag{29.42}\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the beta function}\n\n\nF(\\mu, \\sigma, \\nu) =\n\\begin{cases}\n\\frac{1}{2} I_{\\frac{\\nu  \\sigma ^2}{(x-\\mu )^2+\\nu  \\sigma  ^2}}\\left(\\frac{\\nu }{2},\\frac{1}{2}\\right),                & x\\leq \\mu  \n\\\\ \\frac{1}{2} \\left(I_{\\frac{(x-\\mu )^2}{(x-\\mu )^2+\\nu  \\sigma   ^2}}\\left(\\frac{1}{2},\\frac{\\nu }{2}\\right)+1\\right), & \\text{Otherwise}\n\\end{cases}\n\\tag{29.43}\nwhere I_w(u,v) is the regularized incomplete beta function:\n\n\\\\ I_w(u,v) = \\frac{B(w; u, v)}{B(u,v)}\n\nwhere B(w; u,v) is the incomplete beta function:\n\nB(w; u,v) =\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t\n\nAnd B(u,v) is the (complete) beta function\n\n\\mathbb{E}[X] = \\begin{cases}\n  \\mu,               & \\text{if }\\nu &gt; 1\n  \\\\\\text{undefined} & \\text{ otherwise}\n\\end{cases}\n\\tag{29.44}\n\n\\mathbb{V}ar[X] = \\frac{\\nu \\sigma^2}{\\nu-2}\n\\tag{29.45}\nThe t distribution is symmetric and resembles the Normal Distribution but with thicker tails. As the degrees of freedom increase, the t distribution looks more and more like the standard normal distribution.\n\n\n\n\n\n\n\nFigure 29.1: William Sealy Gosset AKA Student\n\n\n\n\n\n\n\n\nTipHistorical Note on The William Sealy Gosset A.K.A Student\n\n\n\n The student-t distribution is due to Gosset, William Sealy (1876-1937) who was an English statistician, chemist and brewer who served as Head Brewer of Guinness and Head Experimental Brewer of Guinness and was a pioneer of modern statistics. He is known for his pioneering work on small sample experimental designs. Gosset published under the pseudonym “Student” and developed most famously Student’s t-distribution – originally called Student’s “z” – and “Student’s test of statistical significance”.\nHe was told to use a Pseudonym and choose ‘Student’ after a predecessor at Guinness published a paper that leaked trade secrets. Gosset was a friend of both Karl Pearson and Ronald Fisher. Fisher suggested a correction to the student-t using the degrees of freedom rather than the sample size. Fisher is also credited with helping to publicize its use.\nfor a full biography see (Pearson et al. 1990)",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-exponential-distribution",
    "href": "A03.html#sec-the-exponential-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.11 The Exponential Distribution",
    "text": "29.11 The Exponential Distribution\n\n\n29.11.1 Story\nThe Exponential distribution models the waiting time between events for events with a rate lambda. Those events, typically, come from a Poisson process\nThe exponential distribution is often used to model the waiting time between random events. Indeed, if the waiting times between successive events are independent then they form an Exp(λ) distribution. Then for any fixed time window of length t, the number of events occurring in that window will follow a Poisson distribution with mean tλ.\n\nX \\sim Exp[\\lambda]\n\\tag{29.46}\n\n\n29.11.2 PDF\n\nf(x \\mid \\lambda) = \\frac{1}{\\lambda} e^{- \\frac{x}{\\lambda}}(x)\\mathbb{I}_{\\lambda\\in\\mathbb{R}^+ } \\mathbb{I}_{x\\in\\mathbb{R}^+_0 } \\quad \\text{(PDF)}\n\\tag{29.47}\n\n\n29.11.3 CDF\n\nF(x \\mid \\lambda) = 1 - e^{-\\lambda x} \\qquad \\text{(CDF)}\n\n\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda x_i}\n\\tag{29.48}\n\n\\begin{aligned} \\ell(\\lambda) &= \\log \\mathcal{L}(\\lambda) \\\\ &= \\sum_{i=1}^n \\log(\\lambda) - \\lambda x_i \\\\\n&= n\\log(\\lambda) - \\lambda\\sum_{i=1}^n x_i \\end{aligned}\n\\tag{29.49}\n\n\n29.11.4 Moments\n\n\\mathbb{E}(x)= \\lambda\n\\tag{29.50}\n\n\\mathbb{V}ar[X]= \\lambda^2\n\\tag{29.51}\n\n\\mathbb{M}_X(t)= \\frac{1}{1-\\lambda t} \\qquad t &lt; \\frac{1}{\\gamma}\n\\tag{29.52}\n\n\n29.11.5 Special cases:\n\nWeibull Y = X^{\\frac{1}{\\gamma}}\nRayleigh Y = \\sqrt{\\frac{2X}{\\lambda}}\nGumbel Y=\\alpha - \\gamma \\log(\\frac{X}{\\lambda})\n\n\n\n29.11.6 Properties:\n\nmemoryless\n\n\n\nCode\nimport numpy as np\nfrom scipy.stats import expon\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = expon.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\n\nmean=1.00, var=1.00, skew=2.00, kurt=6.00\n\n\nCode\nx = np.linspace(expon.ppf(0.01), expon.ppf(0.99), 100)\nax.plot(x, expon.pdf(x), 'r-', lw=5, alpha=0.6, label='expon pdf')\n\nrv = expon()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\nr = expon.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n\n(array([0.91444551, 0.72246248, 0.6466797 , 0.58100129, 0.41933137,\n       0.36880952, 0.31828766, 0.18187867, 0.17682648, 0.13135681,\n       0.09093933, 0.12125244, 0.06567841, 0.03031311, 0.07073059,\n       0.04041748, 0.00505219, 0.02526093, 0.02020874, 0.02526093,\n       0.01010437, 0.01010437, 0.01010437, 0.01515656, 0.01010437,\n       0.        , 0.        , 0.02526093, 0.00505219, 0.        ,\n       0.01010437]), array([7.00414267e-04, 1.98634570e-01, 3.96568725e-01, 5.94502880e-01,\n       7.92437036e-01, 9.90371191e-01, 1.18830535e+00, 1.38623950e+00,\n       1.58417366e+00, 1.78210781e+00, 1.98004197e+00, 2.17797612e+00,\n       2.37591028e+00, 2.57384443e+00, 2.77177859e+00, 2.96971274e+00,\n       3.16764690e+00, 3.36558106e+00, 3.56351521e+00, 3.76144937e+00,\n       3.95938352e+00, 4.15731768e+00, 4.35525183e+00, 4.55318599e+00,\n       4.75112014e+00, 4.94905430e+00, 5.14698845e+00, 5.34492261e+00,\n       5.54285676e+00, 5.74079092e+00, 5.93872508e+00, 6.13665923e+00]), [&lt;matplotlib.patches.Polygon object at 0x779a76f2c940&gt;])\n\n\nCode\nax.set_xlim([x[0], x[-1]])\n\n\n(0.010050335853501442, 4.605170185988091)\n\n\nCode\nax.legend(loc='best', frameon=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-lognormal-distribution",
    "href": "A03.html#sec-lognormal-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.12 LogNormal Distribution",
    "text": "29.12 LogNormal Distribution\nThe long normal arises when the a log transform is applied to the normal distribution.\n\n\n\\text{LogNormal}(y\\mid\\mu,\\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\ \\sigma} \\, \\frac{1}{y} \\ \\exp \\! \\left( - \\, \\frac{1}{2} \\, \\left( \\frac{\\log y - \\mu}{\\sigma} \\right)^2 \\right) \\ \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)\n\\tag{29.53}",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-pareto-distribution",
    "href": "A03.html#sec-pareto-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.13 Pareto Distribution",
    "text": "29.13 Pareto Distribution\n\n\n\\text{Pareto}(y|y_{\\text{min}},\\alpha) = \\frac{\\displaystyle\n\\alpha\\,y_{\\text{min}}^\\alpha}{\\displaystyle y^{\\alpha+1}} \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{y_{min} \\in \\mathbb{R}^+}\\mathbb{I}_{y\\ge y_{min} \\in \\mathbb{R}^+}\n\\qquad \\text (PDF)\n\\tag{29.54}\n\n\\mathrm{Pareto\\_Type\\_2}(y|\\mu,\\lambda,\\alpha) = \\\n\\frac{\\alpha}{\\lambda} \\, \\left( 1+\\frac{y-\\mu}{\\lambda}\n\\right)^{-(\\alpha+1)} \\! \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\lambda \\in \\mathbb{R}^+}\\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{y\\ge \\mu \\in \\mathbb{R}}\n\\qquad \\text (PDF)\n\\tag{29.55}\n\n\\mathbb{E}[X]=\\displaystyle{\\frac{\\alpha y_\\mathrm{min}}{\\alpha - 1}}\\mathbb{I}_{\\alpha&gt;1} \\qquad \\text (expectation)\n\\tag{29.56}\n\n\\mathbb{V}ar[X]=\\displaystyle{\\frac{\\alpha y_\\mathrm{min}^2}{(\\alpha - 1)^2(\\alpha - 2)}}\\mathbb{I}_{\\alpha&gt;2} \\qquad \\text (variance)\n\\tag{29.57}",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-weibull-distribution",
    "href": "A03.html#sec-weibull-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.14 Weibull Distribution",
    "text": "29.14 Weibull Distribution\n\n\n29.14.1 PDF\n\n\\text{Weibull}(y|\\alpha,\\sigma) =\n\\frac{\\alpha}{\\sigma} \\, \\left( \\frac{y}{\\sigma} \\right)^{\\alpha - 1} \\, e^{ - \\left( \\frac{y}{\\sigma} \\right)^{\\alpha}} \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-chi-squared-distribution",
    "href": "A03.html#sec-chi-squared-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.15 Chi Squared Distribution",
    "text": "29.15 Chi Squared Distribution\n\nThe chi squared distribution is a special case of the gamma. It is widely used in hypothesis testing and the construction of confidence intervals. It is parameterized using parameter \\nu for the degrees of predom\n\n29.15.1 PDF:\n\n\\text{ChiSquare}(y\\mid\\nu) = \\frac{2^{-\\nu/2}}     {\\Gamma(\\nu / 2)} \\,\ny^{\\nu/2 - 1} \\, \\exp \\! \\left( -\\, \\frac{1}{2} \\, y \\right) \\mathbb{I}_{\\nu \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}} \\qquad \\text (PDF)\n\\tag{29.58}\n\n\n29.15.2 CDF:\n\n{\\frac {1}{\\Gamma (\\nu/2)}}\\;\\gamma \\left({\\frac {\\nu}{2}},\\,{\\frac {x}{2}}\\right) \\qquad \\text (CDF)\n\\tag{29.59}\n\n\n29.15.3 MOMENTS\n\n\\mathbb{E}[X]=\\nu\n\\tag{29.60}\n\n\\mathbb{V}ar[X] = 2\\nu\n\\tag{29.61}",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-logistic-distribution",
    "href": "A03.html#sec-logistic-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.16 Logistic Distribution",
    "text": "29.16 Logistic Distribution\n\n\\text{Logistic}(y|\\mu,\\sigma) = \\frac{1}{\\sigma} \\\n\\exp\\!\\left( - \\, \\frac{y - \\mu}{\\sigma} \\right) \\ \\left(1 + \\exp\n\\!\\left( - \\, \\frac{y - \\mu}{\\sigma} \\right) \\right)^{\\!-2} \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}}  \\qquad \\text (PDF)\n\\tag{29.62}",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-f-distribution",
    "href": "A03.html#sec-f-distribution",
    "title": "29  Appendix: Continuous Distributions",
    "section": "29.17 F Distribution",
    "text": "29.17 F Distribution\n   The F-distribution or F-ratio, arises frequently as the null distribution of a test statistic, in the analysis of variance (ANOVA) and other F-tests.F DistributionF-ratio\n\n29.17.1 PDF\n\n\\frac {\\sqrt {\\frac {(d_{1}x)^{d_{1}}d_{2}^{d_{2}}}{(d_{1}x+d_{2})^{d_{1}+d_{2}}}}}{x\\,\\mathrm {B} \\!\\left({\\frac {d_{1}}{2}},{\\frac {d_{2}}{2}}\\right)}\n\\tag{29.63}\n\n\n29.17.2 CDF\n\n\\mathbb{I}_{\\frac {d_{1}x}{d_{1}x+d_{2}}}\\left({\\tfrac {d_{1}}{2}},{\\tfrac {d_{2}}{2}}\\right)\n\\tag{29.64}\n\n\n29.17.3 Moments\n\n\\mathbb{E}[X]=\\frac {d_{2}}{d_{2}-2}\n\\tag{29.65}\n\n\\mathbb{V}ar[X] = {\\frac {2\\,d_{2}^{2}\\,(d_{1}+d_{2}-2)}{d_{1}(d_{2}-2)^{2}(d_{2}-4)}}\n\\tag{29.66}\n\n\n\n\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” The Annals of Applied Statistics 2 (4). https://doi.org/10.1214/08-aoas191.\n\n\nGhosh, Joyee, Yingbo Li, and Robin Mitra. 2018. “On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression.” Bayesian Analysis 13 (2). https://doi.org/10.1214/17-ba1051.\n\n\nPearson, E. S., W. S. Gosset, R. L. Plackett, and G. A. Barnard. 1990. Student: A Statistical Biography of William Sealy Gosset. Clarendon Press. https://books.google.co.il/books?id=LBDvAAAAMAAJ.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A05.html",
    "href": "A05.html",
    "title": "30  Appendix: Exponents & Logarithms",
    "section": "",
    "text": "30.1 Exponents\nExponents are of the form a^x where:\nRecall that a^0 = 1. Exponents have the following useful properties\nNote: that the first property requires that both terms have the same base a.\nWe cannot simplify a^x ·b^y if a \\ne b.",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A05.html#sec-exponents",
    "href": "A05.html#sec-exponents",
    "title": "30  Appendix: Exponents & Logarithms",
    "section": "",
    "text": "a (called the base) and\nx (called the exponent) is any real number.\n\n\n\na^x· a^y = a^{x+y}\n(a^x)^y = a^{x·y}\n\n\n\n\nOne common base is the number e which is approximately equal to 2.7183.\nThe function e^x is so common in mathematics and has its own symbol e^x = \\exp(x).\nBecause e &gt; 0 we have e^x &gt; 0 for all real numbers x\n\\lim_{x \\to \\infty} x = e^{−x} = 0.",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A05.html#sec-natural-logarithms",
    "href": "A05.html#sec-natural-logarithms",
    "title": "30  Appendix: Exponents & Logarithms",
    "section": "30.2 Natural Logarithms",
    "text": "30.2 Natural Logarithms\nWe will need to manipulate long products of probabilities. Since there often comprise small fractions, their calculation on computers can be problematic due to the underflow of floats. We will therefore prefer to convert these products into sums of logarithms.\n\nDefinition 30.1 (The Logarithm) A log is the inverse of a power. We can use (Equation 30.1).\n\ny = a^x \\implies log_a(y) = x\n\\tag{30.1}\n\n\nDefinition 30.2 (The Natural log) The natural logarithm function has base e and is written without the subscript\n\nlog_e(y) = log(y)\n\\tag{30.2}\n\n\nTheorem 30.1 (Logs take positive values) logs only exist for values greater than 0\n\n\\forall x(e^x &gt; 0) \\implies \\exists \\log(y) \\iff {y &gt; 0}\n\n\nWe can use the properties of exponents from the previous section to obtain some important properties of logarithms:\n\nDefinition 30.3 (Log of a product) we can use Equation 30.3 to convert a log of a product to a sum of logs.\n\n\\log(x·y) = \\log(x) + \\log(y)\n\\tag{30.3}\n\n\nDefinition 30.4 (Log of a quotient) we can use Equation 30.4 to convert a log of a quotient to a difference of logs.\n\n\\log(\\frac{x}{y}) = log(x) − log(y)\n\\tag{30.4}\n\n\nDefinition 30.5 (Log of a power) we can use Equation 30.5 to convert a log of a variable raised to a power into the product.\n\n    \\log(x^b) = b \\cdot log(x)\n\\tag{30.5}\n\n\nDefinition 30.6 (Log of one) we can use (Equation 30.6) to replace a log of 1 with zero since $x(x^0 = 1) $\n\n    \\log(1)=0\n\\tag{30.6}\n\n\n30.2.1 Log of exponent\nwe can use (Equation 30.7) to cancel a log of an exponent since the log is the inverse function of the exponent.\n\nexp(log(y)) = log(exp(y)) = y\n\\tag{30.7}\n\nExample 30.1 (Logarithm) \n    \\begin{aligned}\n    log \\frac{5^2}{10}= 2 log(5) − log(10) ≈ 0.916.\n    \\end{aligned}\n\n\n\nDefinition 30.7 (Change of base for a log) we can use (Equation 30.8) to change the base of a logarithm.\n\n    \\log_b(a)=\\frac{\\log_c(a)}{\\log_c(n)}\n\\tag{30.8}\n\n\nDefinition 30.8 (Derivative of a Log) we can use (Equation 30.9) to differentiate a log.\n\n    \\frac{d}{dx} \\log_(x)=\\frac{1}{x}\n\\tag{30.9}\n\n\nBecause the natural logarithm is a monotonically increasing one-to-one function, finding the x which maximizes any (positive-valued function) f(x) is equivalent to maximizing log(f(x)).\nThis is useful because we often take derivatives to maximize functions.\nIf f(x) has product terms, then log(f(x)) will have summation terms, which are usually simpler when taking derivatives.",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A07.html",
    "href": "A07.html",
    "title": "31  Appendix: The Law of Large Numbers",
    "section": "",
    "text": "31.1 Law of large numbers\nSuppose we observe data D=\\{x_1, \\ldots, x_n\\} with each x_i \\sim F .\nBy the strong law of large numbers the empirical distribution \\hat{F}_n based on data D=\\{x_1, \\ldots, x_n\\} converges to the true underlying distribution F as n \\rightarrow \\infty almost surely:\n\\hat{F}_n\\overset{a. s.}{\\to} F\nThe Glivenko–Cantelli asserts that the convergence is uniform. Since the strong law implies the weak law we also have convergence in probability:\n\\hat{F}_n\\overset{P}{\\to} F\nCorrespondingly, for n \\rightarrow \\infty the average \\text{E}_{\\hat{F}_n}(h(x)) = \\frac{1}{n} \\sum_{i=1}^n h(x_i) converges to the expectation \\text{E}_{F}(h(x)) .",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Appendix: The Law of Large Numbers</span>"
    ]
  },
  {
    "objectID": "A08.html",
    "href": "A08.html",
    "title": "32  Appendix: The Central Limit Theorem",
    "section": "",
    "text": "32.1 Central Limit Theorem\nThe Central Limit Theorem is one of the most important results in statistics, stating that with sufficiently large sample sizes, the sample average approximately follows a normal distribution. This underscores the importance of the normal distribution, as well as most of the methods commonly used which make assumptions about the data being normally distributed.\nLet’s first stop and think about what it means for the sample average to have a distribution. Imagine going to the store and buying a bag of your favorite brand of chocolate chip cookies. Suppose the bag has 24 cookies in it. Will each cookie have the exact same number of chocolate chips in it? It turns out that if you make a batch of cookies by adding chips to dough and mixing it really well, then putting the same amount of dough onto a baking sheet, the number of chips per cookie closely follows a Poisson distribution. (In the limiting case of chips having zero volume, this is exactly a Poisson process.) Thus we expect there to be a lot of variability in the number of chips per cookie. We can model the number of chips per cookie with a Poisson distribution. We can also compute the average number of chips per cookie in the bag. For the bag we have, that will be a particular number. But there may be more bags of cookies in the store. Will each of those bags have the same average number of chips? If all of the cookies in the store are from the same industrial-sized batch, each cookie will individually have a Poisson number of chips. So the average number of chips in one bag may be different from the average number of chips in another bag. Thus we could hypothetically find out the average number of chips for each bag in the store. And we could think about what the distribution of these averages is, across the bags in the store, or all the bags of cookies in the world. It is this distribution of averages that the central limit theorem says is approximately a normal distribution, with the same mean as the distribution for the individual cookies, but with a standard deviation that is divided by the square root of the number of samples in each average (i.e., the number of cookies per bag).",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Appendix: The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "A08.html#sec-cl-theorem",
    "href": "A08.html#sec-cl-theorem",
    "title": "32  Appendix: The Central Limit Theorem",
    "section": "",
    "text": "Theorem 32.1 (Central Limit Theorem) Let X_1, ..., X_n be independent and identically distributed (IID) with \\mathbb{E}(X_i) = \\mu and Var(X_i) = \\sigma^2 &lt;\\infty\nThen:\n\n\\lim_{n\\to\\infty} \\sqrt{n} \\sum_{i=0}^{n} \\frac{1}{n}\\frac{(X_i-\\mu)}{\\sigma} = \\sum_{i=0}^{n} \\frac{X_i-\\mu}{\\sqrt{n} \\sigma} = N(0, 1)\n\nThat is, \\hat{X_n} is approximately normally distributed with mean µ and variance \\frac{\\sigma}{2/n} or standard deviation \\frac{\\sigma}{\\sqrt{n}}.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Appendix: The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "A09.html",
    "href": "A09.html",
    "title": "33  Appendix: Conjugate Priors",
    "section": "",
    "text": "33.1 Conjugate Priors",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Appendix: Conjugate Priors</span>"
    ]
  },
  {
    "objectID": "A09.html#sec-conjugate-priors",
    "href": "A09.html#sec-conjugate-priors",
    "title": "33  Appendix: Conjugate Priors",
    "section": "",
    "text": "Table 33.1: Conjugate prior\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\nConjugate prior\nPosterior\nPosterior predictive\n\n\n\n\n\\text{Bernoulli}(p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left( \\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}\\right)}\n{\\displaystyle \\mathbb{P}r({\\tilde {x}}=1)={\\frac {\\alpha '}{\\alpha '+\\beta '}}}\n\n\n\\text{Binomial}(trials=m,p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left(\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}\\right)}\n{\\displaystyle \\operatorname {BetaBin} ({\\tilde {x}}|\\alpha ',\\beta ')}\n\n\n\\text{NegBinomial}(fails=r)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left( \\alpha +rn,\\beta + \\sum _{i=1}^{n} x_{i}\\right)}\n{\\displaystyle \\operatorname {BetaNegBin} ({\\tilde {x}}|\\alpha ',\\beta ')}\n\n\n\\text{Poisson}(rate=\\lambda)\n\\text{Gamma}(k,\\theta)\n{\\displaystyle \\text{Gamma}\\left( k+\\sum _{i=1}^{n}x_{i},\\ {\\frac {\\theta }{n\\theta +1}}\\!\\right)}\n{\\displaystyle \\operatorname {NB} \\left({\\tilde {x}}\\mid k',{\\frac {1}{\\theta '+1}}\\right)}\n\n\n\\text{Poisson}(rate=\\lambda)\n\\text{Gamma}(\\alpha,\\beta)\n{\\displaystyle\\text{Gamma}\\left( \\alpha +\\sum _{i=1}^{n}x_{i},\\ \\beta +n\\!\\right)}\n{\\displaystyle \\operatorname {NB} \\left({\\tilde {x}}\\mid \\alpha ',{\\frac {\\beta '}{1+\\beta '}}\\right)}\n\n\n\\text{Categorical}(probs=p,cats=k)\n\\text{Dir}(\\alpha_k)\\mathbb{I}_{k\\ge1}\n{\\displaystyle \\text{Dir}\\left({ {\\boldsymbol {\\alpha }}+(c_{1},\\ldots ,c_{k})}\\right)}\n{\\displaystyle {\\begin{aligned}\\mathbb{P}r({\\tilde {x}}=i)&={\\frac {{\\alpha _{i}}'}{\\sum _{i}{\\alpha _{i}}'}}\\\\&={\\frac {\\alpha _{i}+c_{i}}{\\sum _{i}\\alpha _{i}+n}}\\end{aligned}}}\n\n\n\\text{Multinomial}(probs=p,cats=k)\n\\text{Dir}(\\alpha_k)\\mathbb{I}_{k\\ge1}\n{\\displaystyle \\text{Dir}\\left({ {\\boldsymbol {\\alpha }}+\\sum _{i=1}^{n}\\mathbf {x} _{i}\\!}\\right)}\n{\\displaystyle \\operatorname {DirMult} ({\\tilde {\\mathbf {x} }}\\mid {\\boldsymbol {\\alpha }}')}\n\n\n\\text{Hypergeometric}(pop=n)\n\\text{BetaBinomial}(\\alpha,\\beta,n=N)\n{\\displaystyle \\text{BetaBinomial}\\left({\\displaystyle \\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}}\\right)}\n\n\n\n\\text{Geometric}(p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle\\text{Beta}\\left( \\alpha +n,\\,\\beta +\\sum _{i=1}^{n}x_{i}\\right)}",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Appendix: Conjugate Priors</span>"
    ]
  },
  {
    "objectID": "A10.html",
    "href": "A10.html",
    "title": "34  Appendix: Link Function",
    "section": "",
    "text": "34.1 Link function\nIn statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\nThe link function provides the relationship between the linear predictor and the mean of the distribution function. There are many commonly used link functions, and their choice is informed by several considerations. There is always a well-defined canonical link function which is derived from the exponential of the response’s density function. However, in some cases, it makes sense to try to match the domain of the link function to the range of the distribution function’s mean, or use a non-canonical link function for algorithmic purposes, for example Bayesian probit regression.\nWhen using a distribution function with a canonical parameter \\theta , the canonical link function is the function that expresses \\theta in terms of \\mu i.e. \\theta =b(\\mu) . For the most common distributions, the mean μ is one of the parameters in the standard form of the distribution’s Density function, and then b(\\mu ) is the function as defined above that maps the density function into its canonical form. When using the canonical link function, b(\\mu )=\\theta =\\mathbf {X} {\\boldsymbol {\\beta }} , which allows \\mathbf{X}^{T}\\mathbf{Y} to be a sufficient statistic for \\beta .\nFollowing is a table of several exponential-family distributions in common use and the data they are typically used for, along with the canonical link functions and their inverses (sometimes referred to as the mean function, as done here).\nIn the cases of the exponential and gamma distributions, the domain of the canonical link function is not the same as the permitted range of the mean. In particular, the linear predictor may be positive, which would give an impossible negative mean. When maximizing the likelihood, precautions must be taken to avoid this. An alternative is to use a non-canonical link function.\nIn the case of the Bernoulli, binomial, categorical and multinomial distributions, the support of the distributions is not the same type of data as the parameter being predicted. In all of these cases, the predicted parameter is one or more probabilities, i.e. real numbers in the range [0,1]. The resulting model is known as Logistic regression (or Multinomial logistic regression in the case that K-way rather than binary values are being predicted).\nFor the Bernoulli and binomial distributions, the parameter is a single probability, indicating the likelihood of occurrence of a single event. The Bernoulli still satisfies the basic condition of the generalized linear model in that, even though a single outcome will always be either 0 or 1, the expected value will nonetheless be a real-valued probability, i.e. the probability of occurrence of a “yes” (or 1) outcome. Similarly, in a binomial distribution, the expected value is Np, i.e. the expected proportion of “yes” outcomes will be the probability to be predicted.\nFor categorical and multinomial distributions, the parameter to be predicted is a K-vector of probabilities, with the further restriction that all probabilities must add up to 1. Each probability indicates the likelihood of occurrence of one of the K possible values. For the multinomial distribution, and for the vector form of the categorical distribution, the expected values of the elements of the vector can be related to the predicted probabilities similarly to the binomial and Bernoulli distributions.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A10.html#sec-link-function",
    "href": "A10.html#sec-link-function",
    "title": "34  Appendix: Link Function",
    "section": "",
    "text": "Common distributions with typical uses and canonical link functions\n\n\n\n\n\n\n\n\n\n\nDistribution\nSupport\nUses\nLink name\nLink fn\nMean fn\n\n\n\n\nNormal\n\\mathbb{R}\n\nIdentity\n\\mathbf {X} {\\boldsymbol {\\beta }}=\\mu\n\n\n\nExpoential\n\\mathbb{R}^+_0\n\nNegative inverse\n\\mathbf {X} {\\boldsymbol {\\beta }}=-\\mu^{-1}\n\n\n\nGamma\n\\mathbb{R}^+_0\n\nNegative inverse\n\\mathbf {X} {\\boldsymbol {\\beta }}=-\\mu^{-1}\n\n\n\nInverse Gamma\n\\mathbb{R}^+_0\n\nInverse squared\n\\mathbf {X} {\\boldsymbol {\\beta }}=\\mu^{-2}\n\n\n\nPoisson\n\\mathbb{N}_0\n\nLog\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\mu)\n\n\n\nBernuolli\n\\{0,1\\}\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})\n\n\n\nBinomial\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{n-\\mu})\n\n\n\nCategorical\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})\n\n\n\nMultinomial\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A10.html#credits",
    "href": "A10.html#credits",
    "title": "34  Appendix: Link Function",
    "section": "Credits:",
    "text": "Credits:\nThis page is based on the Generalized linear model article on Wikipedia, which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. By Wikimedia contributors, available under CC BY-SA 3.0.\nThe text has been modified for clarity and conciseness.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A11.html",
    "href": "A11.html",
    "title": "35  Bayes by backprop",
    "section": "",
    "text": "35.1 Introduction\nThis appendix reviews of a method to introduce weight uncertainty into neural networks called the “Bayes by Backprop” method introduced in (Blundell et al. 2015). where, the main question is how to determine the parameters of the distribution for each network weight. I learned about it from Probabilistic Deep Learning with TensorFlow 2 by Dr Kevin Webster, S reading from based this on\nThe authors note that prior work which considered uncertainty at the hidden unit (H_i) an approach that allows to state the uncertainty with respect to a particular observation and which is an easier problem since the number of weight is greater by two orders of magnitude. But considering the uncertainty in the weights is complementary, in the sense that it captures uncertainty about which neural network is appropriate, leading to regularization of the weights and model averaging. This weight uncertainty can be used to drive the exploration/exploitation in contextual bandit problems using Thompson sampling . Weights with greater uncertainty introduce more variability into the decisions made by the network, naturally leading to exploration. As more data are observed, the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the environment is better understood.\nIn a traditional neural network, as shown in the upper left, each weight has a single value. But in the true value for these weights is not certain. Much of this uncertainty comes from imperfect training data, which is an approximation of the the distribution of the data generating process from which the data were sampled. Recall that this is called epistemic uncertainty, which we expect to decrease as the amount of training data increases.\nIn this method, we want to include such uncertainty in deep learning models. This is done by changing each weight from a single deterministic value to a probability distribution. We then learn the parameters of this distribution. Consider a neural network weight w_i . In a standard (deterministic) neural network, this has a single value \\hat{w}_i , learnt via backpropagation. In a neural network with weight uncertainty, each weight is represented by a probability distribution, and the parameters of this distribution are learned via backpropagation. Suppose, for example, that each weight has a normal distribution. This has two parameters: a mean \\mu_i and a standard deviation \\sigma_i .\nSince the weights are uncertain, the feedforward value of some input x_i is not constant. A single feedforward value is determined in two steps: 1. Sample each network weight from their respective distributions – this gives a single set of network weights. 2. Use these weights to determine a feedforward value \\hat{y}_i .\nHence, the key question is how to determine the parameters of the distribution for each network weight. The paper introduces exactly such a scheme, called Bayes by Backprop.",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#introduction",
    "href": "A11.html#introduction",
    "title": "35  Bayes by backprop",
    "section": "",
    "text": "Fig. 1 from (Blundell et al. 2015) contrasting traditional and Bayesian neural networks\n\n\n\n\n\nClassic deterministic NN: w_i = \\hat{w}_i\nNN with weight uncertainty represented by normal distribution: w_i \\sim N(\\hat{\\mu}_i, \\hat{\\sigma}_i) .",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#bayesian-learning",
    "href": "A11.html#bayesian-learning",
    "title": "35  Bayes by backprop",
    "section": "35.2 Bayesian learning",
    "text": "35.2 Bayesian learning\nNote: We use the notation P to refer to a probability density. For simplicity, we’ll only consider continuous distributions (which have a density). In the case of discrete distributions, P would represent a probability mass and integrals should be changed to sums. However, the formulae are the same.\nWhat you need to know now is that Bayesian methods can be used to calculate the distribution of a model parameter given some data. In the context of weight uncertainty in neural networks, this is convenient, since we are looking for the distribution of weights (model parameters) given some (training) data. The key step relies on Bayes’ theorem. This theorem states, in mathematical notation, that\n\n\\mathbb{P}r(w \\mid D) = \\frac{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)}{\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'}\n\nwhere the terms mean the following:\n\nD is some data, e.g. x and y value pairs: D = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\} . This is sometimes called the evidence.\nw is the value of a model weight.\n\\mathbb{P}r(w) is called the prior. This is our “prior” belief on the probability density of a model weight, i.e. the distribution that we postulate before seeing any data.\n\\mathbb{P}r(D \\mid w) is the likelihood of having observed data D given weight w . It is precisely the same likelihood used to calculate the negative log-likelihood.\n\\mathbb{P}r(w \\mid D) is the posterior density of the distribution of the model weight at value w , given our training data. It is called posterior since it represents the distribution of our model weight after taking the training data into account.\n\nNote that the term {\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'} = \\mathbb{P}r(D) does not depend on w (as the w' is an integration variable). It is only a normalization term. For this reason, we will from this point on write Bayes’ theorem as\n\n\\mathbb{P}r(w \\mid D) = \\frac{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)}{\\mathbb{P}r(D)}.\n\nBayes’ theorem gives us a way of combining data with some “prior belief” on model parameters to obtain a distribution for these model parameters that considers the data, called the posterior distribution.",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#bayesian-neural-network-with-weight-uncertainty-in-principle",
    "href": "A11.html#bayesian-neural-network-with-weight-uncertainty-in-principle",
    "title": "35  Bayes by backprop",
    "section": "35.3 Bayesian neural network with weight uncertainty – in principle",
    "text": "35.3 Bayesian neural network with weight uncertainty – in principle\nThe above formula gives a way to determine the distribution of each weight in the neural network:\n\nPick a prior density \\mathbb{P}r(w) .\nUsing training data D , determine the likelihood \\mathbb{P}r(D \\mid w) .\nDetermine the posterior density \\mathbb{P}r(w \\mid D) using Bayes’ theorem.\n\nThis is the distribution of the NN weight.\nWhile this works in principle, in many practical settings it is difficult to implement. The main reason is that the normalization constant {\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'} = \\mathbb{P}r(D) may be very difficult to calculate, as it involves solving or approximating a complicated integral. For this reason, approximate methods, such as Variational Bayes described below, are often employed.",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#variational-bayes",
    "href": "A11.html#variational-bayes",
    "title": "35  Bayes by backprop",
    "section": "35.4 Variational Bayes",
    "text": "35.4 Variational Bayes\nVariational Bayes methods approximate the posterior distribution with a second function, called a variational posterior. This function has a known functional form, and hence avoids the need to determine the posterior \\mathbb{P}r(w \\mid D) exactly. Of course, approximating a function with another one has some risks, since the approximation may be very bad, leading to a posterior that is highly inaccurate. In order to mediate this, the variational posterior usually has a number of parameters, denoted by \\theta , that are tuned so that the function approximates the posterior as well as possible. Let’s see how this works below.\nInstead of \\mathbb{P}r(w \\mid D) , we assume the network weight has density q(w \\mid \\theta) , parameterized by \\theta . q(w \\mid \\theta) is known as the variational posterior . We want q(w \\mid \\theta) to approximate \\mathbb{P}r(w \\mid D) , so we want the “difference” between q(w \\mid \\theta) and \\mathbb{P}r(w \\mid D) to be as small as possible. This “difference” between the two distributions is measured by the Kullback-Leibler divergence D_{\\text{KL}} (note that this is unrelated to the D we use to denote the data). The Kullback-Leibler divergence between two distributions with densities f(x) and g(x) respectively is defined as\n\nD_{KL} (f(x) \\parallel g(x)) = \\int f(x) \\log \\left( \\frac{f(x)}{g(x)} \\right) \\text{d} x\n\nNote that this function has value 0 (indicating no difference) when f(x) \\equiv g(x) , which is the result we expect. We use the convention that \\frac{0}{0} = 1 here.\nViewing the data D as a constant, the Kullback-Leibler divergence between q(w \\mid \\theta) and \\mathbb{P}r(w \\mid D) is hence:\n\n\\begin{aligned}\n  D_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D)) &= \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta)}{\\mathbb{P}r(w \\mid D)} \\right) \\text{d} w \\\\\n&= \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta) \\mathbb{P}r(D)}{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)} \\right) \\text{d} w \\\\\n&= \\int q(w \\mid \\theta) \\log \\mathbb{P}r(D) \\text{d} w + \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta)}{\\mathbb{P}r(w)} \\right) \\text{d} w - \\int q(w \\mid \\theta) \\log \\mathbb{P}r(D \\mid w) \\text{d} w \\\\\n&= \\log \\mathbb{P}r(D) + D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w))\n\\end{aligned}\n\nwhere, in the last line, we have used\n\n\\int q(w \\mid \\theta) \\log \\mathbb{P}r(D) \\text{d}w = \\log \\mathbb{P}r(D) \\int q(w \\mid \\theta) \\text{d} w = \\log \\mathbb{P}r(D)\n\nsince q(w \\mid \\theta) is a probability distribution and hence integrates to 1. If we consider the data D to be constant, the first term is a constant also, and we may ignore it when minimizing the above. Hence, we are left with the function\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w))\n\\end{aligned}\n\nNote that this function depends only on \\theta and D , since w is an integration variable. This function has a nice interpretation as the sum of: - The Kullback-Leibler divergence between the variational posterior q(w \\mid \\theta) and the prior \\mathbb{P}r(w) . This is called the complexity cost, and it depends on \\theta and the prior but not the data D . - The expectation of the negative log likelihood \\log \\mathbb{P}r(D \\mid w) under the variational posterior q(w \\mid \\theta) . This is called the likelihood cost and it depends on \\theta and the data but not the prior.\nL(\\theta \\mid D) is the loss function that we minimize to determine the parameter \\theta . Note also from the above derivation, that we have\n\n\\begin{aligned}\n\\log \\mathbb{P}r(D) &= \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) - D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) + D_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D))\\\\\n&\\ge \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) - D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) =: ELBO\n\\end{aligned}\n\nwhich follows because D_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D)) is non negative. The final expression on the right hand side is therefore a lower bound on the log-evidence, and is called the evidence lower bound, often shortened to ELBO. The {ELBO} is the negative of our loss function, so minimizing the loss function is equivalent to maximizing the ELBO.\nMaximizing the ELBO requires a trade off between the KL term and expected log-likelihood term. On the one hand, the divergence between q(w \\mid \\theta) and \\mathbb{P}r(w) should be kept small, meaning the variational posterior shouldn’t be too different to the prior. On the other, the variational posterior parameters should maximize the expectation of the log-likelihood \\log \\mathbb{P}r(D \\mid w) , meaning the model assigns a high likelihood to the data.",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#a-backpropagation-scheme",
    "href": "A11.html#a-backpropagation-scheme",
    "title": "35  Bayes by backprop",
    "section": "35.5 A backpropagation scheme",
    "text": "35.5 A backpropagation scheme\n\n35.5.1 The idea\nWe can use the above ideas to create a neural network with weight uncertainty, which we will call a Bayesian neural network. From a high level, this works as follows. Suppose we want to determine the distribution of a particular neural network weight w .\n\nAssign the weight a prior distribution with density \\mathbb{P}r(w) , which represents our beliefs on the possible values of this network before any training data. This may be something simple, like a unit Gaussian. Furthermore, this prior distribution will usually not have any trainable parameters.\nAssign the weight a variational posterior with density q(w \\mid \\theta) with some trainable parameter \\theta .\nq(w \\mid \\theta) is the approximation for the weight’s posterior distribution. Tune \\theta to make this approximation as accurate as possible as measured by the ELBO.\n\nThe remaining question is then how to determine \\theta . Recall that neural networks are typically trained via a backpropagation algorithm, in which the weights are updated by perturbing them in a direction that reduces the loss function. We aim to do the same here, by updating \\theta in a direction that reduces L(\\theta \\mid D) .\nHence, the function we want to minimise is\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) \\\\\n&= \\int q(w \\mid \\theta) ( \\log q(w \\mid \\theta) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w) ) \\text{d}w.\n\\end{aligned}\n\nIn principle, we could take derivatives of L(\\theta \\mid D) with respect to \\theta and use this to update its value. However, this involves doing an integral over w , and this is a calculation that may be impossible or very computationally expensive. Instead, we want to write this function as an expectation and use a Monte Carlo approximation to calculate derivatives. At present, we can write this function as\n\nL(\\theta \\mid D) = \\mathbb{E}_{q(w \\mid \\theta)} ( \\log q(w \\mid \\theta) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w) )\n\nHowever, taking derivatives with respect to \\theta is difficult because the underlying distribution the expectation is taken with respect to depends on \\theta . One way we can handle this is with the reparameterization trick.\n\n\n35.5.2 The reparameterization trick\nThe reparameterization trick is a way to move the dependence on \\theta around so that an expectation may be taken independently of it. It’s easiest to see how this works with an example. Suppose q(w \\mid \\theta) is a Gaussian, so that \\theta = (\\mu, \\sigma) . Then, for some arbitrary f(w; \\mu, \\sigma) , we have\n\n\\begin{aligned}\n\\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\int q(w \\mid \\mu, \\sigma) f(w; \\mu, \\sigma) \\text{d}w \\\\\n&= \\int \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{1}{2 \\sigma^2} (w - \\mu)^2 \\right) f(w; \\mu, \\sigma) \\text{d}w \\\\\n&= \\int \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left( -\\frac{1}{2} \\epsilon^2 \\right) f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right) \\text{d}\\epsilon \\\\\n&= \\mathbb{E}_{\\epsilon \\sim N(0, 1)} (f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right) )\n\\end{aligned}\n\nwhere we used the change of variable w = \\mu + \\sigma \\epsilon . Note that the dependence on \\theta = (\\mu, \\sigma) is now only in the integrand and we can take derivatives with respect to \\mu and \\sigma:\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{\\epsilon \\sim N(0, 1)} (f \\left( w; \\mu, \\sigma \\right) ) = \\mathbb{E}_{\\epsilon \\sim N(0, 1)} \\frac{\\partial}{\\partial \\mu} f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right)\n\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\sigma} \\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\frac{\\partial}{\\partial \\sigma} \\mathbb{E}_{\\epsilon \\sim N(0, 1)} (f \\left( w; \\mu, \\sigma \\right) ) = \\mathbb{E}_{\\epsilon \\sim N(0, 1)} \\frac{\\partial}{\\partial \\sigma} f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right)\n\\end{aligned}\n\nFinally, note that we can approximate the expectation by its Monte Carlo estimate:\n\n\\begin{aligned}\n\\mathbb{E}_{\\epsilon \\sim N(0, 1)}  \\frac{\\partial}{\\partial \\theta} f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right) \\approx \\sum_{i}  \\frac{\\partial}{\\partial \\theta} f \\left( \\mu + \\sigma \\epsilon_i; \\mu, \\sigma \\right),\\qquad \\epsilon_i \\sim N(0, 1).\n\\end{aligned}\n\nThe above reparameterization trick works in cases where we can write the w = g(\\epsilon, \\theta) , where the distribution of the random variable \\epsilon is independent of \\theta .\n\n\n35.5.3 Implementation\nPutting this all together, for our loss function L(\\theta \\mid D) \\equiv L(\\mu, \\sigma \\mid D) , we have\n\nf(w; \\mu, \\sigma) = \\log q(w \\mid \\mu, \\sigma) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w)\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu} L(\\mu, \\sigma \\mid D) \\approx \\sum_{i} \\left( \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\mu} \\right)\n\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\sigma} L(\\mu, \\sigma \\mid D) \\approx \\sum_{i} \\left( \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} \\epsilon_i + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\sigma} \\right)\n\\end{aligned}\n\n\nf(w; \\mu, \\sigma) = \\log q(w \\mid \\mu, \\sigma) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w)\n\nwhere w_i = \\mu + \\sigma \\epsilon_i, \\, \\epsilon_i \\sim N(0, 1) . In practice, we often only take a single sample \\epsilon_1 for each training point. This leads to the following backpropagation scheme:\n\nSample \\epsilon_i \\sim N(0, 1) . 2. Let w_i = \\mu + \\sigma \\epsilon_i\nCalculate\n\n\n\\nabla_{\\mu}f = \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\mu} \\hspace{3em} \\nabla_{\\sigma}f = \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} \\epsilon_i + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\sigma}\n\n\nUpdate the parameters with some gradient-based optimizer using the above gradients.\n\nThis is how we learn the parameters of the distribution for each neural network weight.\n\n\n35.5.4 Minibatches\nNote that the loss function (or negative of the ELBO) is\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) \\\\\n& = D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\sum_{j=1}^N \\log \\mathbb{P}r(y_j, x_j \\mid w_j)\n\\end{aligned}\n\nwhere j runs over all the data points in the training data (N in total) and w_j = \\mu + \\sigma \\epsilon_j is sampled using \\epsilon_j \\sim N(0, 1) (we assume a single sample from the approximate posterior per data point for simplicity).\nIf training occurs in minibatches of size B , typically much smaller than N , we instead have a loss function\n\n\\begin{aligned}\nD_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\sum_{j=1}^{B} \\log \\mathbb{P}r(y_j, x_j \\mid w_j).\n\\end{aligned}\n\nNote that the scaling factors between the first and second terms have changed, since before the sum ran from 1 to N , but it now runs from 1 to B . To correct for this, we should add a correction factor \\frac{N}{B} to the second term to ensure that its expectation is the same as before. This leads to the loss function, after dividing by N to take the average per training value, of\n\n\\begin{aligned}\n\\frac{1}{N} D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\frac{1}{B} \\sum_{j=1}^{B} \\log \\mathbb{P}r(y_j, x_j \\mid w_j).\n\\end{aligned}\n\nBy default, when Tensorflow calculates the loss function, it calculates the average across the minibatch. Hence, it already uses the factor \\frac{1}{B} present on the second term. However, it does not, by default, divide the first term by N . In an implementation, we will have to specify this. You’ll see in the next lectures and coding tutorials how to do this.\n\n\n35.5.5 Conclusion\nWe introduced the Bayes by Backpropagation method, which can be used to embed weight uncertainty into neural networks. Good job getting through it, as the topic is rather advanced. This approach allows the modelling of epistemic uncertainty on the model weights. We expect that, as the number of training points increases, the uncertainty on the model weights decreases. This can be shown to be the case in many settings. In the next few lectures and coding tutorials, you’ll learn how to apply these methods to your own models, which will make the idea much clearer.\n\n\n35.5.6 Further reading and resources\n\nBayes by backprop paper (Blundell et al. 2015)\nWikipedia article on Bayesian inference\n\n\n\n\n\n\n\nBlundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015. “Weight Uncertainty in Neural Networks.” https://doi.org/10.48550/ARXIV.1505.05424.",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A12.html",
    "href": "A12.html",
    "title": "36  Bayesian Books in R & Python",
    "section": "",
    "text": "36.1 Introduction to Probability\nThere are many books in R and Python that can help you learn more about these languages and how to use them for data analysis.\nHere are some of the most popular books on R and Python:",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#introduction-to-probability",
    "href": "A12.html#introduction-to-probability",
    "title": "36  Bayesian Books in R & Python",
    "section": "",
    "text": "Introduction to Probability by Dennis L. Sun",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#sec-bayesian-books",
    "href": "A12.html#sec-bayesian-books",
    "title": "36  Bayesian Books in R & Python",
    "section": "36.2 Books in R",
    "text": "36.2 Books in R\n\nR for Data Science by Hadley Wickham & Garrett Grolemund\nAdvanced R by Hadley Wickham\nggplot2: Elegant Graphics for Data Analysis (3e)\nR Graphics Cookbook, 2nd edition\nAn Introduction to Statistical Learning\nEngineering Production-Grade Shiny Apps\nForecasting: Principles and Practice (3rd ed)\nExploratory Data Analysis with R Roger D. Peng\nModern R with the tidyverse by Bruno Rodrigues\nModern Statistics with R by Benjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton\nMastering Shiny by Hadley Wickham, Winston Chang, and Joe Cheng\nLearning Statistics with R by Danielle Navarro\nText Mining with R by Julia Silge and David Robinson",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#books-in-python",
    "href": "A12.html#books-in-python",
    "title": "36  Bayesian Books in R & Python",
    "section": "36.3 Books in Python",
    "text": "36.3 Books in Python\n\nAn Introduction to Statistical Learning with python by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\nPython for Data Analysis by Wes McKinney of Pandas infamy parquet and Apache Arrow\nPython Data Science Handbook by Jake VanderPlas\nThink Stats by Allen B. Downey\nThink Bayes by Allen B. Downey\nProbabilistic Programming & Bayesian Methods for Hackers by Cameron Davidson-Pilon",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "37  References",
    "section": "",
    "text": "Aldrich, John. 2008. “R. A. Fisher on Bayes and Bayes’\nTheorem.” Bayesian Analysis 3 (March). https://doi.org/10.1214/08-BA306.\n\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. 1980. Regression\nDiagnostics. John Wiley & Sons, Inc. https://doi.org/10.1002/0471725153.\n\n\nBernoulli, J. 1713. Ars Conjectandi [the Art of Conjecturing].\nImpensis Thurnisiorum. https://books.google.co.il/books?id=Ba5DAAAAcAAJ.\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning.\nInformation Science and Statistics. Springer (India) Private Limited. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nBlundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan\nWierstra. 2015. “Weight Uncertainty in Neural Networks.” https://doi.org/10.48550/ARXIV.1505.05424.\n\n\nCasella, G., and R. L. Berger. 2002. Statistical Inference.\nDuxbury Advanced Series in Statistics and Decision Sciences. Thomson\nLearning. http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&Roger%20L.Berger--Statistical%20Inference.pdf.\n\n\nCook, R. Dennis. 1977a. “Detection of Influential Observation in\nLinear Regression.” Technometrics 19 (1): 15. https://doi.org/10.2307/1268249.\n\n\n———. 1977b. “Detection of Influential Observation in Linear\nRegression.” Technometrics 19 (1): 15. https://doi.org/10.2307/1268249.\n\n\nFinetti, Bruno de. 1937. “La Prévision: Ses Lois Logiques, Ses\nSources Subjectives.” Annales de l’Institut Henri\nPoincaré 7 (1): 1–68.\n\n\n———. 2017. “Theory of Probability.” Edited by Antonio Machí\nand Adrian Smith. Wiley Series in Probability and Statistics,\nJanuary. https://doi.org/10.1002/9781119286387.\n\n\nFisher, R. A. 1925. Statistical Methods for Research Workers.\n1st ed. Edinburgh Oliver & Boyd.\n\n\nGelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D.\nB. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman\n& Hall/CRC Texts in Statistical Science. Taylor & Francis. https://books.google.co.il/books?id=ZXL6AQAAQBAJ.\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su.\n2008. “A Weakly Informative Default Prior Distribution for\nLogistic and Other Regression Models.” The Annals of Applied\nStatistics 2 (4). https://doi.org/10.1214/08-aoas191.\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation,\nGibbs Distributions, and the Bayesian Restoration of Images.”\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nPAMI-6 (6): 721–41. https://doi.org/10.1109/tpami.1984.4767596.\n\n\nGhosh, Joyee, Yingbo Li, and Robin Mitra. 2018. “On the Use of\nCauchy Prior Distributions for Bayesian Logistic Regression.”\nBayesian Analysis 13 (2). https://doi.org/10.1214/17-ba1051.\n\n\nHärdle, Wolfgang Karl, and Léopold Simar. 2019. Applied Multivariate\nStatistical Analysis. Springer International Publishing. https://doi.org/10.1007/978-3-030-26006-4.\n\n\nHobbs, N. Thompson, and Mevin B. Hooten. 2015. Bayesian Models: A\nStatistical Primer for Ecologists. STU - Student edition. Princeton\nUniversity Press. http://www.jstor.org/stable/j.ctt1dr36kz.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical\nMethods. Springer New York. https://doi.org/10.1007/978-0-387-92407-6.\n\n\n(https://math.stackexchange.com/users/25097/autolatry), Autolatry. n.d.\n“Why Square Brackets for Expectation.” Mathematics Stack\nExchange. https://math.stackexchange.com/q/1302543.\n\n\nJackman, Simon. 2009. “Bayesian Analysis for the Social\nSciences.” Wiley Series in Probability and Statistics,\nOctober. https://doi.org/10.1002/9780470686621.\n\n\nJeffreys, H. 1983. Theory of Probability. International Series\nof Monographs on Physics. Clarendon Press.\n\n\nJohnson, R. A., and D. W. Wichern. 2001. Applied Multivariate\nStatistical Analysis. Pearson Modern Classics for Advanced\nStatistics Series. Prentice Hall. https://books.google.co.il/books?id=QBqlswEACAAJ.\n\n\nKruschke, John K. 2011. Doing Bayesian Data Analysis: A Tutorial\nwith R and BUGS. Burlington, MA: Academic\nPress. http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855.\n\n\nMcElreath, Richard. 2015. Statistical Rethinking, a Course in r and\nStan.\n\n\nMoivre, Abraham De. 1718. The Doctrine of Chances. H. Woodfall.\nhttps://tellingstorieswithdata.com.\n\n\nMorita, Satoshi, Peter F Thall, and Peter Müller. 2008.\n“Determining the Effective Sample Size of a Parametric\nPrior.” Biometrics 64 (2): 595–602.\n\n\nPearson, E. S., W. S. Gosset, R. L. Plackett, and G. A. Barnard. 1990.\nStudent: A Statistical Biography of William Sealy Gosset.\nClarendon Press. https://books.google.co.il/books?id=LBDvAAAAMAAJ.\n\n\nPoisson, S. -D. 2019. “English Translation of Poisson’s\n\"Recherches Sur La Probabilité Des Jugements En Matière Criminelle Et En\nMatière Civile\" / \"Researches into the Probabilities of Judgements in\nCriminal and Civil Cases\".” https://arxiv.org/abs/1902.02782.\n\n\nPolya, G. 1945. How to Solve It. Princeton University Press. https://doi.org/10.1515/9781400828678.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” In The\nFoundations of Mathematics and Other Logical Essays, edited by R.\nB. Braithwaite, 156–98. McMaster University Archive for the History of\nEconomic Thought. https://EconPapers.repec.org/RePEc:hay:hetcha:ramsey1926.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with r.\nSpringer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nSpanos, A. 2019. Probability Theory and Statistical Inference.\nCambridge University Press. https://books.google.co.il/books?id=9nCiDwAAQBAJ.\n\n\nVanderPlas, J. 2016. Python Data Science Handbook: Essential Tools\nfor Working with Data. O’Reilly Media. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nWiesenfarth, Manuel, and Silvia Calderazzo. 2020. “Quantification\nof Prior Impact in Terms of Effective Current Sample Size.”\nBiometrics 76 (1): 326–36. https://doi.org/https://doi.org/10.1111/biom.13124.\n\n\nWikipedia contributors. 2023a. “68–95–99.7 Rule —\nWikipedia.” https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule.\n\n\n———. 2023b. “Functional (Mathematics) —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Functional_(mathematics)&oldid=1148699341.",
    "crumbs": [
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>References</span>"
    ]
  }
]