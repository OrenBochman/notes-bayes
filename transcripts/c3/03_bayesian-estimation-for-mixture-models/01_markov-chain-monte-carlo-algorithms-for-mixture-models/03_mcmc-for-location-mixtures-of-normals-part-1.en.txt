To illustrate the MCMC algorithm
that we just discussed, let's consider an example that is a mixture of two normals. So the density of the mixture model that
we're going to try to fit is going to take the form Omega times the
square root of 2 Pi, Sigma x of minus 1
over 2 Sigma squared, x minus Mu 1 squared, plus 1 minus Omega 1 over
the square root of 2 Pi, Sigma x of minus 1
over 2 Sigma square, x minus Mu 2 square. This is a mixture of two normals. The weights are Omega and 1 minus Omega for each one
of the two components. The means are Mu 1 and Mu 2 for each one
of the components. The standard deviation is
Sigma for both of them. So this is a location mixture of two normals with a
common escape parameter. Before we can derive a Gibbs
sample for this algorithm, we need to first setup priors because we're using a Bayesian
Approach for inference. So we start with the
prior for Omega. In this case, we are going to use a Beta distribution with
parameters a1 and a2 say. Be careful, this is exactly the same Dirichlet
Distribution that we just used for the general
derivation in the Special k, where k is equal to two. In particular, if
Omega is a Beta a1, a2, then Omega 1 minus Omega. So the joint distribution
of the two follows a Dirichlet with
parameters a1, a2. So we're just specializing
the Dirichlet distribution to the specific case where
we have 2 components. In addition to a prior for
the weights for Omega, we need a prior for the means, and we need a prior for
the standard deviation, or for the variance of the model. We're going to use the
same prior for both means. We're going to call that
prior a normal distribution, with parameters Eta
and Tau square, and that implies
that the density for my prior is going
to be of the form 1 over square root 2 Pi Tau x of minus 1 over 2 Tau square, Mu_k minus Eta squared. As we will see in a minute, this choice of normal prior for the mean of each one of the components is
computationally convenient, and will lead to
very simple updates for the value of the parameters. Finally, we need a prior
for Sigma squared. Again, for convenience, we're
going to use an inverse Gamma prior that you may
already be familiar with, with parameters say p and q, which implies that
the density of the prior is proportional to
1 over Sigma squared, to the d plus 1, x of minus q divided
by Sigma squared. Again, just like in the case of the means, for the variances, it's going to be extremely
computationally convenient to use this type of conditionally
conjugate prior. Lets proceed first to write the full joint
posterior distribution of all the parameters
in the model. We are going to do
that starting with the likelihood for
the complete data. That means the
likelihood that includes both the observations and
the missing data that corresponds to see that
component indicators that we have introduced
in previous lectures. So that means that we want
the posterior distribution of Omega, Mu 1, Mu 2, Sigma square, and the vector of components
c given the data. If you recall, we
defined this as being proportional to the
joint distribution of the data given the
components parameters Mu 1, Mu 2, Sigma square, and the indicators C, multiplied by the
joint distribution of the Indicators given, in this case, the weight. Omega multiplied by the
priors on all the parameters. So the prior on Omega, the prior on Mu 1, the prior on Mu 2, and the prior on Sigma square. Now, given the choice
that we had for the structure of the likelihood and the priors in this model, this is going to
be proportional to the first term is the
product over the components. Though in this case, it's only
two of them multiplied by the product over
the observations, such that c_i is equal to k. So just the observations in the case group of the livelihood, that is 1 over square
root 2 Pi Sigma x of minus minus 1
over 2 Sigma square, xi minus Mu_k square. So this corresponds to the
first term in this expression. The second term in this
expression corresponds to our Beta prior in this case because we only
have two components. So this corresponds
to Omega raised to the sum from i equals 1 to n, of the indicator c_i is
equal to k, in this case, equal to 1 because it's the first component
in the mixture, multiplied by 1 minus Omega, sum from 1 to n, of indicators that
c_i is equal to two. So this expression corresponds
to the second term. Then we can just write the priors that we
discussed before. So the prior for Omega is
Omega raised to the a1 minus 1 times 1 minus Omega
raised to the a2 minus 1. The prior for Mu_k is the
product from k equals 1 to k of 1 over square
root 2 Pi Tau, x of minus 1.5 Tau, square Mu_k minus Eta square. Finally, the prior
for Omega that is proportional to 1
over Sigma square, to the d plus 1, x of minus q over Sigma squared. So this is the form of my full posterior
distribution, and from this, I'm going to want to obtain full conditional posterior for Omega given the rest
of the parameters, I'm going to want to
get a full posterior for Mu_k given all
the parameters, a full posterior for Sigma squared given
all the parameters, and finally, a full posterior
distribution for each 1 of the c_i's given the
rest of the parameters. We're going to do
that by selecting out from this full
posterior distribution, the specific terms that contain the variable of
interest in each case, and trying to
recognize those terms as forming a kernel of a family that we know and can recognize. Let's start with the full
posterior for Omega, which is very simple the full posterior for
Omega given the rest of the parameters is going
to be proportional to just the terms that contain Omega in
this big expression. As you can see, the likelihood for the data does not contain Omega anywhere, and the priors for Mu 1, Mu 2, and Sigma, they are these
expressions down here, do not contain Omega anyway. So the only terms
that are left are this piece here that corresponds to P of
Omega, the prior, and this piece here
that corresponds to the distribution of the
indicators given the weight. So this just becomes
proportional to Omega to the sum from 1 to n, of indicators that
c_i is equal to 1, multiplied by 1 minus Omega, sum of indicators that c_i is equal to 2 multiplied
by Omega a1 minus 1, 1 minus Omega, a2 minus 1. We can obviously
pull terms together. So we can pull the two Omegas together and the
two 1 minus Omegas together to just write this in the form of Omega raised to, let's call it a1 star
minus 1, 1 minus Omega, a2 star minus 1, where a1 star is just a1
plus this exponent here. A_2 star is my original
a_2 plus the number of observations
that are currently assigned to the second component. So we can recognize
this kernel here as exactly the same kernel of the Beta distribution
that we use as a prior. So that just means that the full conditional of
Omega given the rest of the parameters is Beta with this updated parameters
a_1 star and a_2 star. By the way, in the future
it will be convenient to simply call n_1 the sum from one to N of the Indicators
of c_i is equal to 1, and n_2 the sum from I equals 1, to N of the Indicators
that c_i is equal to 2. So as we did with the general
formulation of the model. In that way, this expressions can be written in a
slightly shorter way, and I will be using this type of notation later on for
other full conditionals. Let's proceed now to derive the full conditional
distribution for the Indicators c_i that tells us what component generated
the Ith observation. In other words, we want the
density or the probability. Because c_i can only
take two values, you can only come from
component one or component two, it is enough that we'll look at the probability that
c_i is equal to 1. Then we can always obtain
the probability that c_i is equal to 2 by
doing the complement. So this expression
is proportional too, only the terms in here
that contains c_i and in particular this
contains c_i equal to 1. If we look at this carefully, there are only two terms, one that comes from this product, and again this is a product
over many observations, but only one term in that big product is going to participate in this
full conditional. Similarly here, there is
only one term that has to do with c_i equal to 1. That is a term that
has to read Omega. So this becomes Omega, that is the term
that comes from here multiplied by this single
term that comes from here. That is 1 over square
root 2 pi Sigma exp minus 1 over 2 Sigma square x_i minus Mu 1 square. Which means that once we
renormalize the probability of c_i equals to 1 given
all the parameters, it's going to be equal to Omega exp of minus 1 over 2 Sigma square
x_i minus Mu 1 square, divided by Omega exp minus 1 over 2 Sigma square
x_i minus Mu 1 square, plus 1 minus Omega exp of minus 1 over 2 Sigma square
x_i minus Mu 2 square. As you can see, I did a
couple of simplifications, in particular 1 over
square root of 2 pi Sigma is going to appear in
front of each one of these terms so I can
cancel them out. That's the simplification
that happens because I have the same Sigma for all
components in my mixture. Let's look now at the
full conditional for Sigma square for the
variance of our kernels. We want the full
posterior distribution of Sigma square given the
rest of the parameters. Again, all the terms that are involved in this double
product here, involve Sigma. So I need to retain
all of those for the purpose of computing
the full posterior. The other term that
contains Sigma is the terms that come from
the prior down here. So I can ignore all these pieces that have to
do with the prior on Omega, the prior on Mu, and
the distribution of the Indicators given the weight. So that means that this term is going to be proportional too. I'm going to start to do a couple of simplifications here. So one of them is, I'm going to write Sigma as Sigma squared raised to the 0.5. So I'm going to have 1 over Sigma square raised to the 0.5. Then I have a product over the components times a product over the observations that
belong to that component. If you look at this carefully, we basically have as many
terms as observations, and all of them
involve this 1 over Sigma squared to the 0.5. So in reality I can just
raise this to the end. There is the number
of observations multiplied by the exponential, and now I do need to
take my terms inside. So this is minus 1
over 2 Sigma square. This products becomes sums. So I have a double sum
from one to capital K, and then for the I such
that c_i is equal to K of x_i minus Mu_k squared. Of course the terms
from the prior 1 over Sigma square to the D plus 1, exp minus Q over Sigma squared. Again as I did before, I can try to pull together terms that have
the similar structure. So this is going to be equal
to 1 over Sigma square. Let me call it D star
plus 1 times exp of minus 1 over Sigma square Q star. Where D star is just my
original D plus N halves, and Q star is my renal
Q plus the term that is in here that is 0.5 of
the double sum over the case first and over the Is that
belong to the case component of x_i minus Mu sub K square. So in other words, we get a posterior
distribution that has exactly the same form as the
prior that we started with, but with updated parameters
D star and Q star which allows me to just
say that Sigma square given all their
parameters just follows an inverse Gamma
distribution with parameters D star and Q star.