Now that we have
a clear structure for the likelihood
function that we will be using and we have prior distributions for
all of our parameters, we can proceed to derive the posterior distribution
that we will be working with. So we want to write down the joint posterior distribution. In that joint posterior
distribution includes, of course, the weights and the
parameters of the components, but it also involves the
vector of indicators C, that we have
introduced to simplify the structure of the
likelihood, and by definition, this quantity is proportional to the distribution of the data, given all the parameters, multiplied by the distribution of the missing data parameters, given Omega and Theta, multiplied by the prior
distribution on the parameters, multiplied by the prior
distribution on the base. So this is the
general expression, the general form, for that
posterior distribution. Now, we have already seen what the form of the
different terms is. In particular, this
joint distribution of the data can be written
as a double product. First-order components, and then over the groups of
observations that belong to each one of
those components of g_k x_i, given Theta_k. So that is the first piece
that we're interested in. The second piece, we have already seen also how
to write it down. This is going to be the
product from k equals one, to capital k of Omega_k. Some of these indicators
of C_i is equal to k from one to n. This
is our second piece. Then we discussed using a Dirichlet
distribution for this. So ignoring some
proportionality constants, this becomes the product
from k equals one to capital k of Omega_k
raised to the a_k minus 1. That's this piece. Then finally, we're going
to have another product. So typically, we use independent priors for
each one of the data case. As I said, we'll
typically try to pick them so that they are conjugate to this kernel,
g_k, but for now, I'm going to write it
in general form by writing this as p_k of Theta_k, and that's what the last
term in the expression is. So we have written down a
full expression for this. Now, it should be more or less clear how
we need to proceed. So we need full conditionals for all the parameters
in the model. In particular, we
are going to need a full conditional for Omega, given all the other parameters, we're going to need a full
conditional for each one of the C_is given the rest
of the parameters, and we're going to need
a full conditional for each one of the data case, given the rest of the parameters. So to derive these
full conditionals, what we will do is we will
pick particular pieces from this expressions to retain and to construct this
particular four conditionals. Let's proceed now
to derive each one of the four conditionals
that we need to derive a Gibbs sampler or a Markov Chain Monte Carlo
algorithm for this problem. Let's just start with the full posterior
distribution for the weights, and please note
that we're going to work with all the
weights as a block, so we're going to try to
sample them all together, and rather than looking at
each one of them at a time. So this full conditional
distribution is made up of the terms in this full
posterior distribution that involves Omega k, and if you look at this
expression carefully, you will see that
this piece doesn't depend on Omega k anyway, and that this piece
doesn't depend on any of the Omega case either, so it's just this two pieces in the middle that we
need to consider. Furthermore, the two
pieces are very similar, so both in both products over K of the weight
raised to some power, so we can actually combine the two expressions together
and just write them as the product from one
to capital k of Omega _k raised to the sum of
these indicator functions, plus a_k minus 1. This looks exactly like
the prior that we used, except that now, we have
updated parameters. So I could write this as the product of Omega_k
raised to the a_k, call them stars, minus one, where a_k a star, is just the original a_k
plus the sum from one to n of the indicators of C_i equals to k. So just
doing this little change, makes it very clear
that the form of the posterior is exactly
the same form as the prior. In other words, this
a conditionally conjugate prior for our problem, and that just means that Omega is going to be distributed
as a Dirichlet, given all the other parameters, but with this updated parameters, a_1 star all the way to a_k star, and this is very interesting because essentially,
a posteriori, we know that the
expected value of Omega given all the
other parameters, so this is the expected value
of the full conditional. This is not expected value
of the marginal posterior, but this is the expected value of the full conditional
that is going to be a_k star divided by the sum from L equals
one to k of a sub L, a star, but this is just a_k plus the sum from one
to n of these indicators, c_i equals to k, divided by n plus the sum from L equals one to
capital K of the a_l. N just comes from
the fact that if I sum over all the components, then the sum of those values
is going to be n. So this is just the number of
observations that are currently assigned to
the case component, and if the values of a, k are small, then this is
just roughly speaking. So approximately, the
proportion of observations in component K. This has a very nice analogy with the computations that we
did in the EM algorithm. If you remember the way in which we computed the
weights in that case, or the MLE for the weights, was by essentially
computing a quantity that could also be interpreted
as, roughly speaking, the proportion of
observations in that step of the algorithm that were
assigned to that component. So this provides a mirror image to what we did with
the EM algorithm, but that has a Bayesian flavor rather than a frequentist flavor. Let's continue now with the full conditional
posterior distribution for the indicators, for the c_is. I'm interested in
the probability that c_i is equal to K given the data. As before, this is going
to be proportional to just the terms in this large product
that depends on c_i, and if you look at it carefully, c_i only appears in this
two terms of the product. These have nothing
to do with c_i. In particular, it appears
in a single term within this really large product and in a single term within
this product. So the term that
depends on C surviving equal to K in here is Omega_k. The term that depends on
c_i equal to k in here, it's just g_k of x_i given Theta, and this is true for
every k from one to capital K. Remember that c_i is a discrete
random variable, taking values between
one and k because it indicates which component
generated the observation. So if I want to get rid of the proportionality
sign and actually being able to write what
the probability is, I just need to normalize this by dividing over the sum
of these quantities over k. So that means that
p of c_i equals to k, given all the other parameters, is equal to Omega_k, g_k, x_i, given Theta_k, divided
by the sum from l equals one to capital
k of Omega l, g_l of x_i, given Theta_l. If you look at this
expression carefully, you will realize that it is very similar to the
expression that we used when computing
in the EM algorithm, the weights associated which
is one of the observations. In fact, it is the
same expression, and this is just what we called in the EM algorithm, V_ik. Finally, let's consider the full conditional
posterior distribution for the data case. So we need p of Theta k given all the other
parameters in the model. Again, we just pick from this whole product the terms that have to do with Theta k, in this case, it is the two in the middle that do
not depend on it, and within this big expression, we just have a few terms
that contain Theta k, and those correspond to
the observations that are currently assigned to
that particular component. So this expression is
proportional to the product over the i's that have been assigned to the kth
component of g_k, x_i Theta k, and among
this product, again, there is a single
term that belongs to Theta k. So the form of the full conditional
posterior distribution for the parameter Theta
k is simply this. Now, without a specific
choice of G and P, it is hard to further
simplify this expression. But what I do want to
note here is that if this prior p_k is conjugate
to this kernel g_k, then we typically know what family this posterior
distribution will belong to, and that will make computation much simpler because
you will typically be able to sample from that full posterior
conditional distribution in using a direct sampler. This will become a little
bit more clear once we do an example
with mixture models, which is what we're
going to do next.