So far we have discussed mixture models as probability
distributions for data. We have discussed how to
generate data from that model, but our ultimate goal
is not to use necessarily this mixture models as a way to generate data, it's as a way to model data. From that observed data
try to reconstruct the parameters of the potential mixture model
that generated it. So we're interested in estimation of the
parameters of the model. So learning what I call data
k in previous lectures. To do that, we're going to
use likelihood functions, and in particular
we're going to discuss false maximum
likelihood estimation in Bayesian estimation
for mixture models. But both types of
estimation tools require that we write down the likelihood function
for the data. So in this lecture we
are going to discuss how that likelihood function
is constructed. We're going to construct two different types of
likelihood functions. First, we are going to
construct what's called the observed data likelihood, that just uses
the original representation of the mixture model
as a weighted sum. Then we will also discuss what's called the complete data
likelihood that relies on the hierarchical
representation that we discussed in
the previous lecture. But let me start first with
the observed data likelihood. Our assumption is typically
going to be that we have observations X_1 up to X_n. So these are observations
that have been collected. Our assumption it's
that this X_i are independent, and
identically distributed. We will usually call
that abbreviated iid. The assumption is
that each one of these X_i's just comes
from the same density, f, where, F of X just takes the form
of the mixture models that we have been
discussing so far. In this case I'm going to allow the components
to be different, but I'm going to make
the potential parameters of this [inaudible] G_K explicit
just for simplicity. So remember that the
likelihood function, is just a function that is proportional to the joint
distribution of the data. In this case, because
we're assuming that the observations are independent and identically
distributed from F, then the distribution of
the data can be obtained just by multiplying together
the densities or the probability mass functions for each one of the observations. So I'm going to call the
likelihood function, L, because the data is observed, and the parameters are
the pieces that are unknown. The likelihood function is really a function of the
parameters in the model, it's not a function
of the observations. Even though when we're thinking about it terms
of generating it, we're thinking about it as what is the probability distribution, the joint probability
distribution of the data. So the likelihood function, is a function of the weights, Omega one, Omega sub K. This is one set of
parameters that I don't know, and that I want to try to learn. This density is also a function of the parameters of each one of the components
of the mixture. So in this case, Theta one up to theta
K. As I said before, because the observations are independent and
identically distributed, I can write this
just as product over all the observations of
the individual densities, and the densities
take the form of a sum of Omega sub k. Now, I'm going to have, g_k of x_i. As I said before, I want
to make the dependence on the parameter explicit, so I'm going to write
it in this way. So this is the observed
data likelihood for a mixture model, it's just the product of weighted sums of
the different component, and each term in the product is evaluated in a different i. So this pattern is going
to be very important. We have a product and a sum. The product is over
the observations, the sum is over components. This doesn't commute. So you have to be
careful that you have first the product,
and then the sum. The densities, the
components of the mixture, and the weights depend
on the component index k. The observation comes in or the i index comes in
through the observation. So you have n times
k terms if you will, and they are all
different from each other because they all depend
on both the combination of i and k. As you can see this relatively
complicated structure, we have a product
and we have a sum. Eventually, we probably
are going to want to work with the luck-likelihood rather than with the likelihood, which means that there is not as simple simplification
for this expression. So although this is in principle the likelihood function
that we are interested in, it's one that it's really
difficult to work with. So what we'll do now is to write a different representation
of the likelihood that we are going to call
the complete data likelihood, in which we introduce
these latent variable c that we discussed before when we talked about the
hierarchical representation. That will allow us to have likelihood function
that depends on more parameters because it
will depend also on the c's. But where the structure that you get on the right is much simpler, and easier to work with. We just discussed
the observed data likelihood, we're going to discuss
now the complete data. Like that, and the idea now is that we are going to augment our parameter
space by also considering the
later observation C that we discussed before
another Heracles representation, that just tell us to which component each
observation belongs. What is really
important to remember here is that we are going to have one C for each observation
that we have. Indeed, remember that when we do it, the Heracles representation, we wrote it as x, and in this case because we
have multiple observations, so I'm going to add
the index sub i, x sub i. Given C sub i, it's going to come from
g of c sub i, of x_i. We define the probability
that C sub i is equal to k as being equal to omega k. So I'm going to have C_1
up to C_n that are iid. Remember that iid represents independent and
identically distribute. They are all going to have
the same distribution, where the probability of
C sub i is equal to k, is omega sub k. Now, there are at least a couple of different ways in which we can write the complete
data likelihood. One of them is by writing
the complete data likelihood. So in this case, we have the omegas as before, the data as before. If you will the Cs up
to n rather on up to k, and the observations themselves are still independent than
identically distributed. So I'm still going to have
the product from i equals one to n. What's going to change is whether
I'm going to write here, and what I'm going
to write here is the essentially the
probability associated with each one of the components. So one way to do that is to introduce a series
of indicators. So I can think about this as the product from k
equals one, two. K of the weight associated
with the k component, multiplied by the density of the corresponding
component in x sub i, and then I need to move
the bracket around. Then, each one of
these components is present or not depending on whether C sub i is
equal to k or not. So the notation one
C sub i equals to k, is just one or zero, if C sub i is equal to
k, and zero otherwise. So basically, what we're saying here is that
we're going to write the likelihood function
as a dual product. So since some sense, the sum became now a product. We have the same terms
that we had before when we were
dealing with the sum, but instead, we're adding
this exponent here, that it's either one or zero. So what this exponent
does is that it will essentially make all, but one of this terms in
this internal product, go away for each observation. So that you will only retain the term that you
have in here that corresponds to the component to which the
observation belongs to. So this structure of
the likelihood that includes the C sub i is going
to be extremely useful for us to do
computation later. Now, this can be rewritten in
a couple of different ways. One of the ways in
which this can be rewritten is by breaking this apart into the terms
that correspond to the gs and the terms that
correspond to the omegas. So let me write
first the terms that correspond to the gs, that still remains
as a double product. We have g sub k, x sub
i to the indicator, that is why it's
equal to k. Then, we have a second
product here that I actually want to write in
the reverse order up to n, up to k, excuse me. Product from one to n of omega k, raised to the
indicators of C sub i, is equal to k. What
is interesting here is because the basis of this expression is
the same for every i, this particular double product can also be written
as the product from k equals one to
capital k of omega sub k, sum from one to n of the indicators of C sub i equal to k. If you think about
what the sum represents, the sum just represents
the number of observations in the sample that belong to up to component k. So that provides
a very nice break, and a very nice interpretation for the likelihood function, and for a complete data
likelihood function. You have one piece of
the likelihood function that corresponds to the
distribution of the observations. Basically, if you know what component generated
each observation, you just use the density of that component in this product. So this simplifies
this expression substantially. You are going for each i. You're just going to retain
one of the key terms that corresponds to the exponent
that is equal to one. Then, you have a second term that corresponds to this product
of the weights. In every case, the weight
is going to be raised to essentially the number
of observations, that belong to that component.