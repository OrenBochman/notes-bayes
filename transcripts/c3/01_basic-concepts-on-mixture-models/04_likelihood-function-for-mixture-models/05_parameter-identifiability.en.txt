Today, I want to discuss identifiability in
mixture models. You may recall that
the statistical model is identifiable if two different
values for the parameters, any two different values
fro the parameters, always raise two different probability
distributions for the data. Now, unfortunately, mixture models are not
fully identifiable. So let me show you three ways in which a mixture models are
not fully identifiable. One of them is what's sometimes
called label switching. To illustrate label switching, let's look at a mixture
of two normals. For example, I call this F1 of x, and let's say that
it's a mixture where the first component has
70 percent probability. It's a normal with variance
one, and mean zero. So this is the density. The second component
has mass or weight 0.3, maybe a standard
deviation of two, and mean of one. So in the notation that we
have been using so far, this would be a mixture where
Omega 1 is equal to 0.7, Omega 2 is equal to 0.3, Mu 1 is equal to zero, Mu 2 is equal to one. Sigma 1 is equal to one, and Sigma 2 is equal to two. So keep these numbers
in mind for a second. Now, I could write
a second mixture, F2 of x. That is essentially
the same expression that I have up here, but I'm just going to
flip what I'm calling Component 1 in what I'm
calling Component 2. So I'm going to say this
is 0.31 over square root 2 Pi to exp one-half, x minus 1, divided by 2 squared, plus 0.71 over square root 2 Pi, exp minus one-half x squared. Now, if I'm, again, going to try to use
the notation that I had before to recognize
where the terms are, if I write it this way by just switching
the order of the terms, where I have is
my Omega 1 is 0.3, my Omega 2 is 0.7. My Mu 1 is one, my Mu 2 is zero. My Sigma 1 is two, and my Sigma 2 is one. In other words,
what I was calling one here has become two in here, and what I was calling
two here has become one. Now, these two are
exactly the same models. There is really
no practical difference between them except for the labels that I'm
using for the components. Essentially, if you have more
components in the mixture, you're going to have
many more permutations that you could do
of the terms that, at the end of the day,
represent the same mixture. So the fact that you can switch this order that translates into basically
switching what names you use to label the components, is what we call label switching. So label switching
is going to have a number of implications
for us later on. One of them is that, for example, some of our algorithms
that we're going to use to fit our models may give us two answers that
represent the same mixture, but where the only thing
that is really different is the order in which the component appears
in the response. We need to be able to recognize those two answers that
just differ from the other as essentially representing
the same final model that we're working with. So this is something that we
are going to have to keep in mind when we do computation
with our models all the time. Label switching is not
the only way in which identifiability problems come up when working with mixture models. There are at least two
more ways in which these identifiability
issues can manifest. Again, let me illustrate
them with a little example. So let us start with our same mixture that we used to illustrate
label switching. So the first components, it is going to be a mixture
of two normals with the first component
being a standard normal, and the second component being a Gaussian with mean one and standard
deviation two. Let's consider
another to make sure. So one of them is going to be of the form that is going to
have three components, I'm going to call this F2. The first component is going
to be exactly the same, but what I'm going to do
is I'm actually going to split this component
into two pieces, say, one that has weight 0.01, and the same mean and
standard deviation. Then a third component
with the same parameters, but the leftover weight. So clearly, this is
a mixture of one component, this is a mixture of
three components. In practice, we can understand that because the
parameters are the same, I can collapse
these two terms into one. But from a representational
point of view, these are different models. So this is another way in which the model is
not identifiable. We have different sets
of parameters down here, actually even
different values of K, but they both correspond to the same density at
the end of the day. In practice, this
becomes a problem when you're fitting
a model in which you have, for example,
overestimated the number of components that are included. One of the things
that may arise is that some of the components have, if not identical parameters, at least parameters
that are very, very close to each other. That, in practice,
could be collapse into a single one without really affecting the fitting
of your model. So this is going to become
important particularly when we think about how to select the number of
components in the model. Another way in which
this can arise is that, of course, I can retain
my original components. So call this F3 of x,
just another mixture. So I can work with the mixture
that I had before, and I can add one or a hundred, if I want, components
that have zero weight. So I can add 0 times, for example, 1 over
square root 2 Pi, times 3x minus one-half, x minus 100 divided by 3 squared. So again, from a
representational point of view, this corresponds to
a three-component mixture. But because this weight is zero, it's clearly in practice just identical to the first one
that we started with. Again, this may arise in
practice, for example, you may not have weights
that are exactly zero. But if you overestimate the number of components that
you have in the mixture, one possibility is that you end up in
a situation like this. Where another possibility
is that you end up in a situation that looks
a little bit like this, where you have
one or various components that have weights
that are really, really tiny, for
practical purposes, zero. Even though you think that you're fitting a model that has, say, three components, in
practice you end up fitting a model that has
only two components. So these three ways in
which mixture models can be non-identifiable,
label switching, components that have
the same parameters and just split weights, and components with zero weights, are things that you need to
keep in mind when looking and interpreting results for
inference in mixture models.