In the previous lecture,
we introduced mixture model and
their definition, and we wrote the density as a weighted sum of the densities of other
random variables. In this lecture, I
want to introduce the hierarchical representation
of these models, which both provides a very nice interpretation
for mixture models, and will be very
useful in terms of computation later on because most algorithms that are used for estimating
the parameters in a mixture model
are going to rely on some version of
this categorical representation. So to start, let me remind you of the form of the density
of a mixture model. So for a mixture model, we are talking about
probability densities or probability mass
functions depending on whether it's continuous
or discrete, that take the form sum
from k equals one to capital K of Omega_k, G_k of x in the more
general case in which we allow the
different components of the mixture to be belong to different parametric families. What I want to do now is instead think about this distribution, this formula I wrote here as the marginal
distribution that arises from a joint distribution. I'm going to call that, or I'm going to construct that joint distribution in
a hierarchical fashion. So here, what I would say or the implication
of this is that X is distributed as f. So now I want to introduce an indicator. I'm going to call
that indicator c. So this is a random variable
that is discrete. In particular, c is going to take
values in the set 1,2,3, all the way to capital
K. So it's going to take values on the set of
indexes of the mixture. Rather than write
distribution of X directly, what I'm going to do is, I'm going to write the
distribution of X conditional on c and then write
the distribution of c. That is, if I know what the c value
is, essentially, c is going to represent
what component of the mixture I am selecting to generate
my random variates. So if I know what c is, then X is just
distributed as g_c of X. So c is just an index that selects which one of
the components I'm picking. Now, c is going to be
a discrete distribution such that the probability that c is equal to K
is equal to Omega_k, it's equal to the weight of the mixture that corresponds to that particular component. Now, it's not hard to see
that just by definition, the probability of X or
the marginal distribution of X is going to take the form of the sum from k equals
one to capital K of each one of
the values of c. So f of p of x given c times p of c. But just because of the form that I picked
for both c and x given c, pfc is going to take the form p, the probability that
c is equal to K is just Omega_k and the density or the probability mass function of X given that c is equal
to k is just g_k of X. So this provides
an alternative way to represent the random variable
in which we introduce this indicator that
just tells us which component in the mixture that
we're picking in each case. The hierarchical
representation that we just discussed where we've first right x
conditional on c as coming from the density or the distribution g_c of X and as the probability that
c is equal to K being just Omega_k has in addition to providing
a nice interpretation for the mixture model, provides you a simple way to generate random variables from the mixture if you know how to generate random variables
from the g_c. So the algorithm is very simple. It just has two steps. So for each observation
that you want to generate, you're going to do two steps. In the first step, you're going to randomly sample
the component indicators. So you are going
to sample c_i with probability given by the weights Omega_1 up to Omega_K. Given the value of c_i
that you just sampled, you are going to generate
X_i from g of c_i. So this just means
that sample X_i from the c_i component of the mixture. Again, I need to
emphasize that this is a very simple algorithm that allows you to generate
the random values. It does assume that you
know how to generate from each individual component, and we will see in
a minute an example of how to do this using some R code.