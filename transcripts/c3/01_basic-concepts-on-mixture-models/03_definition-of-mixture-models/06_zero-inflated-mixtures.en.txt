Another way in which mixture models give
you flexibility for modeling data is when dealing
with zero-inflated data. In zero-inflated data,
what you get is a larger number of
zeros than you would expect under one of
your standard models. We're going to consider
here two examples. One is zero-inflated counts. Zero-inflated counts
may arise for example, in the study of biological data. When you look at your data, you may see that and you have large number of counts that
are reported as zeros, and that may not be
very well-explained by any standard probability model such as the negative binomial
or the Poisson model. So let's see here
one example of how we can create a
zero-inflated version of the negative
binomial distribution. In this example,
I'm going to plot probability mass functions for values of x between zero and 15, and I am going to create in this first example
two different graphs. One that corresponds to negative binomial or to
data that that follows a negative binomial
distribution with parameters eight in 0.6. So this means that the
probability of success is 0.6 in that trials, the Bernoulli trials to
stop after eight successes. We can easily do that. That's going to be the first graph in
a panel of two graphs, and we know that this is how the negative binomial
distribution looks like. In general, by changing
the parameter eight and 0.6, you can get a wide range of shapes for the negative
binomial distribution, but in all the cases what you
may be doing is essentially a stretching and condensing this particular unimodal shape that you can see here. So if you reduce the probability
of success for example, you may start shrinking
the peak towards the left until a point where
what you see is just a decreasing distribution. But in every case
the distribution will be unimodal that you
generate from this. That may not be very good
in practice because what we may see in real life
is for example, a large number of zero counts. So our first peak
here, a mode here, and then we may see a second mode somewhere
further down the road. One example of this
arises in biology when you consider
the number of nests in bird counts that
contain no eggs versus the number
of eggs that are present in nests
that do have eggs. You typically have a peak
here that corresponds to birds that couldn't mate or that didn't generate any eggs, and then you have
a full distribution for the number of eggs in other nests that typically doesn't just fall
directly from zero, so it's a small and then
has second pick up here. So there are a number
of practical applications in which you want to allow the distribution
to have those two picks, one at zero and one
somewhere away from zero. One way to do that is to combine one of
the standard distributions like the negative binomial
or the Poisson, with a point mass at zero with some additional weight associated with the zero
probability down here. That is just a mixture model of two various special types
of distributions, one that is a point
mass that puts all probability in
assembly location, and a second one that casts a range of values that's why we don't
have just a single value. Graphically, we can get that
by essentially combining, in this case we are giving 20 percent probability to
point mass at location zero. We are giving 80 percent
probability, one minus 0.8, to the negative
binomial distribution that we just saw that
has been plotted here. So this is how we compute the probability mass function
for that mixture of a point mass in the negative
binomial distribution, and we can add it to
the plot very easily, and now what do we have
here in the lower panel is that mixture of the same
distribution that you have up here with a point mass at 20. You can see that now the
probability of having zero counts is 0.2, which is what the weight
associated with the point mass, plus this little bit of
probability here about 0.2 that is given by the negative binomial
distribution to the count zero. What you get for all the
other values is essentially the same numbers
that you are getting here but now multiplied by 0.8, so that when you add
up all the values, you still get a total probability of one over all values
of the random variable. A second example of mixture distribution that
is useful for dealing with zero-inflated data is
the case where you want to mix a point mass with
a continuous distribution. This often happens when
you study time two events. Sometimes the time
two events are zero. Imagine that you are
studying the lifetime of, for example a light bulb that
you buy in the supermarket. For most light bulbs, that's going to be
a very long time, but some of them
come defective from the factory and they never work. So for those items, their lifetime is zero. So you want to be able to combine continuous
distribution for example, a log-normal distribution,
a Weibull distribution, and exponential distribution with discrete point mass at zero that captures
those components that are defective from
the manufacturer. So the way to do
that is again to use a mixture model that involves both appointments at zero
as one of the components, and a second component
that models the lifetime of the components
that are not defective. As before, we're going
to do a little example. We're going to plot the density of
this mixture distribution, in this case over
the range minus 2-15, this is a distribution
that is going to have support only on
the positive numbers. So the plot starting at minus two is just to illustrate how the point mass
affects the distribution, but the density for
negative numbers will be zero as we
will see in a second. We're going to again try to contrast what happens if we don't put it in the point mass
and what happens if we put in the point mass as
one component of the mixture. So first, we are
going to just plot the density of
the log-normal distribution, over this range of values of
x that we just discussed. The parameters of that log-normal distribution
are going to be 1.5 and 0.5. So if you remember, the log-normal distribution
is the distribution of a number whose logarithm follows a normal
distribution with, in this case, mean 1.5 and
a standard deviation 0.5. So this is our reference or
our baseline for comparison. In addition to that, we're going to consider
the mixture of that same density with
a point mass at zero, and in this example, I'm going to give
the point mass at 0.3 in my example that I
discussed before. This means that I expect about 30 percent of
my components to be defective. So when you go to the supermarket
to buy the light bulb, about 30 percent
of the light bulbs come defective from the factory, and then the rest, the other 70 percent is going to have a lifetime that follows this reference distribution
that I discussed before. In this case, I am going to plot both of them in the same graph. We can see here how
the shapes change. So for negative values, both densities are zero, which means that we don't
allow negative values, that makes a lot of sense, we cannot have negative
lifetimes for the light bulbs. The main difference
happens down here. I should note that these are the cumulative
distribution functions, these are not
the densities unlike in all the previous examples
that we had considered. The reason why we
moved to war with the cumulative
distribution function rather than the density, is to be able to
clearly illustrate the effect of the point
mass at zero. So again, coming back here, the difference is going
to happens at zero, whereas the regular log
Gaussian distribution starts at zero, and it starts just increasing smoothly until it reaches one. In the case of the mixture of the point mass and
the log Gaussian distribution, the cumulative distribution
function has a jump at zero. That is where the point mass
that we mixed in is located, and it jumps exactly to 0.3, which is the value of the way that we gave
the point mass. Then starting from there, it essentially takes
the same shape as the log Gaussian density, it's just that it has been moved up or wrapped up so that, or scaled up so that this beginning point
matches the point up here, the 0.3, were
the point mass ends. So this allows us to capture this probability that
an item is defective and therefore doesn't
last anytime all the way to the component
lasting as long as it may last.