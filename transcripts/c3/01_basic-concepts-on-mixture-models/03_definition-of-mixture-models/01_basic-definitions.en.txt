So today, we are going
to be introducing mixture models and introducing the definition of mixture models. Mixture models are
probability distributions that can account for a variety of features in the data including multi-modality
and skewness. The idea of mixture
models is to take multiple probability
distributions and put them together in some sense
using a linear combination. More formally, if we have a random variable x,
with density f(x), this can be the density or
probability density function, or if it's discreet, it can be the probability
mass function (PMF). It is in the continuous case and in the discrete case. We have F(x) that it's the probability distribution function. So that F(x) is
the probability that your random variable is less or equal than a given number. Then for a mixture model, the probability
distribution function it's going to take the form of a sum over k pieces that we're going to consider
the components of some weights times some other group of probability
distribution functions. So these are typically
called the weights, and these are called
the components. While we typically
ask of the weights and the components is that the weights sum up to one. G_1 all the way to G_K are cumulative
distribution functions. Okay, because the
probability mass function takes the form of a sum, the probability mass function or the probability density
function depending on whether it's a discrete or
continuous distribution will take a similar form, where the g just corresponds to the density or the
PMF of Gs of X, depending on whether
again this distribution corresponds to a discrete or a continuous random variable. Let's consider some examples
of mixture models. One that we will encounter repeatedly and we
will work often with is a mixture of
normal distributions. In a mixture of
normal distributions, the density will take
the form sum from one to K of the weights times
the density of a normal, and as you may recall the density of a normal distribution
takes the form, 2 Pi Sigma x minus 1
over 2 Sigma square, x minus mu's_k square. So this is a particular example, where only the mean of each
one of the normals depends on the component index and we allow the variance to be the same
across all the components. So we sometimes call this allocation
mixture of normals. Again, because the only parameter that depends on
the index of the sum is the mean of
the normal distribution that controls the location
of that distribution. But we can write
a more general mixture that will be actually more
flexible and more helpful, that it's allocation
is scaled mixture, where we allow both parameters to depend on the index component. So in that case, we may write again the
sum of the weights, and now 1 over square root 2 Pi, Sigma_ k so that will allow
the variance to be different. Exp minus 1 over 2 Sigma_k, x minus mu_k square. So this is location-scale mixture of normals. As we will see in a minute, this location scale mixture of normals have a lot
of flexibility. They allow you, for example, to capture multi-modal
distributions. Multi-modal distributions
often arise when you have multiple populations and
you are not aware of that. Consider for example, trying to model the weight of various dogs, and you may have
different breeds of dogs within that population. If you have big dogs
and small dogs, they will probably end
up looking a little bit like two humps. So if you think about
this being the weight, and this the probability
mass function associated with the weight, then if you have
Chihuahuas, for example, they will tend to
have lower weights, and if you have German Shepherds, they will tend to have
a higher weight and there will be very little overlap
between the two groups. So a mixture model that allows the means of these two pieces
to be far away from each other and that have
relatively narrow values of Sigma k will allow you to capture a pattern
that looks like this. Another example in which
this kind of mixture models of normals is useful is
if you're trying to capture its skewness in the data. For example, rather
than trying to capture something that is completely
symmetric around the mean, there are situations in
which your distribution may have a very heavy fat tail
in one direction. Note on the other examples of situations where this
arises are, for example, variable such as the population
of countries or states, income, land size for
different countries, all those are variables that
tend to be highly skewed. You have a few countries or a few individuals that
make large incomes, and then you have the majority of your population
somewhere down here. So for distributions that
have that type of pattern, a mixture of normals where the means are different
but not too different and that have relatively
large variances will allow you to obtain shapes that
look a little bit like this. In the examples that we just saw, we used the same distribution, the same kernel for
our components in the mixture. That tends to be
the case that will be very common in many of the applications that
we will be looking at. But in some circumstances, you may want to consider two different or multiple
different families for the different kernels
that you work with. For example, you can consider a mixture of
a normal distribution, where the density of
the mixture will take the form Omega 1 times 1 over
square root 2 Pi Sigma exp minus 1 over 2 Sigma
square x minus mu square, with a second density that, for example, could
be an exponential. In this case, this is
going to be 1 minus w_1. Remember that the weights
need to add up to one. So instead of writing Omega two, I could easily just write
one minus Omega one. Then density of an exponential
with parameter lambda. In this case, because
of the way in which I parameterize the
exponential distribution, lambda just represents
the mean of this exponential. So generally
speaking, we can have different families for each one of the kernels
of the distribution. Another example of using
two different kernels for the mixture comes up
for zero-inflated data. This arises very often
when you are dealing with discrete data and graphically, the motivation for dealing
with a zero inflated data comes from situations where
you are counting for example, the number of eggs that
you see in a nest. If you just look at
nest that have eggs, you may see something that looks and you
construct a histogram. You may obtain something that looks a little bit like this. But once you take into
account nest that have no eggs. So this would be the shape
of what you're observing. Excuse me, 1, 2, 3, 4, 5. This is the number of eggs. This is the frequency. This is only for
nests that have eggs. But then when you add
the nest that have no eggs, you may see suddenly a spike. So you may have a large number of nests that have no eggs and then the rest of the shape
that you observed before, 0, 1, 2, 3, 4, 5. So a shape like this
can be captured by a number of
standard distributions that you may be familiar with, for example the Poisson
distribution or the negative
binomial distribution. But a shape like this won't
be captured naturally by any of these
standard distributions that you may be familiar with. So one way to deal
with the situation is to consider a mixture of the Poisson or the
negative binomial with essentially something that
puts a lot of weight on zero. So for example, we could
work with a mixture in which we have
our first component and, in this case this is
the probability mass function. Here this was the probability
density function. Now this is going to be discrete. So for example, we can take a Poisson distribution
with parameter lambda. So that'll be e to
the minus lambda, lambda to the x over x factorial. So this is the probability
mass function for the Poisson and, then mix that with
a point mass at zero. So this is, essentially a probability
mass function that puts all
probability on zero, that it's the number
here in the subscript. So by working with
a mixture of this type, we allow to have on one hand, the regular shape that is controlled by this piece
of the mixture and then the spike at
zero is essentially controlled or modeled by
this point mass at zero. To slightly clarify the notation, we just discussed
two different examples, one in which the components of the mixture belong
to the same family, one in which the components of the mixture belong
to different families. So the most general notation
for the mixture will be the one that
we introduced in the very beginning
in which we write the probability mass function or the probability density function, depending on the type
of random variable, as sum of omega kG_x. In this notation, we allow
G sub k to belong to any family and the families could be different
for different case. When all the Gs belong
to the same family, then it's usually
useful to slightly alter the notation and write
this in the following form. Again, it's still a sum. But we're going to make
the dependence of each one of the components on it's parameters explicit and we will
write it in this form. So in this case, this is the parameters
of the family. In the case of the normal, this were mu and sigma squared. So the mean and
the variance of the normal. This makes explicit that the functional form of
G is always the same. So in every case, you have one over square root two pi times sigma k
times an exponential. What changes between
one component and the next is just the specific
values of some of the parameters that make
that functional form. So sometimes we will use this notation in
the most general case. But when we are working
with families in which the G is the same for everybody and the only thing that
changes is a parameter, we'll use this
alternative notation to make that explicit and clarify. Now one situation that we need
to understand a little bit more as whether the properties of this mixture models that
we are working with and, the two key properties
that one is interested in when you look at probability mass
functions is what is the expected value and what is the variance of
the random variable. Those are actually relatively
simple to obtain because of the relatively simple structure
of these functions. Okay. So first, we are interested
in the expectation of x when the probability
mass function or the probability density
function can be written in the form of a mixture. For now, I'm going to write
it in the most general form. So I'm going to consider
currents that are potentially different
from each other. So I want the expected value of x under f. So
this lowercase f here just indicates what is
the probability mass function or probability density function that I'm using to compute
to compute the expectation. So by definition, this expected value
is just the integral, in principle between minus
infinity and infinity. If we're talking about
a unidimensional. Random variable of
x times f of x dx. Because of the form of f, we can just substitute that back, and what we get is
the integral of x sum from k equals 1 to K of Omega k, g sub k of x dx, between minus infinity
and infinity. Now, we know that because
the sum is finite, we can exchange the
integral under sum. So these are two
operations that commute. So instead we could write this as the sum from minus infinity to infinity of Omega k. So here it's important to note is that Omega k
doesn't depend on x, so it can come out of
the integral itself. So we get the integral between minus infinity and infinity of xg sub k of x dx. But now if you consider
what this integral is, that integral just
by definition turns out to be again
an expected value for x, but instead of being
computed under f it gets computed under g sub
k. So the expected value turns out to be the sum from k equals 1 to
infinity of Omega sub k, times the expected value
but now under g sub k of that
random variable X. This provides a very
clear interpretation of what the mixture is doing. So remember this Omega sub
ks add up to one, so you can think about
them as weights. So the expected value
of the mixture is just the way that some of the expected value of each one of the components
in the mixture. In addition to the expectation, we're interested in the variance
of the random variable. The procedure to compute the variance is going
to be very similar, it's just as likely
longer calculation. So if you remember the variance, again on the f of
the random variable X, is defined as the expected value
under f of x squared, minus the expected
value under f of x along raised to the square. This calculation we have already done and we know what
the answer for it is. The one that we need
to compute now is this expected value of the
square of the random variable. So let's do that. The expected value under f of x squared is,
again by definition, the integral of x squared, sum from k equals 1 to K of Omega k g sub k of x dx. That using the same argument
that we did before, becomes the integral or the sum of the integral or Omega k times
the integral of x square, g sub k of x dx. For the same, we just need
the same argument as before. This is just the expected value
of x square under g sub k. So this is
the sum from k equals 1 to K of Omega sub
k expected value, but now under g sub
k of x square. Now, let me turn
this formula on it's head. So we know we can compute this or at least relate
this expected value of x square with the variance and the expected value of
the original random variable, by basically substituting back to the right the expected
value of x square. So that just means that this expression can
also be written as the sum from k equals
1 to K of Omega sub k, that multiplies the variance
on the g sub k of x plus the expected value under g sub k of x squared. Now, substituting back into the definition of
the variance for f, what we have is
the variance under f for x is going to be equal to
this expression, this sum minus the expected value under f, but that expected value
that we just computed is the sum of Omega sub k times
the expected value under g sub k of x squared. Now, one thing that
I want to highlight is that there is
no easy simplification here. So we cannot cancel
the terms that involve the expectation of g sub k square here with the terms that
appear on this side. So unlike the case of the expectation where we had a really nice simple expression, where we could say that
the expected value under f was just a weighted average
of the expected values under g sub k. In general, the variance of f won't be
just something as simple as a linear combination
of the variances of the different components
of the mixture. Except in one very simple case. If the expected value of g sub k is the same for
all of the components, and you can say that they are the same and
that they are zero. So if the expected value under g sub k of x is
equal to 0 for all k, then under this k, what we get is that
the variance under f of x can be written simply as the sum of Omega k times
the variance under g sub k of x. That's because this term becomes zero and this term becomes zero, and you only retain
Omega sub k times the variance of g sub k.
But that is in general, one of the few circumstances in which you can write
the variance of f as a weighted average
of the variances of the g sub k. Otherwise, you have a much more
complicated structure for the distribution.