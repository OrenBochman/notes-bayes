During our lecture, we discussed that mixture
models are very flexible and can capture different shapes for
the density of data. Now, I want to illustrate that using three
different examples. In the first example, I will show you
how mixture models can capture multimodal
distributions, and in particular in this case, we will discuss
a bimodal distribution. We're going to consider
a mixture of two normals. This is our first example. We're going to plot
a mixture of two components. The first component
has a weight of 0.06. Obviously, the second
component needs to have a weight equal
to one minus that, so it's 0.4, and it's
a mixture of two normals. The first normal is going to have a zero mean in
standard deviation of one and the second component
is going to have a mean of five and
a standard deviation of two. What this code will do is it will provide us with a graph of the density associated with this mixture of
two normal distributions. So the first thing
that we do is we create a grid of
values, in this case, a 100 values in between minus five and 12 on which we are going to compute the value of the density. The second line of code
can compute square, the densities at each
one of these x value. If you remember from
your hours lessons, the function dnorm just
computes the density of a normal distribution with mean, in this case zero and
standard deviation one at a set of values
given by the x value. The other two lines of code here just create the plot
of the density. The first one just plays
around with the margins. This is something
very common just to make the figure as
square as possible. This second line, actually
creates the plot. So let's execute these commands. We can see on
the right-hand panel, down here, that now we have the density of that mixture
of two normals, the value of the density
is in the y-axis, the value of x is in the x-axis, and you can see that
we have obtained two moles or two peaks
in this distribution, and that is because the means of the two distributions
are pretty far away. So one of them is zero, the other one is five and when compared with
the standard deviation, so this has a standard
deviation of only one and this has a standard
deviation of only two. So when you have mixtures in this case of normals were
the means are very far away, when you compare them with
the standard deviations, then you will obtain distributions
that are multimodal. If I were to have a third
component in this mixture, then you and again, I were to pick values
for the mean and the standard deviation that allow the peaks to be well separated, then I would see for example an extra peak
somewhere down here. So in that regard, the mixture of model of
normals allows you to capture multimodality in
the data if it is present. Another example that is
interesting is to show how a mixture of normals can capture a skewed
distributions. If you remember
a skewed distributions, are distributions
where one tail seems to tamper out more slowly
than the other tail. So to illustrate that, again we're going to look at
a mixture of two normals. We're going to look at the same x region
that we had here. So we're still going to plot the density between
minus five and 12. What changes now is
both the weights and the means of the distributions that are involved in the mixture. The first weight in
this case is instead of 0.6, we're going to use a weight of 0.55 with the weight of
the error component being 0.45. Then my components
are going to have, the first one has mean zero and a standard deviation
of square root of two and my second component is
going to have a mean of three with a standard
deviation of four. You can see that now both the means are closer to each other instead of
being zero and five, they're zero and three. I'm using much larger
standard deviations for each one of the components. So instead of one, we are using
a square root of two and instead of two, we're using four. So if we go ahead and
again create a plot, you can see that now the two peaks are
not distinguishable. Again, that's because
the modes of the two normals, the means of the two normals are much closer to each
other than they were before, especially when you
compare them against the standard deviations
of the distributions. You can see that
the right tail in this case tapers off much more slowly or so
appears in the graph, than the tail on
the left hand side. So this is an example of how
a mixture of normals can give you a skewed
distribution for your data. Finally, I want to illustrate one last property and
it's that mixture of normals can give you heavy-tailed
distributions or things that resemble heavy-tailed
distributions. The example here, it's
going to involve, instead of a mixture
of two normals, is going to involve a mixture
of three normals with weights: 0.4, 0.4 and 0.2. We're going to plot it now
over a slightly wider region. My region is going to go
between minus 12 and 12. All three mixtures
have the same or all three components of
the mixture have the same mean, zero, zero, and zero. What changes across
the different components is the standard deviation associated with each one of them. So the standard deviation for the first one is
the square root of two, the standard deviation for the second one is
square root of 16. I could have just written four in there but I wanted
to keep the pattern. Finally, a square root of 20 is the standard deviation for the last component
in the mixture. So to illustrate the fact that we are getting heavy tails, I want to try to compare the plot of this density
with the plot of the density of a normal distribution
that has zero mean, which is going to be
the mean of this mixture. Remember the formulas that
we discussed in the lecture, and that has standard deviation that is the same standard
deviation that the mixture. So in other words, I'm making this normal density half the first two moments
be the same as the first two moments of the mixture of three normals
that I have above, and that will allow me to make a fair comparison of how
the tails look like. So I match the first
two moments and then given that I have matched
the first two moments, I want to see how different the tails of the two
distributions are. So let's go ahead and plot it. Again, we compute
the three values and then we plot both sets
of graphs and I'm also going to add a little legend that
tells me which graph corresponds to the mixture and which graph corresponds to
the Gaussian distribution. This is how the result
looks like. The mixture model is drawn in a thick solid line whereas
the Gaussian distribution, the Single Gaussian is drawn
using the dashed line. Again, you can see how
things are very different. Even though I have
the same mean and the same standard deviation
for both distributions, you can see that
the solid line tends to fall more slowly once I
get to high values, whereas the Gaussian distribution
tends to fall faster. So this is what we
call heavier tails. So at least for the purpose of the range
in which I am plotting, it would seem like the graph with the solid line tends to be above the graph for the dashed lines for either very small values or for very large values
of the variable x. So this illustrates how this mixture of three normals
is still symmetric. That's very important
to highlight. This is a symmetric distribution
just like the Gaussian because the means are the
same for all the components. They have the same variance but the mixture has
a much heavier tails.