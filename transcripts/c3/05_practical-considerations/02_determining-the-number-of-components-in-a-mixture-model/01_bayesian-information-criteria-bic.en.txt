Until this point, we have sidestep
an important question, how to select the number of mixture components that you
are going to include in your model today. I want to address some of that, and
I want to talk about selecting, The number of components. Now, knowing what the number of components
says is important for multiple reasons. If you are in a density
estimation problem, remember that k controls how much smoothness you
get out of your density estimate. So it will play an important role in the
quality of the estimators that you get. If you are working on clustering problems, the number of components can typically be
a parameter that is important on its own. For example, in our iris example,
where we were trying to identify species, given some morphological
characteristics of the flowers. One important question is are we
dealing with two species, or are we dealing with three? So do we have enough differences
between versicolor and virginica to really tell that
the two species are different? Or the differences are not large enough,
and we can only treat them as a single one? So this question of how many components
you have in the mixture can be an important one in clustering. And you may want to be able
to answer that question in a statistically appropriate manner. So in terms of tools for
doing model selection, and in the end, selecting the number of components
is a problem of model selection. One that is very popular,
and that can be used for mixture models is the Bayesian
information criteria, Or BAC for short. BAC is going to be the primary tool
that we're going to use to select the number of components in
the context of the Algorithm. And you may be familiar
already with the BAC. The BAC for a model, and I'm going to
index the model using the letter J. It is defined as -2 times
the logarithm of the likelihood of that model evaluated
in the maximum likelihood estimator of the parameters for
that model, plus the number of parameters
that are present in the model. Multiplied by the logarithm
of the number of data points. And you want to think about this guy as the number of effective independent parameters, and again, this is just your likelihood function. And this is the MLE,
the maximum likelihood estimator for the parameters in the model. Now, as you can see,
the BAC is made up of two terms, and these two terms can
be easily interpreted. So the term on the left is
essentially a goodness of fit test. So remember that one way to think
about what the likelihood does is tell you how well
the model fits the data. So if this term here is high, then it's because the model
fits the data well. And then the term on the right
is a complexity term that just penalizes models that
have high complexity. So if the model doesn't fit very well,
then this number will be large. If the model has a lot of parameters,
this number will be large. And so this expression, just traits
of complexity and goodness of fit. So in the end, you're going to tend
to models that have enough complexity to explain the data well. But not so much complexity, that the
additional parameters that you're adding just provide some marginal
increase in this quantity up here. Now, let's talk about the two terms in the
BAC in the particular context of mixture models the likelihood function. In this case,
we have already defined it before. This is the likelihood for
the observed data, not the complete data
likelihood that we use for the Algorithm to wear either Q function,
this is just the observed data likelihood. And the observed data likelihood,
in this case, so my parameters for the mixture
model as the vector of omegas in the parameters of the components
of the mixture model. So these two things is
what I called data before. And of course, j, the index that
identifies what model I'm working with is really the number of
components that I have in my mixture. So think about this as a vector
that has length capital K, depending on how many components I have. And similarly, I have as many theta Ks
as components I have in the mixture. And we have written this expression down before, this is just the product over the observations of the sum over the components of g sub
k x sub I given theta. So this is the expression for
the likelihood for the mixture model. We know that we can use
the Algorithm to get the MLS for these parameters for
any K that we choose, so it's easy to evaluate the first
piece of the BAC for our mole. Now, for
the second piece that less r sub j, that is the number of independent
parameters in the model. We basically remember we have omega,
and we have the thetas. So Omega has length capital K, but
those parameters are not all independent. Remember that those parameters
need to add up to 1, because they represent
the weights in the moment. So therefore, we have capital K +, sorry, -1 independent parameters that come from this piece. And then it's going to depend
on exactly what kernel you use, but for now,
I'm going to call it the sum from k = 1 of capital K of
the dimension of theta k. So each component is going to contribute
a certain number of parameters to the mixture. And we basically need to think about
the sum of all of those as the second component of this. So let's talk about a couple
of specific examples, just to make this a little bit clearer. So if you are talking about location, Mixture of univariate Gaussians,
for example. Then this term that I'm
calling the sum from k = 1 to capital K of
the dimension of theta K. Becomes you have 1 standard deviation,
1 variance that is going to be the same, because I'm only doing a location mixture. So you're going to have
1 parameter from there. And then each Gaussian kernel that
you're including the mixture give you capital K means that
you need to include. So you have + capital K. This is essentially sigma, and
this is essentially a mu 1, Up to mu capital K. On the other hand,
if you are doing location, And scale, Mixture, Of P variate Gaussians, Then the same quantity sum from
k = 1 to capital K becomes. Now, you have one vector mu
term you that has P entries for each one of the components. So you're going to have P times capital K terms that come from mu 1 up to mu k each one with a mentioned P. And then you're also going to have
capital K variance covariance matrix has those variance
covariance matrices are P by P. But you don't have P square independent
terms in a variance covariance matrix. Because remember that by
matrices have to be positive, definite integrals,
they need to be symmetric. So what you really have
is P times P + 1 divided by 2 elements that are independent. So exactly how you count the number
of parameters that are associated with the kernel is going to do depend
on exactly which kernel you define. And it's not really possible to
give a very general formula, but these two examples should give you some
guidance on how you need to pick those numbers for problems in with your work.