Even though the Bayesian information
criteria contains the word Bayesian in it it's not really a fully
Bayesian procedure. And it's most helpful in the context
of problems in which you can compute or do you obtain the maximum likelihood
estimators of the parameters as part of your inference. Now when you are using the MCMC
algorithm to feel a fully Bayesian model computing the MLS is not a straightforward
just from the output of the algorithm and therefore we'd like to use the different
mechanisms to provide full inference. And in particular we would
like to obtain a mechanism to find the full posterior distribution
for the number of components. We have just like to treat it as
another parameter in our model. So that's what I want to discuss today, I want to discuss fully
Bayesian inference for capital K,
the number of components in the model. And what we're going to do for
inference on the number of components in the model
is we're going to try to draw a difference between
what we're calling capital K, which is the maximum number
of components in the mixture. And something that I'm
going to call k star, that is the number of components that actually generated, Observations. So this is distinction is important
because they are Bayesian approach to selecting the number of components
that are present in the sample, which essentially boils
down to this K star. Is to say I'm going to allow a very large
number of components in the mixture in principle, thinking that not all of
those components will be occupied by observations or
would have generated observations. And I'm going to let the data tell
me how to train from that maximum number of components capital
K to a smaller number K star. That is the one that is actually
being used in the data set. So the implicit assumption
is that K star is less and often much less than capital K,
the largest number. Now to do this successfully, we need
to be a little bit careful about how we elicit the prior distribution for
the data. Remember that so far we have been using a prior for
omega, that is a dirichlet. And in particular we've been using
a very particular type of the dirichlet that is dirichlet 1, 1,
1 that just boils down to a uniform distribution on the simplex. Now this prior has been helpful
while we keep capital K fixed and we have a good idea
about what K star is and in particular we think K star and
K are about the same body. But if we were going to move now into the
sort of setting where we're going to say that K star is a much smaller number
than capital K, then using a prior of this type won't work because basically
what will happen is that as we increase K we're also increasing what our prior
guess is about capital K star. So in other words, our prior will be,
if we use this prior, the maximum number that we select as
capital K will have a lot of influence in the posterior inference
that we get out of our model. So instead, I want to work with a different
version of the dirichlet prior and it's one that will allow us to
keep the behavior of the prior in some sense stable as
capital K increases. And prior is a prior of
the form 1 over capital K, 1 over capital K or more generally I don't have to do once,
I can use some other constant value. And we'll discuss in a minute how
to select those constant values call it alpha and alpha. So a prior of this type, the advantage that it has is that
as I grow this capital K that is just going to be in some sense a tuning
parameter and in model in my algorithm. As capital K grows what I'm saying is
that every individual component has a priori smaller and smaller and
smaller probability. And what does that does is that a priori the value of capital K is going to
be bounded by a certain quantity. So if omega one to omega capital K is given at the dirichlet prior with parameters alpha over K, alpha over K all the way for
all K values that I need up there. Then there is a little mathematical
result that tells us that if I want to compute the limit when
capital K goes to infinity. Remember, that this is the maximum
number of components that I allowed of the expected value of K star that is among the maximum components that I'm allowing
among the capital K components. How many of them are actually
occupied by data or were generated or
generated data in my sample. So which ones are filled if you will,
so that this quantity is equal in the limit to the sum from i = 1 to n of alpha divided by alpha + i- 1. And in turn this quantity can be roughly approximated by alpha log of n + alpha- 1 divided by alpha. So the first part of
the result is a little bit of outside the scope of this class. So I won't be working out the proof for
this, but the second piece is actually
relatively straightforward. So this approximation you see the alpha
log n + alpha -1 divided by alpha. And that approximation just comes from
basically thinking about the sum as approximation, a Riemann sum
approximation of an integral. So one way to think about it
is to say that this quantity just comes from doing
the integral between 1 and n of essentially a function. Now I'm going to replace i that is kind
of discrete with a continuous index. So I'm just going to say alpha
divided by alpha + x- 1 dx. And we can easily solve this integral and
the solution to this integral is just this expression here
that involves the logarithm. So that is kind of what justifies this
approximation as I said the first result I'm really not going to justify in detail. But now that we know that
the expected value of K star is at least approximately equal to alpha log n + alpha- 1 divided by alpha. We have a way to elicit the value of alpha
that we want to use for our model if we have a way to elicit an expected value
for the number of occupied components. So give our, so we need a rough guess. Of what the expectation of K star is. If you remember for
example in the in the gala six data set that we have been
discussing across the class. We had a good reason to think that K
star was around six because the data was collected from six different
iconic sections of the of the sky. So that means that making a guess that
K star is around six makes sense. And once you know, what the guess is for
this expected value, then you have a nonlinear equation, but
that you cannot easily solve by hand. But you can easily solve
it in the computer and we'll see an example later on. And once you solve that equation, so
given this value you solve for alpha in that equation because you also know
the number of observations that you have. And so that gives you a mechanism to
elicit the prior that you're going to use or the value of alpha that you're going to
use to define your prior for the omegas. [SOUND]