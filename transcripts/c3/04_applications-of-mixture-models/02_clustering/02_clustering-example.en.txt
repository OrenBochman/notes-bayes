Today we're going to illustrate how to
use mixture models for clustering using a real data set, that real data
set is called the iris data set. It's a very famous one. It was collected in the mid-1970s by
a biologist, whose last name was Anderson, and then it was analyzed by Fisher
in a famous paper in statistics. So this data set contains
150 observations, 50 for each one of three
species of the Iris plant. And we're looking at the flower of
the plant, in particular, for each flower. We have four different characteristics,
the length, and the width of the sample, and
the length and the width of the petal. In the three species of
plant of the iris blender, we're looking at here are Iris setosa,
iris versicolor, and iris virginica. If you just do iris in R, you can see the information that is here
on the right hand side of the screen. So this is another data set
that is included with R. Okay, so
let's start by just plotting the data, to get a sense of how
this data set looks like. So we have this for
parables and observation. So what I'm doing here is a scatter plot,
where I looked at every pair of variables, and scatter plot of 150 observations for
each one of them. And I'm using both colors and letters to represent
the true cluster membership. So in this case, we know what the species
the plants belong to beforehand. And we will use that to test
the results of our algorithm, to try to understand what the algorithm is doing,
or the different algorithms are doing. And a couple observations
are probably important here. So the first one is that you can see
pretty much in every scatter plot, that the setosa flowers are clearly
different from the other two species. So they definitely have
a very different petal. They're completely apart
on both characteristics, and they also,
the sepal is also slightly different, at least the sepal width. The other thing too that is
worthwhile noting is that even though the other two species are different,
there is some overlap between the plants. And in particular, you can see that if
you were to look mostly at sepal length, the distinction between, or
just look at sepal length and width. You would see that versicolor and
virginica are really, really hard to differentiate,
based on the sepal. Really, the different comes from
including information about the petal is really what allows you to distinguish
these two species of plants, okay? So this is the truth that we will
eventually be comparing against. Now, what we're going to do
is we're going to fit, or use two different approaches for
this problem first. We're going to use a mixture of normals,
in this case. We're going to assume that we have
three components in the mixture. And we're going to fit
a mixture of four tetraborate, if you will, Gaussian distributions,
that allow for different means and different variances. And you can see how allowing for different means and different variances
is going to be important here. So the variance of the,
for example, setosa, so sorry, virginica and versicolor,
you can see that they're very elongated, and they're clearly kind of diagonal
oriented in this direction. Whereas, if you look at setosa,
setosa tends to be much more, the variance covariance matrix
tends to be, in some sense, much more compact,
as you can see in these two graphs. So it's clear that the different
components of the mixture, the different clusters will
have very different shapes. And so a more flexible model, like
a mixture of Gaussians that allows for mixing in the variance covariance
matrix is going to be important. We're going to try to compare the results
from that mixture model with those that you obtained by
using k-means clustering, which does not allow that different
variance covariance matrix. It actually doesn't even allow
the different dimensions to have different parts. So this is a nice test case,
in which we can show that allowing for this more general model actually leads
to better clustering results, okay? So otherwise, the code that we're
going to use is very similar to the one we have used in the past, so
you will be very familiar with this. We're going to do 15
runs of the algorithm, just to make sure that we actually
reach a global mode in the algorithm. So let me do that, sorry, we need to provide all the information to the code. So now that it is running,
it will take a few seconds to finish in, so
it's currently on the first try now. It started a second try. In every case, the initial point of
the algorithm is different, and so in some cases, it's faster, and
it converges in first steps. That's because the initial step, or the initial iteration is closer
to the mode of the algorithm. So let's give it a few seconds for it to finish running all
the different iterations. What we're going to do once it's done is
we're going to pick the best hydration, the one that leads to the highest
value of the Q function. And we're going to try to use
the values of the probabilities of membership to each one of the clusters, to decide on the indicator okay? So now that it has finished, I'm going
to decide for the best run out of 15. What is the membership
of the observations? And then the next thing that I'm
going to do is I'm going to plot the observations, okay? And so now what we have is essentially something
that looks a lot like the previous graph. We're trying, but the numbers now correspond to just
the cluster that the algorithm identified, and we can see that it has separated. It was setosa, so if you remember,
these clusters down here are setosa. So those have been separated reasonably
well from the rest of the observations, so no problem there. And it has also been able
to separate virginica and versicolor reasonably well, but we need to still investigate in a little bit
more detail what is happening down here. One way to do that is by using
the adjusted rand index. This is a measure of how well to, or how close to each other two
partitions of the data are. And what I'm doing here is I'm comparing
the partition that we generated against the true partition of the data
that is given by the species indicators. And if you remember what the adjusted
rand index is, and how to interpret it, the adjusted rand index can
take values of up to one. And if it's one, it's because
the two clusters are identical, or the two partitions are identical. And then values that are close
to zero indicate that two partitions are not really looking similar,
okay? So if we compute the ARI, we can see that the ARI in this case is
0.9, which is a reasonably large number. So we're not perfect. We are not matching exactly
the true species classification, but we are pretty close to the true one. Now I want to contrast that
against the results of you're doing k-means clustering. And there is a function in R that
allows you to do k-means clustering, just k-means. And one of the things that you need
to give to this function is, again, the number of clusters. We're also going to use three-fourths for
the algorithm, and we need to give it how many
repetitions of the algorithm. In this case, we're going to use 15, just like for our e/m example. And again, we're going to run this and
compute the adjusted rand index. So again,
we run the algorithm we pick the best, or what this function really does is
it takes the best of the 15 runs. And again, we display the data,
not surprisingly. It's not hard to separate setosa from
the other two, but we can see now that the separation of virginica and
versicolor, it's a little bit odd. And we can verify that the results
are not as good as the ones that we had obtained with the mixture
model by looking at the ARI. And you can see that the ARI,
in this case, is only 0.73, which is much lower than this 0.9
that we have obtained before. In other words, the partition
structure that comes from using the k-means algorithm looks less
like through partition than the one that you get from using
the more flexible mixture bond. So this is an important observation. If your data doesn't really
have the characteristics that are supported by the k-means
clustering algorithm. Then you're typically much better
off by trying to use some more general algorithm that
is based on mixtures.