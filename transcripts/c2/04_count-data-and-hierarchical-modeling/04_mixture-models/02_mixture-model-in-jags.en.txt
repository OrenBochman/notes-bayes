For an example of a mixture model, we're
going to use the data in mixture.csv. Let's go ahead and
read that in using read.csv, with header=FALSE, and
take a look at that. The header=FALSE was because there
was no name for the variable, so this is a default name
given to variable 1. Let's check the number of rows in dat. We have 200 observations of this variable. Let's take a look at it, see what we have. First, we will create a variable, name it y, and that will be dat$V1. And let's do a histogram of y. So the values of y range from about -5,
maybe, up to 4. And we see we have
possibly two populations. We can also view this with
a density estimate, so we can plot density(y). And again, we can see this might
represent two different populations, but we only have one variable here,
the response variable. We have no covariance that might perhaps
explain why there's a group here, around -2. And there's another group up here,
close to +2. Perhaps, in the context of our
example with company growth, this might represent
companies that are growing. This population over here might be
companies that are not growing. But we donâ€™t know which companies
fall in which categories, we'll have to try and
infer it with a mixture model. First, we'll load the JAGS library, Which is actually the rjags library,
there we go. And we'll create the model string. Let's do a mixture of normal distributions
with two mixture components. We'll start with the likelihood
portion as usual. for i in 1:length of the data, y[i] will come from a normal distribution, Where we're going to index mu
by which category it belongs to. And then we're going to assume that
they all have the same precision. So which normal distribution y[i] comes
from will depend on our variable z[i]. This is our auxiliary,
or latent, variable. Because the auxiliary variable z
is not observed in the dataset, like we had with anova,
we're going to have to learn z. So we have to give it
a prior distribution. So z[i] will come from
a categorical distribution. A categorical distribution
is like a Bernoulli, except with possibly many categories. Here there are only two categories,
category one or category two. The probabilities of being
in those categories will be saved in a variable called omega. And now we need to come up with priors for
the two different mus. mu[1], we'll say comes from
a normal distribution, and we'll help this model
identify the two populations. Remember, we had one that was negative and
one that was positive. So we'll say mu[1] should
be the smaller of the two. And we'll give it a prior mean of -1,
but with a fairly large variance. Now, given mu[1], mu[2] is going
to follow a normal distribution. With a positive mean,
the same variance, but it'll be constrained, or
forced, to be larger than mu[1]. This is going to guarantee that
mu[2] is larger than mu[1], which helps our mixture model. Sometimes mixture models are susceptible to what is called
an identifiability problem. We can't identify which group is which, the labels might switch between
groups one and groups two. If we force the mus to be ordered and
give them a prior which helps them identify themselves, then we can
avoid the identifiability issue. As usual,
we'll put a prior on that precision. We'll give it a prior sample size of 1,
a prior guess of 1. And instead of monitoring the precision,
we'll monitor the standard deviation. Our latent variables z[i]
can be either a 1 or a 2. Depending on the probability of being 1 or
2, which is in our probability vector omega. One prior distribution for probability
vectors is the Dirichlet distribution, we're going to give the Dirichlet
distribution to omega. The Dirichlet distribution has
a vector of shape parameters. They're very similar to
the beta shape parameters, where you add the number
of counts to these alphas. To update a Dirichlet distribution just
like you update a beta distribution in the posterior. We'll use a uniform symmetric Dirichlet
prior with shape parameter 1 and 1. As usual,
we have to put this into a string, And save that model string. We'll set the seed. And now we need to create the data for
jags, it is just a list of y. Let's save the parameters
we want to look at. We want to save mu, both mus, sigma, the standard deviation in
our normal likelihood. We want to save omega, the probabilities
of being in either of the two categories. And we could monitor z, but as you know, there are just as many
zs are there are ys. That would be monitoring 200 parameters,
we don't want to do that. Let's look at only a few of these zs. Let's pick out a few observations for which the posterior distribution
of z might be interesting. Let's look at the first 50 values of y. And for reference,
let's look at the density estimate again. It looks like the first observation
is pretty clearly in this population here on the left,
in population one. So we would expect the posterior
probability that z[1] is equal to the group 1
should be pretty high. So to verify that, let's monitor z[1]. If you look a little further down,
at observation 31, that's -0.37, so that's right around here. If this is a mixture of
two normal distributions, it's fairly ambiguous which population
an observation of -0.37 will belong to. So let's monitor z[31],
that one should be interesting. Next, let's look at observation 49. This one is 0.03, or close to 0.04, that's right over here. It's fairly ambiguous, but it looks
like it probably belongs to group two. Let's monitor z[49]. Finally, if we look at observation 6,
the value is 3.75, it's way over here. Pretty clearly,
it's a member of population two. So we can verify that by looking at
the posterior distribution for z[6]. Okay, it now remains to initialize and
run this model. I've also changed the data_jags list
right here, so that the name of y is y. So that it knows the name
of this variable here. Let's run the initialization and
burning period. And then we'll run the actual simulations. Again, we won't go deep into
the convergence diagnostics, but let's at least look at the trace plots. These appear to be mixing quite well. Let me get the plot again,
yeah, pretty good mixing. Notice that these zs,
they are discrete variables, so this is bouncing back and
forth between 1 and 2. That's just fine. Okay. Next, let's look at a posterior
summary from these chains. And again, for reference, let's compare
this to the density estimate for y. Mu[1] has a posterior mean of -2,
which looks about right for this normal distribution. And mu[2] has a posterior mean about 1.5,
that seems reasonable. So it looks like we have
identified two populations. It also looks like the two
normal distributions have a standard deviation of about 1. That looks probably like it's true. And it suggests that the probability
of being in group 1 is about 39%, and the probability of being
in group 2 is about 61%. So about a 39% chance of being down here
and about a 61% chance of being up here. That looks about right as well. Rather than looking at the posterior
means of these z variables, let's look at them graphically instead. I've created here a plot from
the coda package densplot for these four different zs. So we can look at them all at once. Remember, the first observation
associated with z[1] was very clearly in group 1,
it was far over to the left. And you can see here,
almost all of the posterior probability is that it is within group 1. Observation 31 was more ambiguous, we weren't sure which
population it belonged to. The posterior distribution of z
suggests that there's a little more posterior probability that it
was in group two than in group one. Observation 49 was positive,
it was about 0.03. And it belonged fairly clearly
to population 2's territory. And as we can see,
the posterior probability that observation 49 is in
group 2 is pretty high. The 6th observation was above 3,
it was very positive, and it was very clearly part of population 2. And the posterior for
z[6] suggests that this observation is clearly
a member of group 2. The hierarchical structure of this
mixture model was very useful to us In identifying groupings of variables
that were not explicit in the data. We were able to infer them
using this mixture model.