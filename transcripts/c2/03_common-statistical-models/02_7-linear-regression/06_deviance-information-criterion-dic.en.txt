We have now proposed
three different models. How do we compare their
performance on our data? In the previous course,
we discussed estimating parameters in models using the maximum
likelihood method. We can choose between competing
models using a similar idea. We will use a quantity known as
the deviance information criterion, often referred to as the dic, which
essentially calculates the postural mean of the log likelihood and
adds a penalty for model complexity. Let's calculate the dic for
our first two linear models. The first one was our
simple linear regression. To calculate this,
we'll use the dic.samples function, and we give it our first model, mod1. And these samples from
a posterior distribution. So we need to give it a specified number
of iterations, and we can run it. Before we interpret these results,
let's also run the dic for model 2 where we added oil as
another explanatory variable. Let's run those samples as well. The first number in the output
called mean deviance is the Monte Carlo estimated
posterior mean deviance. Deviance equals negative 2 times
the log likelihood plus a constant that's irrelevant for comparing models,
so we're not going to worry about it. Because of that negative 2 factor, a smaller deviance means
a higher likelihood. Next, we're given a penalty term for
the complexity of our model. This penalty is necessary, because we
can always increase the likelihood of the model by making it more
complex to fit the data exactly. We don't want to do this because
models that are over fit usually generalize poorly. This penalty is roughly equal to
the effective number of parameters in your model. You can see this here. In the first model, we had a variance
parameter and two coefficients for a total of three parameters
close to the 2.8 we got here. In model 2, we added one more coefficient, one more parameter than the previous
model which we can see. Reflected which we can see
reflected in the penalty. We add these two quantities,
the mean deviance and the penalty term to get the penalized
deviance, the third term. This is the estimated dic. A better fitting model
has a lower dic value. In this case, the dic is lower for the second model than it is for
the first model. So the dic would select
model 2 over model 1. The gains we receive in
deviance by adding the is_oil covariant outweigh the penalty for
adding that extra parameter to our model. The final dic calculation for
the second model is lower than the first. So we're going to prefer
this second model. We encourage you to explore
different model specifications and compare their fate to the data using dic. Wikipedia has a good
introduction to the to the dic. And you can also find out more
details about the jags implementation through the r jags package documentation. If we type in ?dic.samples,
we can get that information. You might want to try fitting
the model 3 with the t likelihood. Calculate its dic, to see how it
compares with the other two models.