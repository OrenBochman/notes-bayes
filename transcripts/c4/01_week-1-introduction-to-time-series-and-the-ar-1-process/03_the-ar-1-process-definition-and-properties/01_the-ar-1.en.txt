The first process we
are going to study is the autoregressive of order
1 time series process. Here, a process is an
autoregressive of order 1 process, if you can write the
process in this way. If you think about y_t, you can write y_t as a linear function of the
past value of the process, so y_t minus 1. Phi here is the so-called
AR coefficient. There is that linear
structure plus a noise. Here we're going to assume that the Epsilon t is the noise. Here are independent,
identically distributed random variables, normal zero v. That's the assumption, that's the basic process that
we're going to study. This is going to be, we will show a zero
mean process. We are going to study the
properties of this process. The first thing you notice when you look at this equation
is that if you write y_t, here is a function
of y_{t-1}. You can now assume that the same structure is
valid for y_{t-1}. You can write down y_{t-1} as a function of y_{t-2}. If we do that, we
can have Phi now I am writing y_{t-1} using the same auto-regressive
structure. This is going to be y_{t-2}
plus Epsilon_{t-1}, plus Epsilon t, which gives me Phi square, y_{t-2} plus Phi
Epsilon{t-1}  plus Epsilon t. What we
see here now is that the process is now greater than as a
function of y_{t-2}, the Phi appears to
the power of 2. You will notice if you continue
applying this recursion, that the power for the Phi
goes with the lag on the y_t. Then you start having
terms that are just linear combinations
of the Epsilon t's. If you were to apply
this several more times and get to k steps here, you can write down the process just using this recursion over
and over again. Now this is going to be a
function of y_{t-k} Phi^k. Then you're going to
have all those functions of the Epsilon t's that you will be able
to write as k minus 1, Phi^j, Epsilon_{t - j}. This is the form of the process
that you can write down. If you were to repeat this infinitely many times and if you assume that Phi is
between minus 1 and 1, you can write down y_t as this infinite sum of
functions of the Epsilon t's. You can show essentially
that we're writing down as long as the Phi is
between  -1 and 1. We're writing y_t as this infinite order
moving average process. This is like a linear filter around the Epsilon_{t-j}'s. Using the distributional
assumptions on the Epsilon t's, we can find what the moments of the process are
in particular, if you think about the
expected value of y_t. Well, the expected value of y_t is expected
value of the sum. The expected value of the sum is the sum of the expected values. This is a constant and
we know that each of the Epsilon t's has mean zero. Expected value of this
process is zero. This representation
gives us a zero mean autoregressive
process of order 1. If I want to use the same kind of reasoning to compute the variance
of the process, we will see here. Now the variance of the process is the
variance of this sum. I can write the variance
of the sum as the sum of the variances because the
Epsilons are independent. They are all also
identically distributed. I can write this down as, sum j equals zero
up to infinity. This is a constant, so there is a square
that appears here, because this is the variance, and the variance of each
of these Epsilons is v. I can write this down as v. Now, because, again, we're assuming that Phi
is between minus 1 and 1, this summation is
going to converge, and that is simply going to
be one minus Phi squared. This gives me the
variance of the process. As you can see, does
not depend on the time. We are assuming
that phi is here, which implies that the process
is going to be stationary. We will talk about that too. This gives me the mean
and the variance. We can also compute the ACF, and we will do that next. In the case in which the Phi
is between minus 1 and 1, we can use the equation
that writes down yt, the AR1, as an infinite sum of these Epsilon t values and
past values of the Epsilon ts. This moving average way of
writing the AR1 process, to compute the
autocorrelation function and the autocovariance function. Let's just work first with
the autocovariance function. If we are working here
again with Gamma h, this is just we show that the expected value
of the AR1 is zero. We can write down the
autocovariance of the process as the
expected value of yt, y_{t-h}. Now we can write yt as that infinite sum of the Epsilon ts and past
values of Epsilon ts, and we're going to
do the same for the y_{t-h}. If
we were to do that, we would get that this
is expected value of this is the sum j equals zero up to infinity of Phi to the power of
j Epsilon_{t-j}. Then I can write down the
y_{t-h} in the same way. Just to use a
different index here, this would be Phi
to the power of k, and then we have
Epsilon_{t -h}  minus k. If I think about this, the first summation. We have, when j is zero, this is Phi to the
power of zero, Epsilon t is only
the first term. Then I have plus Phi
Epsilon_{t- 1} plus Phi square
Epsilon_{t-2}, h plus, let's do one more, h plus 1 Epsilon
t minus h plus 1, which gives me minus
h minus 1 and so on. That's the first
infinite sum times, and the second one
I can do the same. The first term is going to
be epsilon_{t-h} plus Phi Epsilon t minus h minus 1 plus Phi square Epsilon t minus
h minus 2, and so on. If you look at this
expression here, if I want to compute
expected value, and you look at all the
cross products here, Anything that has
a cross product of epsilon t minus some lag times Epsilon t
minus some other lag, if the time is not the same
in the cross products, the expected value
is going to be zero because that corresponds to the covariance and they are
independently distributed. We know that the Epsilon
ts are all independent. The only terms that are
going to appear here in these cross products are the ones where the Epsilon
t has the same lag. For example, in this case I can combine this one, that is, Phi to the power of h Epsilon t minus h
with this one here, that gives me that
Epsilon t minus h Square and that gives me just the variance
of the process. Only those terms are going
to appear in the equation. I can write down this as
the expected value of, so just do that one, here's the first
one that appears. Then I can combine this one and this one
have the same lag. I have Phi h plus 1 times Phi Epsilon t minus h minus
one square and so on. The next one would be. We can see when we look
at this expression now, if I were to compute the
expected value, again, this is a constant, would come out of the
expected value if you wish. Then I have the expected
value of the square terms. Those are all going
to be equal to v, which is the variance
of the process, of the epsilon T's. I can write this down as v, and I have the summation, let's write it as a
function of j here, of v to the power of h plus j times phi j. The variance is just
coming again from the expected value of
the epsilon t squares. This is the covariance function. If you simplify this
expression further, you see that the phi
to the power of h, comes out of the summation, and then we have
these terms here. If the phi is again assumed
to be between minus 1 and 1, this summation is
going to converge, and I can write this
down as this expression. Here I'm assuming that say
h is any positive integer, and this is my auto
covariance function. If I want to compute the
autocorrelation function, is simply going to be the auto covariance divided by the variance of the process. But we computed before the
variance of the process is that V over 1 minus phi squared. This is the other assumption
here between minus 1 and 1. This is going cancel, the terms here are
going to cancel and I'm going to get phi to the power of h. If you're working with
a negative integer, if h happens to be
a negative integer, you're going to have
the same structure only that now the phi is going to be to the power of
the absolute value of that. In general, for any h, positive or negative,
we have that the auto-correlation
is going to be of the form phi to the power of the absolute value
of h. Similarly, if we want to write down
the auto-covariance, it's going to be phi to the power of the
absolute value of h v over 1 minus phi squared. This gives me the
theoretical autocovariance and autocorrelation functions
for the AR1 process. We can see that assuming again that phi is
between minus 1 and 1, this is a decaying function
as a function of the lag. The larger the value of phi is, so if you have a phi that is
positive and close to one, the decay is going to
be slower than if you have a phi that is positive
and close to zero. Also, if the phi is negative, you're going to have also
that exponential decay as a function of h, but it's going to be
an oscillatory decay. It's going to be
alternating between positive and negative
values depending on h here.