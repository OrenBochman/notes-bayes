We will now discuss Bayesian
inference in the case of the autoregressive
model of order one. As you know, in the case of
the autoregressive model, you have two options for
the likelihood function. I will discuss a model
in which you use a likelihood function that is based on the conditional
information, so the conditional likelihood. Then you have to pick a prior distribution for the
parameters of the model. In this part, we will discuss
Bayesian inference with the conditional likelihood
and the reference prior making a connection
with the regression model. Recalling the AR1 case, we have a model that
looks like this, and then the Epsilons we're
assuming they are iid, normally distributed
with zero mean and variance V. We have
two parameters here, Phi, the AR coefficient, and v the variance. When we work with a
conditional likelihood, we discussed we can write this model as a linear
regression model, so in this case, what that means is we're
writing the model as y equals x Beta plus some
other vector Epsilon. In the case of the AR1 you can write with the
conditional likelihood you're conditioning on
the first observation, so your y is the vector that
goes from y_2 up to y_T, the total number of
observations you have. X in this case is just a vector as well
with the past values, and then Beta is just a scalar, in this case you have Phi, and the Epsilon is
normally distributed. It's a multivariate
normal in this case. The mean is going to be
the vector of zeros here, and the covariance
matrix is going to be v, the variance of the process,
times the identity. In this case, the identity
matrix is going to be of the same dimension that
corresponds to the y, so it's going to be T minus
1 times T minus 1 Identity. Using this form, we can write first the conditional
likelihood in terms of this normal
distribution. If I think about writing
the likelihood here, it's y_2 up to capital T given Phi and v and y_1. I can write this down in terms of density that looks like this, so I have my one
over two Pi v here. Again, this is the v, my y vector here and my Beta here are going to be
given by this expression, so this is going to be a
function of phi as well, and then I have to pick a prior distribution for
the model parameters. We can work with different
kinds of priors; we can work with conjugate
priors and we can work with non conjugate priors in the case of this likelihood, or the other likelihood. What we're going to do
here is we're going to use the reference prior
for several reasons, one of the most
important reasons we're working with that prior is that you get closed form posterior distributions
for the model parameters, and you can do posterior inference in a
computationally easy way. The reference prior, if I think
about my parameters here, we are going to use something
that looks like this. This is my V. Then
we're going to combine. To get the posterior
distribution, we just combine the likelihood is proportional to the
prior times the likelihood. In this case, again, because we are working with
the conditional likelihood, we're conditioning on
the first observation. Combining this piece
with this piece, we can get the
posterior inference and the results from
regression models is that essentially when you have
something of this form and you use your reference prior, you're going to get that
the posterior distribution, so there are two
important results here. One is that the
conditional posterior for beta given v, X and y is
normally distributed here. Centered at beta hat, the maximum
likelihood estimator. Then with a variance
covariance matrix that has the form v, X transpose X inverse. That's the form of the conditional for beta given
v and all the observations. Then the other result that we
have is the marginal for v is when we integrate
out the beta is going to have a form with this particular prior that
leads to an inverse gamma. The inverse gamma prior. I'm going to use this
notation to say that this is distributed
as an inverse gamma. The parameters of
the inverse gamma in the case of the AR(1), with the conditional likelihood, is going to have T
minus 2 over 2 here. Then it's going to be that
Q that we defined before, evaluated at this
beta MLE over 2. What is the maximum likelihood
estimator for beta here? which is, as we say, this phi. Well, the estimator,
as we saw last time, you can get it by computing, assuming that the X
matrix is full rank. You're going to get it just
using these expressions. You just do X transpose X
inverse, X transpose y. As we saw before, in this case, those
operations are easy. X transpose X is just multiplying this vector
transpose by itself. Then just inverting that. Then we just multiply
this one times this. For our beta hat here, which is exactly my phi,
my parameter estimate, we get the maximum
likelihood estimator, which in the case of the
AR is y_t y_{t-1}, t goes from two to
capital T over, so this is the expression here. Then for this v, the Q expression that
we wrote before, if you recall, we can view this as a
quadratic expression. In this case again, the
Beta hat MLE is just, the phi hat MLE. That's going to be
just the sum from two to capital T y_t. We now have all the elements
to obtain posterior samples from the posterior distribution
of phi and v as follows. Since the v is distributed
as an inverse Gamma, we can sample from this
inverse Gamma distribution. We know what the parameters
of the inverse gamma are and then
conditional on a value that we sample that
v. We can then sample for the
distribution of beta, which in this case is the
conditional distribution of phi given v, X, and y. We obtain samples for v and phi.