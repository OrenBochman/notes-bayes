We will now talk about the auto-covariance
function as it allows us to characterize a lot
of the properties of a stationary time
series process. In general, even if the
process is not stationary, we can talk about the auto-covariance function
of the process. Again, we're working
with y_t here. We're going to assume that the process has first and second moments
that are finite, so we're going to
be able to compute expected values and covariances. We define the auto-covariance
function here between, this is the covariance,
any two points. We're going to have
this as the covariance between y_t and y_s. This is just by definition
expected value of y_t minus Mu_t times
y_s minus Mu_s. Here Mu_t is the
expected value of y_t, and Mu_s is the
expected value of y_s. If the process is second-order stationary or strongly
stationary then we know that the expected
value is really not dependent on time and
the covariance is also going to be a function
of the distance between t and s. When we have stationarity, we have the expected value
of y_t is Mu for all t. We can write down
the covariance is really a function
of the distance between t and s. In particular, we can write down the covariance as a
function of the lag. If you have an h, here is an integer
greater than zero. We can write down the covariance as a function of that lag h. This just we can think of it as the
covariance between y_t and y_t minus h, and it's just a
function of the lag. This is just for the
auto-covariance function. We can also talk about the autocorrelation or
in this case again, if this is the
covariance we are using this Gamma function
for the covariance, we can use the autocorrelation, this is usually called the ACF. In the general case is
going to be a function of t and s is just going to be given by we have the auto-covariance in
the numerator and then we're going to standardize by dividing by the
standard deviation of the process at time
t. Which is just Gamma t, t gives me the variance of
the process at time t and if I take the square root I
get the standard deviation. Then again, Gamma s, s gives me the variance
of the process at time s. If I take the square root I get the standard deviation. This is the
standardized version, this is now an autocorrelation
and because it's a correlation function it's always going to be between
minus one and one. When we have stationarity, then we can also write
in the same way that we wrote the auto-covariance
as a function of a lag, we can write down
the autocorrelation, the ACF as, now I'm going to use rho here of the lag and this is just, we take the auto-covariance for lag h and divide
by the variance, the variance is when I have
h equals to zero here. Here we have again, the autocorrelation
function written as a function of the lag when
the process is stationary. Here again, this is
just the variance of the process y_t which we know is going to be
a constant over time. It doesn't depend on time. We just described the
autocovariance function of a time series process. Now, we can obtain estimates of this function by using the
data that is available. Here we're going to
assume that we collect the data from one up to capital T, and we are going to
use these to obtain what is called the sample ACF, so you can get the sample
autocovariance function and the sample autocorrelation
function as well. Here assuming stationarity, we can obtain Gamma hat h, so this is an estimate for
the function Gamma h which is the auto-covariance
function of the process y_t and we're using the hat to indicate that
this is an estimate. Here if we have the
estimate is going to be based on these
observed data y1 up to capital T. This is just
the sum for t equals 1 to t minus h of y_t plus h minus y-bar times
y_t minus y-bar. Here y-bar is simply the average
of all the observations. We can write that down
as just the average. This is our estimate that gives us the sample ACF based on my data. I can also get the
sample auto-correlation. This is the sample
autocovariance, I can get the ACF
usually refers to the autocorrelation functions, so I can get the sample
auto-correlation function by simply plugging in the estimate for
the autocovariance and divide that by the
estimate for the variance. This gives me my estimate
for the sample ACF. What we will do in the
class is talk about some classes of models like
the autoregressive processes. They have a particular
characterization of the auto-covariance and the
autocorrelation functions. Once we observe some data we can get estimates for those
functions and see whether or not those estimates
are consistent with the theoretical properties
of a particular model. For example, the case of
the autoregressive process.