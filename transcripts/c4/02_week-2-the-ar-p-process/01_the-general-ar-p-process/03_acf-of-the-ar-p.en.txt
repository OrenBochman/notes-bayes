In the case of an autoregressive
process of order p, the autocorrelation function
can be written then as the solution of a homogeneous
difference equation. Again, we are working
with the stationary case. You can recall the
notation we were using. Rho is a function
of the lag here. For an AR(P) we can write
this difference equation. The autocorrelation function has to satisfy this equation. You can show that the
solution of this equation is related to the characteristic
roots of the AR polynomial. In particular, if we think that the AR process that we are working with has r
reciprocal roots. I'm going to call them Alpha_1 up to Alpha_r and they
are all distinct. Let's assume that
each of these roots has a specific multiplicity. Then, again, for each of these roots and their
multiplicities, they have to satisfy that
when we add them all we get the order of the model
which is P. For this case, we can write down
the solution or the autocorrelation function
as a function that is a sum of r components where each of those components is related to one of the roots. I'm going to have Alpha_1^h. Each of these p's here are
polynomials on h. The degree of the polynomial
is going to depend on the multiplicity of the root. Each of the polynomials
here on h is a polynomial of order, the multiplicity of the root
minus 1, m_j minus 1. In the case in which you have exactly P different
reciprocal roots, so the number of different reciprocal
roots you have in the AR characteristic
polynomial is exactly P. Each root is going to
have multiplicity 1. Therefore, each of these
polynomials is going to be a polynomial of order 0 on h, so it's just going
to be a constant. Usually we work with that case. I'm now going to talk about two examples that we
have considered before. In the case of the AR(1), we saw earlier that we can write down the autocorrelation
function is just, has this form, is
the form Phi^h, where Phi is the AR coefficient. This form is consistent
with this result. In the case of a
stationary AR 1, we can show that the
reciprocal root of the characteristic
polynomial is Phi. So what we get here is we have a single
root, multiplicity 1. We have the root^h
multiplied by a constant. In this case, a
constant is equal to 1. You can show that that
constant is equal to 1 just using properties
of autocorrelations. In the case of the AR 2, we're also going
to have this form. We discussed three cases in terms of the number of
reciprocal roots you can have. You can have two real valued
different reciprocal roots, each with multiplicity 1. You can have a pair
of complex roots. They are different. They have a modulus and a frequency
associated to them. You also have the case of
one single reciprocal rule, real valued with multiplicity 2. For instance, if you have
something that looks like this, so I have a pair of complex
valued reciprocal roots, I can write them in this
polar representation. R is the modulus of the reciprocal root and
Omega here is the frequency. Using this result because
each has multiplicity 1, I'm going to again have that. I can write a constant
times Alpha_1^h plus another constant
times Alpha_2^h, given that these are complex reciprocal roots and they appear as
complex conjugates. I can simplify this equation
and then get something of the form plus aother constant. Here, c and d are constants. My Omega is my
frequency of the roots, r is the modulus. Then we can see again that
in the case of stable, stationary processes, my modulus of the reciprocal
root is smaller than one. We are going to have an exponential decay multiplied
by these cosine waves. so
there is going to be this oscillatory behavior. In this case again, we
have exponential decay. We can see that we can find the properties of the
autocorrelation function. They are related to the reciprocal roots of the
characteristic polynomial. We can also think about the partial
autocorrelation function. For the partial
autocorrelation function is possible to use the Durbin
Levinson recursion again to express the partial
autocorrelation coefficients as functions of the autocovariance and
the autocorrelation. In the case of an AR(P), you can show that it
has to be the case that after the lag of
the order of the model, all the partial
autocorrelation coefficients are going to be zero. That's a property for the partial autocorrelation
for the AR(P), it is zero after lag equal to the model order P.
In the case of the ACF, the autocorrelation
function, we have that exponential decay assuming that all the roots have
moduli smaller than one. If we want to get the sample
partial autocorrelation, we can also put the
sample autocorrelations into the Durbin
Levinson recursion and we can get the
expressions for that.