Now, that we have discussed the characteristic polynomial and the state-space
representation for a general AR(P) process, we are going to discuss a
few examples just to try to understand what this means in particular cases
of AR processes. One of them is going to be, we're going to go back
to the AR(1) and see what is the forecast
function and how that relates to what we
learned before. The other example
we are going to consider here is the AR(2), the autoregressive
process of order 2. Let's begin with the first case, which is again the AR(1). In the AR(1), we are assuming that as usual
the model looks like this. Now if I think about the state-space
representation for these process is going to be, remember we need to have
something that looks like this X_t, X_t G X_t minus 1 and then plus Omega
t. In this case, because I have order 1, my X_t is going to be a
one-dimensional scalar component, the same thing with F, and then G is going to
be just my Phi. Here F is going to be one, X_t is going to be y_t, and then G is going to be Phi. If you think about how we
define the forecast function, we said the forecast function
is just the suspected value of y_t plus h given everything
that we have observed. I think about predicting h steps ahead given all the
information I have up to time t. We said that the state-space
representation allows us to have something
that looks like this. Given that these
are all scalars, what we are going to have
is that we can write down the forecast
function in terms of the reciprocals root of the
characteristic polynomial. In the case of the
characteristic polynomial, I'm going to have for the AR(1), so before I write down
the expression here, is going to be an
order 1 polynomial. This is the form
of the polynomial. If I want to find the roots of the characteristic
polynomial and this implies that I
have one over Phi. That's my root of the
characteristic polynomial. For the process to be stable, I need this root to have modulus greater than one or
the reciprocal root, which is going to be phi, to have modulus
smaller than one. I have that this is, we are going to make here the assumption that we are
working with processes that have Phi values that are
between minus 1 and one. Again, going back to
the forecast function, since the G matrix is Phi, I can write these down as just something that
is the power of Phi. It's going to be, I'm going
to have some constant here, whatever that is that comes
from this expression. Then I'm going to have
my Phi to the power of h. That's the expression
we get for the AR(1), and we can see that it has the same form that
we discussed for the auto-correlation
function of an AR process. If you remember, the
auto-covariance and the auto-correlation function of an AR process is decaying exponentially as a power of
h just in the same form that this forecast function is decaying exponentially on Phi. If the Phi negative, we're going to have
an oscillatory decay. If the Phi positive, we're just going to have
an exponential decay. It follows exactly
the same structure as the auto-covariance
function for the AR(1). There is that correspondence and here we are just thinking about how we're going to predict
the process h steps ahead. Our second example is
just the AR(2) process. Let's think about what
kind of structure we have for the forecast
function of this process. In an AR(2) we are going to regress on the first two
values of the process. We have to AR coefficients,
Phi_1 and Phi_2. The characteristic
polynomial is going to be given another two
polynomial in this case. Then again, if we think
about stable processes, they are going to have characteristic roots that
have moduli larger than 1 and the reciprocal roots
have moduli smaller than 1. If I think about finding the
roots of this polynomial, this is the same as
the just getting 2 squared plus Phi_1u
minus 1 equals to 0. Then this is a
quadratic polynomial. If I want to find the roots, I can just think
about this equation. Looking at this expression, we can think about three
possible cases depending on what happens with the expression that is here in the square root. I can have values of
Phi_1 and Phi_2 that make this expression different
from zero and positive. In that case, I'm going to have two different real
reciprocal roots. I can think about
having values of Phi_1 and Phi_2 that make
this expression equal to 0. In that case, we can
again think about having a single reciprocal
root with multiplicity 2. Then I can think about cases
in which I'm going to have values that make this expression inside the square root negative. In that case we are going to
have complex-valued roots. Let's think about those cases. The first case we can
describe this the case in which Phi_1 squared plus 4
Phi_2 is greater than 0. In that case, we are going
to have two characteristic. They are both real valued. I can write them down as u_1. If I think now about
the reciprocal roots, I'm going to have just
these are real valued. I'm going to have
Alpha 1 is going to be 2 Phi_2 over minus Phi_1. Then I'm going to
have my Alpha_2 plus Phi_1. So I have these two real-valued
reciprocal roots. So I can write down the
forecast function again f_t(h), which is my expected
value of y_t plus h, given all the observations up to time t, I can write it down. I'm going to have
two components. One is going to be related to
the first reciprocal root, and the other one is going to be related to the second
reciprocal root. I have a power of
h here for both. Then I'm going to have to let's call it c_t1 and c_t2 here. These are constants.
As you can see, I'm going to have two components
in my forecast function. If one root has a modulus that is larger
than the other one, that one is going to have
a larger contribution to the forecast function h steps
ahead than the other one. Again, we have an
exponential decay assuming that the
process is stable, but we have two components are contributing to the
forecast function. The second case I'm going to discuss in terms of the
characteristic roots here is the case in which you
have values of Phi_1 and Phi_2 that lead to negative values inside
this square root. So here we would have the case in which
Phi_1 square plus 4. This is a negative value. Because we have something negative inside of
the square root, what this means is
we're going to have two complex valued roots and therefore two complex
valued reciprocal roots. Since they are complex valued, they are going to appear as a
pair of complex conjugates. We can do the calculation
so I can write down u1 and u2 and then think about
the reciprocal of those. I'm just writing the
reciprocal roots. In this case we can write them like this over two. Once you do all
your calculations and take the reciprocals. This is a pair of
complex conjugates. One root is a complex
conjugate of the other one. These are different
reciprocal roots. Because they are complex valued, we can write them
in a different way. We can write them as Alpha 1 r exponential of i Omega, let's call it, and Alpha 2 r exponential
of minus i Omega. If I write these in
polar coordinates, I'm going to have a modulus. The modulus is the same for each of these and then I'm
going to have a frequency. The modulus r the
frequency is Omega here. One is the complex pair of the other one and if
you think about again, the form that this is going to have in the
forecast function, we can write down the
forecast function. Again, we have the contribution. You can think of this as a constant times Alpha 1 to the power of h and
then plus another constant times Alpha 2 to the power of h. But because they are
complex conjugates, we can write down the
expression in terms of sinusoidal form as follows. If I just do this, this is what we were
describing before. There is an Alpha 2 to the
power of h. Using these here, we can rewrite everything and the fact that they are
complex conjugates, also the eigenstructure
is going to be like that. We can write down in terms of something that
looks like this. This is the cosine of that frequency times h. h is
the number of steps ahead, plus some random phase
here that depends on the information up to time t.
Then here for my amplitude, I'm going to have again, something that depends on the information I
have up to time t, and then I have the modulus of the reciprocal root to the
power of h. You can see here, there is also some
exponential decay, assuming that we are in
the stable case where r is essentially
between zero and one, because that's the modulus
of the reciprocal root. Then I have an exponential
decay as a power of h, but it's a sinusoidal. It has a sinusoidal form. This is just a cosine wave
and it's going to be having this is what we call
quasiperiodic behavior. The periodicity here is
given by this Omega. There is a random phase
and that's why we say that the form of the forecast function
is quasiperiodic. When you have an
ar(2) depending on the structure of the roots, you can also have a
case in which you have a quasiperiodic process. The final case for this is the case in which
you have values of Phi 1 and Phi 2 that give you a zero here
inside the square root. In that case, you
are going to have a single root with
multiplicity two. You can think about how many
times the root repeats. We did not write the general expression for
the forecast function in the case where you have repeated roots with multiplicity
higher than one, but you can show that the
forecast function of this, in this particular case, when you have a repeated root, is going to be of the form. You have your Alpha, which is that single
reciprocal root that we have here to the power of
h times a_t plus b_t times h. You have
here a polynomial of order one in h multiplied by that Alpha h. This is the case of the process when
you have a single root. In practice, we always work
with cases one and two, and we will see that
in the examples.