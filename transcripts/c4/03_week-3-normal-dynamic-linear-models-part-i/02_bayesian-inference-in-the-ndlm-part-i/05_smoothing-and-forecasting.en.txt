[MUSIC] We know,
we now discuss the smoothing equations for the case of the normal
dynamic linear model. When we are assuming that both the
variance at the observation level is known and the covariance matrix at
the system level is also known. Recall we have two equations here, we have the observation equation, where yt is modeled as Ft'θt
+ noise the noise is N(0,vt). And we're assuming that the vt is given. We are also assuming that we know Ft for
all t. And then in the evolution
equation we have θt= Gtθ(t-1)+noise. And then again, the assumption for the wt is here is that they are normally
distributed with mean zero, and these variants co variance matrix,
capital Wt. So we can summarize
the model in terms of Ft, Gt, Vt and Wt, that are given for all t. We have discussed the filtering equations. So the process for obtaining
the distributions of θt given Dt, as we collect observations
over time is called filtering. Now we will discuss what
happens when we do smoothing, meaning when we revisit
the distributions of θt, given now that we have received
a set of observations. So Just to illustrate the process, we have here, θ0,θ1 all way up to θ4. And we can assume just for
the sake of the example, that we are going to
receive three observations. So we are going to proceed with
the filtering, and then once we receive the last observation at time three,
we're going to go backwards and we're going to revisit the distributions
for the state parameters. So just to remind you how
the filtering works, we move forward, before we receive any observations. In the normal dynamic linear model,
when we have all the variances known. The conjugate prior
distribution is a normal, with mean m0 and variance C0. So this is specified by the user,
before collecting any observations. We can then use the structure of
the model, meaning the system equation and the observation equation to obtain
the distribution of θt, given D0. Before observing the first y. This gives us first
the distribution of θt, θ1 given D0, which is normal a1 R1. And then we can also get the one
step ahead forecast distribution for y1 given D0, which is a normal f1 and q1. And we have discussed how to obtain these
moments using the filtering equations. Then we received the first observation,
and the first observation can allows us
to update the distribution of θ1. So we obtain now the distribution
of θ1 given y1, and whatever information we
have at D0. So this gives us m1 C1. And using again the structure of
the model, we can get the prior distribution for θ2 given the one and
that's a normal a2 R2. And then the one step ahead forecast
distribution now for y2 given D1 and that's a normal f2 q2. So we can receive y2 update
the distribution of θ2 and we can continue this process,
now get the priors at T=3. And then once we get the observation
at T=3, we update the distribution. And we can continue like this
with the prior for θ4 and so on. Let's say that we stop here, at T=3. And now we are interested
in answering the question. Well, what is the distribution for
example of θ2 given that, now, I obtain not only y1 and
y2, but also y3. I want to revisit that distribution
using all that information. Same thing for
say, the distribution of θ0, given the D0, y1, y2 and y3. So that's what it's called smoothing. So the smoothing equations,
allow us to obtain those distributions. So just to talk a little bit
about the notation again, in the normal dynamic linear model
where vt and wt are known for all t's. We have that this is a normal,
so the notation here, the T, is larger than t, here. So we're looking at the distribution
of θt, now in the past and that one follows a normal
distribution with mean aT(t-T). So the notation here for
the subscript T means that I'm conditioning on all
the information I have to T. And then the variance covariance
matrix is given by this, RT(t-T). So this is just going to indicate how many
steps I'm going to go backwards as you will see in the example. So we have some recursions in the same
way that we have the filtering equations. Now we have the smoothing equations. And for these smoothing
equations we have that the mean. You can see here, that whenever you're
computing a particular step t- T, you're going to need a quantity that you
computed in the previous step, t-T+1. So you're going to need that,
is a recursion, but you're also going to need mt and
and at+1. So those are quantities that you
computed using the filtering equations. So in order to get
the smoothing equations, you first have to proceed
with the filtering. Similarly for RT(t-T), you have also that depends on something you
previously obtained. And then you also have the Ct,
the Rt+1 and so on. So those quantities you computed when you
were updating the filtering equations. The recursion begins with aT(0)
meaning that you are not going to go backwards any points in time. So that is precisely
the mean is going to be whatever you computed with the filtering equations of up to T, that's mT. And then RT(0) is going to be CT. So just to again illustrate how
this would work in the example, if we start here right? If we condition, so
the first step would be to compute again to initialize using
the distribution of θ3 given D3. And that is a normal with mean a3(0) and
variance covariance matrix R3(0), But those are precisely m3 and
C3 respectively. Then we go backwards one step. And if we want to look at what
is the distribution of θ2, now conditional on D3. That's a normal with mean a3(-1) and variance covariance matrix R3(-1). So if you look at the equations down here,
you will see that, in order to compute a3 (-1), and R3(-1). You're going to need m2,C2,
a3,R3 and then what you computed here these moments in the previous step,
a3(0) and R3(0). Then you obtain that distribution and you can now look at
the distribution of θ1 given D3, that's the normal a3(-2), R3(-2). And once again, to compute these moments,
you're going to need m1,C1,a2,R2 and then you're going to need a3(-1),R3(-1). And you can continue all the way down to
θ0 given D3 using these recursions. So the smoothing equations allow us to,
just compute all these distributions. And the important equations
work basically because of the linear and Gaussian structure in the normal
dynamic linear model. In a similar way, we can compute
the forecasting distributions. Now we are going to be looking forward,
and in the case of forecasting, we are interested in
the distribution of θ(t+h) given Dt. And now h is a positive lag. So here we assume that is h≥0. So we are going to have
the recursion is a N(at(h), Rt(h)). The mean is at(h) and we are going to use the structure of the
model to obtain these recursions, again. So here we are using the system equation,
and the moment at(h) depends
on what you computed at at(h-1) the previous lag, times Gt+h. And then, would you initialize
the recursion with at(0)=mt. Similarly, for
the covariance matrix h steps ahead, you're going to have a recursion
that depends on Rt(h-1). And then you're going to need
to input also Gt+h and Wt+h. To initialize,
the recursion with Rt(0)= Ct. So you can see that in order
to compute these moments, you're going to need mt and
Ct to start with. And then you're also going to
have to input all the G's and the W's for the number of
steps ahead that you require. Similarly, you can
compute the distribution, the h steps ahead distribution
of yt+h given Dt. And that one also follows a normal,
with mean ft(h), qt(h). And now we also have a recursion here,
ft(h) depends on at(h) and as we said,
at(h) depends on at(h-1) and so on. And qt(h) is just given
by these equations. So once again, you have to have
access to Ft+h for all the h, a number of steps ahead that you
are trying to compute this distribution. And then you also have to
provide the observational variance for every h value. So that you get vt+h. So this is specified in
the modeling framework as well. If you want proceed with
the forecasting distributions.