[MUSIC] I will now discuss Bayesian inference in
the case of the normal dynamic linear model where both
the observational variance and the system variance are known. We will talk about filtering equations,
smoothing equations and also forecasting in this
setting using Bayesian approach. So recall we are working with
a model that looks like this. And then this is my first equation,
the observation equation and I have a system equation
that looks like this. We are going to assume that Vt and
Wt are known for every t. And we also know what the Ft's and
the Gt's are here. So the response is a uni-dimensional
yt and then I have, say, theta t is a vector of a given dimension,
depending on the structure of the model. So we are interested in performing
Bayesian inference in this setting and we talked about different
kinds of distributions. One is the filtering distribution that
allows us to update the distribution of theta_t as we receive observations and
information over time. The other one is smoothing
equations that allows us to just revisit the past once we
have observed a chunk of data. So I will be talking about those and
also smoothing. In a Bayesian setting,
you have to set a prior distribution. We will work with the prior
distribution that is conjugate. In this case I have to begin
with distribution at time zero. So before I know, I have seen any data
at all, I have this prior distribution. D0 stands for the information
that I have before collecting any data. And we are going to assume, That this theta0  follows a normal
distribution with m0 mean and variance covariance matrix C0. So these are also specified when
you're working with this model. So we assume that this m0 and C0 is known. Once we have this setting
using these equations, we can obtain the filtering equations. So the first assumption
is going to be that we have, A structure. So for theta t -1 given D_{t-1}
is going to have this normal structure which is
going to happen basically because we're using this conjugate prior. And because we have normal
structure in the model, is going to lead to
the following distribution. So the first one is the prior at time t. So if I want to think about
why my distribution for the t is given the information
I have up to t- 1, I can look at the equations of
the model and use this second equation. And by looking at this equation, if I
condition on the information I have up to t -1, I can see that, say, theta t is
written as a linear function of, theta, t -1 and
I have the assumption of normality here. Therefore, say, theta t going to follow
a normal distribution with some mean and some variance. So now we're going to compute this mean
and this variance using this equation. So if you think about
the expected value of theta t, given D_{t -1}, that's just
going to be Gt is a constant here. So I have my Gt and
then I have expected value of theta t -1 given Gt -1 plus expect
the value of this omega t. But omega t is a zero mean,
normally distributed quantity, so it's just going to be zero. Using the assumption that
I have this structure, then I have that the expected
value of theta t, given D t -1 is Gt times m_{t-1}. We're going to call this quantity at,
so we have here at. For the variance covariance matrix, then we just have to compute,
do the same type of operation. And again, we can use this equation and see that we obtain this Gt
variance of theta t -1, given Dt -1 Gt transposed. And then we have now
the variance of the omega, the variance of the omega is just Wt. So we have Gt, this is Ct -1 Gt transpose plus this Wt. So we can call this quantity Rt and just have the form of this
prior distribution at time t. I can now think about another distribution which is the distribution
of yt given D_{t -1}. So this is the so called one-step ahead, Forecast, And in the one-step ahead forecast again
is a similar type of structure. So now we're going to use the first
equation rather than the second equation and we see that yt is written in terms
of a linear function of theta t. And we have also the Gaussian
in assumption here. So again the yt is going to
be normally distributed, And we just have to compute the mean and
the variance for this yt. So using the first equation, we have the expected value of yt given Dt -1 is just Ft transpose expected value of theta t given Dt -1. And we computed this before,
so this is, again, the expected value of theta t given
Dt -1 is what we computed here. So this is to be Ft transpose at. And we are going to call this little ft. Then, for the variance, Again, we use this equation,
we have this component, so we are going to get Ft transpose. Then I have the variance of
theta t given Dt- 1 Ft plus, now we have the variance of this term, and this term,
the observational variance is Vt. So again, this one we computed before,
is what we have called Rt. So this gives me Ft transpose Rt, Plus Dt and I'm going to call this qt. So my final distribution,
the one-step ahead forecast distribution, tells me that this follows a normal ft qt. The next equations we are going
to discuss are the equations that tell me about what is
the distribution of theta t once we incorporate
the information provided by yt. The next distribution is
the posterior of theta t given Dt. So that's, theta t given Dt. And we can write Dt as whatever information we have at time t- 1. And the new data point with this just yt. So we just want to update
the distribution of theta t given that we have received this additional
data point at time t. There are two ways of
computing this distribution. One uses normal theory,
the other one uses Bayes' theorem. And you obtain that the distribution
of theta t given Dt is going to be a normal, with mean we
call it mt and variance Ct. We will see how to obtain
this distribution or the moments of this distribution
using normal theory. >> So, again, we can write down, if we think about just
combining the vector theta t with the observation
Yt given Dt -1, right? We have information about
theta t given the t -1. That's the prior for theta t a time t,
based on the information at t -1. And then we also computed before the one
step ahead forecast distribution for yt given D_{t -1}. So we know that when we combine these two in a single vector, we're going to have a multivariate normal distribution and the first component is going to be at. The second component is what we
have called Ft, so that's the mean. And then for the covariance matrix. We're going to have now,
what goes here is just the variance of theta t given Dt -1,
which we have called Rt. What goes here is the variance of yt
given Dt -1 and we have called this qt. And now we have to compute
the covariance between theta t and yt, and that goes here. And the covariance between yt and theta t, which is just the transpose of that,
is going to go here. So if I think about computing
the covariance of theta t and yt given Dt -1,
I can write yt using the first equation here as a function of theta t. That's going to give us, Ft transpose theta t plus vt given Dt -1. And in this one we can see
that this is going to give us basically the variance of
theta t given Dt -1 and then multiplied by Ft transpose
transpose which gives me the Ft. So this is going to be variance
of theta t given Dt -1 times Ft. And then there is a term that combines
the theta t with the noise but they are independent, so
the covariance is going to be zero. So this one is simply
going to be my Rt Ft, so this goes here, And what goes here is just the covariance of
yt with theta t or the transpose of this. So this is going to give me Ft
transpose Rt transpose, but Rt is a covariance matrix, so
Rt transpose is equal to Rt. So now I have my full
multivariate distribution and I can use properties of the multivariate
distribution to compute the distribution of, theta_t,
given yt and Dt- 1. So that's going to be
a conditional distribution, I'm going to condition on the yt. And when I combine yt and Dt -1 that gives me just
the information up to time t. So we are interested in just finding, say, theta t given yt and Dt -1 which is the same as theta t given Dt. We partition the normal
distribution in this way, so I can just think about this
is the first component and then I have these different
pieces in my covariance matrix. And we know from normal theory
that if we have a distribution, if we have a vector that is
partitioned into vectors here where they are normally distributed. And I have my mean partition here and let's say I have one component here, Then we know that if I wanted to compute
the distribution of X1 conditional on X2, that's going to give me normal,
let's say alpha star. And let's call this one the sigma star, where alpha star is going to be my alpha 1 plus sigma 12 inverse. And then I have X1- alpha 2 and then I have my sigma star. And this one gives me my sigma 11 -, 21. So this is a result from normal theory. So if I want my conditional
distribution of X1 given X2 I can apply these equations. So we notice we have the same
type of structure here. If I partition my vector and
in theta_t and yt. And now I condition on, I take the distribution of
theta t conditioning on yt. I'm going to have that same structure
where this is normal, mt Ct. And my mt using normal theory, again, is going to be at plus, then I have this piece here, sigma 22 which gives me the qt inverse. And then I have, yt- ft. So that's my mean and
my covariance matrix. It's going to be Rt- qt inverse and then I have this transpose again. So if we simplify things a bit here and
we call et, it's just the error that
we make when we compare yt, which is the observation
with the prediction, right? And then I also use the notation I call at, let's call here At Rt Ft qt inverse. Then we can write this down,
to mean, we can write as at, Plus, And the covariance matrix. We can write it as Rt, At qt At transpose. So this gives me the posterior mean
after receiving this yt observation. And you can see that you can write down
the posterior mean, has this usual form of the prior plus something that relates to
the error that I make with the prediction. So the yt appears there and then is weighted by this
quantity that we just call at. And for the covariance structure,
we are also incorporating information about the prior and
what the yt observation provides. So this gives us our filtering
equation for theta t given Dt. And now we can apply all these
equations as we receive observations from t equals 1 all the way to capital T. If we happen to have capital T
observations in the time series, we can do this filtering process and obtain these
distributions as we receive information. [MUSIC]