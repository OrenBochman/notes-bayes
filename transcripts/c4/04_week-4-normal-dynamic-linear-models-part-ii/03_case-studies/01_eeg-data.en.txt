We know discuss our first case study
type of example. In this case, we're going to use a time-varying
autoregressive model to analyze some EEG data. EEG stands for
electroencephalogram data. This is a brain signal of
a particular channel that was recorded on a subject that received
electroconvulsive therapy. It's just an
electroencephalogram recorded at a particular
location of the patient's scalp. There is a lot of information that you can
get about these data. This dataset was recorded
at Duke University. There are multiple papers
where we analyze these data. You can also look at
a full analysis and discussions and references
to the papers in the book, Prado Ferreira and West 2021. The idea here is also
to provide a little bit of comparison and relationship between what we are learning here with
dynamic linear models and what we learned earlier in the course with
autoregressive processes. Let me just show you the data. Then before we do that, we will need here the file that contains all the functions to compute the
filtering, smoothing, and forecasting
distributions using an unknown v at the
observational level. Then I'm putting
all the functions to compute the optimal
discount factor also in this other file that you will have
to source this. If you open this file, you will see that it
contains the functions we described before to obtain
the optimal discount factors. Let's just source those in. Here I'm providing the EEG data. It corresponds to
the middle portion of an EEG that was recorded at a sampling rate of 256 hertz or 256
observations per second. The data was subsampled every fifth observation to
make it a little smaller. Here I'm just plotting
that dataset. What you look at here is the voltage level
for the EEG data. We have these 3,600
observations. We're going to use a model that is a time-varying
autoregressive model, meaning you have an
autoregressive structure in terms of a
particular model order. Here we're going to use
a model order of 12. We're going to regress yt
on the 12th past values, of yt. What happens here is the AR coefficients are
going to be time-varying. That's why we are using a dynamic linear model for
this particular example. The total number of
observations is 3,600, I'm just going to do the
analysis with the first 3,200 only and then we'll see
what the results are. I'm just going to use
this first portion here of this EEG. When you look at the
picture of the data, again, if you were to look at the characteristics in terms of the frequency and the amplitude, you would see that the data
has a higher frequency or lower quasi-periodicity
at the beginning and a higher quasi-periodicity
towards the end. Then also the amplitude
changes over time. Here we are just defining the data and now we
are setting up the matrices. If you remember how we
set up the matrices here, the Gt is an identity, and then the Ft is
going to contain the past values of the yts. For each time t we are going
to have 12 past values. We build that and get the Ft matrices over time
and then the Gt matrix. The Gt is constant over time, the Ft changes over time. We set up the initial
state matrices. The model order is 12, so we are going to
have a distribution for the state parameter vector
of dimension 12 that is a normal with m0 v times C0_star. Here we have just that matrix. Here this is a vector of zeros. I'm just setting
everything at zero. Then the C_star matrix is a diagonal matrix of
dimension 12 times 10. Each of the components has
a variance of 10 here. This is conditional, again, this would be multiplied by v. The full covariance
structure will be given by v times C0_star is a
conditional structure. Then the parameters for
the prior distribution, the number of degrees of
freedom I'm assuming here is 1. Then the estimate, a priori for the observational
variance is 100. We set up those. We also keep those in objects
that then we can pass to the functions that
compute the filtering, smoothing, and
forecasting distributions. Here I'm going to
specify the Wt using a discount factor
so we are going to define a set of values here. I'm going to have a
lower bound that is 0.95 and an upper
bound that is one and then I'm going to
have increments of 0.001 to define
that grid of values. Just to compute the
optimal discount factor. Then I'm going to use
the mean square error as the measure to pick that optimal discount
factor. We run this. It takes a little bit
just because we have a fairly large range
of grid of values. Once we compute the
optimal discount factor, the function, if you remember, is going return the
filtering moments or the moments for the
filtering distribution, the moments for the
smoothing distributions, as well as the forecasting distributions with
the optimal value. If you change, you may choose
a different measurement. There are, if you recall, four different measurements to compute the optimal value
of the discount factor. Once this is done, I will show you the
results of the analysis. Let me just show you here. The optimal discount
factor that was selected here is 0.994. It's a fairly large
discount factor, meaning that the variations in the state parameters are
occurring smoothly over time, but there are changes. We can now retrieve the results in terms
of the filtering. distributions. We can compute the credible intervals for that. We can compute the results for the smoothing distributions and obtain credible intervals
for that as well. I'm just going to plot
here the results of these. This would be my data
and these would be the blue that I superimpose on that, is just the mean of the smoothing distribution based on the model and this discount
factor that we chose. It's capturing the main
characteristics of the data. But in this particular case, this information is not necessarily what we
want to look at. We want to try to interpret
what results this is giving us in terms of the time-frequency
characteristics of the data. One way of obtaining that information is to
think back in terms of the characteristic
polynomial and the reciprocal roots of that
characteristic polynomial. We are going to have a
characteristic polynomial for each time t now, because we have AR coefficients
that change over time. For each of those roots and for each polynomial
we know that we can also obtain a
spectral representation that changes over time. If you think about
this spectral density that is associated to each of those autoregressive
processes at time t, you can just represent
that in terms of different spectral density. Let's look at what
the results are. The next piece of code
is just computing those reciprocal roots of the characteristic
polynomial at each time t. Then looking at what is the modulus of the reciprocal roots and what is the period of the
reciprocal roots. Here we have a
model order of 12. As you recall, you can have complex valued or real
valued reciprocal roots, as we did in the
case of the the standard
auto-regressive model where the parameters are
not changing over time. Because you can have a different number
of roots in terms of how many pairs of complex roots you have and how many
real roots you have, this model is time-varying. You could potentially
have also changing number of complex valued and real
valued reciprocal roots. What I'm going to plot here, is I'm going to look
at the root that has the largest modulus. So the reciprocal rule that has the largest modulus over time. Then you will see here
that root happens to be complex so you do get a
pair of complex valued roots. There is a period
that is associated to that dominant root
in terms of the modulus. I'm going to plot the modulus of that root and also
the periodicity. What you see here, once you run the code. This is that trajectory based on the mean of this
smooth distribution for the state
parameter vector. The AR coefficients
at each time t here. You can see it starts very high, is very persistent
at the beginning of the time period and then it oscillates essentially
between 0.9. One, and you can see that
it decreases towards the end of the seizure here or towards the end of
the EEG recording. But this is a fairly
persistent group. When you look at the
corresponding frequency, and here I'm plotting that frequency in a
scale that is hertz, so if you remember
the original data was recorded at 256 hertz, but it was sub-sampled
every fifth observation. So if I want to plot
the results in terms of hertz here for that
corresponding frequency, I have to do this
transformation. What you see here, is that this dominant
quasiperiodic component has a frequency that starts a little bit higher
than five hertz, and then it decreases
towards the end of the EEG recording which is something we saw also when
we were plotting the data. If you were to look
at the data and zoom in into different portions
at the beginning of the EEG and at the end
you would see that this is consistent
with that behavior. In particular, neuroscientists like to talk about different frequency
bands in the brain, so we see here that this
dominant component is mostly in the delta
frequency band for the period of recording, at the beginning is in
the Theta frequency band. At the beginning, when
the seizure is starting, it begins in the Theta band and it decreases to the delta band. Just to again, remind
you of how this works, if I think about the spectral representation
of this process over time, you can specify some
time points here, and then look at what
is the estimate of the spectral density
that corresponds to that autoregressive process
for that particular time. I'm just going to
use the function, arma.spec from the
library astsa, we have used this
function before, and I'm just going to plot. Here we're just plotting
the spectral density estimated using
again the results from the smooth distribution, so this is based on that mean of the smooth distribution
at each time t, and we're going to use the
estimate of the observational variance that we get after
fitting the entire data set. What we get from the
dynamic linear model. Here the results, so this corresponds to
just one spectral density, you see that if you
were to translate this, so now the frequency appears
plotted between 0.5, if you were to do the transformation to
the original scale, you would see that the peak of these spectral density is precisely at that value that was around those that went from essentially five hertz down towards the
end of the seizure. We can see, I'm
now going to plot the different spectral
densities here, so just going back just to show you the plot a
little bit larger here, so I'm picking again different times during the EEG recordings, so this corresponds
to the 1.96 seconds, 9.77 seconds and so on, so this is towards the
beginning of the recording, and then this is towards
the end of the recording. You can see that at the
beginning of the recording, I have a power that is dominated mostly by a single
quasiperiodic component, that is that one that we just
plotted at the beginning, that begins in the Theta band. Then if you look at the
location of the peak, you see that the location of this dominant peak
is decreasing, so the frequency is decreasing towards the
end of the recording. Also, we see that the power of these spectral
densities tends to decrease also as the
recording gets to the end. There are other
components that are also contributing in terms of the
spectral characteristics. This is the nice feature of using a time-varying
autoregressive model. You can capture
what's happening with the data and what is the
spectral representation, and what is the interpretation
that we can give to these analyses in
terms of something that neuroscientists
can understand.