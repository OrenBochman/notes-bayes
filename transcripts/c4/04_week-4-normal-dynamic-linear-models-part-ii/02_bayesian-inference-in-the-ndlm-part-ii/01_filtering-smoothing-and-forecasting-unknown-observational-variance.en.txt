[MUSIC] We will now discuss the filtering, and
smoothing, and forecasting distributions. In the case of the normal
dynamic linear model, where we assume that the observational
variance is constant over time. So we're going to call that variance v and
so vt is equal to v for
all t, but v is unknown. So we're going to place a prior
distribution on that variance. We are going to write down
the model in terms of again, an observation equation and
a system equation. The observation equation has the same
form that we discussed before. We're now the observational variance vt is changed to be always v,
but now v is unknown. And then the system equation
we are going to write down as an equation that is
conditional on the variance v. So we have that say theta t is equal to Gt
theta t minus one plus the system noise. And we are assuming that this is
normally distributed with mean zero and now the variance co-variance
matrix is v times Wt star. So here we assume then that Wt
star is a known quantity for all the times t from one and so on. And then we're also assuming that Ft and
Gt are given. So what kind of prior
distribution can we use now? The structure of the prior in
this case that is conjugated with the model that we are proposing
is a normal inverse gamma prior that means for theta zero given
the zero we condition on v. We're going to use a normal prior
distribution with mean m0 and variance co-variance
matrix v times C0 star. And then the distribution for
v given D0 is an inverse gamma with
parameters n0 half and d0 half. d0 can be written in
terms of n0 times s0, so here n0 is the degrees
of freedom parameter. So the higher the value of
the degrees of freedom is the more information you
are inputting in this prior. And then s0 is a prior estimate
of the variance v before you obtain any observations, so
this is how you can view the prior. So this is a conditional prior structure,
if you use this prior structure, you can obtain the filtering
equations in closed form. The smoothing equations
also in closed form. So we are going to discuss
those equations now, so again, we are assuming that at time t- 1. We have this conditional
structure in which the prior the posterior at time t -1,
given the t -1, and v has this normal
structure conditional on v. And then we given the t minus one,
has this inverse gamma structure. Now, my parameters are indexed it in time, do I have nt- 1/2, and dt- 1/2, and the dt- 1 has the same structure,
nt- 1 times St- 1. So, if I have this condition structure,
we can show, and this structure, as we said, is true for
t = 0, as we wrote down before. We can show that the prior
distribution at time t given Dt- 1 and conditional on v is a normal at vRt star. And now I have equations just to compute
the value of the mean at is Gt * mt- 1. So this is exactly the same equation
we had before when we were discussing the case of the DLM with known variances at
both the observational and system level. And now we have thi Rt
star that has the structure Gt Ct- 1, Gt transpose plus Wt star. And then we can obtained
as well the one step ahead forecast distribution for yt given Dt- 1, conditional on v is given
by this equation here. It's a normal Ft vqt star,
where Ft is Ft transports t, and qt star is Ft transpose,
t, Rt star Ft + 1. So we have again,
conditional structures on v and then we can obtain
the posterior distribution for the variance after I get
my first observation, yt. And this is going to be given in terms of
an inverse gamma nt over to dt Over two. And the degrees of freedom parameter
is updated in such a way that it was the number of degrees of
freedom you had at step t- 1 + 1. This plus one has to do with
the observation you just collected. And then you can compute
the dt as nt * St where St is given by this equation here,
so it depends on St- 1. It depends on the number
of degrees of freedom, it depends on et which is
the given by yt- ft, so that's the prediction error. So yt is the new observation you just
obtained and then qt is st- 1qt star. Finally, you can then obtain
the posterior distribution for thetat given Dt conditional on
v that's a normal mt vCt star. And you can obtain the equations for
the moments of that normal distribution using
this equation for the mean. And then we have Ct star this is
the equation that allows us to update the co-variance matrix. So this is the distributions we have
conditional on v for theta t and yt. We can also write down,
we can integrate out the variance here. And when we integrate out the variance,
given that the variance has an inverse gamma distribution, we obtain
for theta t now unconditional on v. So it only depends on
the information up to t- 1, that's a student  t distribution with
nt- 1 degrees of freedom and at and Rt are. The location and scale parameters for
that distribution, so Rt is St- 1Rt star. Similarly, we get students
distributions for the one step ahead
prediction distribution for yt given Dt- 1 and for
theta t given Dt the posterior for theta t at time,
t is a student T nt mt Ct. So we can just write everything
conditional on v as we did before. Or if we want to just look at
the marginal distributions and conditional on v,
we obtained student t distributions. We now discuss the smoothing equations
in the case of the unknown variance at the observational level. So we have the same type of
notation we're assuming here we're looking at the distribution
of theta t given D capital T. So capital T here is greater or
equal than the lower case t. And before we have
a normal distribution now we're going to have
a student T distribution. The number of degrees of freedom
is going to be n, capital T. And then we're going to have mean
a capital T, lowercase t- capital T. And there is going to be a recursion
for the means of this distribution. And then we have for the scale parameter
here we have R capital T(t- T). And then that's multiplied by s capital T,
divided by s lower case t. So in this case we have if you recall
this is going to be the estimate that we're going to get for the variance
after receiving capital T observations. While this one is the estimate for
the observational variance that we get from the model after
receiving lower case t observations. So the recursions are going to
work in the same way. I'm not writing down the equations, they are just exactly
the equations that we had before. And then for forecasting if I want to
look at the distribution of theta t + h. So h here is greater equal than
zero given the information up to Dt, that one is going to be a student
T with nt degrees of freedom. And then I have my recursions also for
at(h) and Rt(h) as we had before. So as in the case with
the known observational variance, we are going to need the Gt + h here for all the values of h to
compute these moments and also the Wt + h for all the values of h. So we need to specify
those matrices in there. Everything else stays the same,
and now we have that for the predictive distribution for
h steps ahead for yt. Given Dt is also now
a student T with nt degrees of freedom, we have ft(h) and qt(h). The ft(h) is as before,
the qt(h) is slightly different now because
we don't have vt + h. We're assuming that v is constant and
unknown. So what goes in here, you can see
is the estimate that we have for that variance v  at time t. So this allows us to compute
the forecasting distributions.