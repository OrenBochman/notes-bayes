As we move over to continuous
random variables the math gets a little bit more complicated and
requires a little bit of calculus. We can define a continuous random variable
based on its probability density function or PDF for short. The PDF is sort of proportional to the
probability that the random variable will take a particular value. In differential calculus sense because it
can take an infinite number of possible values. The key idea is that if you
integrate the PDF over an interval, it gives you the probability that the
random variable would be in that interval. As a specific example,
let's consider a uniform distribution. X is uniformly distributed between 0 and
1. So it can take any value between 0 and
1 and they're all equally likely. This case, we can write the density,
f(x) as being 1, if x is in the interval 0,
1 and 0 otherwise. We could also represent this
as an indicator function and it's just the indicator function,
that X is between 0 and 1. We can then think about how we
might plot this as a function of X. So as X goes from 0 to 1,
what does this look like? Well, it's 1. In that region, and it's 0 otherwise. Very simple, the most simple,
probably, density function we have. We can then ask questions
about probabilities. What's the probability that X
will be between 0 and one-half? In order to get this probability, we can
just integrate the density function. So this is the integral from
0 to one-half of f of x dx, the integral from 0 to
one-half of just dx, because it's 1 in that range and
so this is just one-half. If we look at the picture, we could see the picture here and it's the area there. We could ask a similar question. What's the probability X is
between 0 to one-half, inclusive, versus not including the end points? Well, the integral doesn't really
depend on the end points being in or not, you're going to get the same answer,
so this is one-half. In the calculus sense, we can also ask what's the probability
that X is equal to one-half? Well, if we integrate from one-half
to one-half, we just going to get a 0 [COUGH] because there are an infinite
number of possible outcomes. The probability of taking any
particular 1 is going to be 0. Some key rules for
probability density functions. It is going to be always true
that the integral from minus infinity to infinity of
f(x)dx has to add up to 1, the probability 1 something happens. Also has to be true that densities are
non-negative for all possible values of X. We can write the expected value for a continuous random variable as
the integral of X times f(x) dx so this is analogous to the sum that
we have for a discrete variable. In general,
the expected value of some function of x, g(x), we can write as
the interval of g(x)f(x)dx. If we take the expectation of some
constant, times a random variable x. That's just the constant times
the expected value of X, we can pull the constant out. Similarly, if we're looking at
the expectation of a sum of two random variables,
that's the sum of the two expectations. Finally, if we have independence,
if X is independent of Y, that symbol means independent Then the expected value of X times Y is just the product of the expected values.