Let's look at another simple example. This one just has two possible outcomes. In this way we can compare
both the frequentist and the Bayesian approaches to inference. Suppose your brother has a coin
which you know to be loaded so that it comes up heads 70% of the time. He then comes to you with some coin, you're not sure which one and
he wants to make a bet with you. Betting money that it's
going to come up heads. You're not sure of it's the loaded coin or
if it's just a fair one. So he gives you a chance to flip it
five times and just check it out. You flip it five times and
you get two heads and three tails. And now you have a decision point to make. Which coin do you think it is and
how sure are you about that? So in order to perform inference,
we need to define a likelihood. In this case, we'll start by defining
the unknown parameter theta, and this is either that the coin
is fair or it's a loaded coin. So our unknown parameters,
which coin it is. Is it a fair coin or
is it the loaded coin? Our data are going to be
5 flips of this coin, so the binomial of 5 flips and
the question is what's that probability? So we can write a likelihood, f of x
given theta, as if it's the fair coin, this'll be 5 choose x,
one-half to the fifth. If it's the loaded coin,
this will be 5 choose x, 0.7 to the x, 0.3 to the 5 minus x. We can rearrange this also to
write it with indicator functions. Now in this case, we've observed x=2,
what's our likelihood? We can write f of theta given x=2. In this case I'm just going to
switch back to using f instead of using a capital L for likelihood. When the frequent is paradigm,
we're typically using capital Ls. As we move over to the Bayesian paradigm,
we're just going to use fs everywhere. It's really just a notational thing. We can now plug in x=2 and we get 0.3125 if theta equals fair, and 0.1323 if theta equals loaded. So we see that having observed two heads,
the likelihood is higher for theta equals fair than for
theta equals loaded. Thus, we can say the maximum
likelihood estimate, theta hat, is that this is a fair coin. That's our maximum likelihood estimate. Given the data,
it is most likely that the coin is fair. That's a good point estimate, but
then, how do we answer the question, how sure are you? This is not a question that's easily
answered in the frequentist paradigm. Another question is that we might
like to know what is the probability that theta equals fair,
given, we observed two heads. In the frequentist paradigm,
the coin is a physical quantity. It's a fixed coin, and therefore it has
a fixed probability of coming up heads. It either is the fair coin,
or it's the loaded coin. In this case,
the probability is fair given x=2 is just a probability that's
fair because it's a fixed coin. And this probability is either 0 or 1. This is not a particularly
satisfying answer. As a result, Letâ€™s take a look at how
we solve this problem under Bayesian inference and
see what type of answer we get there.