Choosing a uniform prior depends upon
the particular parameterization. For example,
thinking about normal distribution, Suppose, I used a prior which is uniform
on the log scale for sigma squared, so F of sigma squared is proportional
to one over sigma squared. Suppose somebody else decides,
they just want to put a uniform prior on sigma squared itself,
F of sigma squared is proportional to one. These are both uniform on certain scales,
or certain parameterizations, but they are different priors. And so when we compute the posteriors,
we will get different posteriors. The key thing is that uniform priors
are not invariant with respect to transformation. Depending upon how you
parameterize the problem, you can get different answers
by using a uniform prior. One attempt to round to
this is the Jeffreys prior. Jeffreys prior is to find this prior
proportional to the square root of the fisher information. In most cases,
this will be an improper prior. For the example of normal data. The Jeffreys prior is exactly
the prior we have seen before. It's uniform for mu, and then for signal
squared it's uniformed on the log scale. This prior will then be invariant
transformation will be putting the same information into the prior. Even if we use a different
parameterization for the normal. In the example of Bernoulli or
binomial, the Jeffreys prior turns out to be theta to the minus one-half,
one minus theta, to the minus one-half, which is a beta distribution with
parameters one-half and one-half. This is a rare example where the Jeffreys
prior turns out to be a proper prior. You'll note that this prior actually
does have some information in it. It's equivalent to an effective
sample size of one data point. However this information
will then be the same, not depending on
the prioritization we use. This case we have theta as a probability. But another alternative,
used in probabilities calculations, sometimes we model things
on a logistics scale. And in that case,
we can transfer everything and using the Jeffreys prior,
we'll maintain the exact same information. Other possible approaches to
objective basing inference include priors such as reference
priors and maximum entropy priors. Finally, I'd like to mention a related
concept which is empirical basing analysis. The idea in empirical base is that you
use the data to help inform your prior, such as using the mean of the data to
set the mean of the prior distribution. This approach often leads to reasonable
point estimates in your posterior. However, it's sort of cheating,
because you're using the data twice. And as a result, it may lead to
improper uncertainty estimates.