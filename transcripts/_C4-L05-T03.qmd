We now talk about how to obtain maximum likelihood
estimation and Bayesian inference using the conditional
likelihood function in this case of the AR2 process. In this particular
example is the a AR2. Here I'm first again, just sampling from a
specific AR2 process so that we have the data here. I'm using an AR2 with one pair of complex
reciprocal roots. The modulus of the
reciprocal root is 0.95 and the period is 12, so we're going to observe this quasi-periodicity in
the sampled time series. Then these are 300 observations, the standard deviation of
the innovation is one, and we are using the
arima.sim function as before. As you recall, in the case of the
conditional likelihood we have a correspondence between the conditional
likelihood and the equations of the
linear regression model. Here I'm just applying
the equations we discussed before and we can write everything in terms of a regression model
in vector form. So we have a response y and X design matrix and
then we obtain, just reading in, so I'm looking at as a conditional likelihood
so I'm going to condition on the
first P observations. P in this particular
case is two and I have my X matrix and my maximum likelihood
estimator for Phi is just obtained using
the equation X transpose X_inverse times X transpose y. This is my MLE for
the AR coefficients, and then I have the unbiased
estimator for v which we called S2 here using again, the equations that
we discussed before. If you look at the estimates, again the AR coefficients, the true values of
the AR coefficients, we can compare with the estimated AR coefficients
so the true values are here in the Phi
and the true value is for the AR coefficient
in the first lag. The Phi 1 is about 1.65
which is what we obtain also in the maximum
likelihood estimate using the conditional
likelihood, and then the second coefficient is around negative 0.9 which is again close to what
we get in terms of the estimate of the maximum
likelihood estimator. s square is an estimate for v and here the true
variance is one. Again this is based
estimator using all these is based on these
data that we simulated. Now we can just run in
the case of the AR2 we can obtain
posterior inference using these
conditional likelihood and the reference prior, and in that case we can do everything using
direct sampling. I'm going to use the
library MASS here just to use the functions to sample random variables
from that library. If you recall the form of the posterior distribution is
such that the marginal for the variance given the data is just an inverse Gamma
distribution so we can sample from
this inverse Gamma using the function rgamma and then taking the
inverse of that with the corresponding parameters
that we have discussed again, the equations before. In the step 2, conditional on v, we can sample Phi from a normal distribution
in this case is a multivariate normal bivariate in this case since
we have an AR2, and here I'm just
simply obtaining samples from that distribution. I set the number of
samples to be 1,000. You can choose as many or as few posterior
samples as you want. Then based on these
we can look at posterior estimates in terms of graphical representations of
those like we can look at these histograms of the samples
for Phi and the variance. Here we can see if we zoom in here I have three histograms, the first one is the
histogram that corresponds to the samples of the first
AR coefficient Phi_1, the red line corresponds
to the true value and this is just the posterior
distribution for Phi_1. Similarly this histogram
gives me representation of the posterior distribution
for Phi_2 based on those 1,000 samples from
the posterior distribution. Then I have also the samples for the v and then the true
value which is one. We can see that we're getting those estimates
and we can then look also at graphical
representations for functions of
those parameters. For example, we could also summarize the posterior
distribution for the modulus and the period of the reciprocal roots of the
characteristic polynomial. So in this case,
the good thing with simulation-based
posterior sampling is that if we have the samples of the AR
coefficients and the variance, we can simply just do
any transformations for any functions of
those and obtain the posterior summaries of
those transform values. In this case, the modulus, we can obtain it using
a transformation of the Phi_2 coefficients
so if we have samples of those we can
just look at those, and then for the period we can also do the same
so this is a function now of the modulus and this first coefficient so we
just look at that and then can obtain just
the summaries of those histograms for the posterior distribution
of the modulus. We can see here
the true value was 0.95 and this is
a histogram with the posterior samples for that modulus based on those 1,000 samples
from the posterior, and this is what I get in terms of the histogram for the period. The true value for
the period was 12 in this particular example. This gives you also
uncertainty quantification for those parameters
of the distribution. Another thing we discussed is we can fix the model
order and then obtain posterior inference
but in practice we usually also have uncertainty on the model order and we said, well, we can use
different procedures to try to look at that problem. One of the methods
that we can use is just pick the model order using some model
selection criteria. This code just looks at how
you can use AIC and BIC for finding the optimal
model order and then conditional on that
optimal model order then you can proceed with
your posterior inference. In this case, again, this is for the
simulated dataset, I'm going to assume
that I don't know the model order and we're going to set the maximum
model order to be 10. I'm going to be searching for
the optimal model order in AR models that have a maximum model order of 10 and then we will
see what happens here. For each of those
model orders I can just keep all the matrices, the X matrix and the
Y matrix just to proceed with the maximum
likelihood inference, and the maximum
likelihood inference is just using I just created
this function here to compute the MLE and it returns essentially all the
quantities that we need to have here, the degrees of freedom,
the residuals, the MLE, and so on for a given
model order with a given structure in terms
of the X which depends on the data that you
have and the Y that also depends on the
data that you have. There is this AIC BIC function
that simply uses calls this MLE function and then computes the AIC and the BIC
with different penalties. As we said, if you look a little bit inside this
function you will see that the AIC has a penalty in terms of the number of
parameters in the model which is 2 times P and in this case we have
a log N times P, so this is just how
to compute this AIC. So it's just again
computing those. Based on this information
that we have we can plot the AIC and the BIC for different values of
the model orders and highlight in this case we're
looking at the minimizes, so is the difference
between the value and the minimum
value of the AIC. In this case, we want to look at the value that
minimizes this quantity and we see that the
model order for both the AIC and the BIC, the optimal model order
happens to be two. Once we have that
optimal model order we can then say, well, I can choose the model
order that minimizes one of the two criteria and then in
this case I'm choosing BIC. For this particular example, both model selection
criteria are giving me the same answer in terms
of the model order, this doesn't have
to be the case for all the datasets so you will have different answers in terms of what AIC is telling you is the optimal model order and what BIC is telling you that is
the optimal model order. In this example, they
gives you the same answer. Then you can proceed
with the posterior mean, the maximum likelihood
estimation, the reference and
the posterior mean, and the posterior mean of
the standard deviation, and then you can look
at estimates also based on those posterior
means of the AR coefficients, you can obtain the estimate
for the modulus and the period of the
reciprocal roots, so I'm just here using
that optimal model order looking at estimates of those based on the
posterior distribution. Here again, you can just pick one of the two model selection
criteria to obtain your optimal model order and then do your inference
conditional on that.