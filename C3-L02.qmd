---
title : 'Maximum Likelihood Estimation of Mixture Models'
subtitle : 'Bayesian Statistics: Mixture Models'
categories:
  - Bayesian Statistics
keywords:
  - Mixture Models
  - Maximum Likelihood Estimation
  - notes
---

Maximum likelihood estimation is the most common approach to estimate the parameters of statistical models. However, attempting to obtain maximum likelihood estimates (MLEs) $\hat{\omega}$ and $\hat{\theta}$ by directly maximizing the observed-data likelihood

$$
\mathcal{L}(\omega,\theta) = \arg \max_{\omega,\theta} 
\prod_{i=1}^{n} \sum_{k=1}^{K} \omega_k g_k(x_i|\theta_k)
$$

is not feasible, as it is a non-convex optimization problem. 

Using numerical optimization methods, such as the Newton-Raphson algorithm, can be challenging due when there are many components in the mixture.

It worthwhile mentioning that MLE is more of a frequentist approach, as it provides point estimates of the parameters rather than a distributional view. In contrast, Bayesian methods we will consider later provide a full posterior distribution of the parameters, which is more informative and allows for uncertainty quantification.

## EM algorithms for Mixture Models

EM algorism comes up a lot in NLP and other fields so it is worthwhile to understand it the way we will do so in the course.

It also important that the EM algorithm we use for mixture models is from the 1970s and is not the same as the general EM algorithm.

The EM algorithm is iterative and consists of two steps: the E-step and the M-step. The E-step computes the expected value of the complete-data log-likelihood given the observed data and the current parameter estimates, while the M-step maximizes this expected log-likelihood with respect to the parameters. However before we start these steps we need to se initial values for the parameters.

E step: Set

$$
Q(\omega,\theta \mid \omega^{(t)}, \theta^{(t)},x) = E_{c \mid \omega^{(t)},\theta^{(t)}, x} \left[ \log p(x,c \mid \omega,\theta) \right]
$$ {#eq-q-function}


Where $c$ is the latent variable indicating the component from which each observation was generated, $\omega$ are the weights, and $\theta$ are the parameters of the Gaussian components (means and standard deviations).


M step: Set

$$
\hat{\omega}^{(t+1)},\hat{\theta}^{(t+1)} = \arg \max_{\omega,\theta} Q(\omega,\theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)},y)
$$ {#eq-mstep}

where $\hat{\omega}^{(t)}$ and $\hat{\theta}^{(t)}$ are the current estimates of the parameters, and $y$ is the observed data.

These two steps are repeated until convergence, which is typically defined as the change in the full-data log-likelihood $Q$ function being below a certain threshold.

A key point is that if we condition each component independently on the $\omega, \theta, x$ we can write

$$
p(c_i=k \mid \omega, \theta, x_i) = \frac{\omega_k g_k(x_i \mid \theta_k)}{\sum_{j=1}^{K} \omega_j g_j(x_i \mid \theta_j)}= v_{ik}(\omega, \theta)
$$

where the value of $v_{ik}$ is interpreted as the probability that the $i$-th observation comes from the $k$-th component of the mixture assuming the population parameters $\omega$ and $\theta$.

$$

## EM for general Mixture 


## EM for location Mixture of Gaussians


## EM example 1


## Sample code for EM example 1


## EM example 2


## Sample code for EM example 2


## Mixture of Log Gaussians


## HW: The EM for ZIP mixtures

Data on the lifetime (in years) of fuses produced by the ACME Corporation is available in the file fuses.csv:



Provide the EM algorithm to fit the mixture model



## HW+: The EM for Mixture Models




