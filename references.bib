@article{Wiesenfarth2020Quantification,
author = {Wiesenfarth, Manuel and Calderazzo, Silvia},
title = {Quantification of prior impact in terms of effective current sample size},
journal = {Biometrics},
volume = {76},
number = {1},
pages = {326-336},
keywords = {Bayesian adaptive clinical trial design, prior-data conflict, prior effective sample size, prior elicitation, prior information, robust priors},
doi = {https://doi.org/10.1111/biom.13124},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13124},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13124},
abstract = {Abstract Bayesian methods allow borrowing of historical information through prior distributions. The concept of prior effective sample size (prior ESS) facilitates quantification and communication of such prior information by equating it to a sample size. Prior information can arise from historical observations; thus, the traditional approach identifies the ESS with such a historical sample size. However, this measure is independent of newly observed data, and thus would not capture an actual “loss of information” induced by the prior in case of prior-data conflict. We build on a recent work to relate prior impact to the number of (virtual) samples from the current data model and introduce the effective current sample size (ECSS) of a prior, tailored to the application in Bayesian clinical trial designs. Special emphasis is put on robust mixture, power, and commensurate priors. We apply the approach to an adaptive design in which the number of recruited patients is adjusted depending on the effective sample size at an interim analysis. We argue that the ECSS is the appropriate measure in this case, as the aim is to save current (as opposed to historical) patients from recruitment. Furthermore, the ECSS can help overcome lack of consensus in the ESS assessment of mixture priors and can, more broadly, provide further insights into the impact of priors. An R package accompanies the paper.},
year = {2020}
}

@book{deMoivre1718doctrine,
  title = {The Doctrine of Chances},
  author = {Abraham De Moivre},
  year = 1718,
  publisher = {H. Woodfall.},
  url = {https://tellingstorieswithdata.com},
}

@book{alexander2022telling,
  title = {Telling Stories with Data},
  author = {Rohan Alexander},
  year = 2022,
  publisher = {CRC Press},
  url = {https://tellingstorieswithdata.com},
}
@incollection{amarel1981representations,
  title = {On Representations of Problems of Reasoning about Actions},
  author = {Saul Amarel},
  year = 1981,
  booktitle = {Readings in Artificial Intelligence},
  publisher = {Morgan Kaufmann},
  pages = {2--22},
  doi = {https://doi.org/10.1016/B978-0-934613-03-3.50006-4},
  isbn = {978-0-934613-03-3},
  url = {https://www.sciencedirect.com/science/article/pii/B9780934613033500064},
  editor = {Bonnie Lynn Webber and Nils J. Nilsson},
  abstract = {Publisher Summary This chapter discusses the basic issues of choice of representation for problems of reasoning about actions. The general problem of representation is concerned with the relationship between different ways of formulating a problem to a problem solving system and the efficiency with which the system can be expected to find a solution to the problem. An understanding of the relationship between problem formulation and problem solving efficiency is a prerequisite for the design of procedures that can automatically choose the most appropriate representation of a problem—they can find a point of view of the problem that maximally simplifies the process of finding a solution. The chapter discusses a specific problem of transportation scheduling—the missionaries and cannibals problem—to evaluate the effects of alternative formulations of this problem on the expected efficiency of mechanical procedures for solving it and also to examine the processes that come into play when a transition takes place from a given problem formulation into a better one.},
}
@misc{blöbaum2022dowhy,
  title = {DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models},
  author = {Patrick Blöbaum and Peter Götz and Kailash Budhathoki and Atalanti A. Mastakouri and Dominik Janzing},
  year = 2022,
  eprint = {2206.06821},
  archiveprefix = {arXiv},
  primaryclass = {stat.ME},
}
@book{carlin2008bayesian,
  title = {Bayesian Methods for Data Analysis},
  author = {Carlin, B.P. and Louis, T.A.},
  year = 2008,
  publisher = {CRC Press},
  series = {Chapman \& Hall/CRC Texts in Statistical Science},
  isbn = 9781584886983,
  url = {https://books.google.co.il/books?id=GTJUt8fcFx8C},
}
@misc{chadha2020distilled,
  title = {Distilled Notes for the Natural Language Processing Specialization on Coursera (offered by deeplearning.ai)},
  author = {Chadha, Aman},
  year = 2020,
  url = {www.aman.ai},
  note = {Accessed: 2023-03-21},
  howpublished = {\url{https://www.aman.ai}},
}
@book{chambers2008software,
  title = {Software for data analysis programming with R},
  author = {Chambers, John M.},
  year = 2008,
  publisher = {Springer},
  address = {New York; London},
  isbn = {0387759360 9780387759364},
  url = {http://www.amazon.de/Software-Data-Analysis-Programming-Statistics/dp/0387759352},
  added-at = {2013-09-01T13:14:37.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/281e36af33c66c45a7e1db48cc910f0ad/vivion},
  description = {Software for Data Analysis: Programming with R (Statistics and Computing): Amazon.de: John Chambers: Englische Bücher},
  interhash = {e1dde4b9a5a216be4f75516a7519ba1b},
  intrahash = {81e36af33c66c45a7e1db48cc910f0ad},
  keywords = {data programming r statistics},
  refid = 436978847,
  timestamp = {2013-09-01T13:14:37.000+0200},
}
@misc{chen1996empirical,
  title = {An Empirical Study of Smoothing Techniques for Language Modeling},
  author = {Stanley F. Chen and Joshua T. Goodman},
  year = 1996,
  url = {https://arxiv.org/abs/cmp-lg/9606011},
  eprint = {cmp-lg/9606011},
  archiveprefix = {arXiv},
  primaryclass = {cmp-lg},
  abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
}
@book{davidson1996principles,
  title = {Principles of Statistical Data Handling},
  author = {Davidson, Fred},
  year = 1996,
  doi = {10.4135/9781483348902},
  url = {https://methods.sagepub.com/book/principles-of-statistical-data-handling},
  city = {Thousand Oaks, California},
}
@book{de_bono2002lateral,
  title = {Lateral thinking : a textbook of creativity},
  author = {De Bono, Edward},
  year = 2002,
  publisher = {Penguin Books},
  keywords = {algorithms, logic},
  language = English,
}
@book{dromey1982how,
  title = {How to Solve It by Computer},
  author = {Dromey, R. G.},
  year = 1982,
  publisher = {Prentice-Hall, Inc.},
  address = {USA},
  isbn = {0134340019},
  keywords = {algorithms, math, logic},
}
@article{duchi2011adaptive,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author = {Duchi, John and Hazan, Elad},
  year = 2011,
  month = {07},
  journal = {Journal of Machine Learning Research},
  volume = 12,
  pages = {2121--2159},
  url = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
}
@book{galef2021scout,
  title = {The Scout Mindset: Why Some People See Things Clearly and Others Don't},
  author = {Galef, J.},
  year = 2021,
  publisher = {Penguin Publishing Group},
  isbn = 9780735217553,
  url = {https://books.google.co.il/books?id=wJ0jEAAAQBAJ},
  lccn = 2020024770,
}
@book{gelman2013bayesian,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John and Stern, Hal and Dunson, David and Vehtari, Aki and Rubin, Donald},
  year = 2013,
  month = 11,
  pages = {},
  doi = {10.1201/b16018},
  isbn = 9780429113079,
}
@article{good1953population,
  title = "The population frequencies of species and the estimation of population parameters",
  author = {Good, I. J.},
  year = 1953,
  month = 12,
  journal = {Biometrika},
  volume = 40,
  number = {3-4},
  pages = {237--264},
  doi = {10.1093/biomet/40.3-4.237},
  issn = {0006-3444},
  url = {https://doi.org/10.1093/biomet/40.3-4.237},
  abstract = "A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r= 1, 2, 3, ...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as ‘the frequency of the frequency r’.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's ‘characteristic’ and Shannon's ‘entropy’. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.",
  eprint = {https://academic.oup.com/biomet/article-pdf/40/3-4/237/492571/40-3-4-237.pdf},
}
@misc{hinton2012improving,
  title = {Improving neural networks by preventing co-adaptation of feature detectors},
  author = {Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
  year = 2012,
  doi = {https://doi.org/10.48550/arXiv.1207.0580},
  url = {https://arxiv.org/pdf/1207.0580},
  eprint = {1207.0580},
  archiveprefix = {arXiv},
  primaryclass = {cs.NE},
}
@article{hopfield1982neural,
  title = {Neural networks and physical systems with emergent collective computational abilities},
  author = {Hopfield, J. J.},
  year = 1982,
  month = apr,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = 79,
  number = 8,
  pages = {2554--2558},
  issn = {0027-8424},
  url = {http://view.ncbi.nlm.nih.gov/pubmed/6953413]},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  added-at = {2011-06-02T00:22:00.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2a3074d3b833b6b02b6fc991407e86804/mhwombat},
  citeulike-article-id = 8137707,
  citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/6953413]},
  citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=6953413]},
  file = {:neural_nets/Hopfield_1982__pnas00447-0135.pdf:PDF},
  groups = {public},
  interhash = {cf60d9bad127184617e0ad10ae86b78b},
  intrahash = {a3074d3b833b6b02b6fc991407e86804},
  keywords = {network neural, seminal},
  pmid = {6953413]},
  posted-at = {2010-10-28 14:55:59},
  priority = 2,
  timestamp = {2016-07-12T19:25:30.000+0200},
  username = {mhwombat},
}
@article{jacobs1991adaptive,
  title = {Adaptive Mixtures of Local Experts},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year = 1991,
  journal = {Neural Computation},
  volume = 3,
  number = 1,
  pages = {79--87},
  doi = {10.1162/neco.1991.3.1.79},
  url = {http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf},
}
@misc{jelliti2020nlp,
  title = {NLP Specialization Courses Notes (offered by deeplearning.ai)},
  author = {Jelliti, Ibrahim},
  year = 2020,
  journal = {GitHub repository},
  publisher = {GitHub},
  url = {https://github.com/ibrahimjelliti/Deeplearning.ai-Natural-Language-Processing-Specialization},
  note = {Accessed: 2023-03-21},
  howpublished = {\url{https://github.com/ibrahimjelliti/Deeplearning.ai-Natural-Language-Processing-Specialization}},
  commit = {403196f6dc8808020b8d826890fb0ee554e34a4f},
}
@book{jurafsky2000speech,
  title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author = {Jurafsky, D. and Martin, J.H.},
  year = 2000,
  publisher = {Prentice Hall},
  series = {Prentice Hall series in artificial intelligence},
  isbn = 9780131227989,
  url = {https://books.google.co.il/books?id=85BvQgAACAAJ},
  lccn = 99087845,
}

@book{kerns2018introduction,
  title = {Introduction to Probability and Statistics Using R},
  author = {G. Jay Kerns},
  year = 2018,
  file = {:IPSUR.pdf:PDF},
  groups = {Ecologia Numérica, Modelação Ecológica},
}

@book{kruschke2011doing,
  title = {Doing Bayesian data analysis: a tutorial with {R} and {BUGS}},
  author = {Kruschke, John K.},
  year = 2011,
  publisher = {Academic Press},
  address = {Burlington, MA},
  isbn = {9780123814852 0123814855},
  url = {http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855},
  abstract = {There is an explosion of interest in {Bayesian statistics}, primarily because recently created computational methods have finally made Bayesian analysis tractable and accessible to a wide audience. Doing Bayesian Data Analysis, A Tutorial Introduction with R and BUGS, is for first year graduate students or advanced undergraduates and provides an accessible approach, as all mathematics is explained intuitively and with concrete examples. It assumes only algebra and a rustya calculus. Unlike other textbooks, this book begins with the basics, including essential concepts of probability and random sampling. The book gradually climbs all the way to advanced hierarchical modeling methods for realistic data. The text provides complete examples with the R programming language and BUGS software (both freeware), and begins with basic programming examples, working up gradually to complete programs for complex analyses and presentation graphics. These templates can be easily adapted for a large variety of students and their own research needs. The textbook bridges the students from their undergraduate training into modern Bayesian methods.},
  added-at = {2016-05-11T11:06:36.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2f7502614d5262ce0f764de992271e896/thoni},
  description = {Amazon.com: Doing Bayesian Data Analysis: A Tutorial with R and BUGS (8601300089751): John K. Kruschke: Books},
  interhash = {92085c933548da90543a89cac3ca2f76},
  intrahash = {f7502614d5262ce0f764de992271e896},
  keywords = {analysis bayesian data},
  refid = 653121532,
  timestamp = {2016-09-06T08:23:07.000+0200},
}

@inproceedings{kusner2017counterfactual,
  title = {Counterfactual Fairness},
  author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year = 2017,
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume = 30,
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
}

@book{mackay2003information,
  title = {Information Theory, Inference, and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = 2003,
  publisher = {Copyright Cambridge University Press},
  added-at = {2007-05-24T14:43:04.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/24c23fea472f6e75c0964badd83883d77/tmalsburg},
  interhash = {86f621d9d6f9f159448f768d792d4511},
  intrahash = {4c23fea472f6e75c0964badd83883d77},
  keywords = {bayesianinference book informationtheory neuralnetworks patternrecognition probabilitytheory},
  timestamp = {2007-05-24T14:43:04.000+0200},
}

@article{morita2008determining,
  title={Determining the effective sample size of a parametric prior},
  author={Morita, Satoshi and Thall, Peter F and M{\"u}ller, Peter},
  journal={Biometrics},
  volume={64},
  number={2},
  pages={595--602},
  year={2008},
  publisher={Wiley Online Library}
}

@inproceedings{mothilal2020explaining,
  title = {Explaining machine learning classifiers through diverse counterfactual explanations},
  author = {Ramaravind K. Mothilal and Amit Sharma and Chenhao Tan},
  year = 2020,
  month = jan,
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  publisher = {ACM},
  doi = {10.1145/3351095.3372850},
  url = {https://doi.org/10.1145%2F3351095.3372850},
}
@inbook{n2003raven,
  title = "Raven Progressive Matrices",
  author = "John and Raven, Jean",
  year = 2003,
  booktitle = "Handbook of Nonverbal Assessment",
  publisher = "Springer US",
  address = "Boston, MA",
  pages = "223--237",
  doi = "10.1007/978-1-4615-0153-4_11",
  isbn = "978-1-4615-0153-4",
  url = "https://doi.org/10.1007/978-1-4615-0153-4_11",
  editor = "McCallum, R. Steve",
  abstract = "The Raven Progressive Matrices (RPM) tests measure ``general cognitive ability'' or, better, eductive, or ``meaning making,'' ability (Raven, Raven, {\&} Court, 1998a,2000). The term ``eductive'' comes from the Latin root educere, which means, ``to draw out.'' The basic version of the test, known as the Standard Progressive Matrices (or SPM), consists of five sets of items of the kind shown in Figures 11.1 and 11.2. Within each set, the items become progressively more difficult. At the beginning of each set, the items, although easy again, follow a different logic. The sets in turn become progressively more difficult. The five sets offer those taking the test five opportunities to become familiar with the method of thought required to solve the problems. In addition to the Standard series, there is the Coloured Progressive Matrices (CPM), which is designed to spread the scores of children and less able adults and the Advanced Progressive Matrices (APM), developed to spread the scores of the top 20{\%} of the population.",
}
@book{navas2013triz,
  title = {TRIZ: Design {Problem Solving} with Systematic Innovation},
  author = {Helena V. G. Navas},
  year = 2013,
  booktitle = {Advances in Industrial Design Engineering},
  publisher = {IntechOpen},
  address = {Rijeka},
  doi = {10.5772/55979},
  url = {https://doi.org/10.5772/55979},
  abstract = {The Scout Mindset challenges readers to move beyond gut reactions and preconceptions and rethink problems. The book offers instructions for overcoming bias and central beliefs to gather more objective data. Julia Galef encourages readers to act more like scouts than soldiers and gather information without judging to make more informed decisions. The text outlines the common reasons folks jump to conclusions and offers advice on how to avoid incorrect assumptions and conduct level-headed analyses. The Scout Mindset is a call to action for objectivity and an instruction manual for breaking away from unhelpful mental patterns that can lead to poor choices.},
  keywords = {problem solving},
  editor = {Denis A. Coelho},
  chapter = 4,
}
@article{ney1994structuring,
  title = {On structuring probabilistic dependences in stochastic language modelling},
  author = {Hermann Ney and Ute Essen and Reinhard Kneser},
  year = 1994,
  journal = {Computer Speech & Language},
  volume = 8,
  number = 1,
  pages = {1--38},
  doi = {https://doi.org/10.1006/csla.1994.1001},
  issn = {0885-2308},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230884710011},
  abstract = {In this paper, we study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a Germany database and an English database.},
}
@inproceedings{ng2001discriminative,
  title = {On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes},
  author = {Ng, Andrew and Jordan, Michael},
  year = 2001,
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {MIT Press},
  volume = 14,
  pages = {},
  url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf},
  editor = {T. Dietterich and S. Becker and Z. Ghahramani},
}
@book{polya2014how,
  title = {How to Solve It: A New Aspect of Mathematical Method},
  author = {Polya, G. and Conway, J.H.},
  year = 2014,
  publisher = {Princeton University Press},
  series = {Princeton Science Library},
  pages = 288,
  isbn = 9780691164076,
  url = {https://books.google.co.il/books?id=Zu2hEAAAQBAJ},
  lccn = 2014941268,
  howpublished = {Paperback},
  keywords = {algorithms, math, logic, geometry},
  biburl = {https://www.bibsonomy.org/bibtex/2237322f30081bafcc21a4dfb67d46d94/chato},
  abstract = {A perennial bestseller by eminent mathematician G. Polya, <I>How to Solve It</I> will show anyone in any field how to think straight.<P>In lucid and appealing prose, Polya reveals how the mathematical method of demonstrating a proof or finding an unknown can be of help in attacking any problem that can be "reasoned" out--from building a bridge to winning a game of anagrams. Generations of readers have relished Polya's deft--indeed, brilliant--instructions on stripping away irrelevancies and going straight to the heart of the problem.<P>},
}
@book{rizzo2019statistical,
  title = {Statistical computing with R Maria L. Rizzo.},
  author = {Rizzo, Maria L},
  year = 2019,
  booktitle = {Statistical computing with R},
  publisher = {CRC Press, Taylor & Francis Group},
  address = {Boca Raton},
  series = {Chapman & Hall/CRC The R Series},
  isbn = 9780429192760,
  abstract = {Computational statistics and statistical computing are two areas that employ computational, graphical, and numerical approaches to solve statistical problems, making the versatile R language an ideal computing environment for these fields. This second edition continues to encompass the traditional core material of computational statistics, with an},
  edition = {Second edition.},
  language = {eng},
  keywords = {Mathematical statistics -- Data processing; Statistics -- Data processing; R (Computer program language)},
}
@article{russell2019efficient,
  title = {Efficient Search for Diverse Coherent Explanations},
  author = {Chris Russell},
  year = 2019,
  journal = {CoRR},
  volume = {abs/1901.04909},
  url = {http://arxiv.org/abs/1901.04909},
  eprinttype = {arXiv},
  eprint = {1901.04909},
  timestamp = {Mon, 04 Feb 2019 09:14:09 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-1901-04909.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}
@misc{sharma2020dowhy,
  title = {DoWhy: An End-to-End Library for Causal Inference},
  author = {Amit Sharma and Emre Kiciman},
  year = 2020,
  eprint = {2011.04216},
  archiveprefix = {arXiv},
  primaryclass = {stat.ME},
}
@manual{team2021r,
  title = {R: A Language and Environment for Statistical Computing},
  author = {R Core Team},
  year = 2021,
  address = {Vienna, Austria},
  url = {https://www.R-project.org/},
  organization = {R Foundation for Statistical Computing},
}
@misc{wachter2018counterfactual,
  title = {Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR},
  author = {Sandra Wachter and Brent Mittelstadt and Chris Russell},
  year = 2018,
  abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
  eprint = {1711.00399},
  archiveprefix = {arXiv},
  primaryclass = {cs.AI},
}
@book{walpole2007probability,
  title = {Probability \& statistics for engineers and scientists},
  author = {Walpole, Ronald E. and Myers, Raymond H. and Myers, Sharon L. and Ye, Keying},
  year = 2007,
  publisher = {Pearson Education},
  address = {Upper Saddle River},
  added-at = {2011-04-12T12:51:01.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/219a59df44d8aa5a63e6844e0c066cb21/arnsholt},
  edition = {8th},
  interhash = {e1efaca49b5c9c0d8e89a50bccb98901},
  intrahash = {19a59df44d8aa5a63e6844e0c066cb21},
  keywords = {statistics},
  timestamp = {2011-04-29T13:06:24.000+0200},
}
@book{wickham2009ggplot2,
  title = {ggplot2: Elegant Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = 2009,
  publisher = {Springer},
  series = {UseR!},
  doi = {10.1007/978-0-387-98141-3},
  isbn = {978-0-387-98140-6},
  url = {http://ggplot2.org/book/},
  abstract = {Teaches how to create graphics in R using ggplot Discusses the theoretical framework that underlies ggplot. This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkison's Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, it's easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page. Hadley Wickham is an Assistant Professor of Statistics at Rice University, and is interested in developing computational and cognitive tools for making data preparation, visualization, and analysis easier. He has developed 15 R packages and in 2006 he won the John Chambers Award for Statistical Computing for his work on the ggplot and reshape R packages.},
  added-at = {2018-06-18T21:23:34.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/28e95c4e5f32a9fe74e4ce23ca7ca9fd6/pbett},
  citeulike-article-id = 5445806,
  citeulike-attachment-1 = {HadleyWickham2009_ggplot2_book.pdf; /pdf/user/pbett/article/5445806/958182/HadleyWickham2009_ggplot2_book.pdf; d5e08302e67648b3cdbaf619362aa5ff29eb5018},
  citeulike-linkout-0 = {http://ggplot2.org/book/},
  citeulike-linkout-1 = {http://dx.doi.org/10.1007/978-0-387-98141-3},
  citeulike-linkout-2 = {http://www.worldcat.org/isbn/9780387981406},
  citeulike-linkout-3 = {http://books.google.com/books?vid=ISBN9780387981406},
  citeulike-linkout-4 = {http://www.amazon.com/gp/search?keywords=9780387981406&index=books&linkCode=qs},
  citeulike-linkout-5 = {http://www.librarything.com/isbn/9780387981406},
  citeulike-linkout-6 = {http://www.worldcat.org/oclc/416289643},
  comment = {(private-note)Freely available from Springer web site.},
  file = {HadleyWickham2009_ggplot2_book.pdf},
  interhash = {207c42e56f05318e953a3fcaec50ea15},
  intrahash = {8e95c4e5f32a9fe74e4ce23ca7ca9fd6},
  keywords = {visualisation textbook rpackage},
  posted-at = {2014-04-02 19:42:54},
  priority = 2,
  timestamp = {2018-06-22T18:34:20.000+0200},
}
@book{winston1992artificial,
  title = {Artificial Intelligence},
  author = {Winston, Patrick Henry},
  year = 1992,
  publisher = {Addison-Wesley},
  address = {Reading, MA},
  isbn = {978-0-201-53377-4},
  abstract = {This book is one of the oldest and most popular introductions to artificial intelligence. An accomplished artificial intelligence (AI) scientist, Winston heads MIT's Artificial Intelligence Laboratory, and his hands-on AI research experience lends authority to what he writes. Winston provides detailed pseudo-code for most of the algorithms discussed, so you will be able to implement and test the algorithms immediately. The book contains exercises to test your knowledge of the subject and helpful introductions and summaries to guide you through the material.},
  added-at = {2016-09-23T14:14:26.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/270046e5ce145e71ce16c0f29810cc5d1/flint63},
  description = {Edition 2 1984 978-0-201-08259-3},
  edition = 3,
  file = {Amazon Search inside:http\://www.amazon.de/gp/reader/0201533774/:URL;Google Books:http\://books.google.de/books?isbn=9780201533774:URL},
  groups = {public},
  interhash = {b210c9ab9e96b0116c7e27ce4e7959f3},
  intrahash = {70046e5ce145e71ce16c0f29810cc5d1},
  keywords = {01801 101 book shelf ai general},
  timestamp = {2018-04-16T11:33:09.000+0200},
  username = {flint63},
}
@book{McElreath2020Rethinking,
  author = {McElreath, Richard},
  keywords = {book stats},
  title = {Statistical Rethinking, A Course in R and Stan},
  year = 2015
}
@article{Aldrich2008Fisher,
author = {Aldrich, John},
year = {2008},
month = {03},
pages = {},
title = {R. A. Fisher on Bayes and Bayes' theorem},
volume = {3},
journal = {Bayesian Analysis},
doi = {10.1214/08-BA306}
}
@book{Fisher1925Statistical,
  author = {Fisher, R.A.},
  keywords = {imported},
  publisher = {Edinburgh Oliver \& Boyd},
  title = {Statistical methods for research workers},
  edition = 1,
  year = 1925
}
@book{Fisher1932Statistical,
  author = {Fisher, R.A.},
  keywords = {imported},
  publisher = {Edinburgh Oliver \& Boyd},
  title = {Statistical methods for research workers},
  edition = 4,
  year = 1932
}
@book{Fisher1935Design,
  author = {Fisher, R.A.},
  keywords = {imported},
  publisher = {Edinburgh Oliver \& Boyd},
  title = {The Design of Experiments.},
  edition = 1,
  year = 1935
}
@article{bayes1763InverseProbability,
author = {Bayes, Thomas  and Price, null },
title = {LII. An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S},
journal = {Philosophical Transactions of the Royal Society of London},
volume = {53},
number = {},
pages = {370-418},
year = {1763},
doi = {10.1098/rstl.1763.0053},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstl.1763.0053},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1763.0053},
abstract = { Dear Sir, I Now send you an essay which I have found among the papers of our deceased friend Mr. Bayes, and which, in my opinion, has great merit, and well deserves to be preserved. }
}
@article{10.1214/ss/1177012488,
author = {Sandy Zabell},
title = {{R. A. Fisher on the History of Inverse Probability}},
volume = {4},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {247 -- 256},
keywords = {History of statistics, inverse probability, R. A. Fisher},
year = {1989},
doi = {10.1214/ss/1177012488},
URL = {https://doi.org/10.1214/ss/1177012488}
}
@book{dale1999history,
  title={A History of Inverse Probability: From Thomas Bayes to Karl Pearson, 2nd edition},
  author={Dale, A.I. and Buchwald, J.Z. and Toomer, G.J.},
  isbn={9780387988078},
  lccn={99018596},
  series={Sources and Studies in the History of Mathematics and Physical Sciences},
  url={https://books.google.co.il/books?id=dGweIIXbAgMC},
  year={1999},
  publisher={Springer New York}
}
@book{casella2002statistical,
  title={Statistical Inference},
  author={Casella, G. and Berger, R.L.},
  isbn={9780534243128},
  lccn={20010257},
  series={Duxbury advanced series in statistics and decision sciences},
  url={http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&Roger%20L.Berger--Statistical%20Inference.pdf},
  year={2002},
  publisher={Thomson Learning},
}
@book{hobbs2015bayesian,
 URL = {http://www.jstor.org/stable/j.ctt1dr36kz},
 abstract = {Bayesian modeling has become an indispensable tool for ecological research because it is uniquely suited to deal with complexity in a statistically coherent way. This textbook provides a comprehensive and accessible introduction to the latest Bayesian methods-in language ecologists can understand. Unlike other books on the subject, this one emphasizes the principles behind the computations, giving ecologists a big-picture understanding of how to implement this powerful statistical approach.Bayesian Modelsis an essential primer for non-statisticians. It begins with a definition of probability and develops a step-by-step sequence of connected ideas, including basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and inference from single and multiple models. This unique book places less emphasis on computer coding, favoring instead a concise presentation of the mathematical statistics needed to understand how and why Bayesian analysis works. It also explains how to write out properly formulated hierarchical Bayesian models and use them in computing, research papers, and proposals.This primer enables ecologists to understand the statistical principles behind Bayesian modeling and apply them to research, teaching, policy, and management.Presents the mathematical and statistical foundations of Bayesian modeling in language accessible to non-statisticiansCovers basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and moreDeemphasizes computer coding in favor of basic principlesExplains how to write out properly factored statistical expressions representing Bayesian models},
 author = {N. Thompson Hobbs and Mevin B. Hooten},
 edition = {STU - Student edition},
 publisher = {Princeton University Press},
 title = {Bayesian Models: A Statistical Primer for Ecologists},
 year = {2015}
}

@book{spanos2019probability,
  title={Probability Theory and Statistical Inference},
  author={Spanos, A.},
  isbn={9781107185142},
  lccn={2019016182},
  url={https://books.google.co.il/books?id=9nCiDwAAQBAJ},
  year={2019},
  publisher={Cambridge University Press}
}

@MISC {mo-1302543,
    TITLE = {Why Square Brackets for Expectation},
    AUTHOR = {Autolatry (https://math.stackexchange.com/users/25097/autolatry)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/1302543 (version: 2015-05-28)},
    EPRINT = {https://math.stackexchange.com/q/1302543},
    URL = {https://math.stackexchange.com/q/1302543}
}

@misc{enwiki-functional,
    author = "{Wikipedia contributors}",
    title = "Functional (mathematics) --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    url = "https://en.wikipedia.org/w/index.php?title=Functional_(mathematics)&oldid=1148699341",
    note = "[Online; accessed 18-April-2023]"
}

@book{vanderplas2016python,
  title={Python Data Science Handbook: Essential Tools for Working with Data},
  author={VanderPlas, J.},
  isbn={9781491912133},
  lccn={2017385426},
  url={https://jakevdp.github.io/PythonDataScienceHandbook/},
  year={2016},
  publisher={O'Reilly Media}
}

  
@book{bishop2013pattern,
  title={Pattern Recognition and Machine Learning},
  author={Bishop, C.M.},
  isbn={9780387310732},
  series={Information science and statistics},
  url={https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
  year={2006},
  publisher={Springer (India) Private Limited}
}

@book{jeffreys1983theory,
  title={Theory of Probability},
  author={Jeffreys, H.},
  isbn={9780198531937},
  lccn={99175168},
  series={International series of monographs on physics},
  year={1983},
  publisher={Clarendon Press}
}

@article{Dawid1973MarginalizationParadoxes ,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984907},
 abstract = {We describe a range of routine statistical problems in which marginal posterior distributions derived from improper prior measures are found to have an unBayesian property--one that could not occur if proper prior measures were employed. This paradoxical possibility is shown to have several facets that can be successfully analysed in the framework of a general group structure. The results cast a shadow on the uncritical use of improper prior measures. A separate examination of a particular application of Fraser's structural theory shows that it is intrinsically paradoxical under marginalization.},
 author = {A. P. Dawid and M. Stone and J. V. Zidek},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {189--233},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Marginalization Paradoxes in Bayesian and Structural Inference},
 urldate = {2023-04-25},
 volume = {35},
 year = {1973}
}



@book{pearson1990student,
  title={Student: A Statistical Biography of William Sealy Gosset},
  author={Pearson, E.S. and Gosset, W.S. and Plackett, R.L. and Barnard, G.A.},
  isbn={9780198522270},
  lccn={89072137},
  url={https://books.google.co.il/books?id=LBDvAAAAMAAJ},
  year={1990},
  publisher={Clarendon Press}
}

@misc{poisson2019english,
      title={English Translation of Poisson's "Recherches sur la probabilit\'e des jugements en mati\`ere criminelle et en mati\`ere civile" / "Researches into the Probabilities of Judgements in Criminal and Civil Cases"}, 
      author={S. -D. Poisson},
      year={2019},
      eprint={1902.02782},
      archivePrefix={arXiv},
      primaryClass={math.HO}
}

@book{bernoulli1713ars,
  title={Ars conjectandi [The Art of Conjecturing]},
  author={Bernoulli, J.},
  url={https://books.google.co.il/books?id=Ba5DAAAAcAAJ},
  year={1713},
  publisher={Impensis Thurnisiorum}
}
@book{hoff2009,
	title = {A First Course in Bayesian Statistical Methods},
	author = {Hoff, Peter D.},
	year = {2009},
	date = {2009},
	publisher = {Springer New York},
	doi = {10.1007/978-0-387-92407-6},
	url = {http://dx.doi.org/10.1007/978-0-387-92407-6}
}

@book{polya1945,
	title = {How to Solve It},
	author = {Polya, G.},
	year = {1945},
	month = {12},
	date = {1945-12-31},
	publisher = {Princeton University Press},
	doi = {10.1515/9781400828678},
	url = {http://dx.doi.org/10.1515/9781400828678}
}

@misc{ enwiki:poly_urn,
    author = "{Wikipedia contributors}",
    title = "Pólya urn model --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=P%C3%B3lya_urn_model&oldid=1152130337}",
    note = "[Online; accessed 16-May-2023]"
  }

@article{diaconis1980,
	title = {Finite Exchangeable Sequences},
	author = {Diaconis, P. and Freedman, D.},
	year = {1980},
	month = {08},
	date = {1980-08-01},
	journal = {The Annals of Probability},
	volume = {8},
	number = {4},
	doi = {10.1214/aop/1176994663},
	url = {http://dx.doi.org/10.1214/aop/1176994663}
}

@article{kerns2006,
	title = {Definetti{\textquoteright}s Theorem for Abstract Finite Exchangeable Sequences},
	author = {Kerns, G. Jay. and {Székely}, {Gábor J.}},
	year = {2006},
	month = {08},
	date = {2006-08-18},
	journal = {Journal of Theoretical Probability},
	pages = {589--608},
	volume = {19},
	number = {3},
	doi = {10.1007/s10959-006-0028-z},
	url = {http://dx.doi.org/10.1007/s10959-006-0028-z},
	langid = {en}
}

@inbook{barlow1992,
	title = {Introduction to de Finetti (1937) Foresight: Its Logical Laws, Its Subjective Sources},
	author = {Barlow, R. E.},
	year = {1992},
	date = {1992},
	publisher = {Springer New York},
	pages = {127--133},
	doi = {10.1007/978-1-4612-0919-5_9},
	url = {http://dx.doi.org/10.1007/978-1-4612-0919-5_9}
}

@inbook{definetti1992foresight,
	title = {Foresight: Its Logical Laws, Its Subjective Sources},
	author = {de Finetti, Bruno},
	year = {1992},
	date = {1992},
	publisher = {Springer New York},
	pages = {134--174},
	doi = {10.1007/978-1-4612-0919-5_10},
	url = {http://dx.doi.org/10.1007/978-1-4612-0919-5_10}
}

@article{definetti2017theory,
	title = {Theory of Probability},
	author = {de Finetti, Bruno},
	editor = {{Machí}, Antonio and Smith, Adrian},
	year = {2017},
	month = {01},
	date = {2017-01-20},
	journal = {Wiley Series in Probability and Statistics},
	doi = {10.1002/9781119286387},
	url = {http://dx.doi.org/10.1002/9781119286387}
}

@INCOLLECTION{ramsey1926,
title = {Truth and Probability},
author = {Ramsey, Frank P.},
year = {1926},
chapter = {7},
pages = {156-198},
booktitle = {The Foundations of Mathematics and other Logical Essays},
editor = {Braithwaite, R. B.},
publisher = {McMaster University Archive for the History of Economic Thought},
abstract = {Contains two other essays as well: Further Considerations & Last Papers: Probability and Partial Belief.},
url = {https://EconPapers.repec.org/RePEc:hay:hetcha:ramsey1926}
}

@misc{ enwiki-empirical,
    author = "{Wikipedia contributors}",
    title = "68–95–99.7 rule --- {Wikipedia}",
    year = "2023",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule}",
    note = "[Online; accessed 18-May-2023]"
  }
@article{geman1984,
	title = {Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images},
	author = {Geman, Stuart and Geman, Donald},
	year = {1984},
	month = {11},
	date = {1984-11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {721--741},
	volume = {PAMI-6},
	number = {6},
	doi = {10.1109/tpami.1984.4767596},
	url = {http://dx.doi.org/10.1109/TPAMI.1984.4767596}
}

@article{cook1977,
	title = {Detection of Influential Observation in Linear Regression},
	author = {Cook, R. Dennis},
	year = {1977},
	month = {02},
	date = {1977-02},
	journal = {Technometrics},
	pages = {15},
	volume = {19},
	number = {1},
	doi = {10.2307/1268249},
	url = {http://dx.doi.org/10.2307/1268249}
}

@article{williams1987,
	title = {Generalized Linear Model Diagnostics Using the Deviance and Single Case Deletions},
	author = {Williams, D. A.},
	year = {1987},
	date = {1987},
	journal = {Applied Statistics},
	pages = {181},
	volume = {36},
	number = {2},
	doi = {10.2307/2347550},
	url = {http://dx.doi.org/10.2307/2347550}
}

@book{johnson2019applied,
  title={Applied Multivariate Statistical Analysis},
  author={Johnson, R.A. and Wichern, D.W.},
  isbn={978-0-13-187715-3},
  series={Pearson Modern Classics for Advanced Statistics Series},
  url={https://books.google.co.il/books?id=QBqlswEACAAJ},
  year={2001},
  publisher={Prentice Hall}
}
@book{belsley1980,
	title = {Regression Diagnostics},
	author = {Belsley, David A. and Kuh, Edwin and Welsch, Roy E.},
	year = {1980},
	month = {06},
	date = {1980-06-24},
	publisher = {John Wiley & Sons, Inc.},
	doi = {10.1002/0471725153},
	url = {http://dx.doi.org/10.1002/0471725153}
}

@book{härdle2019,
	title = {Applied Multivariate Statistical Analysis},
	author = {{Härdle}, Wolfgang Karl and Simar, {Léopold}},
	year = {2019},
	date = {2019},
	publisher = {Springer International Publishing},
	doi = {10.1007/978-3-030-26006-4},
	url = {http://dx.doi.org/10.1007/978-3-030-26006-4}
}

@book{sheather2009,
	title = {A Modern Approach to Regression with R},
	author = {Sheather, Simon},
	year = {2009},
	date = {2009},
	publisher = {Springer New York},
	doi = {10.1007/978-0-387-09608-7},
	url = {http://dx.doi.org/10.1007/978-0-387-09608-7}
}

@article{jackman2009,
	title = {Bayesian Analysis for the Social Sciences},
	author = {Jackman, Simon},
	year = {2009},
	month = {10},
	date = {2009-10-23},
	journal = {Wiley Series in Probability and Statistics},
	doi = {10.1002/9780470686621},
	url = {http://dx.doi.org/10.1002/9780470686621}
}

@article{gelman2008,
	title = {A weakly informative default prior distribution for logistic and other regression models},
	author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
	year = {2008},
	month = {12},
	date = {2008-12-01},
	journal = {The Annals of Applied Statistics},
	volume = {2},
	number = {4},
	doi = {10.1214/08-aoas191},
	url = {http://dx.doi.org/10.1214/08-AOAS191}
}

@article{ghosh2018,
	title = {On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression},
	author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
	year = {2018},
	month = {06},
	date = {2018-06-01},
	journal = {Bayesian Analysis},
	volume = {13},
	number = {2},
	doi = {10.1214/17-ba1051},
	url = {http://dx.doi.org/10.1214/17-BA1051}
}

@article{bianchi2010,
	title = {Tempered infinitely divisible distributions and processes},
	author = {Bianchi, M and Bianchi, M and {Рачев}, {Светлозар Тодоров} and Rachev, Svetlozar Todorov and Kim, Y S and Kim, Y S and Fabozzi, F J and Fabozzi, F J},
	year = {2010},
	date = {2010},
	journal = {Teoriya Veroyatnostei i ee Primeneniya},
	pages = {59--86},
	volume = {55},
	number = {1},
	doi = {10.4213/tvp4176},
	url = {http://dx.doi.org/10.4213/tvp4176},
	langid = {ru}
}

@article{satheesh2003,
	title = {A Supplement To The Bose-Dasgupta-Rubin (2002) Review Of Infinitely Divisible Laws And Processes},
	author = {Satheesh, S.},
	year = {2003},
	date = {2003},
	doi = {10.48550/ARXIV.MATH/0305126},
	url = {https://arxiv.org/abs/math/0305126}
}

@article{bose2002contemporary,
  title={A contemporary review and bibliography of infinitely divisible distributions and processes},
  author={Bose, Arup and Dasgupta, Anirban and Rubin, Herman},
  journal={Sankhy{\=a}: The Indian Journal of Statistics, Series A},
  pages={763--819},
  year={2002},
  publisher={JSTOR}
}

@Misc{gimond2021tukeyedar,
  title = {tukeyedar: A package of Tukey inspired EDA functions},
  author = {Manuel Gimond},
  url = {https://mgimond.github.io/tukeyedar/},
  year = {2021},
}

@book{tukey1970exploratory,
  title={Exploratory Data Analysis},
  author={Tukey, J.W.},
  isbn={9780608082257},
  lccn={71083638},
  series={Exploratory Data Analysis},
  url={https://books.google.co.il/books?id=F6IIxgEACAAJ},
  year={1970},
  publisher={Addison Wesley Publishing Company}
}

@Manual{le2022treeheatr,
  title = {treeheatr: Heatmap-Integrated Decision Tree Visualizations},
  author = {Trang Le and Jason Moore},
  year = {2022},
  note = {https://trangdata.github.io/treeheatr/index.html,
https://trangdata.github.io/treeheatr-manuscript/},
}

@Manual{Barret2022GGally,
  title = {GGally: Extension to 'ggplot2'},
  author = {Barret Schloerke and Di Cook and Joseph Larmarange and Francois Briatte and Moritz Marbach and Edwin Thoen and Amos Elberg and Jason Crowley},
  year = {2022},
  note = {https://ggobi.github.io/ggally/, https://github.com/ggobi/ggally},
}

@book{jank2011business,
  title={Business Analytics for Managers},
  author={Jank, W.},
  isbn={9781461404064},
  series={Use R!},
  url={https://books.google.co.il/books?id=4-uU74-Jss4C},
  year={2011},
  publisher={Springer New York}
}

@article{Textor2017Dagity,
    author = {Textor, Johannes and van der Zander, Benito and Gilthorpe, Mark S and Liśkiewicz, Maciej and Ellison, George TH},
    title = "{Robust causal inference using directed acyclic graphs: the R package ‘dagitty’}",
    journal = {International Journal of Epidemiology},
    volume = {45},
    number = {6},
    pages = {1887-1894},
    year = {2017},
    month = {01},
    abstract = "{Directed acyclic graphs (DAGs), which offer systematic representations of causal relationships, have become an established framework for the analysis of causal inference in epidemiology, often being used to determine covariate adjustment sets for minimizing confounding bias. DAGitty is a popular web application for drawing and analysing DAGs. Here we introduce the R package ‘dagitty’, which provides access to all of the capabilities of the DAGitty web application within the R platform for statistical computing, and also offers several new functions. We describe how the R package ‘dagitty’ can be used to: evaluate whether a DAG is consistent with the dataset it is intended to represent; enumerate ‘statistically equivalent’ but causally different DAGs; and identify exposure-outcome adjustment sets that are valid for causally different but statistically equivalent DAGs. This functionality enables epidemiologists to detect causal misspecifications in DAGs and make robust inferences that remain valid for a range of different DAGs. The R package ‘dagitty’ is available through the comprehensive R archive network (CRAN) at [https://cran.r-project.org/web/packages/dagitty/]. The source code is available on github at [https://github.com/jtextor/dagitty]. The web application ‘DAGitty’ is free software, licensed under the GNU general public licence (GPL) version 2 and is available at [http://dagitty.net/].}",
    issn = {0300-5771},
    doi = {10.1093/ije/dyw341},
    url = {https://doi.org/10.1093/ije/dyw341},
    eprint = {https://academic.oup.com/ije/article-pdf/45/6/1887/11120744/dyw341.pdf},
}
@article{blundell2015,
	title = {Weight Uncertainty in Neural Networks},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	year = {2015},
	date = {2015},
	doi = {10.48550/ARXIV.1505.05424},
	url = {https://arxiv.org/abs/1505.05424}
}


@misc{Webster, 
author = {Dr Kevin Webster},
title = {Emmy {Amalie} {Noether}},
howpublished = "\url{http://www-groups.dcs.st-and.ac.uk/~history/Biographies/Noether_Emmy.html}",
year = {2014}, 
note = "Accessed 11/02/16",
}


@article{cook1977a,
	title = {Detection of Influential Observation in Linear Regression},
	author = {Cook, R. Dennis},
	year = {1977},
	month = {02},
	date = {1977-02},
	journal = {Technometrics},
	pages = {15},
	volume = {19},
	number = {1},
	doi = {10.2307/1268249},
	url = {http://dx.doi.org/10.2307/1268249}
}


@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society: series B (methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

@misc{BibEntry2024Sep,
	title = {{Statistics professor wins prestigious professional statistics society award {\textendash} Baskin School of Engineering}},
  author = {Melissa Weckerle},
	year = {2022},
	month = July,
	note = {[Online; accessed 17. Sep. 2024]},
	howpublished = "\url{https://engineering.ucsc.edu/news/statistics-professor-wins-zellner-medal}"
}

@article{Durbin1960Fitting,
 ISSN = {03731138},
 URL = {http://www.jstor.org/stable/1401322},
 abstract = {Le but de cette communication est un examen des méthodes d'estimation efficaces des paramètres dans quelques modèles employés dans l'analyse des séries temporelles. On considère les modèles suivants: Le modèle autorégressif: ut+α 1ut-1+....+α kut-k=ε t. (1) Régression avec variables x indépendentes et y retardés: yt+α 1yt-1+....+α pyt-p=β 1x1t+....β qxqt+ε t. (2) Régression avec variables x indépendentes et perturbations autorégressives: yt=β tx1t+....+β qxqt+ut (3) avec ut+α 1ut-1+....+α put-p=ε t. Modèle défini par des moyennes mobiles: ut=ε t+β 1ε t-1+....+β hε t-h. (4) Modèle autorégressif et erreurs définies par moyennes mobiles: ut+γ 1ut-1+....+γ put-p=ε t+δ 1ε t-1+....+δ qε t-q. (5) Dans chaque cas, il est supposé que {ε t} définit une série de variables aléatoires indépendentes avec la même distribution. Pour les modèles (1) et (2) les qualités de l'estimation par les moindres carrés sont examinées. Pour le modèle (3) on considère le cas simple yt=β xt+ut avec ut+α ut-1=ε t. Soient a, b, c les coefficients de régression (par les moindres carrés) de yt sur -yt-1′xt′xt-1. Il est démontré que β̂=(1+ar)b+(a+r)c/1+2ar+a2, lorsque r=Σ xtxt-1/Σ xt2, est un estimateur asymptotiquement efficace (à variance minimum) de β. Une extension au cas général est indiquée. Le traitement des modèles (4) et (5) est basé sur l'idée d'un ajustement aux données par la méthode des moindres carrés d'un modèle autorégressif d'ordre k, lorsque k est grand. En examinant les distributions multi-dimensionnelles des coefficients obtenus, on trouve que les estimations efficaces de β 1,...,β h dans (4) sont les solutions d'une série d'équations linéaires. Cette méthode permet d'obtenir des estimations de δ 1,...,δ q dans (5), en supposant que les valeurs γ 1,...,γ p sont connues. En utilisant ces deux méthodes alternativement, on obtient une méthode itérative pour l'ajustement du modèle (5). Une méthode d'estimation plus simple mais moins efficace, qui pourrait être utilisée pour obtenir des valeurs de départ pour l'itération est indiquée.},
 author = {J. Durbin},
 journal = {Revue de l'Institut International de Statistique / Review of the International Statistical Institute},
 number = {3},
 pages = {233--244},
 publisher = {[International Statistical Institute (ISI), Wiley]},
 title = {The Fitting of Time-Series Models},
 urldate = {2024-09-17},
 volume = {28},
 year = {1960},
 context = 
}

@article{Levinson1947Wiener,
  author = {Levinson, Norman},
  title = {The Wiener (Root Mean Square) Error Criterion in Filter Design and Prediction},
  journal = {Journal of Mathematics and Physics},
  volume = {25},
  number = {1-4},
  pages = {261-278},
  doi = {https://doi.org/10.1002/sapm1946251261},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sapm1946251261},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sapm1946251261},
  year = {1946}
}

@article{Trench1964ToeplitzMI,
 title = {An Algorithm for the Inversion of Finite Toeplitz Matrices},
 ISSN = {03684245},
 URL = {http://ramanujan.math.trinity.edu/wtrench/research/papers/TRENCH_RP_6.PDF},
 author = {William F. Trench},
 journal = {Journal of the Society for Industrial and Applied Mathematics},
 number = {3},
 pages = {515--522},
 publisher = {Society for Industrial and Applied Mathematics},
 urldate = {2024-09-17},
 volume = {12},
 year = {1964}
}

@article{Zohar1969ToeplitzMI,
  title={Toeplitz Matrix Inversion: The Algorithm of W. F. Trench},
  author={Shalhav Zohar},
  journal={J. ACM},
  year={1969},
  volume={16},
  pages={592-601},
  url={https://api.semanticscholar.org/CorpusID:3115290}
}

@book{broemeling2011bayesian,
  title={Bayesian Analysis of Time Series},
  author={Broemeling, Lyle D},
  year={2019},
  publisher={CRC Press},
  isbn={9781138591523},
  pages={290}

}

@book{broemeling2018bayesian,
  title={Bayesian Inference for Stochastic Processes},
  author={Broemeling, Lyle D},
  year={2018},
  publisher={CRC Press},
  isbn={9781138196131},
  pages={432},
}

@book{cowpertwait2009introductory,
  title={Introductory Time Series with R},
  author={Cowpertwait, P.S.P. and Metcalfe, A.V.},
  isbn={9780387886985},
  lccn={2009928496},
  series={Use R!},
  url={https://books.google.co.il/books?id=QFiZGQmvRUQC},
  year={2009},
  publisher={Springer New York}
}

@book{cressie2011statistics,
  title={Statistics for Spatio-Temporal Data},
  author={Cressie, N. and Wikle, C.K.},
  isbn={9780471692744},
  lccn={2010033576},
  series={CourseSmart Series},
  url={https://books.google.co.il/books?id=-kOC6D0DiNYC},
  year={2011},
  publisher={Wiley}
}

@book{davidson2015bayesian,
  title={Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference},
  author={Davidson-Pilon, C.},
  isbn={9780133902921},
  series={Addison-Wesley Data \& Analytics Series},
  url={https://books.google.co.il/books?id=rMKiCgAAQBAJ},
  year={2015},
  publisher={Pearson Education}
}

@misc{enwiki-LevinsonRecursion,
    author = "{Wikipedia contributors}",
    title = "Levinson recursion --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2024",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Levinson_recursion&oldid=1229942891}",
    note = "[Online; accessed 17-September-2024]"
}

@misc{enwiki-YuleWalkerEquations,
    author = "{Wikipedia contributors}",
    title = "Autoregressive model --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2024",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Autoregressive_model&oldid=1233171855#Estimation_of_AR_parameters}",
    note = "[Online; accessed 17-September-2024]"
}


@book{gelman2013bayesian,
  title={Bayesian Data Analysis, Third Edition},
  author={Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
  isbn={9781439840955},
  lccn={2013039507},
  series={Chapman \& Hall/CRC Texts in Statistical Science},
  url={https://books.google.co.il/books?id=ZXL6AQAAQBAJ},
  year={2013},
  publisher={Taylor \& Francis}
}

@book{nielsen2019practical,
  title={Practical Time Series Analysis: Prediction with Statistics and Machine Learning},
  author={Nielsen, A.},
  isbn={9781492041603},
  url={https://books.google.co.il/books?id=xNOwDwAAQBAJ},
  year={2019},
  publisher={O'Reilly Media}
}

@book{pfaff2008analysis,
  title={Analysis of Integrated and Cointegrated Time Series with R},
  author={Pfaff, B.},
  isbn={9780387759678},
  lccn={2008930126},
  series={Use R!},
  url={https://books.google.co.il/books?id=ca5MkRbF3fYC},
  year={2008},
  publisher={Springer New York}
}

@book{prado2023time,
  title={Time Series: Modeling, Computation, and Inference},
  author={Prado, R. and Ferreira, M.A.R. and West, M.},
  isbn={9781032040042},
  series={Chapman \& Hall/CRC Texts in Statistical Science},
  url={https://books.google.co.il/books?id=pZ6lzgEACAAJ},
  year={2023},
  publisher={CRC Press}
}

@book{ravishanker2022dynamic,
  title={Dynamic Time Series Models using R-INLA: An Applied Perspective},
  author={Ravishanker, N. and Raman, B. and Soyer, R.},
  isbn={9781000622874},
  lccn={2022004441},
  url={https://books.google.co.il/books?id=e6h6EAAAQBAJ},
  year={2022},
  publisher={CRC Press}
}

@book{rios2012bayesian,
  title={Bayesian Analysis of Stochastic Process Models},
  author={Rios Insua, David and Ruggeri, Fabrizio and Wiper, Michael P},
  year={2012},
  publisher={John Wiley \& Sons},
  isbn={9780470975916},
  pages={300},
}

@book{theodoridis2015ML,
  title={Machine Learning: A Bayesian and Optimization Perspective},
  author={Theodoridis, S.},
  isbn={9780128015223},
  url={https://books.google.co.il/books?id=hxQRogEACAAJ},
  year={2015},
  publisher={Elsevier Science}
}

@book{von2002statistical,
  title={Statistical Analysis in Climate Research},
  author={von Storch, H. and Zwiers, F.W.},
  isbn={9781139425094},
  url={https://books.google.co.il/books?id=bs8hAwAAQBAJ},
  year={2002},
  publisher={Cambridge University Press}
}

@article{walker1931periodicity,
  author = {Walker, Gilbert Thomas },
  title = {On periodicity in series of related terms},
  journal = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
  volume = {131},
  number = {818},
  pages = {518-532},
  year = {1931},
  doi = {10.1098/rspa.1931.0069},
  URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1931.0069},
  eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.1931.0069},
  abstract = { An important extension of our ideas regarding periodicity was made in 1927 when Yule pointed out that, instead of regarding a series of annual sunspot numbers as consisting merely of a harmonic series to which a series of random terms were added, we might suppose a certain amount of causal relationship between the successive annual numbers. In that case the system might be regarded as a physical system possessing one or more natural oscillations of its own, all subject to damping; and the effect of annual random disturbances would be to produce a fairly smooth curve with periods varying in amplitude and length, essentially as the sunspot numbers vary. If we call the departures from their mean of our series u1, u2.., Yule showed that the consequence of a single natural period is an equation like ux = kux-1 - ux-2 + vx, where vx represents the “accidental” external “disturbance”; and if there are two natural periods, ux = k1 (ux-1 + ux-3) - k2ux-2 - ux-4 + vx }
}

@book{west2013bayesian,
  title={Bayesian Forecasting and Dynamic Models},
  author={West, M. and Harrison, J.},
  isbn={9781475793659},
  lccn={89011497},
  series={Springer Series in Statistics},
  url={https://books.google.co.il/books?id=NmfaBwAAQBAJ},
  year={2013},
  publisher={Springer New York}
}

@book{woodward2022time,
  title={Time Series for Data Science: Analysis and Forecasting},
  author={Woodward, W.A. and Sadler, B.P. and Robertson, S.},
  isbn={9781000555332},
  lccn={2021048204},
  series={Chapman \& Hall/CRC Texts in Statistical Science},
  url={https://books.google.co.il/books?id=_W16EAAAQBAJ},
  year={2022},
  publisher={CRC Press}
}

@article{yule1927periodicities,
  author = {Yule, George Udny },
  title = {VII. On a method of investigating periodicities disturbed series, with special reference to Wolfer's sunspot numbers},
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume = {226},
  number = {636-646},
  pages = {267-298},
  year = {1927},
  doi = {10.1098/rsta.1927.0007},
  URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1927.0007},
  eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1927.0007},
  abstract = { If we take a curve representing a simple harmonic function of the time, and superpose on the ordinates small random errors, the only effect is to make the graph somewhat irregular, leaving the suggestion of periodicity still quite clear to the eye. Fig. 1 (a) shows such a curve, the random errors having been determined by the throws of dice. If the errors are increased in magnitude, as in fig. 1 (b), the graph becomes more irregular, the suggestion of periodicity more obscure, and we have only sufficiently to increase the “errors” to mask completely any appearance of periodicity. But, however large the errors, periodogram analysis is applicable to such a curve, and, given a sufficient number of periods, should yield a close approximation to the period and amplitude of the underlying harmonic function. When periodogram analysis is applied to data respecting any physical phenomenon in the expectation of eliciting one or more true periodicities, there is usually, as it seems to me, a tendency to start from the initial hypothesis that the periodicity or periodicities are masked solely by such more or less random superposed fluctuations— fluctuations which do not in any way disturb the steady course of the underlying periodic function or functions. It is true that the periodogram itself will indicate the truth or otherwise of the hypothesis made, but there seems no reason for assuming it to be the hypothesis most likely a priori. }
}

@book{BMCP2021,
  title = {{Bayesian Modeling and Computation in Python}},
  author = {Martin, Osvaldo A. and Kumar, Ravin and Lao, Junpeng},
  year = {2021},
  month = dec,
  address = {{Boca Raton}},
  isbn = {978-0-367-89436-8},
}
