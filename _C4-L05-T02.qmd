One of the big
questions when you're working with
autoregressive processes is what model order you should use for your
particular data. One thing that you can
do is you can look at the sample ACF and PACF of the data that you
have and try to begin with a model
order that reflects, the behavior that
you see in those sample ACF's and PACF's. But you can also have a more
formal approach in which you consider using model
order selection criteria. Model selection criteria
to pick the model order. You could even also be more
sophisticated and assume that the model order is also uncertain and consider
that as a random variable. Put a prior on that
random variable, and then perform
Bayesian inference. I'm going to just
discuss here how we can use two model
selection criteria. One is the AIC, Akaike's Information Criterion,
and the other one is BIC, to choose the model order. These are usually implemented
in software packages in R and you can use them in
practice for your given data. The first thing is if you're working with the
conditional likelihood, you can also work with
the full likelihood. But if you're working with
the conditional likelihood, you have to evaluate these model selection criteria
using the same data. Let's say that you have a
set candidate of models. You have to pick a
maximum model order, I'm going to call that p-star. For example, you could
have p-star to be 20. Then you're going to consider all the possible
orders from 0-20. So your model orders that
you will be considering go from one or from zero
all the way to p-star. Here for each of those model
orders that you consider, if you're working with a
conditional likelihood, you're going to have estimates
of the model parameters. You're going to
have your phi hats using maximum
likelihood estimation. For each of those model orders, you're going to have Sp, I'm going to call it Sp square. It's your S square that you get from each of those model orders. Now p here, I'm putting that subscript there just to indicate
that this is related to the model with model
order p. Then you can evaluate everything
using your data and your data for evaluation
here is going to go from p star plus 1 all the way to T. It's important that you just
use this data when you're computing your regressions so that you evaluate everything on the same set of data points. You can write down the Akaike's Information
Criterion in this case, is a function that is going
to have two components. One has to do with how good
is the fit of the model. The other one is penalizing the number of parameters you
have in the model. Usually in these model
selection criteria, we have those two components. You can write this down as the number of
observations you have, which is, again, we're conditioning on the first
p star observations. We're only going
to evaluate this on the Data starting
from p star plus 1 up to T. Then you're going to have
the log of that Sp squared. That's the part of the AIC that has to do with the
goodness of fit. Then there is a penalty for
the number of parameters. In this case, AIC uses
a penalty of two times p. What you do is you fit
all these model orders. It can go from one to p star, from zero to p star
if you want to incorporate the white
noise component as well. Then you just get an
AIC for each of these, and then you compare
all of those. There is going to be essentially a value
for each of these p. Then you look at the optimal value
is the one that minimizes that AIC expression. You look at those values, you pick the model order
that minimizes the AIC. You can also use BIC. In the BIC, you're going to have the same component here related
to the goodness of fit. You're going to have
the same piece. But now BIC is going to penalize the number of parameters in the model
in a different way. You're going to have
something that looks like p log of the T minus p star. Here we had a penalty. We had two times the
number of parameters. Here we have the
number of parameters times something that depends on the number of
observations you have. It's a different penalty. You may get different results
again here you evaluate your BIC for each of the model orders you
are considering, and then you pick
the model order that minimizes this expression. As you can see, what happens is
there is a balance usually between the more
parameters you consider, the better is going
to be your model fit. But you have that penalty
that you are overfitting, you maybe overfitting, you have too many parameters
in your model. These model selection
criteria try to balance those components and give a penalty for the
number of parameters. When you run things in practice, you may get the
same model order, the same optimal model
order for AIC or BIC, or you may get
different numbers. You can also look at
other quantities. You can look at posterior predictive distributions and see how well the model
does in terms of those posterior
predictive densities. You can, as I said before, think about considering p as another variable
in your model. We will stay simple here, and we will consider just these
model selection criteria.