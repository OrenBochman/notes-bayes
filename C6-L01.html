<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="dcterms.date" content="2025-07-02">
<meta name="keywords" content="Gaussian Processes, Kriging, Optimal Interpolation">
<meta name="description" content="Gaussian Processes Tutorial">

<title>104&nbsp; Gaussian Processes for Regression – Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C6-L02.html" rel="next">
<link href="./C6-L00.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-cec409635dab7d42c7b562035797153a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-cec409635dab7d42c7b562035797153a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(images/banner_deep.jpg);
background-size: cover;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C6-L00.html">Bayesian Non-Parametric Models</a></li><li class="breadcrumb-item"><a href="./C6-L01.html"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Gaussian Processes for Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C6-L00.html">Bayesian Non-Parametric Models</a></li><li class="breadcrumb-item"><a href="./C6-L01.html"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Gaussian Processes for Regression</span></a></li></ol></nav>
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Gaussian Processes for Regression</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Bayesian Statistics - Nonparametric Methods</p>
                  <div>
        <div class="description">
          Gaussian Processes Tutorial
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Bayesian Statistics</div>
                <div class="quarto-category">Nonparametric Methods</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 2, 2025</p>
      </div>
    </div>
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Gaussian Processes, Kriging, Optimal Interpolation</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C1-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Concept to Data Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Paradigms of probability - M1L1HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law - M1L2HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probability and Bayes’ Theorem - M1L2HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Distributions - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Random Variables - M1L3HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Homework on Distributions - M1L3HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Frequentist Inference - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Frequentist MLE - M2L3HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Bayesian Inference - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference - M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Priors - M3L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities - M3L6HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Homework on Priors - M2L7HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Poisson Data - M3L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Homework on Poisson Data - M3L8HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Beta Bernoulli - M3L8HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Homework on Exponential Data - M4L9HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Normally distributed Data - M4L10</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Homework on Normal Data - M4L10HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Non-Informative Priors - M4L11</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Homework Alternative Priors - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Brief Review of Regression - M4L12</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW2</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C2-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Techniques and Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm - M2L4HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Gibbs sampling - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Homework Gibbs-Sampling algorithm - M2L22HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Assessing Convergence - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Homework on M-H algorithm M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Linear regression - M3L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Homework on Linear Regression Model Part 1 - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Homework on Deviance information criterion - M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">ANOVA - M3L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Homework on ANOVA - M3L8HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Homework on Multiple Factor ANOVA - M3L8HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Logistic regression - M3L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression - M3L9HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Poisson regression - M4L10</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Homework on Poisson regression - M4L10HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Hierarchical modeling - M4L11</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Capstone Project - M4L12</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models - M4L12HW1</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C3-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mixture Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Definitions of Mixture Models - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Likelihood functions for Mixture Models - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Homework The Likelihood function - M1L2HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Homework Identifiability - M1L2HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Homework The likelihood function M1L2HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Homework on simulating from a Poisson Mixture Model - M1L2HW4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model - M1L2HW5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Homework Sim mixture of exponential distributions - M1L2HW6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture models - M2L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">MCMC for Mixture Models - M4L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Density Estimation - M4L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Computational Considerations - M5L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Determining the number of components - M5L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Homework on Bayesian Information Criteria (BIC) - M5L09HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Homework on BIC for zero-inflated mixtures - M5L09HW4</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time Series Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Stationarity, The ACF and the PCF M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">The AR(1) process: definitions and properties - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">The AR(1): MLE and Bayesian inference - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">The AR(p) process - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">Bayesian Inference in the AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Quiz: Spectral representation of the AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1 - M3L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 1 M3L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Seasonal NDLMs M4L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 2 - M4L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C5-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Capstone Project</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Homework - Practice Quiz for Week 1 – M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Homework - first-step-for-the-project – M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Model Selection Criteria - M2L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Bayesian location mixture of AR(p) models - M3L3</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C6-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Non-Parametric Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C6-L01.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Gaussian Processes for Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C6-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Dirichlet Process</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Appendix: Bayes by backprop</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Appendix: Yule-Walker Equations &amp; Durbin-Levinson Recursion</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Moore-Penrose Inversion &amp; Cholesky Decomposition</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Appendix: Inequalities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Appendix: Wold’s theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#why-gps" id="toc-why-gps" class="nav-link active" data-scroll-target="#why-gps"><span class="header-section-number">104.1</span> Why GPs</a></li>
  <li><a href="#more-examples" id="toc-more-examples" class="nav-link" data-scroll-target="#more-examples"><span class="header-section-number">104.2</span> More Examples:</a></li>
  <li><a href="#roadmap" id="toc-roadmap" class="nav-link" data-scroll-target="#roadmap"><span class="header-section-number">104.3</span> Roadmap</a></li>
  <li><a href="#a-bayesian-approach" id="toc-a-bayesian-approach" class="nav-link" data-scroll-target="#a-bayesian-approach"><span class="header-section-number">104.4</span> A Bayesian approach</a></li>
  <li><a href="#sec-multivariate-gaussian-locations" id="toc-sec-multivariate-gaussian-locations" class="nav-link" data-scroll-target="#sec-multivariate-gaussian-locations"><span class="header-section-number">104.5</span> Multivariate Gaussian using locations</a></li>
  <li><a href="#demo-1-draws-from-a-multivariate-normal-distribution" id="toc-demo-1-draws-from-a-multivariate-normal-distribution" class="nav-link" data-scroll-target="#demo-1-draws-from-a-multivariate-normal-distribution"><span class="header-section-number">104.6</span> Demo 1 : Draws from a multivariate normal distribution</a></li>
  <li><a href="#sec-gaussian-processes" id="toc-sec-gaussian-processes" class="nav-link" data-scroll-target="#sec-gaussian-processes"><span class="header-section-number">104.7</span> Gaussian processes</a></li>
  <li><a href="#demo-2-draw-from-a-gaussian-process" id="toc-demo-2-draw-from-a-gaussian-process" class="nav-link" data-scroll-target="#demo-2-draw-from-a-gaussian-process"><span class="header-section-number">104.8</span> Demo 2: Draw from a Gaussian process</a></li>
  <li><a href="#note-on-dimension-of-the-input" id="toc-note-on-dimension-of-the-input" class="nav-link" data-scroll-target="#note-on-dimension-of-the-input"><span class="header-section-number">104.9</span> Note on “dimension” of the input</a></li>
  <li><a href="#inference-about-unknowns-given-data." id="toc-inference-about-unknowns-given-data." class="nav-link" data-scroll-target="#inference-about-unknowns-given-data."><span class="header-section-number">104.10</span> Inference about Unknowns given data.</a></li>
  <li><a href="#sec-squared-exponential-kernel-revisited" id="toc-sec-squared-exponential-kernel-revisited" class="nav-link" data-scroll-target="#sec-squared-exponential-kernel-revisited"><span class="header-section-number">104.11</span> Squared exponential kernel revisited</a>
  <ul class="collapse">
  <li><a href="#demo-3-learning-the-signal-and-the-length-scale-and" id="toc-demo-3-learning-the-signal-and-the-length-scale-and" class="nav-link" data-scroll-target="#demo-3-learning-the-signal-and-the-length-scale-and"><span class="header-section-number">104.11.1</span> Demo 3 – Learning the signal and the length scale and</a></li>
  </ul></li>
  <li><a href="#what-next" id="toc-what-next" class="nav-link" data-scroll-target="#what-next"><span class="header-section-number">104.12</span> What next</a></li>
  <li><a href="#observation-noise" id="toc-observation-noise" class="nav-link" data-scroll-target="#observation-noise"><span class="header-section-number">104.13</span> Observation noise</a></li>
  <li><a href="#what-uncertainty-are-we-quantifying" id="toc-what-uncertainty-are-we-quantifying" class="nav-link" data-scroll-target="#what-uncertainty-are-we-quantifying"><span class="header-section-number">104.14</span> What uncertainty are we quantifying?</a></li>
  <li><a href="#sec-other-sources-of-uncertainty" id="toc-sec-other-sources-of-uncertainty" class="nav-link" data-scroll-target="#sec-other-sources-of-uncertainty"><span class="header-section-number">104.15</span> Some other sources of uncertainty</a></li>
  <li><a href="#sec-extrapolation" id="toc-sec-extrapolation" class="nav-link" data-scroll-target="#sec-extrapolation"><span class="header-section-number">104.16</span> Extrapolation</a></li>
  <li><a href="#more-than-one-input" id="toc-more-than-one-input" class="nav-link" data-scroll-target="#more-than-one-input"><span class="header-section-number">104.17</span> More than one input</a></li>
  <li><a href="#sec-high-points" id="toc-sec-high-points" class="nav-link" data-scroll-target="#sec-high-points"><span class="header-section-number">104.18</span> Some high points of what got cut for time</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Credit
</div>
</div>
<div class="callout-body-container callout-body">
<p>the following material is based on the <a href="https://tamarabroderick.com/tutorial_2024_cps-fr.html">Gaussian Processes for Regression</a> and tutorial and code by Tamara Broderick. <a href="https://tamarabroderick.com/tutorial_2025_asa_cirs_webinar.html"></a> Note that the <a href="https://github.com/tbroderick/gps_tutorial/blob/2025asa_cirs/demos.ipynb">code</a> is under the <a href="">MIT license</a></p>
</div>
</div>
<section id="why-gps" class="level2 page-columns page-full" data-number="104.1">
<h2 data-number="104.1" class="anchored" data-anchor-id="why-gps"><span class="header-section-number">104.1</span> Why GPs</h2>

<div class="no-row-height column-margin column-container"><div id="fig-nonparametrics-gps1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonparametrics-gps1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="C6-SL01.png" class="lightbox" data-gallery="slides" title="why GP1"><img src="C6-SL01.png" class="img-fluid figure-img" style="width:53mm"></a></p>
<figcaption>why GP1</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonparametrics-gps1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.1: ocean current vector field [Ryan, Özgökmen 2023; Zewe 2023; Gonçalves et al 2019; Lodise et al 2020; Berlinghieri et al 2023]
</figcaption>
</figure>
</div><div id="fig-nonparametrics-gps2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonparametrics-gps2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="C6-SL02.png" class="lightbox" data-gallery="slides" title="why GP2"><img src="C6-SL02.png" class="img-fluid figure-img" style="width:53mm"></a></p>
<figcaption>why GP2</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonparametrics-gps2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.2: rocket booster surrogate model [Gramacy,Lee 2008] [Gramacy 2020]
</figcaption>
</figure>
</div></div>
<ul>
<li>Why Gaussian processes (GPs)?</li>
<li>Example 1:
<ul>
<li>The ocean current (velocity vector field) varies by space &amp; time</li>
<li>Scientists get sparse observations of the current from buoys</li>
<li>Goal: estimate the current</li>
<li>c.f. [Ryan, Özgökmen 2023; Zewe 2023; Gonçalves et al 2019; Lodise et al 2020; Berlinghieri et al 2023]</li>
</ul></li>
<li>Challenges:
<ul>
<li>Sparse &amp; expensive data, not on a grid</li>
<li>Current is highly nonlinear but smooth in space-time</li>
<li>We want uncertainty quantification</li>
<li>c.f.
<ul>
<li>[Gramacy,Lee 2008]</li>
<li>[Gramacy 2020]</li>
</ul></li>
</ul></li>
<li>Example 2: Surrogate model
<ul>
<li>The lift force of a rocket booster varies as a function of
<ul>
<li>speed at re-entry,</li>
<li>angle of attack, and</li>
<li>sideslip angle</li>
</ul></li>
<li>Scientists can run expensive simulations at chosen input settings</li>
<li>Goal: estimate how lift varies as a function of input settings</li>
</ul></li>
<li>Example 3: learn (&amp; optimize) performance in machine learning as a function of tuning parameters
<ul>
<li>c.f. [Snoek et al 2012; 2015; Garnett 2023]</li>
</ul></li>
<li>Motif:
<ul>
<li>Sparse, noisy, costly but but smooth data.</li>
<li>Output may have a nonlinear relationship to the inputs.</li>
<li>Want uncertainty quantification.</li>
<li>Low-dimensional inputs.</li>
</ul></li>
<li>Bonus benefits:
<ul>
<li>Ease of use (software, tuning)</li>
<li>Supports optimization of outcome</li>
<li>Predictions &amp; uncertainties over derivatives &amp; integrals</li>
<li>Module in more-complex methods</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>⚡ Overthinking parameters in Bayesian non-parametric models
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Let’s talk about the elephant in the room non-parametrics do have parameters!</p></li>
<li><p>The classical form of a distribution based on a few fixed number parameters is replaced in these by a notion similar to a euclidean line which can be arbitrary extended. This is the nature of the <strong>non-parametric</strong> parameter vector <span class="math inline">\theta</span>.</p></li>
<li><p>To make the analogy more precise and what the above and many examples illustrate is that we often are interested in some behavior Y but as we get more and more data X, we will notice that there are new types of behavior that we did not observe earlier.</p></li>
</ul>
</div>
</div>
</section>
<section id="more-examples" class="level2" data-number="104.2">
<h2 data-number="104.2" class="anchored" data-anchor-id="more-examples"><span class="header-section-number">104.2</span> More Examples:</h2>
<ul>
<li>new species are being daily</li>
<li>new friend set appear as social networks expand</li>
<li>new entities appear in images as the data set expands</li>
<li>new wikipedia articles are discovered as we click on wikilinks</li>
<li>new alleles are identified as we sequence more genomes</li>
</ul>
</section>
<section id="roadmap" class="level2" data-number="104.3">
<h2 data-number="104.3" class="anchored" data-anchor-id="roadmap"><span class="header-section-number">104.3</span> Roadmap</h2>
<ul>
<li>A Bayesian approach</li>
<li>What is a Gaussian process?
<ul>
<li>Popular version using a squared exponential kernel</li>
</ul></li>
<li>Gaussian process inference.
<ul>
<li>Prediction &amp; uncertainty quantification.</li>
</ul></li>
<li>Goal:
<ul>
<li>Learn the mechanism behind standard GPs to identify benefits and pitfalls</li>
</ul></li>
</ul>
</section>
<section id="a-bayesian-approach" class="level2 page-columns page-full" data-number="104.4">
<h2 data-number="104.4" class="anchored" data-anchor-id="a-bayesian-approach"><span class="header-section-number">104.4</span> A Bayesian approach</h2>

<div class="no-row-height column-margin column-container"><div id="fig-nonparametics-posterior" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonparametics-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-SL04.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;104.3: posterior"><img src="C6-SL04.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonparametics-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.3: posterior
</figcaption>
</figure>
</div><div id="fig-nonparametics-joint" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonparametics-joint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-SL05.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;104.4: joint"><img src="C6-SL05.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonparametics-joint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.4: joint
</figcaption>
</figure>
</div></div>
<p><span class="math display">
\underbrace{
  \mathbb{P}r(unknowns \mid data) }_{
    \substack{
      \text{Given the data we’ve seen} \\
      \text{what do we know about } \\
      \text{the underlying function?}
    }
  }
\propto
\underbrace{
  \mathbb{P}r(data \mid unknowns)
  \mathbb{P}r(unknowns)}_{  
    \substack{
      \text{A (statistical) model} \\
      \text{that can generate} \\
      \text{functions and data} \\
      \text{of interest}
    }
  }
</span></p>
</section>
<section id="sec-multivariate-gaussian-locations" class="level2 page-columns page-full" data-number="104.5">
<h2 data-number="104.5" class="anchored" data-anchor-id="sec-multivariate-gaussian-locations"><span class="header-section-number">104.5</span> Multivariate Gaussian using locations</h2>

<div class="no-row-height column-margin column-container"><div id="fig-nonparametrics-multivariate-gaussian-locations" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" width="53mm">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonparametrics-multivariate-gaussian-locations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-SL06.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;104.5: "><img src="C6-SL06.png" id="fig-nonparametrics-multivariate-gaussian-locations" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-nonparametrics-multivariate-gaussian-locations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.5
</figcaption>
</figure>
</div></div><ul>
<li><span class="math inline">M =2</span> bivariate Gaussian <span class="math inline">[y^{(1)} y^{(2)}] \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</span></li>
<li>with <span class="math inline">\mu = [0,0]^\top</span> and <span class="math inline">k = \sigma^2 \begin{bmatrix} 1 &amp; \rho \\ \rho &amp; 1\end{bmatrix}</span> where <span class="math inline">\rho</span> is the correlation between the two dimensions.</li>
<li>What if we let the correlation <span class="math inline">\rho</span> depend on the input <span class="math inline">x</span>?
<ul>
<li>Let <span class="math inline">\rho(x)= \rho(\mid x^{(1)} - x^{(2)} \mid)</span></li>
<li>Where the correlation goes to 1 as the x’s get close</li>
<li>And goes to 0 as the x’s get far</li>
</ul></li>
<li>Next: Use a Similar setup but an M-long Gaussian instead of just a bivariate
<ul>
<li>We have M locations (need not be in order): <span class="math inline">X^{(m)} \in \mathbb{R}^d</span></li>
<li>We generate <span class="math inline">[y^{(1)}, \ldots,y^{(m)}]^\top \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{K})</span></li>
<li>with <span class="math inline">\mu=0_M</span> and <span class="math inline">K</span> such that <span class="math inline">\mathbf{K}_{ij} = \sigma^2 \rho\mid (x^{(i)}, x^{(j)}\mid)</span></li>
<li>Let’s try <span class="math inline">\rho(\Delta)=\exp(-\tfrac{1}{2} \Delta^2)</span>
<ul>
<li><span class="math inline">\rho(0)=1</span> ,</li>
<li><span class="math inline">\lim_{\Delta \to \infty} \rho(\Delta)=0</span> and</li>
<li><span class="math inline">\rho(\Delta)</span> is monotonically decreasing</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="demo-1-draws-from-a-multivariate-normal-distribution" class="level2" data-number="104.6">
<h2 data-number="104.6" class="anchored" data-anchor-id="demo-1-draws-from-a-multivariate-normal-distribution"><span class="header-section-number">104.6</span> Demo 1 : Draws from a multivariate normal distribution</h2>
<div id="lst-demo-01-setup" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="1">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-demo-01-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;104.1
</figcaption>
<div aria-describedby="lst-demo-01-setup-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState()</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process <span class="im">import</span> GaussianProcessRegressor</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process.kernels <span class="im">import</span> RBF</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<div id="cell-fig-demo-01-gp-draws-1" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="annotated-cell-2"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-2-1"><a href="#annotated-cell-2-1" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="annotated-cell-2-2"><a href="#annotated-cell-2-2" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="annotated-cell-2-3"><a href="#annotated-cell-2-3" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">1.0</span>,</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="1">1</button><span id="annotated-cell-2-4" class="code-annotation-target"><a href="#annotated-cell-2-4" aria-hidden="true" tabindex="-1"></a>  length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>)</span>
<span id="annotated-cell-2-5"><a href="#annotated-cell-2-5" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">3</span></span>
<span id="annotated-cell-2-6"><a href="#annotated-cell-2-6" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="annotated-cell-2-7"><a href="#annotated-cell-2-7" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">100</span></span>
<span id="annotated-cell-2-8"><a href="#annotated-cell-2-8" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span></span>
<span id="annotated-cell-2-9"><a href="#annotated-cell-2-9" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span></span>
<span id="annotated-cell-2-10"><a href="#annotated-cell-2-10" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="2">2</button><span id="annotated-cell-2-11" class="code-annotation-target"><a href="#annotated-cell-2-11" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx)</span>
<span id="annotated-cell-2-12"><a href="#annotated-cell-2-12" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,twosd,<span class="op">-</span>twosd,color<span class="op">=</span><span class="st">"gray"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="annotated-cell-2-13"><a href="#annotated-cell-2-13" aria-hidden="true" tabindex="-1"></a>xdraw<span class="op">=</span>np.array([<span class="fl">0.0</span>,<span class="fl">0.05</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">;</span> </span>
<span id="annotated-cell-2-14"><a href="#annotated-cell-2-14" aria-hidden="true" tabindex="-1"></a>y_all_draws<span class="op">=</span>gp.sample_y(xdraw,random_state<span class="op">=</span>rng,</span>
<span id="annotated-cell-2-15"><a href="#annotated-cell-2-15" aria-hidden="true" tabindex="-1"></a>                        n_samples<span class="op">=</span>n_samples)<span class="op">;</span> </span>
<span id="annotated-cell-2-16"><a href="#annotated-cell-2-16" aria-hidden="true" tabindex="-1"></a>marker_collection<span class="op">=</span>[<span class="st">"o"</span>,<span class="st">"^"</span>,<span class="st">"x"</span>]</span>
<span id="annotated-cell-2-17"><a href="#annotated-cell-2-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ydraw <span class="kw">in</span> <span class="bu">enumerate</span>(y_all_draws.T): </span>
<span id="annotated-cell-2-18"><a href="#annotated-cell-2-18" aria-hidden="true" tabindex="-1"></a>  plt.scatter(</span>
<span id="annotated-cell-2-19"><a href="#annotated-cell-2-19" aria-hidden="true" tabindex="-1"></a>    xdraw,ydraw,</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="3">3</button><span id="annotated-cell-2-20" class="code-annotation-target"><a href="#annotated-cell-2-20" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>marker_collection[np.mod(i,n_samples)])</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="4">4</button><span id="annotated-cell-2-21" class="code-annotation-target"><a href="#annotated-cell-2-21" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])</span>
<span id="annotated-cell-2-22"><a href="#annotated-cell-2-22" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))</span>
<span id="annotated-cell-2-23"><a href="#annotated-cell-2-23" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca()</span>
<span id="annotated-cell-2-24"><a href="#annotated-cell-2-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="annotated-cell-2-25"><a href="#annotated-cell-2-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span>
<span id="annotated-cell-2-26"><a href="#annotated-cell-2-26" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="st">"Multivariate normal draws; correlation decreases as x distance increases"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="4" data-code-annotation="1">set the kernel</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="11,12" data-code-annotation="2">coloring 2 std dev range</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="20" data-code-annotation="3">plotting the draws</span>
</dd>
<dt data-target-cell="annotated-cell-2" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="21,22,23,24,25,26" data-code-annotation="4">make the plot</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-draws-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-draws-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-draws-1-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;104.6: Multivariate normal 2 draws"><img src="C6-L01_files/figure-html/fig-demo-01-gp-draws-1-output-1.png" width="619" height="449" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-01-gp-draws-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.6: Multivariate normal 2 draws
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-demo-01-gp-draws-2" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="annotated-cell-3"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-3-1"><a href="#annotated-cell-3-1" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="annotated-cell-3-2"><a href="#annotated-cell-3-2" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="annotated-cell-3-3"><a href="#annotated-cell-3-3" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">1.0</span>,</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="1">1</button><span id="annotated-cell-3-4" class="code-annotation-target"><a href="#annotated-cell-3-4" aria-hidden="true" tabindex="-1"></a>length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>)</span>
<span id="annotated-cell-3-5"><a href="#annotated-cell-3-5" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="annotated-cell-3-6"><a href="#annotated-cell-3-6" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">100</span></span>
<span id="annotated-cell-3-7"><a href="#annotated-cell-3-7" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span></span>
<span id="annotated-cell-3-8"><a href="#annotated-cell-3-8" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span></span>
<span id="annotated-cell-3-9"><a href="#annotated-cell-3-9" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(</span>
<span id="annotated-cell-3-10"><a href="#annotated-cell-3-10" aria-hidden="true" tabindex="-1"></a>  start<span class="op">=</span>xstart,</span>
<span id="annotated-cell-3-11"><a href="#annotated-cell-3-11" aria-hidden="true" tabindex="-1"></a>  stop<span class="op">=</span>xstop,</span>
<span id="annotated-cell-3-12"><a href="#annotated-cell-3-12" aria-hidden="true" tabindex="-1"></a>  num<span class="op">=</span>numx)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="2">2</button><span id="annotated-cell-3-13" class="code-annotation-target"><a href="#annotated-cell-3-13" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx)</span>
<span id="annotated-cell-3-14"><a href="#annotated-cell-3-14" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,</span>
<span id="annotated-cell-3-15"><a href="#annotated-cell-3-15" aria-hidden="true" tabindex="-1"></a>  twosd,<span class="op">-</span>twosd,</span>
<span id="annotated-cell-3-16"><a href="#annotated-cell-3-16" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="annotated-cell-3-17"><a href="#annotated-cell-3-17" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="annotated-cell-3-18"><a href="#annotated-cell-3-18" aria-hidden="true" tabindex="-1"></a>xdraw<span class="op">=</span>np.array([<span class="fl">0.0</span>,<span class="fl">0.05</span>,<span class="fl">3.0</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="annotated-cell-3-19"><a href="#annotated-cell-3-19" aria-hidden="true" tabindex="-1"></a>y_all_draws<span class="op">=</span>gp.sample_y(</span>
<span id="annotated-cell-3-20"><a href="#annotated-cell-3-20" aria-hidden="true" tabindex="-1"></a>  xdraw,random_state<span class="op">=</span>rng,n_samples<span class="op">=</span><span class="dv">3</span>)</span>
<span id="annotated-cell-3-21"><a href="#annotated-cell-3-21" aria-hidden="true" tabindex="-1"></a>marker_collection<span class="op">=</span>[<span class="st">"o"</span>,<span class="st">"^"</span>,<span class="st">"x"</span>]</span>
<span id="annotated-cell-3-22"><a href="#annotated-cell-3-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ydraw <span class="kw">in</span> <span class="bu">enumerate</span>(y_all_draws.T): </span>
<span id="annotated-cell-3-23"><a href="#annotated-cell-3-23" aria-hidden="true" tabindex="-1"></a>  plt.scatter(</span>
<span id="annotated-cell-3-24"><a href="#annotated-cell-3-24" aria-hidden="true" tabindex="-1"></a>    xdraw,</span>
<span id="annotated-cell-3-25"><a href="#annotated-cell-3-25" aria-hidden="true" tabindex="-1"></a>    ydraw,</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="3">3</button><span id="annotated-cell-3-26" class="code-annotation-target"><a href="#annotated-cell-3-26" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>marker_collection[np.mod(i,<span class="dv">3</span>)])</span>
<span id="annotated-cell-3-27"><a href="#annotated-cell-3-27" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])</span>
<span id="annotated-cell-3-28"><a href="#annotated-cell-3-28" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="4">4</button><span id="annotated-cell-3-29" class="code-annotation-target"><a href="#annotated-cell-3-29" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca()</span>
<span id="annotated-cell-3-30"><a href="#annotated-cell-3-30" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="st">"Multivariate normal draws; correlation decreases as x distance increases"</span>) <span class="co"># &lt;4&gt;plot</span></span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="4" data-code-annotation="1">set the kernel</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="13,17" data-code-annotation="2">coloring 2 std dev range</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="26" data-code-annotation="3">plotting the draws</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="29" data-code-annotation="4">make the plot</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-draws-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-draws-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-draws-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;104.7: Multivariate normal three points unevenly spaced"><img src="C6-L01_files/figure-html/fig-demo-01-gp-draws-2-output-1.png" width="603" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-01-gp-draws-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.7: Multivariate normal three points unevenly spaced
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-demo-01-gp-draws-3" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(length_scale<span class="op">=</span><span class="fl">1.0</span>,length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="co"># set the kernel</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">200</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  twosd,<span class="op">-</span>twosd,color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>xdraw<span class="op">=</span>np.linspace(</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  num<span class="op">=</span>numx).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>y_all_draws<span class="op">=</span>gp.sample_y(</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  xdraw,random_state<span class="op">=</span>rng,</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  n_samples<span class="op">=</span><span class="dv">3</span>) </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>marker_collection<span class="op">=</span>[<span class="st">"o"</span>,<span class="st">"^"</span>,<span class="st">"x"</span>]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ydraw <span class="kw">in</span> <span class="bu">enumerate</span>(y_all_draws.T): </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  plt.scatter(xdraw,ydraw,marker<span class="op">=</span>marker_collection[np.mod(i,<span class="dv">3</span>)]) </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  plt.plot(xdraw,ydraw) <span class="co"># plotting the draws</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="st">"Multivariate normal draws; correlation decreases as x distance increases"</span>) <span class="co"># make the plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-draws-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-draws-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-draws-3-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;104.8: Multivariate normal 50 point evenly spaced"><img src="C6-L01_files/figure-html/fig-demo-01-gp-draws-3-output-1.png" width="603" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-01-gp-draws-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.8: Multivariate normal 50 point evenly spaced
</figcaption>
</figure>
</div>
</div>
</div>
<p>we just drew random functions from a type of “Gaussian process”!</p>
</section>
<section id="sec-gaussian-processes" class="level2" data-number="104.7">
<h2 data-number="104.7" class="anchored" data-anchor-id="sec-gaussian-processes"><span class="header-section-number">104.7</span> Gaussian processes</h2>
<ul>
<li>Definition: “A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.” – <span class="citation" data-cites="rasmussen2006gaussian">(<a href="#ref-rasmussen2006gaussian" role="doc-biblioref">Rasmussen and Williams 2006</a>)</span></li>
<li>E.g. the function is a collection indexed by input <span class="math inline">x</span>: <span class="math display">  
  f(x) \sim \mathcal{GP}(m, k)
</span></li>
<li>It is specified by its mean function and covariance function:
<ul>
<li>Mean function <span class="math inline">m(x)=E[f(x)]</span></li>
<li>Covariance function (a.k.a. kernel) <span class="math inline">k(x,x')=\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]</span></li>
<li>A common default (e.g.&nbsp;in software) is <span class="math inline">m(x)=0</span></li>
<li>One very commonly used covariance function is the squared exponential or radial basis function (RBF)</li>
<li>We’ll see a more general form later, but for now we’re using: <span class="math inline">k(x, x') = \sigma^2 \exp\left(-\frac{1}{2} \|x - x'\|^2\right)</span></li>
<li>For now, assume data is observed without noise</li>
</ul></li>
</ul>
<p>Let’s look at a draw from the Gaussian process, i.e.&nbsp;we are just generating synthetic data from the Gaussian process model.</p>
</section>
<section id="demo-2-draw-from-a-gaussian-process" class="level2" data-number="104.8">
<h2 data-number="104.8" class="anchored" data-anchor-id="demo-2-draw-from-a-gaussian-process"><span class="header-section-number">104.8</span> Demo 2: Draw from a Gaussian process</h2>
<div id="cell-fig-demo-01-gp-draws-4" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">1.0</span><span class="op">;</span> </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="co"># set the kernel</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>xobs <span class="op">=</span> np.array([<span class="fl">0.0</span>,<span class="fl">0.05</span>,<span class="fl">0.7</span>,<span class="fl">3.0</span>])<span class="op">;</span> </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>Nobs <span class="op">=</span> xobs.shape[<span class="dv">0</span>]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">50</span><span class="op">;</span> </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span><span class="op">;</span> </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span><span class="op">;</span> x<span class="op">=</span>np.linspace(</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)<span class="op">;</span> </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  x,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>  twosd,</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>  <span class="op">-</span>twosd,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>xcurve<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)<span class="op">;</span> </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>xall <span class="op">=</span> np.concatenate((xcurve,xobs)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>yall <span class="op">=</span> gp.sample_y(xall,random_state<span class="op">=</span>rng,n_samples<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span> </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>ycurve<span class="op">=</span>yall[<span class="dv">0</span>:numx]<span class="op">;</span> </span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>yobs<span class="op">=</span>yall[numx:numx<span class="op">+</span>Nobs]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>plt.plot(xcurve,ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,color<span class="op">=</span><span class="st">"blue"</span>)<span class="op">;</span> </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="st">"A draw from a Gaussian process with "</span> <span class="op">+</span> <span class="bu">str</span>(Nobs) <span class="op">+</span> <span class="st">" simulated data points"</span>) <span class="co"># make the plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-draws-4" class="quarto-float quarto-figure quarto-figure-center anchored" width="580" height="431">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-draws-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-draws-4-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;104.9: "><img src="C6-L01_files/figure-html/fig-demo-01-gp-draws-4-output-1.png" id="fig-demo-01-gp-draws-4" width="580" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-demo-01-gp-draws-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.9
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-demo-01-gp-draws-5" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># just the training data</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,twosd,<span class="op">-</span>twosd,color<span class="op">=</span><span class="st">"gray"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="bu">str</span>(Nobs) <span class="op">+</span> <span class="st">" simulated data points"</span>) <span class="co"># make the plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-draws-5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-draws-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-draws-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;104.10: A draw from a Gaussian process with 4 simulated data points"><img src="C6-L01_files/figure-html/fig-demo-01-gp-draws-5-output-1.png" width="580" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-01-gp-draws-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.10: A draw from a Gaussian process with 4 simulated data points
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="note-on-dimension-of-the-input" class="level2 page-columns page-full" data-number="104.9">
<h2 data-number="104.9" class="anchored" data-anchor-id="note-on-dimension-of-the-input"><span class="header-section-number">104.9</span> Note on “dimension” of the input</h2>
<!--placeholder image-->

<div class="no-row-height column-margin column-container"><div id="fig-nonparametrics-dimension-data" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" width="53mm">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonparametrics-dimension-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-SL05.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;104.11: "><img src="C6-SL05.png" id="fig-nonparametrics-dimension-data" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-nonparametrics-dimension-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.11
</figcaption>
</figure>
</div><div id="fig-nonparametrics-dimension-item" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides" width="53mm">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonparametrics-dimension-item-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-SL07.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;104.12: "><img src="C6-SL07.png" id="fig-nonparametrics-dimension-item" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-nonparametrics-dimension-item-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.12
</figcaption>
</figure>
</div></div>
<ul>
<li>Let’s be careful to separate two types of “dimension”
<ul>
<li>We’re using a superscript <span class="math inline">y^{(1)}</span> to denote (M or N) number of points in the space</li>
<li>We’ll use a subscript for the (D) different elements of a point’s vector</li>
</ul></li>
<li>Note: all of our real-life examples from the start had number of inputs D &gt; 1</li>
<li>D = 1 is much easier to visualize, but might not be representative.</li>
</ul>
</section>
<section id="inference-about-unknowns-given-data." class="level2" data-number="104.10">
<h2 data-number="104.10" class="anchored" data-anchor-id="inference-about-unknowns-given-data."><span class="header-section-number">104.10</span> Inference about Unknowns given data.</h2>
<ul>
<li><p>Let <span class="math inline">X</span> collect the N “training” data points (indexed 1 to N)</p></li>
<li><p>Let <span class="math inline">X'</span> collect the M “test” data points <span class="math inline">X: N\times D</span></p>
<ul>
<li>Where we want to evaluate the function</li>
<li>Indexed N+1:N+M</li>
</ul></li>
<li><p><span class="math inline">K(X,X')</span> os the <span class="math inline">N \times M</span> matrix with (n,m) entry <span class="math inline">k(x^{(n)}, x^{(N+m)})</span></p></li>
<li><p>Then by our model (dimension are annotated in blue) <span class="math display">
\begin{bmatrix}
\overbrace{f(X)}^{\color{blue}N\times 1} \\
\underbrace{f(X')}_{\color{blue}M\times 1}
\end{bmatrix}
\sim \mathcal{N}
\left(
  \underbrace{
    \begin{bmatrix}
    0_N \\
    0_M
    \end{bmatrix}}_{\color{blue}(N+M)\times 1 } ,
  \underbrace{
\begin{bmatrix}
  K(X, X) &amp; K(X, X') \\
  K(X', X) &amp; K(X', X')
\end{bmatrix}}_{
  \color{blue}(N+M)\times (N+M)
}
\right)
</span></p></li>
<li><p>The conditional satisfies <span class="math inline">f(X')\mid f(X), X, X' \sim \mathcal{N}</span></p>
<ul>
<li>Can compute mean (<span class="math inline">\color{blue}M\times 1</span>) &amp; covariance (<span class="math inline">\color{blue}M\times M</span>) in closed form with Gaussian facts</li>
</ul></li>
</ul>
<p><span class="math display">
\begin{align*}
k(x, x') = \sigma^2 \exp\left(-\tfrac{1}{2}(x - x')^2\right), \sigma^2 = 1
\end{align*}
</span></p>
<p>Now we can flip this around using the Bayesian machinery and use that to get information. When we don’t know the function, but we do know the data, which is real life. In real life You know the data, but you don’t know the function and you’re trying to learn that direction.</p>
<p>If we want to draw a curve we need M to be very large. And we will need to generate a <span class="math inline">M \times D</span> matrix of points.</p>
<p>So now what we’re going to do is we’re going to run Gaussian process regression on those points. And what do I mean by, say, running Gaussian process regression I’m literally talking about exactly what we just did on the slide together.</p>
<div id="cell-fig-demo-01-gp-predict" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="annotated-cell-7"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-7-1"><a href="#annotated-cell-7-1" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="1">1</button><span id="annotated-cell-7-2" class="code-annotation-target"><a href="#annotated-cell-7-2" aria-hidden="true" tabindex="-1"></a>  kernel<span class="op">=</span>kern, optimizer<span class="op">=</span><span class="va">None</span>)</span>
<span id="annotated-cell-7-3"><a href="#annotated-cell-7-3" aria-hidden="true" tabindex="-1"></a>gpfit.fit(xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="annotated-cell-7-4"><a href="#annotated-cell-7-4" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(</span>
<span id="annotated-cell-7-5"><a href="#annotated-cell-7-5" aria-hidden="true" tabindex="-1"></a>  xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="annotated-cell-7-6"><a href="#annotated-cell-7-6" aria-hidden="true" tabindex="-1"></a>  return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="annotated-cell-7-7"><a href="#annotated-cell-7-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(</span>
<span id="annotated-cell-7-8"><a href="#annotated-cell-7-8" aria-hidden="true" tabindex="-1"></a>  xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="annotated-cell-7-9"><a href="#annotated-cell-7-9" aria-hidden="true" tabindex="-1"></a>  marker<span class="op">=</span><span class="st">"x"</span>, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="annotated-cell-7-10"><a href="#annotated-cell-7-10" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="annotated-cell-7-11"><a href="#annotated-cell-7-11" aria-hidden="true" tabindex="-1"></a>  xcurve,ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,</span>
<span id="annotated-cell-7-12"><a href="#annotated-cell-7-12" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"original function"</span>)</span>
<span id="annotated-cell-7-13"><a href="#annotated-cell-7-13" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="annotated-cell-7-14"><a href="#annotated-cell-7-14" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred,color<span class="op">=</span><span class="st">"green"</span>, </span>
<span id="annotated-cell-7-15"><a href="#annotated-cell-7-15" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="st">"mean predictions"</span>)</span>
<span id="annotated-cell-7-16"><a href="#annotated-cell-7-16" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="annotated-cell-7-17"><a href="#annotated-cell-7-17" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="annotated-cell-7-18"><a href="#annotated-cell-7-18" aria-hidden="true" tabindex="-1"></a>  mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="annotated-cell-7-19"><a href="#annotated-cell-7-19" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>,label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="annotated-cell-7-20"><a href="#annotated-cell-7-20" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="annotated-cell-7-21"><a href="#annotated-cell-7-21" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="annotated-cell-7-22"><a href="#annotated-cell-7-22" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="annotated-cell-7-23"><a href="#annotated-cell-7-23" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Gaussian process predictions"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-7" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="2" data-code-annotation="1">NOT fitting kernel hyperparameters</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-predict" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-predict-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-predict-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;104.13: Gaussian process predictions"><img src="C6-L01_files/figure-html/fig-demo-01-gp-predict-output-1.png" width="580" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-01-gp-predict-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.13: Gaussian process predictions
</figcaption>
</figure>
</div>
</div>
</div>
<p>We’re saying hey jointly You can think of the joint distribution of the value of the value of the Gaussian process at all of these different X locations, all of these different horizontal locations.</p>
<p>What we would have is we would have everything else. In this plot we’d have The black X’s, that’s our observed data. So we totally know that.</p>
<ul>
<li>We have:
<ul>
<li><p>Our observed data, the black X’s, which is our training data.</p></li>
<li><p>Our predictions. that is the solid green line it is the mean of the Gaussian process at all of these different X locations away from the observations. We are creating it just like we created our draws or like we created our, our big prior Gaussian process before.</p></li>
<li><p>We’re saying, hey, what’s the mean? In this case, it’s non-trivial. It’s not just zero because it’s conditioned on the data.</p></li>
<li><p>We have our uncertainties, our plus or minus two standard deviation interval.</p></li>
</ul></li>
</ul>
<p>Now, before, this was pretty boring. When we were first generating things a priori when we didn’t have any data. The plus or minus two standard deviation interval is just plus or minus two everywhere. Now, because we’ve observed some data, it’s not trivial. We’re actually seeing that it differs depending on where we are in the space.</p>
<p>And I just want to point out some things that might be considered useful about this.</p>
<ol type="1">
<li>When I’m really near my data points
<ul>
<li>we see there’s very little uncertainty.</li>
<li>very little uncertainty going between them. Because this is basically the only way we can smoothly and quickly go between these two points.</li>
</ul></li>
<li>when we get much farther away we have a lot of uncertainty because there’s a good number of different ways that we could have gotten between these two points. It makes sense that our uncertainty would be larger As we get farther away.</li>
</ol>
<div id="cell-fig-demo-01-gp-predict-2" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="annotated-cell-8"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><button class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="1">1</button><span id="annotated-cell-8-1" class="code-annotation-target"><a href="#annotated-cell-8-1" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern, optimizer<span class="op">=</span><span class="va">None</span>)</span>
<span id="annotated-cell-8-2"><a href="#annotated-cell-8-2" aria-hidden="true" tabindex="-1"></a>gpfit.fit(xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="annotated-cell-8-3"><a href="#annotated-cell-8-3" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="annotated-cell-8-4"><a href="#annotated-cell-8-4" aria-hidden="true" tabindex="-1"></a>y_all_draws<span class="op">=</span>gpfit.sample_y(xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),random_state<span class="op">=</span>rng,n_samples<span class="op">=</span><span class="dv">3</span>)</span>
<span id="annotated-cell-8-5"><a href="#annotated-cell-8-5" aria-hidden="true" tabindex="-1"></a>color_options<span class="op">=</span>[<span class="st">"red"</span>,<span class="st">"blue"</span>,<span class="st">"orange"</span>]</span>
<span id="annotated-cell-8-6"><a href="#annotated-cell-8-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="annotated-cell-8-7"><a href="#annotated-cell-8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-8-8"><a href="#annotated-cell-8-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ydraw <span class="kw">in</span> <span class="bu">enumerate</span>(y_all_draws.T): </span>
<span id="annotated-cell-8-9"><a href="#annotated-cell-8-9" aria-hidden="true" tabindex="-1"></a>  plt.plot(xcurve,ydraw,label<span class="op">=</span><span class="st">"random draw"</span>,color<span class="op">=</span>color_options[np.mod(i,<span class="dv">3</span>)])</span>
<span id="annotated-cell-8-10"><a href="#annotated-cell-8-10" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="annotated-cell-8-11"><a href="#annotated-cell-8-11" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="annotated-cell-8-12"><a href="#annotated-cell-8-12" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"green"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>,label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="annotated-cell-8-13"><a href="#annotated-cell-8-13" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="annotated-cell-8-14"><a href="#annotated-cell-8-14" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="annotated-cell-8-15"><a href="#annotated-cell-8-15" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="annotated-cell-8-16"><a href="#annotated-cell-8-16" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Random draws of f from the predictive distribution"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="1" data-code-annotation="1">NOT fitting kernel hyperparameters</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-predict-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-predict-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-predict-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;104.14: Gaussian process predictions with noise"><img src="C6-L01_files/figure-html/fig-demo-01-gp-predict-2-output-1.png" width="580" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-01-gp-predict-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.14: Gaussian process predictions with noise
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now we have these four data points. And so we’ve constrained our random draws of the functions to go through the four data points.</p>
<p>And here are just three draws from it. Because again, it’s just a multivariate Gaussian is all that we’re doing here. We’re just making this multivariate Gaussian the same way we did before by having draws at many dense x values at many dense input values. And so we can do the same thing here. It’s just a different multivariate Gaussian.</p>
<p>This is meant to express Our uncertainty and our best guess, our best guess could be seen as this mean This posterior mean. So these are the mean predictions this solid green line. So that could be thought of as our best guess. And then our uncertainty around that</p>
<p>Is the plus or minus two standard deviation interval conditional on the data that we’ve seen. And now we’re going to dig a little bit more. Into some choices we’ve made and what are their implications.</p>
</section>
<section id="sec-squared-exponential-kernel-revisited" class="level2" data-number="104.11">
<h2 data-number="104.11" class="anchored" data-anchor-id="sec-squared-exponential-kernel-revisited"><span class="header-section-number">104.11</span> Squared exponential kernel revisited</h2>
<p>What if we happened to measure our data on a different scale?</p>
<p>We’ve been using this particular kernel:</p>
<p><span class="math display">
\begin{align*}
k(x, x') = \sigma^2 \exp\left(-\tfrac{1}{2}(x - x')^2\right), \sigma^2 = 1
\end{align*}
</span></p>
<ul>
<li>What do we expect from the scale of f(x) a priori?
<ul>
<li>At one x, with ~95% probability a priori,f(x) is in the interval <span class="math inline">[-2\sigma,2\sigma]</span>.</li>
<li>Marginal variance cannot increase with data</li>
</ul></li>
<li>What counts as “close” in x?</li>
</ul>
<p><span class="math display">
\exp(-\tfrac{1}{2}(2)^2) \approx 0.14 \quad \exp(-\tfrac{1}{3}(3)^2) \approx 0.11 \quad  \exp(-\tfrac{1}{2}(4)^2) \approx 0.0034 \qquad
</span></p>
<div id="cell-fig-demo-01-gp-predict-3" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="annotated-cell-9"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-9-1"><a href="#annotated-cell-9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># observed y values are 100 times what they were in the nice example</span></span>
<span id="annotated-cell-9-2"><a href="#annotated-cell-9-2" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="1">1</button><span id="annotated-cell-9-3" class="code-annotation-target"><a href="#annotated-cell-9-3" aria-hidden="true" tabindex="-1"></a>  kernel<span class="op">=</span>kern, optimizer<span class="op">=</span><span class="va">None</span>)</span>
<span id="annotated-cell-9-4"><a href="#annotated-cell-9-4" aria-hidden="true" tabindex="-1"></a>gpfit.fit(xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="annotated-cell-9-5"><a href="#annotated-cell-9-5" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="annotated-cell-9-6"><a href="#annotated-cell-9-6" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(</span>
<span id="annotated-cell-9-7"><a href="#annotated-cell-9-7" aria-hidden="true" tabindex="-1"></a>  xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="annotated-cell-9-8"><a href="#annotated-cell-9-8" aria-hidden="true" tabindex="-1"></a>  return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="annotated-cell-9-9"><a href="#annotated-cell-9-9" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,<span class="dv">100</span><span class="op">*</span>yobs,color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="annotated-cell-9-10"><a href="#annotated-cell-9-10" aria-hidden="true" tabindex="-1"></a>marker<span class="op">=</span><span class="st">"x"</span>, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="annotated-cell-9-11"><a href="#annotated-cell-9-11" aria-hidden="true" tabindex="-1"></a>plt.plot(xcurve,<span class="dv">100</span><span class="op">*</span>ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,</span>
<span id="annotated-cell-9-12"><a href="#annotated-cell-9-12" aria-hidden="true" tabindex="-1"></a>color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"original function"</span>)</span>
<span id="annotated-cell-9-13"><a href="#annotated-cell-9-13" aria-hidden="true" tabindex="-1"></a>plt.plot(xcurve,mean_pred,color<span class="op">=</span><span class="st">"green"</span>, </span>
<span id="annotated-cell-9-14"><a href="#annotated-cell-9-14" aria-hidden="true" tabindex="-1"></a>label<span class="op">=</span><span class="st">"mean predictions"</span>)</span>
<span id="annotated-cell-9-15"><a href="#annotated-cell-9-15" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="annotated-cell-9-16"><a href="#annotated-cell-9-16" aria-hidden="true" tabindex="-1"></a>  xcurve,</span>
<span id="annotated-cell-9-17"><a href="#annotated-cell-9-17" aria-hidden="true" tabindex="-1"></a>  mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="annotated-cell-9-18"><a href="#annotated-cell-9-18" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"green"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="annotated-cell-9-19"><a href="#annotated-cell-9-19" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="annotated-cell-9-20"><a href="#annotated-cell-9-20" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="annotated-cell-9-21"><a href="#annotated-cell-9-21" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="annotated-cell-9-22"><a href="#annotated-cell-9-22" aria-hidden="true" tabindex="-1"></a>plt.ylim(</span>
<span id="annotated-cell-9-23"><a href="#annotated-cell-9-23" aria-hidden="true" tabindex="-1"></a>  (<span class="op">-</span><span class="dv">100</span><span class="op">*</span><span class="dv">3</span><span class="op">*</span>signal_stddev, <span class="dv">100</span><span class="op">*</span><span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="annotated-cell-9-24"><a href="#annotated-cell-9-24" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Gaussian process predictions"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-9" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="3" data-code-annotation="1">NOT fitting kernel hyperparameters</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-predict-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-predict-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-predict-3-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;104.15: issues when we scale the y values by 100"><img src="C6-L01_files/figure-html/fig-demo-01-gp-predict-3-output-1.png" width="596" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-01-gp-predict-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.15: issues when we scale the y values by 100
</figcaption>
</figure>
</div>
</div>
</div>
<p>In this sample the <span class="math inline">+/-2 \sigma</span> confidence interval is now 100 times smaller than in the nice example, and the mean prediction is also 100 times smaller than in the nice example. So while it is still there it seems to have vanished.</p>
<div id="cell-fig-demo-01-gp-predict-4" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># observed x values are 100 times what they were in the nice example</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  kernel<span class="op">=</span>kern, optimizer<span class="op">=</span><span class="va">None</span>) <span class="co"># NOT fitting kernel hyperparameters</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>gpfit.fit(<span class="dv">100</span><span class="op">*</span>xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>xall<span class="op">=</span>np.sort(np.concatenate((xcurve,xobs)))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>xall.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="dv">100</span><span class="op">*</span>xobs,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>, </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>xall,mean_pred,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"green"</span>, label<span class="op">=</span><span class="st">"mean predictions"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>xcurve,ycurve,</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  linestyle<span class="op">=</span><span class="st">"dashed"</span>,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"blue"</span>, </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="st">"original function"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>xall,mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>  mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>,label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="dv">100</span><span class="op">*</span>xstart,<span class="dv">100</span><span class="op">*</span>xstop])<span class="op">;</span> </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Gaussian process predictions"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-demo-01-gp-predict-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-01-gp-predict-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-01-gp-predict-4-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;104.16: Issue when we scale the x values by 100"><img src="C6-L01_files/figure-html/fig-demo-01-gp-predict-4-output-1.png" width="582" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-01-gp-predict-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.16: Issue when we scale the x values by 100
</figcaption>
</figure>
</div>
</div>
</div>
<p>in this example the points are now so far apart that at most point on the graph we are quickly jumping to the prior and getting the maximum uncertainty.</p>
<div id="lst-estimation" class="cell listing quarto-float quarto-figure quarto-figure-left anchored" data-execution_count="11">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst quarto-uncaptioned" id="lst-estimation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;104.2
</figcaption>
<div aria-describedby="lst-estimation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span><span class="fl">2.0</span><span class="op">**</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.13533528323661267</code></pre>
</div>
</div>
</figure>
</div>
<ul>
<li>What can we do to handle different x and f(x) scales?
<ul>
<li>Normalization in y can help; in x, can still be hiccups</li>
</ul></li>
<li>A common option in practice and in software is to fit the hyperparameters of a more general squared exponential kernel from data
<ul>
<li>More general form of the squared exponential:</li>
</ul></li>
</ul>
<p><span id="eq-squared-exponential-kernel-general"><span class="math display">
k(x, x') = \underbrace{\sigma^2}_{\color{red}\text{signal variance}} \exp\left(-\frac{1}{2}\sum_{d=1}^D \frac{(x_d - x'_d)^2}{\underbrace{l_d^2}_{\color{red}\text{length scale}}}\right), \sigma^2 = 1
\tag{104.1}</span></span></p>
<ul>
<li><p>Parameters (here, f) parametrize the distribution of the data. If we knew them, we could generate the data.</p>
<ul>
<li>GPs: <em>nonparametric</em> model: infinite # of latent params</li>
</ul></li>
<li><p><em>Hyperparameters</em> parametrize the distribution of the parameters. If known, we could generate the parameters.</p></li>
<li><p>Algorithm:</p>
<ul>
<li>Fit a value for the hyperparameters using the data.</li>
<li>Given those values, now compute and report the mean and uncertainty intervals</li>
</ul></li>
</ul>
<section id="demo-3-learning-the-signal-and-the-length-scale-and" class="level3" data-number="104.11.1">
<h3 data-number="104.11.1" class="anchored" data-anchor-id="demo-3-learning-the-signal-and-the-length-scale-and"><span class="header-section-number">104.11.1</span> Demo 3 – Learning the signal and the length scale and</h3>
<div id="cell-fig-demo-03-gp-data-and-ground-truth" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">2.0</span><span class="op">;</span> </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="co"># set the kernel</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span><span class="op">;</span> </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span><span class="op">;</span> </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>Nobs <span class="op">=</span> <span class="dv">10</span><span class="op">;</span> </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>xobs <span class="op">=</span> rng.uniform(xstart,xstop,size<span class="op">=</span>Nobs)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">200</span><span class="op">;</span> </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)<span class="op">;</span> </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,twosd,<span class="op">-</span>twosd,color<span class="op">=</span><span class="st">"gray"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>xcurve<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)<span class="op">;</span> </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>xall <span class="op">=</span> np.concatenate((xcurve,xobs)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>yall <span class="op">=</span> gp.sample_y(xall,random_state<span class="op">=</span>rng,n_samples<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span> </span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>ycurve<span class="op">=</span>yall[<span class="dv">0</span>:numx]<span class="op">;</span> </span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>yobs<span class="op">=</span>yall[numx:numx<span class="op">+</span>Nobs]</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.plot(xcurve,ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,color<span class="op">=</span><span class="st">"blue"</span>)<span class="op">;</span> </span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>  <span class="st">"A draw from a Gaussian process with "</span> <span class="op">+</span> <span class="bu">str</span>(Nobs) <span class="op">+</span> </span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>  <span class="st">" simulated data points"</span>) <span class="co"># make the plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-demo-03-gp-data-and-ground-truth" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo-03-gp-data-and-ground-truth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo-03-gp-data-and-ground-truth-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;104.17: A draw from a Gaussian process with 10 simulated data points"><img src="C6-L01_files/figure-html/fig-demo-03-gp-data-and-ground-truth-output-1.png" width="580" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo-03-gp-data-and-ground-truth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.17: A draw from a Gaussian process with 10 simulated data points
</figcaption>
</figure>
</div>
</div>
</div>
<p>we have drawn 10 points and the ground truth</p>
<p>but what goes to the algorithm is just the data points as follows:</p>
<div id="cell-fig-demo3-gp-data" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># just the training data</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="bu">str</span>(Nobs) <span class="op">+</span> <span class="st">" simulated data points"</span>) <span class="co"># make the plot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-demo3-gp-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo3-gp-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo3-gp-data-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure&nbsp;104.18: the data"><img src="C6-L01_files/figure-html/fig-demo3-gp-data-output-1.png" width="580" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo3-gp-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.18: the data
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-demo3-gp-algorithm" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="annotated-cell-14"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-14-1"><a href="#annotated-cell-14-1" aria-hidden="true" tabindex="-1"></a>signal_stddev_init <span class="op">=</span> <span class="fl">1.0</span><span class="op">;</span> </span>
<span id="annotated-cell-14-2"><a href="#annotated-cell-14-2" aria-hidden="true" tabindex="-1"></a>kern_fit <span class="op">=</span> signal_stddev_init<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="annotated-cell-14-3"><a href="#annotated-cell-14-3" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">1.0</span>,length_scale_bounds<span class="op">=</span>(<span class="fl">0.01</span>, <span class="dv">100</span>))</span>
<span id="annotated-cell-14-4"><a href="#annotated-cell-14-4" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-14" data-target-annotation="1">1</button><span id="annotated-cell-14-5" class="code-annotation-target"><a href="#annotated-cell-14-5" aria-hidden="true" tabindex="-1"></a>  kernel<span class="op">=</span>kern_fit,n_restarts_optimizer<span class="op">=</span><span class="dv">10</span>)</span>
<span id="annotated-cell-14-6"><a href="#annotated-cell-14-6" aria-hidden="true" tabindex="-1"></a>gpfit.fit(xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))<span class="op">;</span></span>
<span id="annotated-cell-14-7"><a href="#annotated-cell-14-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original kernel: "</span> <span class="op">+</span> <span class="bu">str</span>(kern))<span class="op">;</span> </span>
<span id="annotated-cell-14-8"><a href="#annotated-cell-14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fit kernel: "</span> <span class="op">+</span> <span class="bu">str</span>(gpfit.kernel_))</span>
<span id="annotated-cell-14-9"><a href="#annotated-cell-14-9" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(</span>
<span id="annotated-cell-14-10"><a href="#annotated-cell-14-10" aria-hidden="true" tabindex="-1"></a>  xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="annotated-cell-14-11"><a href="#annotated-cell-14-11" aria-hidden="true" tabindex="-1"></a>plt.scatter(</span>
<span id="annotated-cell-14-12"><a href="#annotated-cell-14-12" aria-hidden="true" tabindex="-1"></a>  xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="annotated-cell-14-13"><a href="#annotated-cell-14-13" aria-hidden="true" tabindex="-1"></a>  marker<span class="op">=</span><span class="st">"x"</span>, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="annotated-cell-14-14"><a href="#annotated-cell-14-14" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="annotated-cell-14-15"><a href="#annotated-cell-14-15" aria-hidden="true" tabindex="-1"></a>  xcurve,ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,</span>
<span id="annotated-cell-14-16"><a href="#annotated-cell-14-16" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"original function"</span>)</span>
<span id="annotated-cell-14-17"><a href="#annotated-cell-14-17" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="annotated-cell-14-18"><a href="#annotated-cell-14-18" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred,color<span class="op">=</span><span class="st">"green"</span>, </span>
<span id="annotated-cell-14-19"><a href="#annotated-cell-14-19" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="st">"mean predictions"</span>)</span>
<span id="annotated-cell-14-20"><a href="#annotated-cell-14-20" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="annotated-cell-14-21"><a href="#annotated-cell-14-21" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="annotated-cell-14-22"><a href="#annotated-cell-14-22" aria-hidden="true" tabindex="-1"></a>  mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="annotated-cell-14-23"><a href="#annotated-cell-14-23" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"green"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="annotated-cell-14-24"><a href="#annotated-cell-14-24" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="annotated-cell-14-25"><a href="#annotated-cell-14-25" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="annotated-cell-14-26"><a href="#annotated-cell-14-26" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="annotated-cell-14-27"><a href="#annotated-cell-14-27" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="annotated-cell-14-28"><a href="#annotated-cell-14-28" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Gaussian process predictions"</span>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-14" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-14" data-code-lines="5" data-code-annotation="1">now we ARE fitting the hyperparameters</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/work/bayesian-specilization/env/lib/python3.10/site-packages/sklearn/gaussian_process/_gpr.py:659: ConvergenceWarning:

lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Original kernel: 2**2 * RBF(length_scale=0.3)
Fit kernel: 2.09**2 * RBF(length_scale=0.3)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-demo3-gp-algorithm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-demo3-gp-algorithm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-L01_files/figure-html/fig-demo3-gp-algorithm-output-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure&nbsp;104.19: GP scaled by the algorithm"><img src="C6-L01_files/figure-html/fig-demo3-gp-algorithm-output-3.png" width="580" height="431" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-demo3-gp-algorithm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.19: GP scaled by the algorithm
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="what-next" class="level2" data-number="104.12">
<h2 data-number="104.12" class="anchored" data-anchor-id="what-next"><span class="header-section-number">104.12</span> What next</h2>
<ul>
<li>adding observation noise variance</li>
<li>extrapolation</li>
<li>high-dimensional inputs</li>
</ul>
<p>Always ask: What uncertainty are we quantifying? <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</section>
<section id="observation-noise" class="level2" data-number="104.13">
<h2 data-number="104.13" class="anchored" data-anchor-id="observation-noise"><span class="header-section-number">104.13</span> Observation noise</h2>
<ul>
<li>So far we’ve been assuming that we observed f(x) directly</li>
<li>But often the actual observation y has additional noise:</li>
</ul>
<p><span class="math display">
f \sim \mathcal{GP}(m, k) \quad y^{(n)} = f(x) + \epsilon^{(n)}, \qquad \epsilon^{(n)} \stackrel{iid}{\sim} \mathcal{N}(0, \tau^2)
</span></p>
<ul>
<li>we observe <span class="math inline">\{(x^{(n)}, y^{(n)})\}_{n=1}^N</span> and want to learn the latent <span class="math inline">f</span>
<ul>
<li>The <span class="math inline">y</span> are multivariate-Gaussian distributed with mean and covariance summed.</li>
<li>The mean of <span class="math inline">y^{(n)}</span> is given by the function <span class="math inline">m(x^{(n)})</span>. and <span class="math inline">Cov(y^{(n)},y^{(n')})=k(x^{(n)},x^{(n')})+\tau^2\mathbb{1}_{n=n'}</span></li>
<li>The covariance is given by the kernel function:</li>
</ul></li>
</ul>
<p><span class="math display">
y \sim \mathcal{N}(f(x), \tau^2 I)
</span></p>
<p>Before: <span class="math display">
\begin{bmatrix}
f(X) \\
f(X')
\end{bmatrix}
\sim \mathcal{N}
  \left(    
    \begin{bmatrix}
      0_N \\
      0_M
      \end{bmatrix}
    \begin{bmatrix}
    K(X, X) &amp; K(X, X') \\
    K(X', X) &amp; K(X', X')
    \end{bmatrix}    
  \right)
</span> Now :</p>
<p><span class="math display">
\begin{bmatrix}
y^{(1:N)} \\
f(X')
\end{bmatrix}
\sim \mathcal{N}
  \left(    
    \begin{bmatrix}
      0_N \\
      0_M
      \end{bmatrix}
    \begin{bmatrix}
    K(X, X)+ \tau \mathbb{I} &amp; K(X, X') \\
    K(X', X) &amp; K(X', X')
    \end{bmatrix}    
  \right)
</span></p>
</section>
<section id="what-uncertainty-are-we-quantifying" class="level2 page-columns page-full" data-number="104.14">
<h2 data-number="104.14" class="anchored" data-anchor-id="what-uncertainty-are-we-quantifying"><span class="header-section-number">104.14</span> What uncertainty are we quantifying?</h2>
<ul>
<li>It’s worth being aware that data science (ML/stats/AI) often overloads common colloquial terms with terms of art
<ul>
<li>E.g. “significance”, “bias”, “generalization”</li>
<li>Every precise use of “uncertainty” has this issue
<ul>
<li>E.g. frequentist sampling, Bayesian, etc.</li>
</ul></li>
<li>We should always make sure we can distinguish what is, and what is not, covered by the term of art</li>
</ul></li>
<li>A standard setup (our setup so far):
<ul>
<li>We model the data as generated according to a GP with squared exponential kernel and observation noise</li>
<li>We fit the hyperparameters (the signal variance, the length scale(s), and the noise variance) to single values</li>
<li>The reported uncertainties are what result when the GP model and fitted hyperparameters are exactly correct</li>
</ul>
</li>
</ul>
<div class="no-row-height column-margin column-container"><span class="">Are there other uncertainties that aren’t being quantified here?</span></div></section>
<section id="sec-other-sources-of-uncertainty" class="level2" data-number="104.15">
<h2 data-number="104.15" class="anchored" data-anchor-id="sec-other-sources-of-uncertainty"><span class="header-section-number">104.15</span> Some other sources of uncertainty</h2>
<ul>
<li>There may be multiple sets of substantively different hyperparameter values that are both plausible and consistent with the observed data</li>
<li>What can we do? First: unit test, plot, sense check!
<ul>
<li>Ask what is possible to learn with the data available</li>
<li>Multiple random restarts: plot the results</li>
<li>Bayesian model of the hyperparameters</li>
</ul></li>
<li>A GP with your mean &amp; kernel may be meaningfully misspecified for the data
<ul>
<li>Box: “All models are wrong, but some are useful”</li>
<li>What can we do? First: unit test, plot, sense check!
<ul>
<li>Can change the mean and/or kernel
<ul>
<li>E.g. local/heteroskedastic models, periodic kernels, linear mean function, many many more</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="sec-extrapolation" class="level2" data-number="104.16">
<h2 data-number="104.16" class="anchored" data-anchor-id="sec-extrapolation"><span class="header-section-number">104.16</span> Extrapolation</h2>
<p>Extrapolation: Estimation/prediction beyond the observed data • Compare to interpolation: estimation/prediction within the observed data • When using GPs with a squared exponential kernel: • Data points that are more than a handful of length scales from other data points will revert to prior behavior • Note: extrapolation isn’t a special issue unique to GPs. It’s a fundamentally hard problem for all data analysis methods • To extrapolate, you need to make assumptions • When you have domain knowledge of a system, you might be able to use it to extrapolate • When you’re letting a machine learning method use its defaults, it’s making assumptions. Do you know what those assumptions are?</p>
</section>
<section id="more-than-one-input" class="level2 page-columns page-full" data-number="104.17">
<h2 data-number="104.17" class="anchored" data-anchor-id="more-than-one-input"><span class="header-section-number">104.17</span> More than one input</h2>

<div class="no-row-height column-margin column-container"><div id="fig-multiple-inputs" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multiple-inputs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C6-SL09.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;104.20: histogram of squared inter-point distances"><img src="C6-SL09.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multiple-inputs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;104.20: histogram of squared inter-point distances
</figcaption>
</figure>
</div></div><ul>
<li>Our illustrations have almost all been for one input so far</li>
<li>But in real life, it’s typical to have more than one input</li>
<li>What could go wrong? Previous lessons apply, but also:
<ul>
<li>Possibly different length scales. Check defaults.</li>
<li>Regression in high dimensions is a fundamentally hard problem (without additional assumptions)</li>
</ul></li>
<li>All points are “far away” in high dimensions. Illustration:</li>
<li>Uniformly randomly sample 10,000 points on <span class="math inline">[0,1]^D</span></li>
<li>Make a histogram of squared inter-point distances</li>
<li>Recall: points “far” from data default to the prior mean and variance.</li>
</ul>
</section>
<section id="sec-high-points" class="level2" data-number="104.18">
<h2 data-number="104.18" class="anchored" data-anchor-id="sec-high-points"><span class="header-section-number">104.18</span> Some high points of what got cut for time</h2>
<ul>
<li>We ran out of time! Here are some high-level summary points beyond what we discussed together:
<ul>
<li>There are other challenges with many inputs, both conceptual and practical</li>
<li>Running time for GP regression can be an issue with a large number of training data points
<ul>
<li>In particular, the matrix inverse can be expensive</li>
<li>There are incredibly many papers about fast approximations to the exact Gaussian process
<ul>
<li>Each approximation has pros and cons</li>
</ul></li>
</ul></li>
</ul></li>
<li>Bayesian optimization inherits many of the pros and cons of Gaussian processes for regression
<ul>
<li>Exercise: once you learn about Bayesian optimization, think about how the pros and cons we discussed together might translate there</li>
</ul></li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-rasmussen2006gaussian" class="csl-entry" role="listitem">
Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Adaptive Computation and Machine Learning Series. MIT Press. <a href="https://gaussianprocess.org/gpml/">https://gaussianprocess.org/gpml/</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>This is key as in real life there are many sources of uncertainty and in our models we only get to quantify one!<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C6-L00.html" class="pagination-link" aria-label="Bayesian Non-Parametric Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Bayesian Non-Parametric Models</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C6-L02.html" class="pagination-link" aria-label="Dirichlet Process">
        <span class="nav-page-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Dirichlet Process</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2025-07-02</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Gaussian Processes for Regression"</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Bayesian Statistics - Nonparametric Methods"</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Gaussian Processes Tutorial"</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - Bayesian Statistics</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - Nonparametric Methods</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - Gaussian Processes</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - Kriging</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co">  - Optimal Interpolation</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## Credit</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>the following material is based on the <span class="co">[</span><span class="ot">Gaussian Processes for Regression</span><span class="co">](https://tamarabroderick.com/tutorial_2024_cps-fr.html)</span> and tutorial and code by Tamara Broderick. [](https://tamarabroderick.com/tutorial_2025_asa_cirs_webinar.html) Note that the <span class="co">[</span><span class="ot">code</span><span class="co">](https://github.com/tbroderick/gps_tutorial/blob/2025asa_cirs/demos.ipynb)</span> is under the <span class="co">[</span><span class="ot">MIT license</span><span class="co">]()</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why GPs</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>::: {#fig-nonparametrics-gps1}</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="al">![why GP1](C6-SL01.png)</span>{ .column-margin group="slides" width="53mm"}</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>ocean current vector field <span class="co">[</span><span class="ot">Ryan, Özgökmen 2023; Zewe 2023; Gonçalves et al 2019; Lodise et al 2020; Berlinghieri et al 2023</span><span class="co">]</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>::: {#fig-nonparametrics-gps2}</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="al">![why GP2](C6-SL02.png)</span>{ .column-margin group="slides" width="53mm"}</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>rocket booster surrogate model <span class="co">[</span><span class="ot">Gramacy,Lee 2008</span><span class="co">] [Gramacy 2020]</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Why Gaussian processes (GPs)?</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example 1:</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The ocean current (velocity vector field) varies by space &amp; time</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Scientists get sparse observations of the current from buoys </span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Goal: estimate the current</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>c.f. <span class="co">[</span><span class="ot">Ryan, Özgökmen 2023; Zewe 2023; Gonçalves et al 2019; Lodise et al 2020; Berlinghieri et al 2023</span><span class="co">]</span></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Challenges:</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Sparse &amp; expensive data, not on a grid</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Current is highly nonlinear but smooth in space-time</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We want uncertainty quantification</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>c.f. </span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Gramacy,Lee 2008</span><span class="co">]</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Gramacy 2020</span><span class="co">]</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example 2: Surrogate model</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The lift force of a rocket booster varies as a function of </span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>speed at re-entry, </span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>angle of attack, and </span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>sideslip angle</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Scientists can run expensive simulations at chosen input settings</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Goal: estimate how lift varies as a function of input settings</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example 3: learn (&amp; optimize) performance in machine learning as a function of tuning parameters</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>c.f.  <span class="co">[</span><span class="ot">Snoek et al 2012; 2015; Garnett 2023</span><span class="co">]</span></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Motif: </span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Sparse, noisy, costly but but smooth data.</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Output may have a nonlinear relationship to the inputs.</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Want uncertainty quantification.</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Low-dimensional inputs.</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bonus benefits: </span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Ease of use (software, tuning) </span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Supports optimization of outcome </span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Predictions &amp; uncertainties over derivatives &amp; integrals </span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Module in more-complex methods</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>::: {.callout-important icon=false}</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ⚡ Overthinking parameters in Bayesian non-parametric models</span></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Let's talk about the elephant in the room non-parametrics do have parameters!</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The classical form of a distribution based on a few fixed number parameters is replaced in these by a notion similar to a euclidean line which can be arbitrary extended. This is the nature of the **non-parametric** parameter vector $\theta$.</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>To make the analogy more precise and what the above and many examples illustrate is that we often are interested in some behavior Y but as we get more and more data X, we will notice that there are new types of behavior that we did not observe earlier.</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a><span class="fu">## More Examples:</span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>new species are being daily</span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>new friend set appear as social networks expand</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>new entities appear in images as the data set expands</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>new wikipedia articles are discovered as we click on wikilinks</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>new alleles are identified as we sequence more genomes</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a><span class="fu">## Roadmap</span></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A Bayesian approach</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What is a Gaussian process?</span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Popular version using a squared exponential kernel</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gaussian process inference.</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Prediction &amp; uncertainty quantification.</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Goal:</span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Learn the mechanism behind standard GPs to identify benefits and pitfalls</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Bayesian approach</span></span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a><span class="al">![posterior](C6-SL04.png)</span>{#fig-nonparametics-posterior .column-margin group="slides" width="53mm"}</span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="al">![joint](C6-SL05.png)</span>{#fig-nonparametics-joint .column-margin group="slides" width="53mm"}</span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>\underbrace{</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a>  \mathbb{P}r(unknowns \mid data) }_{</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a>    \substack{</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a>      \text{Given the data we’ve seen} <span class="sc">\\</span> </span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>      \text{what do we know about } <span class="sc">\\</span> </span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>      \text{the underlying function?} </span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>\propto </span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a>\underbrace{</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>  \mathbb{P}r(data \mid unknowns) </span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>  \mathbb{P}r(unknowns)}_{  </span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>    \substack{</span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>      \text{A (statistical) model} <span class="sc">\\</span></span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>      \text{that can generate} <span class="sc">\\</span></span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>      \text{functions and data} <span class="sc">\\</span></span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a>      \text{of interest}</span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multivariate Gaussian using locations {#sec-multivariate-gaussian-locations}</span></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a><span class="al">![](C6-SL06.png)</span>{#fig-nonparametrics-multivariate-gaussian-locations .column-margin group="slides" width="53mm"}</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$M =2$ bivariate Gaussian $<span class="co">[</span><span class="ot">y^{(1)} y^{(2)}</span><span class="co">]</span> \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>with $\mu = <span class="co">[</span><span class="ot">0,0</span><span class="co">]</span>^\top$ and $k = \sigma^2 \begin{bmatrix} 1 &amp; \rho <span class="sc">\\</span> \rho &amp; 1\end{bmatrix}$ where $\rho$ is the correlation between the two dimensions.</span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What if we let the correlation $\rho$ depend on the input $x$?</span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Let $\rho(x)= \rho(\mid x^{(1)} - x^{(2)} \mid)$</span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Where the correlation goes to 1 as the x’s get close</span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>And goes to 0 as the x’s get far</span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Next: Use a Similar setup but an M-long Gaussian instead of just a bivariate</span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We have M locations (need not be in order): $X^{(m)} \in \mathbb{R}^d$</span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We generate $<span class="co">[</span><span class="ot">y^{(1)}, \ldots,y^{(m)}</span><span class="co">]</span>^\top \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{K})$</span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>with</span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a>    $\mu=0_M$ and </span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a>    $K$ such that $\mathbf{K}_{ij} = \sigma^2 \rho\mid (x^{(i)}, x^{(j)}\mid)$</span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Let's try $\rho(\Delta)=\exp(-\tfrac{1}{2} \Delta^2)$</span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a><span class="ss">      - </span>$\rho(0)=1$ , </span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a><span class="ss">      - </span>$\lim_{\Delta \to \infty} \rho(\Delta)=0$ and </span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a><span class="ss">      - </span>$\rho(\Delta)$ is monotonically decreasing</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Demo 1 : Draws from a multivariate normal distribution</span></span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lst-demo-01-setup</span></span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.RandomState()</span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process <span class="im">import</span> GaussianProcessRegressor</span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.gaussian_process.kernels <span class="im">import</span> RBF</span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-draws-1</span></span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Multivariate normal 2 draws"</span></span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>  length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">100</span></span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span></span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span></span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)</span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx) <span class="co"># &lt;2&gt;</span></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,twosd,<span class="op">-</span>twosd,color<span class="op">=</span><span class="st">"gray"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># &lt;2&gt;</span></span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a>xdraw<span class="op">=</span>np.array([<span class="fl">0.0</span>,<span class="fl">0.05</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">;</span> </span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a>y_all_draws<span class="op">=</span>gp.sample_y(xdraw,random_state<span class="op">=</span>rng,</span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a>                        n_samples<span class="op">=</span>n_samples)<span class="op">;</span> </span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>marker_collection<span class="op">=</span>[<span class="st">"o"</span>,<span class="st">"^"</span>,<span class="st">"x"</span>]</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ydraw <span class="kw">in</span> <span class="bu">enumerate</span>(y_all_draws.T): </span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a>  plt.scatter(</span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a>    xdraw,ydraw,</span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>marker_collection[np.mod(i,n_samples)]) <span class="co"># &lt;3&gt;</span></span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop]) <span class="co"># &lt;4&gt;</span></span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev)) <span class="co"># &lt;4&gt;</span></span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca() <span class="co"># &lt;4&gt;</span></span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>) <span class="co"># &lt;4&gt;</span></span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)  <span class="co"># &lt;4&gt;</span></span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="st">"Multivariate normal draws; correlation decreases as x distance increases"</span>) <span class="co"># &lt;4&gt;</span></span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>set the kernel</span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>coloring 2 std dev range</span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>plotting the draws</span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>make the plot</span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-draws-2</span></span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Multivariate normal three points unevenly spaced"</span></span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a>length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">100</span></span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span></span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span></span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(</span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a>  start<span class="op">=</span>xstart,</span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a>  stop<span class="op">=</span>xstop,</span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a>  num<span class="op">=</span>numx)</span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx) <span class="co"># &lt;2&gt;</span></span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a>  twosd,<span class="op">-</span>twosd,</span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># &lt;2&gt;</span></span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a>xdraw<span class="op">=</span>np.array([<span class="fl">0.0</span>,<span class="fl">0.05</span>,<span class="fl">3.0</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a>y_all_draws<span class="op">=</span>gp.sample_y(</span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a>  xdraw,random_state<span class="op">=</span>rng,n_samples<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a>marker_collection<span class="op">=</span>[<span class="st">"o"</span>,<span class="st">"^"</span>,<span class="st">"x"</span>]</span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ydraw <span class="kw">in</span> <span class="bu">enumerate</span>(y_all_draws.T): </span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a>  plt.scatter(</span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a>    xdraw,</span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a>    ydraw,</span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span>marker_collection[np.mod(i,<span class="dv">3</span>)]) <span class="co"># &lt;3&gt;</span></span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])</span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))</span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca() <span class="co"># &lt;4&gt;</span></span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="st">"Multivariate normal draws; correlation decreases as x distance increases"</span>) <span class="co"># &lt;4&gt;plot</span></span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>set the kernel</span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>coloring 2 std dev range</span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>plotting the draws</span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>make the plot</span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-draws-3</span></span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Multivariate normal 50 point evenly spaced"</span></span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(length_scale<span class="op">=</span><span class="fl">1.0</span>,length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="co"># set the kernel</span></span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">200</span></span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span></span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span></span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)</span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,</span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a>  twosd,<span class="op">-</span>twosd,color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a>xdraw<span class="op">=</span>np.linspace(</span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a>  start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,</span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a>  num<span class="op">=</span>numx).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a>y_all_draws<span class="op">=</span>gp.sample_y(</span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a>  xdraw,random_state<span class="op">=</span>rng,</span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a>  n_samples<span class="op">=</span><span class="dv">3</span>) </span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a>marker_collection<span class="op">=</span>[<span class="st">"o"</span>,<span class="st">"^"</span>,<span class="st">"x"</span>]</span>
<span id="cb12-273"><a href="#cb12-273" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ydraw <span class="kw">in</span> <span class="bu">enumerate</span>(y_all_draws.T): </span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a>  plt.scatter(xdraw,ydraw,marker<span class="op">=</span>marker_collection[np.mod(i,<span class="dv">3</span>)]) </span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a>  plt.plot(xdraw,ydraw) <span class="co"># plotting the draws</span></span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])</span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))</span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="st">"Multivariate normal draws; correlation decreases as x distance increases"</span>) <span class="co"># make the plot</span></span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a>we just drew random functions from a type of "Gaussian process"!</span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gaussian processes {#sec-gaussian-processes}</span></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Definition: "A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution." -- <span class="co">[</span><span class="ot">@rasmussen2006gaussian</span><span class="co">]</span></span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>E.g. the function is a collection indexed by input $x$:</span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a>  $$  </span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a>    f(x) \sim \mathcal{GP}(m, k)</span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It is specified by its mean function and covariance function:</span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Mean function $m(x)=E<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>$</span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Covariance function (a.k.a. kernel) $k(x,x')=\mathbb{E}<span class="co">[</span><span class="ot">(f(x) - m(x))(f(x') - m(x'))</span><span class="co">]</span>$</span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a><span class="ss">  -  </span>A common default (e.g. in software) is $m(x)=0$</span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>One very commonly used covariance function is the squared exponential or radial basis function (RBF)</span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We’ll see a more general form later, but for now we’re using: $k(x, x') = \sigma^2 \exp\left(-\frac{1}{2} <span class="sc">\|</span>x - x'<span class="sc">\|</span>^2\right)$</span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>For now, assume data is observed without noise</span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a>Let's look at a draw from the Gaussian process, i.e. we are just generating synthetic data from the Gaussian process model.</span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a><span class="fu">## Demo 2: Draw from a Gaussian process</span></span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-draws-4</span></span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">1.0</span><span class="op">;</span> </span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a>  length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="co"># set the kernel</span></span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a>xobs <span class="op">=</span> np.array([<span class="fl">0.0</span>,<span class="fl">0.05</span>,<span class="fl">0.7</span>,<span class="fl">3.0</span>])<span class="op">;</span> </span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a>Nobs <span class="op">=</span> xobs.shape[<span class="dv">0</span>]</span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">50</span><span class="op">;</span> </span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span><span class="op">;</span> </span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span><span class="op">;</span> x<span class="op">=</span>np.linspace(</span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a>  start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)<span class="op">;</span> </span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a>  x,</span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a>  twosd,</span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a>  <span class="op">-</span>twosd,</span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a>xcurve<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)<span class="op">;</span> </span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a>xall <span class="op">=</span> np.concatenate((xcurve,xobs)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a>yall <span class="op">=</span> gp.sample_y(xall,random_state<span class="op">=</span>rng,n_samples<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span> </span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a>ycurve<span class="op">=</span>yall[<span class="dv">0</span>:numx]<span class="op">;</span> </span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a>yobs<span class="op">=</span>yall[numx:numx<span class="op">+</span>Nobs]</span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a>plt.plot(xcurve,ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,color<span class="op">=</span><span class="st">"blue"</span>)<span class="op">;</span> </span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>)</span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="st">"A draw from a Gaussian process with "</span> <span class="op">+</span> <span class="bu">str</span>(Nobs) <span class="op">+</span> <span class="st">" simulated data points"</span>) <span class="co"># make the plot</span></span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-draws-5</span></span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A draw from a Gaussian process with 4 simulated data points"</span></span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a><span class="co"># just the training data</span></span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,twosd,<span class="op">-</span>twosd,color<span class="op">=</span><span class="st">"gray"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>)</span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="bu">str</span>(Nobs) <span class="op">+</span> <span class="st">" simulated data points"</span>) <span class="co"># make the plot</span></span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a><span class="fu">## Note on "dimension" of the input</span></span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--placeholder image--&gt;</span></span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a><span class="al">![](C6-SL05.png)</span>{#fig-nonparametrics-dimension-data .column-margin group="slides" width="53mm"}</span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a><span class="al">![](C6-SL07.png)</span>{#fig-nonparametrics-dimension-item .column-margin group="slides" width="53mm"}</span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Let’s be careful to separate two types of “dimension” </span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We’re using a superscript $y^{(1)}$ to denote (M or N) number of points in the space</span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We’ll use a subscript for the (D) different elements of a point’s vector</span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Note: all of our real-life examples from the start had number of inputs D &gt; 1</span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>D = 1 is much easier to visualize, but might not be representative.</span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference about Unknowns given data.</span></span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Let $X$ collect the N "training" data points (indexed 1 to N)</span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Let $X'$ collect the M "test" data points $X: N\times D$</span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Where we want to evaluate the function </span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Indexed N+1:N+M</span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$K(X,X')$ os the $N \times M$ matrix with (n,m) entry $k(x^{(n)}, x^{(N+m)})$</span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Then by our model (dimension are annotated in blue)</span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a>\overbrace{f(X)}^{\color{blue}N\times 1} <span class="sc">\\</span> </span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a>\underbrace{f(X')}_{\color{blue}M\times 1}</span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} </span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a>\sim \mathcal{N}</span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a>  \left(</span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a>    \underbrace{</span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a>      \begin{bmatrix}</span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a>      0_N <span class="sc">\\</span> </span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a>      0_M</span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a>      \end{bmatrix}}_{\color{blue}(N+M)\times 1 } ,</span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a>    \underbrace{</span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a>  \begin{bmatrix}</span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a>    K(X, X) &amp; K(X, X') <span class="sc">\\</span></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a>    K(X', X) &amp; K(X', X')</span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a>  \end{bmatrix}}_{</span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a>    \color{blue}(N+M)\times (N+M)</span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a>\right) </span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The conditional satisfies $f(X')\mid f(X), X, X' \sim \mathcal{N}$</span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Can compute mean ($\color{blue}M\times 1$) &amp; covariance ($\color{blue}M\times M$) in closed form with Gaussian facts</span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a>k(x, x') = \sigma^2 \exp\left(-\tfrac{1}{2}(x - x')^2\right), \sigma^2 = 1</span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a>Now we can flip this around using the Bayesian machinery and use that to get information. When we don't know the function, but we do know the data, which is real life. In real life You know the data, but you don't know the function and you're trying to learn that direction.</span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a>If we want to draw a curve we need M to be very large. And we will need to generate a  $M \times D$ matrix of points. </span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a>So now what we're going to do is we're going to run Gaussian process regression on those points. And what do I mean by, say, running Gaussian process regression I'm literally talking about exactly what we just did on the slide together.</span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-predict</span></span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Gaussian process predictions"</span></span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(</span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a>  kernel<span class="op">=</span>kern, optimizer<span class="op">=</span><span class="va">None</span>) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a>gpfit.fit(xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(</span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a>  xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb12-430"><a href="#cb12-430" aria-hidden="true" tabindex="-1"></a>  return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a>plt.scatter(</span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a>  xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a>  marker<span class="op">=</span><span class="st">"x"</span>, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a>  xcurve,ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,</span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"original function"</span>)</span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred,color<span class="op">=</span><span class="st">"green"</span>, </span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="st">"mean predictions"</span>)</span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a>  mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>,label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Gaussian process predictions"</span>)</span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>NOT fitting kernel hyperparameters</span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a>We're saying hey jointly You can think of the joint distribution of the value of the value of the Gaussian process at all of these different X locations, all of these different horizontal locations.</span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a>What we would have is we would have everything else. In this plot we'd have The black X's, that's our observed data. So we totally know that.</span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We have:</span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Our observed data, the black X's, which is our training data.</span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Our predictions. that is the solid green line it is the mean of the Gaussian process at all of these different X locations away from the observations. We are creating it just like we created our draws or like we created our, our big prior Gaussian process before.</span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We're saying, hey, what's the mean? In this case, it's non-trivial. It's not just zero because it's conditioned on the data.</span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We have our uncertainties, our plus or minus two standard deviation interval. </span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>Now, before, this was pretty boring. </span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a>When we were first generating things a priori when we didn't have any data.</span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a>The plus or minus two standard deviation interval is just plus or minus two everywhere. </span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a>Now, because we've observed some data, it's not trivial. We're actually seeing that it differs depending on where we are in the space. </span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a>And I just want to point out some things that might be considered useful about this.</span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>When I'm really near my data points </span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>we see there's very little uncertainty.</span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a><span class="ss">   -  </span>very little uncertainty going between them. </span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a>   Because this is basically the only way we can smoothly and quickly go between these two points.</span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>when we get much farther away we have a lot of uncertainty because there's a good number of different ways that we could have gotten between these two points.  It makes sense that our uncertainty would be larger As we get farther away.</span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-predict-2</span></span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Gaussian process predictions with noise"</span></span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern, optimizer<span class="op">=</span><span class="va">None</span>) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a>gpfit.fit(xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a>y_all_draws<span class="op">=</span>gpfit.sample_y(xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),random_state<span class="op">=</span>rng,n_samples<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a>color_options<span class="op">=</span>[<span class="st">"red"</span>,<span class="st">"blue"</span>,<span class="st">"orange"</span>]</span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ydraw <span class="kw">in</span> <span class="bu">enumerate</span>(y_all_draws.T): </span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a>  plt.plot(xcurve,ydraw,label<span class="op">=</span><span class="st">"random draw"</span>,color<span class="op">=</span>color_options[np.mod(i,<span class="dv">3</span>)])</span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"green"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>,label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Random draws of f from the predictive distribution"</span>)</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>NOT fitting kernel hyperparameters</span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a>Now we have these four data points. And so we've constrained our random draws of the functions to go through the four data points.</span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a>And here are just three draws from it. Because again, it's just a multivariate Gaussian is all that we're doing here. We're just making this multivariate Gaussian the same way we did before by having draws at many dense x values at many dense input values. And so we can do the same thing here. It's just a different multivariate Gaussian.</span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a>This is meant to express Our uncertainty and our best guess, our best guess could be seen as this mean This posterior mean. So these are the mean predictions this solid green line. So that could be thought of as our best guess. And then our uncertainty around that</span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a>Is the plus or minus two standard deviation interval conditional on the data that we've seen. And now we're going to dig a little bit more. Into some choices we've made and what are their implications.</span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a><span class="fu">## Squared exponential kernel revisited {#sec-squared-exponential-kernel-revisited}</span></span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a>What if we happened to measure our data on a different scale? </span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a>We’ve been using this particular kernel:</span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a>k(x, x') = \sigma^2 \exp\left(-\tfrac{1}{2}(x - x')^2\right), \sigma^2 = 1</span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>What do we expect from the scale of f(x) a priori?</span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>At one x, with ~95% probability a priori,f(x) is in the interval $<span class="co">[</span><span class="ot">-2\sigma,2\sigma</span><span class="co">]</span>$.</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Marginal variance cannot increase with data</span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>What counts as “close” in x?</span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a>\exp(-\tfrac{1}{2}(2)^2) \approx 0.14 \quad \exp(-\tfrac{1}{3}(3)^2) \approx 0.11 \quad  \exp(-\tfrac{1}{2}(4)^2) \approx 0.0034 \qquad </span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-predict-3</span></span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "issues when we scale the y values by 100"</span></span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a><span class="co"># observed y values are 100 times what they were in the nice example</span></span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(</span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a>  kernel<span class="op">=</span>kern, optimizer<span class="op">=</span><span class="va">None</span>) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a>gpfit.fit(xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),</span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(</span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a>  xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a>  return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,<span class="dv">100</span><span class="op">*</span>yobs,color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a>marker<span class="op">=</span><span class="st">"x"</span>, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a>plt.plot(xcurve,<span class="dv">100</span><span class="op">*</span>ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,</span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a>color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"original function"</span>)</span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a>plt.plot(xcurve,mean_pred,color<span class="op">=</span><span class="st">"green"</span>, </span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a>label<span class="op">=</span><span class="st">"mean predictions"</span>)</span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a>  xcurve,</span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a>  mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"green"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a>plt.ylim(</span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a>  (<span class="op">-</span><span class="dv">100</span><span class="op">*</span><span class="dv">3</span><span class="op">*</span>signal_stddev, <span class="dv">100</span><span class="op">*</span><span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Gaussian process predictions"</span>)</span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>NOT fitting kernel hyperparameters</span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a>In this sample the $+/-2 \sigma$ confidence interval is now 100 times smaller than in the nice example, and the mean prediction is also 100 times smaller than in the nice example. So while it is still there it seems to have vanished.</span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-01-gp-predict-4</span></span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Issue when we scale the x values by 100"</span></span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a><span class="co"># observed x values are 100 times what they were in the nice example</span></span>
<span id="cb12-574"><a href="#cb12-574" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(</span>
<span id="cb12-575"><a href="#cb12-575" aria-hidden="true" tabindex="-1"></a>  kernel<span class="op">=</span>kern, optimizer<span class="op">=</span><span class="va">None</span>) <span class="co"># NOT fitting kernel hyperparameters</span></span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a>gpfit.fit(<span class="dv">100</span><span class="op">*</span>xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a>xall<span class="op">=</span>np.sort(np.concatenate((xcurve,xobs)))</span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(</span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>xall.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a>  return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-581"><a href="#cb12-581" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="dv">100</span><span class="op">*</span>xobs,</span>
<span id="cb12-582"><a href="#cb12-582" aria-hidden="true" tabindex="-1"></a>  yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>, </span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>xall,mean_pred,</span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"green"</span>, label<span class="op">=</span><span class="st">"mean predictions"</span>)</span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>xcurve,ycurve,</span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a>  linestyle<span class="op">=</span><span class="st">"dashed"</span>,</span>
<span id="cb12-590"><a href="#cb12-590" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"blue"</span>, </span>
<span id="cb12-591"><a href="#cb12-591" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="st">"original function"</span>)</span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a>  <span class="dv">100</span><span class="op">*</span>xall,mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a>  mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,color<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">0.3</span>,label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="cb12-597"><a href="#cb12-597" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="dv">100</span><span class="op">*</span>xstart,<span class="dv">100</span><span class="op">*</span>xstop])<span class="op">;</span> </span>
<span id="cb12-598"><a href="#cb12-598" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Gaussian process predictions"</span>)</span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a>in this example the points are now so far apart that  at most point on the graph we are quickly jumping to the prior and getting the maximum uncertainty.</span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lst-estimation</span></span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span><span class="fl">2.0</span><span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What can we do to handle different x and f(x) scales?</span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Normalization in y can help; in x, can still be hiccups</span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A common option in practice and in software is to fit the hyperparameters of a more general squared exponential kernel from data</span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>More general form of the squared exponential:</span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-618"><a href="#cb12-618" aria-hidden="true" tabindex="-1"></a>k(x, x') = \underbrace{\sigma^2}_{\color{red}\text{signal variance}} \exp\left(-\frac{1}{2}\sum_{d=1}^D \frac{(x_d - x'_d)^2}{\underbrace{l_d^2}_{\color{red}\text{length scale}}}\right), \sigma^2 = 1</span>
<span id="cb12-619"><a href="#cb12-619" aria-hidden="true" tabindex="-1"></a>$$ {#eq-squared-exponential-kernel-general}</span>
<span id="cb12-620"><a href="#cb12-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-621"><a href="#cb12-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Parameters (here, f) parametrize the distribution of the data. If we knew them, we could generate the data.</span>
<span id="cb12-622"><a href="#cb12-622" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>GPs: *nonparametric* model: infinite # of latent params</span>
<span id="cb12-623"><a href="#cb12-623" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*Hyperparameters* parametrize the distribution of the</span>
<span id="cb12-624"><a href="#cb12-624" aria-hidden="true" tabindex="-1"></a>parameters. If known, we could generate the parameters.</span>
<span id="cb12-625"><a href="#cb12-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-626"><a href="#cb12-626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Algorithm:</span>
<span id="cb12-627"><a href="#cb12-627" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Fit a value for the hyperparameters using the data.</span>
<span id="cb12-628"><a href="#cb12-628" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Given those values, now compute and report the mean and uncertainty intervals</span>
<span id="cb12-629"><a href="#cb12-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-630"><a href="#cb12-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-631"><a href="#cb12-631" aria-hidden="true" tabindex="-1"></a><span class="fu">### Demo 3 -- Learning the signal and the length scale and</span></span>
<span id="cb12-632"><a href="#cb12-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-635"><a href="#cb12-635" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-636"><a href="#cb12-636" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo-03-gp-data-and-ground-truth</span></span>
<span id="cb12-637"><a href="#cb12-637" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A draw from a Gaussian process with 10 simulated data points"</span></span>
<span id="cb12-638"><a href="#cb12-638" aria-hidden="true" tabindex="-1"></a>signal_stddev <span class="op">=</span> <span class="fl">2.0</span><span class="op">;</span> </span>
<span id="cb12-639"><a href="#cb12-639" aria-hidden="true" tabindex="-1"></a>kern <span class="op">=</span> signal_stddev<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="cb12-640"><a href="#cb12-640" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb12-641"><a href="#cb12-641" aria-hidden="true" tabindex="-1"></a>  length_scale_bounds<span class="op">=</span><span class="st">"fixed"</span>) <span class="co"># set the kernel</span></span>
<span id="cb12-642"><a href="#cb12-642" aria-hidden="true" tabindex="-1"></a>gp <span class="op">=</span> GaussianProcessRegressor(kernel<span class="op">=</span>kern)</span>
<span id="cb12-643"><a href="#cb12-643" aria-hidden="true" tabindex="-1"></a>xstart<span class="op">=-</span><span class="fl">0.5</span><span class="op">;</span> </span>
<span id="cb12-644"><a href="#cb12-644" aria-hidden="true" tabindex="-1"></a>xstop<span class="op">=</span><span class="fl">3.5</span><span class="op">;</span> </span>
<span id="cb12-645"><a href="#cb12-645" aria-hidden="true" tabindex="-1"></a>Nobs <span class="op">=</span> <span class="dv">10</span><span class="op">;</span> </span>
<span id="cb12-646"><a href="#cb12-646" aria-hidden="true" tabindex="-1"></a>xobs <span class="op">=</span> rng.uniform(xstart,xstop,size<span class="op">=</span>Nobs)</span>
<span id="cb12-647"><a href="#cb12-647" aria-hidden="true" tabindex="-1"></a>numx<span class="op">=</span><span class="dv">200</span><span class="op">;</span> </span>
<span id="cb12-648"><a href="#cb12-648" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)<span class="op">;</span> </span>
<span id="cb12-649"><a href="#cb12-649" aria-hidden="true" tabindex="-1"></a>twosd<span class="op">=</span><span class="dv">2</span><span class="op">*</span>signal_stddev<span class="op">*</span>np.ones(numx) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb12-650"><a href="#cb12-650" aria-hidden="true" tabindex="-1"></a>plt.fill_between(x,twosd,<span class="op">-</span>twosd,color<span class="op">=</span><span class="st">"gray"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>) <span class="co"># coloring 2 std dev range</span></span>
<span id="cb12-651"><a href="#cb12-651" aria-hidden="true" tabindex="-1"></a>xcurve<span class="op">=</span>np.linspace(start<span class="op">=</span>xstart,stop<span class="op">=</span>xstop,num<span class="op">=</span>numx)<span class="op">;</span> </span>
<span id="cb12-652"><a href="#cb12-652" aria-hidden="true" tabindex="-1"></a>xall <span class="op">=</span> np.concatenate((xcurve,xobs)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb12-653"><a href="#cb12-653" aria-hidden="true" tabindex="-1"></a>yall <span class="op">=</span> gp.sample_y(xall,random_state<span class="op">=</span>rng,n_samples<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span> </span>
<span id="cb12-654"><a href="#cb12-654" aria-hidden="true" tabindex="-1"></a>ycurve<span class="op">=</span>yall[<span class="dv">0</span>:numx]<span class="op">;</span> </span>
<span id="cb12-655"><a href="#cb12-655" aria-hidden="true" tabindex="-1"></a>yobs<span class="op">=</span>yall[numx:numx<span class="op">+</span>Nobs]</span>
<span id="cb12-656"><a href="#cb12-656" aria-hidden="true" tabindex="-1"></a>plt.plot(xcurve,ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,color<span class="op">=</span><span class="st">"blue"</span>)<span class="op">;</span> </span>
<span id="cb12-657"><a href="#cb12-657" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>)</span>
<span id="cb12-658"><a href="#cb12-658" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="cb12-659"><a href="#cb12-659" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb12-660"><a href="#cb12-660" aria-hidden="true" tabindex="-1"></a>pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb12-661"><a href="#cb12-661" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(</span>
<span id="cb12-662"><a href="#cb12-662" aria-hidden="true" tabindex="-1"></a>  <span class="st">"A draw from a Gaussian process with "</span> <span class="op">+</span> <span class="bu">str</span>(Nobs) <span class="op">+</span> </span>
<span id="cb12-663"><a href="#cb12-663" aria-hidden="true" tabindex="-1"></a>  <span class="st">" simulated data points"</span>) <span class="co"># make the plot</span></span>
<span id="cb12-664"><a href="#cb12-664" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-665"><a href="#cb12-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-666"><a href="#cb12-666" aria-hidden="true" tabindex="-1"></a>we have drawn 10 points and the ground truth</span>
<span id="cb12-667"><a href="#cb12-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-668"><a href="#cb12-668" aria-hidden="true" tabindex="-1"></a>but what goes to the algorithm is just the data points as follows:</span>
<span id="cb12-669"><a href="#cb12-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-672"><a href="#cb12-672" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-673"><a href="#cb12-673" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo3-gp-data</span></span>
<span id="cb12-674"><a href="#cb12-674" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "the data"</span></span>
<span id="cb12-675"><a href="#cb12-675" aria-hidden="true" tabindex="-1"></a><span class="co"># just the training data</span></span>
<span id="cb12-676"><a href="#cb12-676" aria-hidden="true" tabindex="-1"></a>plt.scatter(xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,marker<span class="op">=</span><span class="st">"x"</span>)</span>
<span id="cb12-677"><a href="#cb12-677" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> pltaxes <span class="op">=</span> plt.gca() <span class="co"># make the plot</span></span>
<span id="cb12-678"><a href="#cb12-678" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> plt.title(<span class="bu">str</span>(Nobs) <span class="op">+</span> <span class="st">" simulated data points"</span>) <span class="co"># make the plot</span></span>
<span id="cb12-679"><a href="#cb12-679" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-680"><a href="#cb12-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-683"><a href="#cb12-683" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-684"><a href="#cb12-684" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-demo3-gp-algorithm</span></span>
<span id="cb12-685"><a href="#cb12-685" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "GP scaled by the algorithm"</span></span>
<span id="cb12-686"><a href="#cb12-686" aria-hidden="true" tabindex="-1"></a>signal_stddev_init <span class="op">=</span> <span class="fl">1.0</span><span class="op">;</span> </span>
<span id="cb12-687"><a href="#cb12-687" aria-hidden="true" tabindex="-1"></a>kern_fit <span class="op">=</span> signal_stddev_init<span class="op">**</span><span class="dv">2</span><span class="op">*</span>RBF(</span>
<span id="cb12-688"><a href="#cb12-688" aria-hidden="true" tabindex="-1"></a>  length_scale<span class="op">=</span><span class="fl">1.0</span>,length_scale_bounds<span class="op">=</span>(<span class="fl">0.01</span>, <span class="dv">100</span>))</span>
<span id="cb12-689"><a href="#cb12-689" aria-hidden="true" tabindex="-1"></a>gpfit <span class="op">=</span> GaussianProcessRegressor(</span>
<span id="cb12-690"><a href="#cb12-690" aria-hidden="true" tabindex="-1"></a>  kernel<span class="op">=</span>kern_fit,n_restarts_optimizer<span class="op">=</span><span class="dv">10</span>) <span class="co"># &lt;1&gt;</span></span>
<span id="cb12-691"><a href="#cb12-691" aria-hidden="true" tabindex="-1"></a>gpfit.fit(xobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),yobs.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))<span class="op">;</span></span>
<span id="cb12-692"><a href="#cb12-692" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original kernel: "</span> <span class="op">+</span> <span class="bu">str</span>(kern))<span class="op">;</span> </span>
<span id="cb12-693"><a href="#cb12-693" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Fit kernel: "</span> <span class="op">+</span> <span class="bu">str</span>(gpfit.kernel_))</span>
<span id="cb12-694"><a href="#cb12-694" aria-hidden="true" tabindex="-1"></a>mean_pred, stddev_pred <span class="op">=</span> gpfit.predict(</span>
<span id="cb12-695"><a href="#cb12-695" aria-hidden="true" tabindex="-1"></a>  xcurve.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), return_std<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-696"><a href="#cb12-696" aria-hidden="true" tabindex="-1"></a>plt.scatter(</span>
<span id="cb12-697"><a href="#cb12-697" aria-hidden="true" tabindex="-1"></a>  xobs,yobs,color<span class="op">=</span><span class="st">"black"</span>,</span>
<span id="cb12-698"><a href="#cb12-698" aria-hidden="true" tabindex="-1"></a>  marker<span class="op">=</span><span class="st">"x"</span>, label<span class="op">=</span><span class="st">"training data"</span>)</span>
<span id="cb12-699"><a href="#cb12-699" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb12-700"><a href="#cb12-700" aria-hidden="true" tabindex="-1"></a>  xcurve,ycurve,linestyle<span class="op">=</span><span class="st">"dashed"</span>,</span>
<span id="cb12-701"><a href="#cb12-701" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"blue"</span>, label<span class="op">=</span><span class="st">"original function"</span>)</span>
<span id="cb12-702"><a href="#cb12-702" aria-hidden="true" tabindex="-1"></a>plt.plot(</span>
<span id="cb12-703"><a href="#cb12-703" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred,color<span class="op">=</span><span class="st">"green"</span>, </span>
<span id="cb12-704"><a href="#cb12-704" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="st">"mean predictions"</span>)</span>
<span id="cb12-705"><a href="#cb12-705" aria-hidden="true" tabindex="-1"></a>plt.fill_between(</span>
<span id="cb12-706"><a href="#cb12-706" aria-hidden="true" tabindex="-1"></a>  xcurve,mean_pred<span class="op">-</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="cb12-707"><a href="#cb12-707" aria-hidden="true" tabindex="-1"></a>  mean_pred<span class="op">+</span><span class="dv">2</span><span class="op">*</span>stddev_pred,</span>
<span id="cb12-708"><a href="#cb12-708" aria-hidden="true" tabindex="-1"></a>  color<span class="op">=</span><span class="st">"green"</span>,alpha<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb12-709"><a href="#cb12-709" aria-hidden="true" tabindex="-1"></a>  label<span class="op">=</span><span class="vs">r"</span><span class="op">+</span><span class="vs">/-2 std dev interval"</span>)</span>
<span id="cb12-710"><a href="#cb12-710" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> </span>
<span id="cb12-711"><a href="#cb12-711" aria-hidden="true" tabindex="-1"></a>plt.xlim([xstart,xstop])<span class="op">;</span> </span>
<span id="cb12-712"><a href="#cb12-712" aria-hidden="true" tabindex="-1"></a>plt.ylim((<span class="op">-</span><span class="dv">3</span><span class="op">*</span>signal_stddev,<span class="dv">3</span><span class="op">*</span>signal_stddev))<span class="op">;</span> </span>
<span id="cb12-713"><a href="#cb12-713" aria-hidden="true" tabindex="-1"></a>plotout <span class="op">=</span> plt.title(<span class="st">"Gaussian process predictions"</span>)</span>
<span id="cb12-714"><a href="#cb12-714" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-715"><a href="#cb12-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-716"><a href="#cb12-716" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>now we ARE fitting the hyperparameters</span>
<span id="cb12-717"><a href="#cb12-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-718"><a href="#cb12-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-719"><a href="#cb12-719" aria-hidden="true" tabindex="-1"></a><span class="fu">## What next</span></span>
<span id="cb12-720"><a href="#cb12-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-721"><a href="#cb12-721" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>adding observation noise variance</span>
<span id="cb12-722"><a href="#cb12-722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>extrapolation</span>
<span id="cb12-723"><a href="#cb12-723" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>high-dimensional inputs</span>
<span id="cb12-724"><a href="#cb12-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-725"><a href="#cb12-725" aria-hidden="true" tabindex="-1"></a>Always ask: What uncertainty are we quantifying? ^<span class="co">[</span><span class="ot">This is key as in real life there are many sources of uncertainty and in our models we only get to quantify one!</span><span class="co">]</span></span>
<span id="cb12-726"><a href="#cb12-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-727"><a href="#cb12-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-728"><a href="#cb12-728" aria-hidden="true" tabindex="-1"></a><span class="fu">## Observation noise</span></span>
<span id="cb12-729"><a href="#cb12-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-730"><a href="#cb12-730" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>So far we’ve been assuming that we observed f(x) directly</span>
<span id="cb12-731"><a href="#cb12-731" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>But often the actual observation y has additional noise:</span>
<span id="cb12-732"><a href="#cb12-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-733"><a href="#cb12-733" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-734"><a href="#cb12-734" aria-hidden="true" tabindex="-1"></a>f \sim \mathcal{GP}(m, k) \quad y^{(n)} = f(x) + \epsilon^{(n)}, \qquad \epsilon^{(n)} \stackrel{iid}{\sim} \mathcal{N}(0, \tau^2)</span>
<span id="cb12-735"><a href="#cb12-735" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-736"><a href="#cb12-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-737"><a href="#cb12-737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we observe $<span class="sc">\{</span>(x^{(n)}, y^{(n)})<span class="sc">\}</span>_{n=1}^N$ and want to learn the latent $f$</span>
<span id="cb12-738"><a href="#cb12-738" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The $y$ are multivariate-Gaussian distributed with mean and covariance summed.</span>
<span id="cb12-739"><a href="#cb12-739" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The mean of $y^{(n)}$ is given by the function $m(x^{(n)})$. and $Cov(y^{(n)},y^{(n')})=k(x^{(n)},x^{(n')})+\tau^2\mathbb{1}_{n=n'}$</span>
<span id="cb12-740"><a href="#cb12-740" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The covariance is given by the kernel function:</span>
<span id="cb12-741"><a href="#cb12-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-742"><a href="#cb12-742" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-743"><a href="#cb12-743" aria-hidden="true" tabindex="-1"></a>y \sim \mathcal{N}(f(x), \tau^2 I)</span>
<span id="cb12-744"><a href="#cb12-744" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-745"><a href="#cb12-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-746"><a href="#cb12-746" aria-hidden="true" tabindex="-1"></a>Before: </span>
<span id="cb12-747"><a href="#cb12-747" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-748"><a href="#cb12-748" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb12-749"><a href="#cb12-749" aria-hidden="true" tabindex="-1"></a>f(X) <span class="sc">\\</span></span>
<span id="cb12-750"><a href="#cb12-750" aria-hidden="true" tabindex="-1"></a>f(X')</span>
<span id="cb12-751"><a href="#cb12-751" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} </span>
<span id="cb12-752"><a href="#cb12-752" aria-hidden="true" tabindex="-1"></a>\sim \mathcal{N}</span>
<span id="cb12-753"><a href="#cb12-753" aria-hidden="true" tabindex="-1"></a>  \left(    </span>
<span id="cb12-754"><a href="#cb12-754" aria-hidden="true" tabindex="-1"></a>    \begin{bmatrix}</span>
<span id="cb12-755"><a href="#cb12-755" aria-hidden="true" tabindex="-1"></a>      0_N <span class="sc">\\</span> </span>
<span id="cb12-756"><a href="#cb12-756" aria-hidden="true" tabindex="-1"></a>      0_M</span>
<span id="cb12-757"><a href="#cb12-757" aria-hidden="true" tabindex="-1"></a>      \end{bmatrix}</span>
<span id="cb12-758"><a href="#cb12-758" aria-hidden="true" tabindex="-1"></a>    \begin{bmatrix}</span>
<span id="cb12-759"><a href="#cb12-759" aria-hidden="true" tabindex="-1"></a>    K(X, X) &amp; K(X, X') <span class="sc">\\</span></span>
<span id="cb12-760"><a href="#cb12-760" aria-hidden="true" tabindex="-1"></a>    K(X', X) &amp; K(X', X')</span>
<span id="cb12-761"><a href="#cb12-761" aria-hidden="true" tabindex="-1"></a>    \end{bmatrix}    </span>
<span id="cb12-762"><a href="#cb12-762" aria-hidden="true" tabindex="-1"></a>  \right) </span>
<span id="cb12-763"><a href="#cb12-763" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-764"><a href="#cb12-764" aria-hidden="true" tabindex="-1"></a>Now :</span>
<span id="cb12-765"><a href="#cb12-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-766"><a href="#cb12-766" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-767"><a href="#cb12-767" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb12-768"><a href="#cb12-768" aria-hidden="true" tabindex="-1"></a>y^{(1:N)} <span class="sc">\\</span></span>
<span id="cb12-769"><a href="#cb12-769" aria-hidden="true" tabindex="-1"></a>f(X')</span>
<span id="cb12-770"><a href="#cb12-770" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} </span>
<span id="cb12-771"><a href="#cb12-771" aria-hidden="true" tabindex="-1"></a>\sim \mathcal{N}</span>
<span id="cb12-772"><a href="#cb12-772" aria-hidden="true" tabindex="-1"></a>  \left(    </span>
<span id="cb12-773"><a href="#cb12-773" aria-hidden="true" tabindex="-1"></a>    \begin{bmatrix}</span>
<span id="cb12-774"><a href="#cb12-774" aria-hidden="true" tabindex="-1"></a>      0_N <span class="sc">\\</span> </span>
<span id="cb12-775"><a href="#cb12-775" aria-hidden="true" tabindex="-1"></a>      0_M</span>
<span id="cb12-776"><a href="#cb12-776" aria-hidden="true" tabindex="-1"></a>      \end{bmatrix}</span>
<span id="cb12-777"><a href="#cb12-777" aria-hidden="true" tabindex="-1"></a>    \begin{bmatrix}</span>
<span id="cb12-778"><a href="#cb12-778" aria-hidden="true" tabindex="-1"></a>    K(X, X)+ \tau \mathbb{I} &amp; K(X, X') <span class="sc">\\</span></span>
<span id="cb12-779"><a href="#cb12-779" aria-hidden="true" tabindex="-1"></a>    K(X', X) &amp; K(X', X')</span>
<span id="cb12-780"><a href="#cb12-780" aria-hidden="true" tabindex="-1"></a>    \end{bmatrix}    </span>
<span id="cb12-781"><a href="#cb12-781" aria-hidden="true" tabindex="-1"></a>  \right) </span>
<span id="cb12-782"><a href="#cb12-782" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-783"><a href="#cb12-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-784"><a href="#cb12-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-785"><a href="#cb12-785" aria-hidden="true" tabindex="-1"></a><span class="fu">## What uncertainty are we quantifying?</span></span>
<span id="cb12-786"><a href="#cb12-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-787"><a href="#cb12-787" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It’s worth being aware that data science (ML/stats/AI) often overloads common colloquial terms with terms of art</span>
<span id="cb12-788"><a href="#cb12-788" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>E.g. “significance”, “bias”, “generalization”</span>
<span id="cb12-789"><a href="#cb12-789" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Every precise use of “uncertainty” has this issue</span>
<span id="cb12-790"><a href="#cb12-790" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>E.g. frequentist sampling, Bayesian, etc.</span>
<span id="cb12-791"><a href="#cb12-791" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We should always make sure we can distinguish what is,</span>
<span id="cb12-792"><a href="#cb12-792" aria-hidden="true" tabindex="-1"></a>and what is not, covered by the term of art</span>
<span id="cb12-793"><a href="#cb12-793" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A standard setup (our setup so far):</span>
<span id="cb12-794"><a href="#cb12-794" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We model the data as generated according to a GP with squared exponential kernel and observation noise</span>
<span id="cb12-795"><a href="#cb12-795" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>We fit the hyperparameters (the signal variance, the length scale(s), and the noise variance) to single values</span>
<span id="cb12-796"><a href="#cb12-796" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The reported uncertainties are what result when the GP model and fitted hyperparameters are exactly correct</span>
<span id="cb12-797"><a href="#cb12-797" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-798"><a href="#cb12-798" aria-hidden="true" tabindex="-1"></a>  <span class="co">[</span><span class="ot">Are there other uncertainties that aren’t being quantified here?</span><span class="co">]</span>{.column-margin}</span>
<span id="cb12-799"><a href="#cb12-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-800"><a href="#cb12-800" aria-hidden="true" tabindex="-1"></a><span class="fu">## Some other sources of uncertainty {#sec-other-sources-of-uncertainty}</span></span>
<span id="cb12-801"><a href="#cb12-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-802"><a href="#cb12-802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There may be multiple sets of substantively different hyperparameter values that are both plausible and</span>
<span id="cb12-803"><a href="#cb12-803" aria-hidden="true" tabindex="-1"></a>consistent with the observed data</span>
<span id="cb12-804"><a href="#cb12-804" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What can we do? First: unit test, plot, sense check!</span>
<span id="cb12-805"><a href="#cb12-805" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Ask what is possible to learn with the data available </span>
<span id="cb12-806"><a href="#cb12-806" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Multiple random restarts: plot the results</span>
<span id="cb12-807"><a href="#cb12-807" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Bayesian model of the hyperparameters</span>
<span id="cb12-808"><a href="#cb12-808" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A GP with your mean &amp; kernel may be meaningfully misspecified for the data</span>
<span id="cb12-809"><a href="#cb12-809" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Box: “All models are wrong, but some are useful”</span>
<span id="cb12-810"><a href="#cb12-810" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>What can we do? First: unit test, plot, sense check!</span>
<span id="cb12-811"><a href="#cb12-811" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Can change the mean and/or kernel</span>
<span id="cb12-812"><a href="#cb12-812" aria-hidden="true" tabindex="-1"></a><span class="ss">      - </span>E.g. local/heteroskedastic models, periodic kernels, linear mean function, many many more</span>
<span id="cb12-813"><a href="#cb12-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-814"><a href="#cb12-814" aria-hidden="true" tabindex="-1"></a><span class="fu">## Extrapolation {#sec-extrapolation}</span></span>
<span id="cb12-815"><a href="#cb12-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-816"><a href="#cb12-816" aria-hidden="true" tabindex="-1"></a>Extrapolation: Estimation/prediction beyond the observed</span>
<span id="cb12-817"><a href="#cb12-817" aria-hidden="true" tabindex="-1"></a>data</span>
<span id="cb12-818"><a href="#cb12-818" aria-hidden="true" tabindex="-1"></a>• Compare to interpolation: estimation/prediction within the</span>
<span id="cb12-819"><a href="#cb12-819" aria-hidden="true" tabindex="-1"></a>observed data</span>
<span id="cb12-820"><a href="#cb12-820" aria-hidden="true" tabindex="-1"></a>• When using GPs with a squared exponential kernel:</span>
<span id="cb12-821"><a href="#cb12-821" aria-hidden="true" tabindex="-1"></a>• Data points that are more than a handful of length scales</span>
<span id="cb12-822"><a href="#cb12-822" aria-hidden="true" tabindex="-1"></a>from other data points will revert to prior behavior</span>
<span id="cb12-823"><a href="#cb12-823" aria-hidden="true" tabindex="-1"></a>• Note: extrapolation isn’t a special issue unique to GPs. It’s a</span>
<span id="cb12-824"><a href="#cb12-824" aria-hidden="true" tabindex="-1"></a>fundamentally hard problem for all data analysis methods</span>
<span id="cb12-825"><a href="#cb12-825" aria-hidden="true" tabindex="-1"></a>• To extrapolate, you need to make assumptions</span>
<span id="cb12-826"><a href="#cb12-826" aria-hidden="true" tabindex="-1"></a>• When you have domain knowledge of a system, you</span>
<span id="cb12-827"><a href="#cb12-827" aria-hidden="true" tabindex="-1"></a>might be able to use it to extrapolate</span>
<span id="cb12-828"><a href="#cb12-828" aria-hidden="true" tabindex="-1"></a>• When you’re letting a machine learning method use its</span>
<span id="cb12-829"><a href="#cb12-829" aria-hidden="true" tabindex="-1"></a>defaults, it’s making assumptions. Do you know what</span>
<span id="cb12-830"><a href="#cb12-830" aria-hidden="true" tabindex="-1"></a>those assumptions are?</span>
<span id="cb12-831"><a href="#cb12-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-832"><a href="#cb12-832" aria-hidden="true" tabindex="-1"></a><span class="fu">## More than one input</span></span>
<span id="cb12-833"><a href="#cb12-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-834"><a href="#cb12-834" aria-hidden="true" tabindex="-1"></a><span class="al">![histogram of squared inter-point distances](C6-SL09.png)</span>{#fig-multiple-inputs .column-margin group="slides" width="53mm"}</span>
<span id="cb12-835"><a href="#cb12-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-836"><a href="#cb12-836" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Our illustrations have almost all been for one input so far</span>
<span id="cb12-837"><a href="#cb12-837" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>But in real life, it’s typical to have more than one input</span>
<span id="cb12-838"><a href="#cb12-838" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What could go wrong? Previous lessons apply, but also:</span>
<span id="cb12-839"><a href="#cb12-839" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Possibly different length scales. Check defaults.</span>
<span id="cb12-840"><a href="#cb12-840" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Regression in high dimensions is a fundamentally hard problem (without additional assumptions)</span>
<span id="cb12-841"><a href="#cb12-841" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>All points are "far away" in high dimensions. Illustration:</span>
<span id="cb12-842"><a href="#cb12-842" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Uniformly randomly sample 10,000 points on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>^D$</span>
<span id="cb12-843"><a href="#cb12-843" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Make a histogram of squared inter-point distances</span>
<span id="cb12-844"><a href="#cb12-844" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Recall: points "far" from data default to the prior mean and variance.</span>
<span id="cb12-845"><a href="#cb12-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-846"><a href="#cb12-846" aria-hidden="true" tabindex="-1"></a><span class="fu">## Some high points of what got cut for time {#sec-high-points}</span></span>
<span id="cb12-847"><a href="#cb12-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-848"><a href="#cb12-848" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We ran out of time! Here are some high-level summary points beyond what we discussed together:</span>
<span id="cb12-849"><a href="#cb12-849" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>There are other challenges with many inputs, both conceptual and practical</span>
<span id="cb12-850"><a href="#cb12-850" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Running time for GP regression can be an issue with a large number of training data points</span>
<span id="cb12-851"><a href="#cb12-851" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>In particular, the matrix inverse can be expensive </span>
<span id="cb12-852"><a href="#cb12-852" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>There are incredibly many papers about fast approximations to the exact Gaussian process</span>
<span id="cb12-853"><a href="#cb12-853" aria-hidden="true" tabindex="-1"></a><span class="ss">      - </span>Each approximation has pros and cons </span>
<span id="cb12-854"><a href="#cb12-854" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bayesian optimization inherits many of the pros and cons of Gaussian processes for regression</span>
<span id="cb12-855"><a href="#cb12-855" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Exercise: once you learn about Bayesian optimization, think about how the pros and cons we discussed together might translate there</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Oren Bochman</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>