---
title: "Week 0: Feynman Notebook on Bayesian Analysis"
subtitle: Time Series Analysis
description: "Notes on Bayesian Time Series Analysis"
date: 2024-10-22
categories: 
  - Coursera 
  - notes
  - Bayesian Statistics
  - Time Series
  - Feynman Notebook
keywords: 
  - time series  
---


## A Feynman Notebook - For Bayesian ~~Time Series~~ Analysis

::: {.callout-important}

## TS Questions --- A Feynman Notebook

Here is where I can collect questions about TS analysis that I have as I go through the course. Hopefully I will be better equipped to answer many of these  them by the end of the course.
:::



1. How does Bayesian TS analysis differ from regular TS analysis?
   1. In the *NDLM* we supply **a Normal Prior**.
   1. We use **Bayesian updating** to update the model parameters from the data.
   1. Rather than using a point estimate, we maintain a distributional view of the parameters. We can propagate these uncertainties into our forecasting, smoothing, and filtering distributions.
1. Fourier analysis is a powerful tool for time series analysis. How does it relate to Bayesian time series analysis?
   1. It's easier to discuss periodicity and frequency components rather than the fourier analysis which is a technique. However, Fourier analysis lets us consider the time series in the time domain and in the frequency domain.
   1. We see in the intro that 
   1. in a Bayesian framework. We can incorporate prior beliefs about the frequency of events.
   1. We can incorporate the outcomes to incorporate seasonal elements into an NDLM
1. How and what type of prior knowledge into time series analysis?
   - In [@west2013bayesian] they authors discuss both is the actual prior.
   - But they also talk about supporting interventions. E.g. when a major competitor goes out of business.The model should be able to handle this information and they make a big issues of how we need to incorporate into the next time step both new expected demand as well as an estimate of its variance which give better estimates of required production.
1. Are there models that are unique to Bayesian time series analysis?
   - Hard to say but DLM seem to be.
1. How does *distributional* thinking affect time series analysis?
   - It gives us confidence bounds on future estimates.
1. How do we represent *uncertainty* in Bayesian time series analysis?
   - We have distribution and we can derive for any point estimate a corresponding credible interval.
   - We can use smoothing to try and reason about trend or seasonality separately.

1. What BTS models are most useful in economics and finance?
1. Is there a cleanup procedure for time series data?
   - Using exponential smoothing
   - Using weighted averaging going back and forward enough steps can smooth seasonal  effects.
   - More generally this is handled by smoothing
   - Going backwards this is can be done using filtering.
1. Is there an Bayesian EDA for time series?
   - we can use differencing to make the time series stationary
   - we can use the ACF and PACF 
   - we can decompose the time series into trend, seasonal, and residual components
   - we can visualize autocorrelation using a  correlogram
   - we can visualize periodicity using a periodogram and spectral density.
   - see [@nielsen2019practical]
1. How do we handle missing data in time series?
1. How do we handle non-stationary time series?   
   - By applying differencing we can make the time series stationary.
1. Are there processes for long term memory in time series?
   - see [@prado2023time p. 124 ] 
   - the book also touches on EKF and MKF
1. Are there processes for power laws.
  - see https://wiki.santafe.edu/images/5/52/Powerlaws.pdf
1. Can BTS handle dynamic systems in time series?
1. Can we model time series with multiple scales?
1. What TS models are useful for regime switching?
   - I recall this came up in @davidson2015bayesian
   - This is also covered in module 3 of the course.
1. How can we simulate time series data?
1. How can we forecast time series data?
1. How can we find periodicity in time series data?
1. Is the Kalman Filter a state-space or dynamic linear model
   - see [@prado2023time p. 141] 
   - the book also touches on EKF and MKF
1. Particle filters AKA Sequential Monte Carlo methods in the book.
   - see [@prado2023time p. 205]
1. Are there Bayesian time series models of contagion? 
1. Are there Bayesian time series models of epidemics?
1. What are Seasonal adjustments?
1. How to do a seasonal adjustment?
1. What are the tools for wrangling and cleaning TS data.
   - [Data Engineering using Wrangling and Cleaning](https://www.youtube.com/watch?v=JZxNvVi1UfA) c.f. [@woodward2022time § 1.4.1.3]
1. How to use the frequency domain as a key part of understanding and analyzing data?
1. How to assess whether an existing trend should be predicted to continue or whether caution should be used in forecasts?
1. Do you really understand AR models or are they just a black box?
1. What is a unit root?
1. How to use tools that would help you decide whether to use a seasonal model and/or include a unit root in models?
1. How to use predictor variables to help forecast some variable such as sales? (i.e. regression on time series data)


### Co-integration

1. What are Cointegrated time series?
   - **Co-integration** [**magic trick**]{.column-margin} is a technique used to find [a long term correlation between time series processes]{.mark} that was introduced in 1987 by  Nobel laureates Robert Engle and Clive Granger
   - also see [@pfaff2008analysis]
1. What are some test for co-integration:
   - Engle-Granger,
   - Johansen Test,
   - the Phillips-Ouliaris test. 
   

### NN and Deep learning

1. How to use neural network methods to forecast time series?  
   - RNNs
   - lstms
   - convolutions
   - GRUs
   - transformers
   - TS foundations models
   - Neural Prophet [citation needed]{.cn}
1. Deep learning foundation models [citation needed]{.cn} pre-train NN model with many time series. Is this a form of Bayesian time series analysis? 
1. How does this BTS relate to deep learning?
   - Diffusion models in DL are autoregressive [citation needed]{.cn}
   - the recently the mamba architecture has been proposed which is an autoregressive state space model. [citation needed]{.cn}


### Web scraping

1. Any tips on scaraping time series data?
  - [Web Scraping using Bots](https://www.youtube.com/watch?v=mg6OK1B-Rac) c.f [@woodward2022time § 1.4.1.4]

### Filtering & Smoothing
 
1. What is smoothing in BTS
   - decomposing the series as a sum of two components: a smooth component, plus another component that includes all the features  that are unexplained by the smooth component.
   - one way is to use a moving average.
   - in the Bayesian context smoothing is the process of estimating the hidden states of a system given the observed data.
1. What is filtering in Bayesian Time Series?
   - in the Bayesian context filtering is the process of estimating the previous hidden states of a system given the observed data. I.e. a retrospective analysis to understand the process better 

   - we want to sample $\mathbb{P}r(\theta_t,k \mid \mathcal{D}_t)$ 
1. What is the Kalman filter?
   - see [@prado2023time p. 141] 
   - the book also toches on EKF and MKF
1. What is a particle filter?
   - see [@prado2023time § 6.2.2] on the The Auxiliary Particle Filter
1. What is the [Butterworth filter](https://en.wikipedia.org/wiki/Butterworth_filter)?
   - The Butterworth filter is a signal processing filter that has a flat response in passband (or as close as possible to flat) - making it good for cleaning up noise !?

### Models:

1. White noise
1. Wiener process (random walk)
1. AR(1): Autoregressive process order 1
1. [Ornstein–Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) a continuous-time version of the AR(1) process
1. AR(p): Autoregressive process order p
1. MA(q): Moving average process order q
1. ARMA(p,q): Autoregressive moving average
1. SARMA: Seasonal ARMA
1. ARIMA: Autoregressive integrated moving average
1. SARIMA: Seasonal ARIMA
1. VAR: Vector autoregressive
1. SVAR: structural vector autoregressive models (SVAR).
1. VECM: Vector error correction models (VECM).
1. GARCH: Generalized autoregressive conditional heteroskedasticity
1. ARCH: Autoregressive conditional heteroskedasticity
1. SMC: Sequential Monte Carlo
1. MDM: Multi-regression dynamic models
1. LTMs: latent threshold models
1. FFBS: Forward Filtering Backward Sampling
1. DLM: Dynamic Linear Models
1. BSTS: Bayesian Structural Time Series
   - https://drive.google.com/file/d/14US56VzanuLt03XBkoAGzLy0gDEreZUc/view
   - 
1. TVAR: Time-varying autoregressive models
1. DGLM: Dynamic Generalized Linear Models


<!-- This is long and should be broken up into subjects -->




What is the relation between Dynamic linear time series models (DLTS) models and Bayesian structural time series models (BSTS) models?

> Dynamic Linear Time Series (DLTS) models and Bayesian Structural Time Series (BSTS) models are both frameworks for modeling time series data, and they share a strong connection, particularly in the way they approach model formulation and uncertainty. Here's a breakdown of their relationship:

1. **Dynamic Linear Time Series** (DLTS) Models:

- DLTS models, often referred to as Dynamic Linear Models (DLMs), are a class of models where the parameters (such as the intercept or slope) evolve over time according to a stochastic process.
They can be written in a state-space form, consisting of:
- Observation Equation: Relates the observed data to the hidden state.
- State Equation: Describes how the hidden state evolves over time.
- These models use Kalman filtering for inference and prediction.
- DLTS models are flexible in handling non-stationarity and time-varying parameters.

> In the course we primarily learned to use Bayesian methods to estimate the parameters of the model.

2. **Bayesian Structural Time Series** (BSTS) Models:

- BSTS models are a Bayesian approach to time series modeling, which generalizes the DLTS framework.
- Like DLTS, they use a state-space form, where the time series is decomposed into different components (e.g., trend, seasonality, regression effects).
- BSTS explicitly incorporates **Bayesian inference**, where prior distributions are placed on the model components and parameters, and inference is conducted using MCMC or other Bayesian methods.
- One of the key advantages of BSTS is its ability to incorporate model uncertainty, allowing the user to specify structural components (such as trend or seasonality) with uncertainty about their presence or importance in the data.

3. Relation Between DLTS and BSTS:

- Bayesian Extension of DLTS: BSTS can be seen as a Bayesian extension of DLTS models. While DLTS uses Kalman filtering for deterministic inference, BSTS uses Bayesian methods to quantify and propagate uncertainty in model components and parameters.
- **Component Decomposition**: Both models can represent the time series in terms of structural components (like trends, seasonal patterns, or covariates), but BSTS allows for **more flexible modeling** of these components using Bayesian priors and hierarchical structures.
- **Handling of Uncertainty**: DLTS models provide point estimates for parameters using Kalman filters, while BSTS incorporates full probabilistic estimates, enabling better uncertainty quantification in the presence of small data, model misspecification, or structural breaks.
- Model Complexity: BSTS models can handle more complex scenarios where the structure of the time series isn't fully known (e.g., unknown seasonality or trends), whereas DLTS is typically used when the structure of the model (e.g., presence of trend or seasonality) is more defined.

- for more information on [BSTS](https://drive.google.com/file/d/14US56VzanuLt03XBkoAGzLy0gDEreZUc/view)

## Software


1. What are the most useful packages in R for time series analysis?
   - **BOA** Bayesian Output Analysis (BOA, Smith 2007) [citation needed]{.cn} and 
   - **CODA** Convergence Diagnosis and Output Analysis for MCMC (CODA, Plummer, Best, Cowles, and Vines 2006). [citation needed]{.cn}
   - **URCA** Unit Root and Cointegration Tests for Time Series Data (URCA, Pfaff 2008)[citation needed]{.cn}.
   - **Vars** VAR Modelling (Vars, Pfaff 2008). [citation needed]{.cn}
   - **BSTS** Bayesian Structural Time Series (BSTS, Scott and Varian 2014). [citation needed]{.cn}
   - **CausalImpact** Causal Impact Analysis (CausalImpact, Brodersen, Gallusser, Koehler, Remy, and Scott 2015). [citation needed]{.cn} builds on BSTS and methods from this [Inferring causal impact using Bayesian structural time-series models](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-1/Inferring-causal-impact-using-Bayesian-structural-time-series-models/10.1214/14-AOAS788.full)
   - **KFAS** Kalman Filter and Smoother for Exponential Family State Space Models (KFAS, Helske 2017). [citation needed]{.cn}
   - **MARSS** Multivariate Autoregressive State-Space Models (MARSS, Holmes, Ward, and Scheuerell 2012). [citation needed]{.cn}
   - **MCMCpack** Markov Chain Monte Carlo (MCMCpack, Martin, Quinn, and Park 2011). [citation needed]{.cn}
   - **MCMCglmm** Markov Chain Monte Carlo Generalized Linear Mixed Models (MCMCglmm, Hadfield 2010). [citation needed]{.cn}
   - **R-INLA** Integrated Nested Laplace Approximations (R-INLA, Rue, Martino, and Chopin 2009). [citation needed]{.cn} used approximate Bayesian inference for Latent Gaussian Models that can be expressed as latent Gaussian Markov random fields (GMRF)

1. What about in python?
