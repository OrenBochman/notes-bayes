<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/lmIXO/maximum-likelihood-estimation-in-the-ar-1 -->



I will now talk about how to perform maximum likelihood estimation in the case of the autoregressive model of order one. 
Recall that we have the equation for
the process. This is a zero mean AR1, looks like this. 
And then the epsilon t's are iid normal (0,v). 
So the parameters of the model here, our phi, the AR coefficient, and the variance associated to the epsilon t's. 
We have two options to perform maximum
likelihood estimation in this case. One case is to consider the full
likelihood, and the other case I will be describing is the case in which we work
only with a conditional likelihood. And in the case of the AR1,
we can condition on the first observation, and then work only with that conditional
likelihood. 
I will begin working with
the full likelihood here. 
If you recall, we are also going
to assume that the process is such that the phi is
in the (-1,1) interval, which implies the process
is going to be stationary. 
And in that case, we said that the yt's
have a particular distribution. 
They are all going to be
normally distributed. In particular, the first y, y1,
is going to be normally distributed. It's going to be zero mean. 
And the variance is that term
that we described before, is that v / (1- phi squared). For the rest of the yt's,
because of the temporal dependency, I have to write down
the distribution of yt given y_{t-1}. 
So for the first y1,
I have this distribution, and for the remaining ones, where yt given y_{t-1}, I have a normal. Now the mean is phi, y_{t-1},
and the variance is v. So combining these two pieces,
if I now think about having observations y1,
all the way to y capital T. I can write down the likelihood
function as a function of v and the phi coefficient as follows. So I can write down, 1 to T given phi and v as the product of the first term, which is this first density. This one depends on phi and v. And then I have the product
from t = 2 up to capital T. Phi and v, so
this piece gives me a normal. So I'm going to have that first piece is going to be 1 / (2pi v) one-half. And then I have my 1- phi squared,
To the power of one-half. And then I have my exponential piece. Over 2v, so that's the first density,
y1 given phi and v. And then I have this product of all these
normal densities here, yt given y_{t-1}. So in this case, when I consider
this product, we're going to have, T-1 terms, so
that's to the power of (T-1)/2. And then I have this exponential. And t equals from 2 up to capital T. And this is divided by 2v. So this is the expression that I obtain. So I can put some terms together here and just write this down,
simplify this expression a bit. And then this is all, Divided by 1/2v. So this gives me the expression for
the full likelihood. If you look at what is in here, this term, I can call this term Q star of phi. So here, Q star, of phi is that y1 squared(1- phi squared). And so this expression, Can be written like this. So if I look at this expression now, if I want to perform maximum
likelihood estimation, I would have to maximize this function
viewed as a function of phi and v. So I have this entire expression here. The Q star is given by this expression. So it has two components, one component
that has to do with the first y. And then the remaining components all go
from this t goes from 2 to capital T. So if you think about the first term,
the term for t = 2, you have the terms y2 and
y1 appearing here. So this piece, we can see later,
is the one that we will use when we are doing maximum likelihood estimation
using the conditional likelihood. And I will call this
expression later Q(phi). But it has these two terms,
the Term that corresponds to this component for the first y and
then the remaining terms. We will study how to perform maximum
likelihood estimation in these two cases. This gives me the full likelihood. We can also work with the so
called conditional likelihood, that conditions on the first observation. So, in here I can write
down my likelihood, now it is going to go from, 2 up to T. I'm conditioning on the first y1. And then I view this again
as a function of phi and v. And the y1 is given. So, in this case,
My likelihood is going to have. All the terms from 2 up to T. And then I'm going to use the fact
that they are all normally distributed conditional on the previous value. So I'm just going to have here. So, we can write down all these terms. We're going to have T -1 of them, and then I'm going to have that
exponential of the sum of the yt minus phi minus 1 squared over 2v, which is what we have called
before the summation here. That goes from t equals 2 up to T
is what we have called Q of phi. So, rewriting this expression. And the nice thing about working
with the conditional likelihood, is that if you view this
as a function of v and phi. What appears in here, is really going to be a quadratic
function of the phi parameter. So it's much easier to maximize
that likelihood function because all the results of the maximization
are going to appear in close form when we work with
the conditional likelihood. Therefore it's easy if we work
with the full likelihood, we need to use a numerical
maximization procedure. So just to show you how to obtain
maximum likelihood estimation in this particular framework using
the conditional likelihood, is useful to see a correspondence
between this likelihood and a linear regression model. So, I can write down for the case of
the conditional likelihood again, something that looks like this. So, I can stack all my ys from
2 up to T in a single vector. I can call that vector y,
then I can put my phi coefficient in here. And then just thinking about what
is my dependency on the past. y2 is going to be dependent on y1,
y3 will be dependent on y2 and so on. And then I'm going to have here, all the way down to y_{T -1}. And then I have my epsilon2 all the way to epsilont. And I'm going to call this an x,
in this case is a vector, but in the general regression
it can be a matrix. And then this here would be my beta, again in the general regression
framework this is usually a vector. And here I'm going to call
this my epsilon vectors. So, if you go and look at the results
in terms of regression model, we have a model that looks like this, And then the epsilon, Follows here a normal zero v times an identity matrix. So when we work with this
expression we know that in order to obtain maximum likelihood estimation, assuming that the matrix
X is a full rank matrix. We obtain maximum likelihood estimation as beta hat, X transpose X, inverse X transpose y. And we can also obtain an estimate for v. That I'm going to call s square,
is my estimate for v here. We can simply obtain this as y, in general X beta hat
transpose y X beta hat. And then here is going to to be divided by the dimension off the y vector. And by that,
I mean how many components I have in that vector minus the dimension
of the beta vector. So we can apply these equations
to the case of here y1, to obtain maximum likelihood
estimators for phi and v. In the case of the maximum
likelihood estimator for phi, I want to call it phi hat and v. We simply take,
if we look at this formula, we're going to take this transpose. So it's a column vector that becomes
a row vector, multiplied by itself. So I'm going to have the summation
of all these square terms, and then I have to take the inverse of that. And then the next piece which
is my X transpose times y is going to be the product of
this vector times this vector. 
So this results in having my maximum likelihood estimator given as, from t2 up to T of y_{t- 1} divided by this sum of the y_{t-1} square, for t2 up to T. That's my maximum likelihood estimator for
phi. And then I can obtain my s square in this case is also just simply taking the sum, when I do this product, I get the some of the yt minus phi hat MLE times y_{t -1} squared. And this goes from 2 of the T, divided by the dimension of this y vector, which is going to be T -1 and
the dimension of the beta here, which in our case is a scalar, so it's 1. 
So I'm going to curve here. T - 2.
This gives me the two estimates for the parameters that we can use for maximum likelihood in the case of phi hat. 
We can also obtain the maximum likelihood estimator for v. 
But we usually work with this unbiased estimator. 
In the case of the full likelihood is not possible to establish a correspondence between the equations that we have for the likelihood of the autoregressive process and the linear regression model. 
So one has to do numerical optimization or numerical maximization if one wants to get the maximum likelihood estimators for the AR coefficient in the case of the AR(1) and also for the variance. 
Let's just look at one example. 
So again recall that we're working with the AR(1). 
And just to simplify things here a little bit I'm going to assume that we only want to perform maximum likelihood estimation for phi and that we know the variance of the process. 
And let's assume that that variance is,For the epsilon ts, they are all normal 0,1. 
So there is no v here, only the phi. So if you recall when we work with
a full likelihood, there are two pieces. 
The first piece is the distribution of y1 given phi which in
this case is normal 0. And then we have that 1
over 1 minus phi square. And here again I'm assuming that
phi is in this interval (-1,1). And then we have the piece that is all
the remaining ts conditional on y. So the yt giving y_{t-1} for
t going from two all the way to capital T. And then we have this is And then 1. So when we put together the likelihood, the full likelihood for this case,
the only parameter is phi. So we're going to have that product of p of y1 given phi times
the remaining terms. And so again if we think about the expressions we have the 1 minus phi square 1 half. And then we have 1 minus phi square over two. And then we have,
The remaining components. So we have T minus 1 of the 2 pi
times the variance, the variance is 1. 
And then I have the summation. This will go from 2 all the way to capital T, Over 2. 
So if I were to put things together again I would have this expression. 
And then I have the exponential. 
And if I were to maximize this expression again I cannot use the results from linear regression to do this. 
I have to use numerical methods like Newton Raphson to maximize this function. 
I can consider the log likelihood. So if I take the log of this, You can see that we will have a term here. 
That is one half log of this. 
And then I have the log of this piece here which just gives me, And this expression if you recall we had called this Q star phi. 
So plus some constant, this piece is just a constant, I'm going to call it K. 
So if I were to look at maximizing this function, I can think about taking first derivatives with respect to phi. 
And then you will see that again the expression that you obtain doesn't allow you to obtain a close form expression for the maximum likelihood estimator of phi. 
Instead, what you will do is you can use a numerical optimization method such as Newton Raphson to obtain the maximum likelihood estimator for phi.