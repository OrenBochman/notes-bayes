---
title : "Mixture Models"
---



## 1. Basic Concepts:

### 1.1 Definition of a finite mixture model

1. Mixture Model (CDF):

$$
F(x) = \sum_{k=1}^K \omega_k G_k(x)
$$ {#eq-mixture-cdf}

where $G_k(x)$ is the CDF of the $k$-th component distribution and $\omega_k$ is the weight for the $k$-th component.

2. Mixture Model (PDF/PMF):



$$
f(x) = \sum_{k=1}^K \omega_k g_k(x)
$$ {#eq-mixture-pdf}

where $g_k(x)$ is the PDF/PMF of the $k$-th component distribution and $\omega_k$ is the weight for the $k$-th component.

3. Example: Exponential Mixture CDF:

$$
F(x) = \omega_1 \left(1 - \exp\left\{\frac{x}{\theta_1}\right\}\right)
     + \omega_2 \left(1 - \exp\left\{\frac{x}{\theta_2}\right\}\right)
     + \omega_3 \left(1 - \exp\left\{\frac{x}{\theta_3}\right\}\right)
$$ {#eq-exponential-mixture-cdf}

4. Example: Exponential Mixture PDF:

$$
f(x) = \frac{\omega_1}{\theta_1} \exp\left\{\frac{x}{\theta_1}\right\}
    + \frac{\omega_2}{\theta_2} \exp\left\{\frac{x}{\theta_2}\right\}
    + \frac{\omega_3}{\theta_3} \exp\left\{\frac{x}{\theta_3}\right\}
$$ {#eq-exponential-mixture-pdf}

5. Gamma Mixture PDF:

$$
f(x) =
\begin{cases}
    \omega \frac{x^{\nu_1-1}}{\Gamma(\nu_1)\lambda_1^{\nu_1}}\ \exp \left\{\frac{x}{\lambda_1}\right\}
    + (1-\omega) \frac{x^{\nu_2-1}}{\Gamma(\nu_2)\lambda_2^{\nu_2}}\ \exp \left\{\frac{x}{\lambda_2}\right\} & x > 0 \\
    0 & \text{otherwise}
\end{cases}
$$ {#eq-gamma-mixture-pdf}

6. Mean and Variance of a Mixture:

$$
\mathbb{E}_F(X) = \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}[X]
$$ {#eq-mixture-mean}

$$
\begin{aligned}
\mathbb{V}ar_F(X) & = \mathbb{E}_F(X^2) - \{\mathbb{E}_F(X)\}^2 \\
& = \sum_{k=1}^K \omega_k \left\{ \mathbb{E}_{G_k}(X^2) \right\} - \left\{ \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}(X) \right\}^2 \\
& = \sum_{k=1}^K \omega_k \left\{ \mathbb{V}ar_{G_k}(X) + [\mathbb{E}_{G_k}(X)]^2 \right\} - \left\{ \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}(X) \right\}^2
\end{aligned}
$$ {#eq-mixture-variance}


7. Special Case (Component means zero):


$$
\mathbb{V}ar_F(X) = \sum_{k=1}^K \omega_k \mathbb{V}ar_{G_k}(X)
$$ {#eq-mixture-variance-zero-mean}

### 1.2 Why finite mixture models?

Finite mixtures of distributions within a single family provide a lot of flexibility. For example, a mixture of Gaussian distributions can have a bimodal density.

8. Example: Bimodal Mixture:

$$
f(x) = 0.6 \frac{1}{\sqrt{2\pi}} \exp\left\{ -\frac{1}{2}x^2 \right\} 
     + 0.4 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left\{ -\frac{1}{2}\frac{(x-5)^2}{4} \right\}
$$ {#eq-bimodal-mixture}

9. Example: Skewed Unimodal Mixture:

$$
f(x) = 0.55 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left\{ -\frac{1}{2} \frac{x^2}{2} \right\}
    + 0.45 \frac{1}{\sqrt{2\pi} 2} \exp\left\{ -\frac{1}{2}\left(\frac{x-3}{2}\right)^2 \right\}
$$ {#eq-skewed-unimodal-mixture}

10. Example: Symmetric Heavy-tailed Mixture:

$$
f(x) = 0.4 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left\{ -\frac{1}{2} \frac{x^2}{2} \right\}
    + 0.4 \frac{1}{\sqrt{2\pi} 4} \exp\left\{ -\frac{1}{2} \frac{x^2}{16} \right\}
    + 0.2 \frac{1}{\sqrt{2\pi} \sqrt{20}} \exp\left\{ -\frac{1}{2} \frac{x^2}{20} \right\}
$$ {#eq-symmetric-heavy-tailed-mixture}

11. Zero-inflated Negative Binomial PMF:

$$
\mathbb{P}r(x) = 
\begin{cases}
    \omega_1 + (1-\omega_1)\theta^r & x=0 \\
    (1-\omega_1) \binom{x+r-1}{x} \theta^r (1-\theta)^x & x>1
\end{cases}
$$ {#eq-zero-inflated-negative-binomial-pmf}

12. Regular Negative Binomial PMF:

$$
p^*(x) = \binom{x+r-1}{x} \theta^r (1-\theta)^x
$$ {#eq-regular-negative-binomial-pmf}

13. Zero-inflated Log-Gaussian PDF:

$$
f(x) = 
\begin{cases}
    \omega_1 \delta_0(x) + (1-\omega_1)\frac{1}{\sqrt{2\pi}\sigma x}\exp\left\{ -\frac{(\ln x - \mu)^2}{2\sigma^2} \right\} & x > 0 \\
    0 & \text{otherwise}
\end{cases}
$$ {#eq-zero-inflated-log-gaussian-pdf}

where $\delta_0(x)$ is the Dirac delta function at $x=0$.

### 1.3 hierarchical representation of finite mixtures

14. Mixture Model (Hierarchical):

$$
X \mid c \sim G_c, \quad \mathbb{P}r(c = k) = \omega_k
$$ {#eq-hierarchical-mixture}

where $G_c$ is the distribution of the $k$-th component and $\omega_k$ is the weight for the $k$-th component.

### 1.4 The likelihood function for mixture models

15. Observed-data Likelihood for a mixture Model

$$
L_O(\theta, \omega; x) \propto \mathbb{P}r(x \mid \theta, \omega) = \prod_{i=1}^n \sum_{k=1}^K \omega_k g_k(x_i \mid \theta_k)
$$ {#eq-observed-data-likelihood}

where $g_k(x_i \mid \theta_k)$ is the PDF/PMF of the $k$-th component distribution evaluated at $x_i$ with parameter $\theta_k$.

16. Mixture Model (Likelihood, complete-data):

$$
L(\theta, \omega; x, c) = \mathbb{P}r(x, c \mid \theta, \omega) = \prod_{i=1}^n \prod_{k=1}^K [\omega_k g_k(x_i \mid \theta_k)]^{1(c_i = k)}
$$ {#eq-complete-data-likelihood}

where $1(c_i = k)$ is an indicator function that is 1 if $c_i = k$ and 0 otherwise.

17. Alternative complete-data likelihood decomposition:

$$
\mathbb{P}r(x, c \mid \theta, \omega) = \mathbb{P}r(x \mid c, \theta) \mathbb{P}r(c \mid \omega)
$$ {#eq-complete-data-likelihood-decomposition}

with

$$
\mathbb{P}r(x \mid c, \theta) = \prod_{i=1}^n g_{c_i}(x_i \mid \theta_{c_i})
$$ {#eq-complete-data-likelihood-x}

$$
\mathbb{P}r(c \mid \omega) = \prod_{k=1}^K \omega_k^{\sum_{i=1}^n 1(c_i = k)}
$$ {#eq-complete-data-likelihood-c}

where $1(c_i = k)$ is an indicator function that is 1 if $c_i = k$ and 0 otherwise.


### 1.5 parameter identifiability

Label switching

TODO : missing formula

$$
f(x) = ...
$$ {#eq-parameter-identifiability-mix1}

TODO : missing formula

$$
f(x) = ...
$$ {#eq-parameter-identifiability-mix2}

Number of components

$$
f(x) = ...
$$ {#eq-parameter-identifiability-mix3}


## 2. Maximum Likelihood Estimation For Mixture Models:


18. Maximum Likelihood Estimator (MLE) for Mixture:


$$
(\hat{\omega}, \hat{\theta}) = \arg\max_{\omega, \theta} \prod_{i=1}^n \sum_{k=1}^K \omega_k g_k(x_i \mid \theta_k)
$$ {#eq-max-observed-data-likelihood}

where $\hat{\omega}$ and $\hat{\theta}$ are the MLEs for the weights and parameters of the mixture components, respectively.

### 2.1 EM Algorithm for Mixture Models:

19. EM algorithm Set Q-function in E step:

E step:

$$
Q(\omega, \theta \mid \omega^{(t)}, \theta^{(t)}, x) = \mathbb{E}_{c \mid \omega^{(t)}, \theta^{(t)}, x} [\log \mathbb{P}r(x, c \mid \omega, \theta)]
$$ {#eq-em-Q-function}


where $Q(\omega, \theta \mid \omega^{(t)}, \theta^{(t)}, x)$ is the expected complete-data log-likelihood given the current estimates of the parameters $\omega^{(t)}$ and $\theta^{(t)}$ and the observed data $x$.

19. EM algorithm Set parameters in M step:

$$
(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)}) = \arg\max_{\omega, \theta} Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, y)   
$$ {#eq-em-m-step}

where $(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)})$ are the updated estimates of the parameters after the M step.

<!-- missing em stuff here -->
$$
\mathbb{P}r(c_i = k \mid \omega, \theta, x) = \frac{\omega_k g_k(x_i \mid \theta_k)}{\sum_{l=1}^K \omega_l g_l(x_i \mid \theta_l)} = v_{i,k}(\omega, \theta)
$$ {#eq-conditional-independence-of-components}

where $v_{i,k}(\omega, \theta)$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\omega$ and $\theta$.

and $(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)})$ are the updated estimates of the parameters after the M step.

Also, remember that:

$$
\mathbb{P}r(x, c | \theta, \omega) =
\prod_{i=1}^n
\prod_{k=1}^K
[\omega_k g_k(x_i | \theta_k)]^{\mathbb{1}(c_i=k)}
$$ {#eq-em-m-step}

The value of $v_{i,k}(\omega, \theta)$ can be interpreted as the probability that observation $i$ was generated from component $k$ if we assume that the true parameters of
the mixture model are $\omega$ and $\theta$.


<!-- missing em stuff here -->
$$
\mathbb{P}r(c_i = k \mid \omega, \theta, x) = \frac{\omega_k g_k(x_i \mid \theta_k)}{\sum_{l=1}^K \omega_l g_l(x_i \mid \theta_l)} = v_{i,k}(\omega, \theta)
$$ {#eq-conditional-independence-of-components}

where $v_{i,k}(\omega, \theta)$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\omega$ and $\theta$.

which implies that 

$$
\log \mathbb{P}r(x_i,c \mid \theta, \omega) = \sum_{n=1}^N \sum_{k=1}^K \mathbb{1}(c_i=k) [\log(\omega_k) + \log(g_k(x_i \mid \theta_k))]
$$ {#eq-log-likelihood}

where $\mathbb{1}(c_i=k)$ is an indicator function that is 1 if $c_i = k$ and 0 otherwise.


Hence 

$$
Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x) = \sum_{i=1}^n \sum_{k=1}^K v_{i,k} \mathbb{E}_{c \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x} \left [\mathbb{1}_{(c_i=k)} \log(\omega_k) + \log(g_k(x_i \mid \theta_k))\right]
$$ {#eq-em-Q-function}

where $v_{i,k}(\omega^{(t)}, \theta^{(t)})$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\omega^{(t)}$ and $\theta^{(t)}$.

and therefore
$$
Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x) = \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)}\left(\hat{\omega}^{(t)}, \hat{\theta}^{(t)}\right) [\log(\omega_k) + \log(g_k(x_i \mid \theta_k))]
$$ {#eq-em-Q-function}

where $v_{i,k}(\hat{\omega}^{(t)}, \hat{\theta}^{(t)})$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\hat{\omega}^{(t)}$ and $\hat{\theta}^{(t)}$.

(Remember that the term that is constant with respect to c can come out of the expectation, and that the expected value of an indicator function is just the probability of the event inside the indicator). 

Hence, roughly speaking, we can see that the Q function is in this case equivalent to a weighted average of the log likelihoods associated with each of the components in the mixture.

## 2.2 The EM algorithm for a Location Mixture of Two Gaussians

$$
f(x\mid \omega, \mu_1,\mu_2, \sigma) = \omega \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{ -\frac{(x - \mu_1)^2}{2\sigma^2} \right\} + (1-\omega) \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{ -\frac{(x - \mu_2)^2}{2\sigma^2} \right\}
$$ {#eq-gaussian-location-mixture}

where $\omega_1$ and $\omega_2$ are the weights of the two components, $\mu_1$ and $\mu_2$ are the means of the two components, and $\sigma^2$ is the common variance.

The expected weights are:

$$
v^{(t+1)}_{i,1} =v^{(t+1)}_{i,1} \left (\hat{\omega}^{(t)},\hat{\mu}^{(t)},\hat{\sigma}^{(t)}\right)=\frac{\hat{\omega}^{(t)}_1 \frac{1}{\sqrt{2\pi}\hat{\sigma}^{(t)}} \exp\left\{ -\frac{(x_i - \hat{\mu}^{(t)}_1)^2}{2\hat{\sigma}^{(t)}} \right\}}{\sum_{k=1}^K \hat{\omega}^{(t)}_k \frac{1}{\sqrt{2\pi}\hat{\sigma}^{(t)}} \exp\left\{ -\frac{(x_i - \hat{\mu}^{(t)}_k)^2}{2\hat{\sigma}^{(t)}} \right\}}
$$ {#eq-location-mixture-weights}

and

$$
v^{(t+1)}_{i,2} = 1 - v^{(t+1)}_{i,1}
$$

Therefore, the Q function is

$$
\begin{aligned}
Q(\omega, \mu_1, \mu_2, \sigma \mid \hat{\omega}^{(t)}, \hat{\mu}^{(t)}_1,\hat{\mu}^{(t)}_2, \hat{\sigma}^{(t)}, x) = \sum_{i=1}^n & v_{i,1}^{(t+1)} \left [\log(\omega) - \frac{1}{2} \log(2\pi) - \log(\sigma) - \frac{(x_i - \mu_1)^2}{2\sigma^2}\right ] \\
+ & v_{i,2}^{(t+1)} \left [\log(1-\omega) - \frac{1}{2} \log(2\pi) - \log(\sigma) - \frac{(x_i - \mu_2)^2}{2\sigma^2}\right] 
\end{aligned}
$$ {#eq-Q-function}

where $v_{i,1}^{(t+1)}$ and $v_{i,2}^{(t+1)}$ are the posterior probabilities of the first and second components given the observed data $x_i$ and the current estimates of the parameters $\hat{\omega}^{(t)}$, $\hat{\mu}^{(t)}_1$, $\hat{\mu}^{(t)}_2$, and $\hat{\sigma}^{(t)}$.

The Q function is a function of the parameters $\omega$, $\mu_1$, $\mu_2$, and $\sigma$ and is used to update the estimates of these parameters in the M step of the EM algorithm.

to maximize Q we compute its partial derivatives with respect to $\omega$, $\mu_k$, and $\sigma$ and set them equal to zero.
The partial derivative with respect to $\omega$ is:


$$
\frac{\partial Q}{\partial \omega} = \sum_{i=1}^n \left [\frac{v_{i,1}^{(t+1)}}{\omega} - \frac{v_{i,2}^{(t+1)}}{1-\omega} \right ] = 0
$$ {#eq-partial-derivative-of-Q-wrt-omega}

$$
\frac{\partial Q}{\partial \mu_k} = \sum_{i=1}^n v_{i,k}^{(t+1)} \frac{1}{\sigma^2} (x_i - \mu_k) = 0
$$ {#eq-partial-derivative-of-Q-wrt-mu}


$$
\frac{\partial Q}{\partial \sigma} = \sum_{i=1}^n \left [\frac{v_{i,1}^{(t+1)}}{\sigma} - \frac{(x_i - \mu_1)^2}{\sigma^3} + \frac{v_{i,2}^{(t+1)}}{\sigma} - \frac{(x_i - \mu_2)^2}{\sigma^3} \right ] = 0
$$ {#eq-partial-derivative-of-Q-wrt-sigma}


By setting (@eq-partial-derivative-of-Q-wrt-omega) equal to zero we get:

$$ 
\begin{aligned}
&\left \{ \sum_{i=1}^n v_{i,2}^{(t+1)} \right \}  \omega^{(t+1)} & = &
\left \{ \sum_{i=1}^n v_{i,1}^{(t+1)} \right \} \left (1 - \omega^{(t+1)}\right) 
\\ \implies & \left \{ \sum_{i=1}^n v_{i,1}^{(t+1)} \right \} & = &
\left \{ \sum_{i=1}^n v_{i,1}^{(t+1)} + \sum_{i=1}^n v_{i,2}^{(t+1)} \right \} \omega^{(t+1)}
\\ \implies & \omega^{(t+1)} & = & \frac{\sum_{i=1}^n v_{i,1}^{(t+1)}}{\sum_{i=1}^n v_{i,1}^{(t+1)} + v_{i,2}^{(t+1)}} 
\\ &  &= & \frac{1}{n} \sum_{i=1}^n v_{i,1}^{(t+1)}
\end{aligned}
$$ {#eq-omega-update}

where $\omega^{(t+1)}$ is the updated estimate of the weight for the first component after the M step.

where we used the fact that $\sum_{i=1}^n v_{i,1}^{(t+1)} + \sum_{i=1}^n v_{i,2}^{(t+1)} = n$.

This means that the new estimate of $\omega$ is the average of the posterior probabilities of the first component over all observations.

by setting (@eq-partial-derivative-of-Q-wrt-mu) equal to zero we get:

$$\begin{aligned}
            && 0 && = & \sum_{i=1}^n v_{i,k}^{(t+1)} (x_i - \mu_k) 
\\ \implies && \sum_{i=1}^n v_{i,k}^{(t+1)} x_i && = & \sum_{i=1}^n v_{i,k}^{(t+1)} \mu_k
\\ \implies && \mu_k^{(t+1)} && = & \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} x_i}{\sum_{i=1}^n v_{i,k}^{(t+1)}}
\end{aligned}
$$ {#eq-em-solving-for-mu}

where we used the fact that $\sum_{i=1}^n v_{i,k}^{(t+1)} = n$.

Note that the partial estimator for μk is a weighted average of the xis, with the weight associated with observation i being proportional to the probability that such observation was generated by component k. Again, if the components are well separated and values of vi,k are all close to either 0 or 1, this weighted average is roughly the average of the observations coming from component k

Finally, making the same argument for the partial derivative with respect to σ, we get:

$$
\begin{aligned}
& 
    \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} &=& \left ( \frac {1}{\sigma^{(t+1)}} \right ) ^ 2  \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)})^2
\\ \implies &
 \sigma^{(t+1)} &=& \sqrt{\frac{\sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)})^2}{\sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)}}}
\end{aligned}
$$ {#eq-em-solving-for-sigma}

### 2.3 general location and scale mixtures of p-variate gaussian distributions

$$
f(x) =
\sum_{k=1}^K
\omega_k
\left( \frac{1}{(2\pi)} \right) ^\frac{q}{2}
|\Sigma_k|-\frac{1}{2} \exp
\left\{
(x - \theta_k)^T \Sigma_k^{-1}(x - \theta_k)
\right\}
$$ {#eq-general-location-scale-mixture}

where $\theta_k$ is the mean of the $k$-th component, $\Sigma_k$ is the covariance matrix of the $k$-th component, and $\omega_k$ is the weight for the $k$-th component.
$$
\begin{aligned}
v_{i,k}^{(t+1)} &= \frac{\omega_k^{(t)} |\Sigma_k^{(t)}|^{-1/2} \exp\left\{-\frac{1}{2}(x_i - \mu_k^{(t)})^T [\Sigma_k^{(t)}]^{-1}(x_i - \mu_k^{(t)})\right\}}{\sum_{l=1}^K \omega_l^{(t)} |\Sigma_l^{(t)}|^{-1/2} \exp\left\{-\frac{1}{2}(x_i - \mu_l^{(t)})^T [\Sigma_l^{(t)}]^{-1}(x_i - \mu_l^{(t)})\right\}} \\
\omega_k^{(t+1)} &= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)}}{\sum_{i=1}^n \sum_{l=1}^K v_{i,l}^{(t+1)}} \\
\mu_k^{(t+1)} &= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} x_i}{\sum_{i=1}^n v_{i,k}^{(t+1)}} \\
\Sigma_k^{(t+1)} &= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)}) (x_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^n v_{i,k}^{(t+1)}}
\end{aligned}
$$ {#eq-em-p-variate-mixture-parameters}

where $v_{i,k}^{(t+1)}$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\omega^{(t)}$, $\mu^{(t)}$, and $\Sigma^{(t)}$.


## 3. MCMC

$$
f(x) = \sum_{k=1}^K \omega_k g_k(x | \theta_k)
$$ {#eq-mcmc-mixture}

where $g_k(x \mid \theta_k)$ is the PDF/PMF of the $k$-th component distribution evaluated at $x$ with parameter $\theta_k$.

$$
\mathbb{P}r(\omega) \;=\;
\frac{\Gamma \bigl ( \sum_{k=1}^K a_k \bigr ) }{\prod_{k=1}^K \Gamma(a_k)}
\prod_{k=1}^K \omega_k^{\,a_k-1},
\quad
\sum_{k=1}^K \omega_k = 1
$$ {#eq-dirichlet-prior}

### 3.1 Markov chain monte carlo algorithms for mixture mod-
els

$$
\mathbb{P}r(x, c \mid \theta, \omega) = \mathbb{P}r(x \mid c, \theta)\ \mathbb{P}r(c \mid \omega)
$$ {#eq-pxc-given-theta-omega}

where 

$$
\mathbb{P}r(x \mid c, \theta) = \prod_{i=1}^n g_{c_i}(x_i \mid \theta_{c_i})
$$ {#eq-px-given-c-theta}

and

$$
\mathbb{P}r(c \mid \omega) = \prod_{i=1}^n \prod_{k=1}^K 
\omega_k ^{\mathbb{1}(c_i = k)} =
\prod_{k=1}^K \omega_k^{\sum_{i=1}^n \mathbb{1}(c_i = k)} 
$$ {#eq-pc-given-omega}

where $1(c_i = k)$ is an indicator function that is 1 if $c_i = k$ and 0 otherwise.

21. Joint posterior (with latent labels)

$$
\mathbb{P}r(c,\theta,\omega \mid x)
\;\propto\;
\Bigl(\prod_{i=1}^n g_{c_i}(x_i\mid \theta_{c_i})\Bigr)
\Bigl(\prod_{k=1}^K \omega_k^{\sum_{i=1}^n1(c_i=k)}\Bigr)
\,p(\omega)\,p(\theta)
$$ {#eq-joint-posterior}

Each of the full conditional distributions can be derived from this joint posterior by retaining the terms that involve the parameter of interest, and recognizing the product of the selected terms as the kernel of a known
family of distributions.

22. Full conditional for $\omega$

$$
\mathbb{P}r(\omega \mid c,\theta,x)
\;\propto\;
\mathbb{P}r(c \mid \omega) \mathbb{P}r(\omega)
\;=\;
\prod_{k=1}^K
\omega_k^{\,a_k + \sum_{i=1}^n1(c_i=k)\;-\;1}
$$ {#eq-full-cond-omega}

This clearly corresponds to the kernel of another Dirichlet distribution
with updated parameters
$$
a_k^* = a_k + m_k, \qquad k = 1, \ldots, K,
$$

where $m_k = \sum_{i=1}^n 1(c_i = k)$ is the number of observations in component $k$.

Full conditional for each component $c_i$

$$
\mathbb{P}r(c_i \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x) \propto \mathbb{P}r(x_i \mid c_i,\theta_k) \mathbb{P}r(c_i \mid \omega)
$$ {#eq-fullcond-ci-1}

hence

$$
\mathbb{P}r(c_i = k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)
\;=\;
\frac{\omega_k\,g_k(x_i\mid \theta_k)}
     {\sum_{l=1}^K \omega_l\,g_l(x_i\mid \theta_l)}
$$ {#eq-fullcond-ci-2}

Note the similarity with the formula for the expected weights $v_{i,k}$ in the
EM algorithm.) 

Full conditional for component parameters $\theta_k$

$$
\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)
\;\propto\;
\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)
\prod_{i:c_i=k} \mathbb{P}r(x_i\mid \theta_k)
$$ {#eq-fullcond-theta_k}

In the most common case in which the priors for $\theta_k$ are independent this is simply:

$$
\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x) 
\;\propto\;
\mathbb{P}r(\theta_k) \prod_{i:c_i=k} \mathbb{P}r(x_i\mid \theta_k)
$$ {#eq-fullcond-theta_k-indep}



### 3.2 The MCMC algorithm for a location mixture of two gaussian distributions

$$
f(x \mid \omega, \mu_1, \mu_2, \sigma) = \omega \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{ -\frac{(x - \mu_1)^2}{2\sigma^2} \right\} + (1-\omega) \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{ -\frac{(x - \mu_2)^2}{2\sigma^2} \right\}
$$ {#eq-mcmc-location-mixture}

25. Gaussian prior for means $\mu\_k$

$$
\mathbb{P}r(\mu_k)
=\frac{1}{\sqrt{2\pi\tau^2}}
\exp\!\Bigl\{-\tfrac{(\mu_k-\eta)^2}{2\tau^2}\Bigr\}
$$ {#eq-prior-muk}

26. Inverse-Gamma prior for variance $\sigma^2$:

with shape parameter $a$ and scale parameter $b$ for $\sigma$

$$
\mathbb{P}r(\sigma^2)
=\frac{1}{b^a\Gamma(a)}
(\sigma^2)^{-\,d-1}
\exp\!\Bigl\{-\tfrac{q}{\sigma^2}\Bigr\}
$$ {#eq-prior-sigma2}



$$
\begin{aligned}
\mathbb{P}r(\mu_k \mid c, \mu_1, \ldots, \mu_{k-1}, \mu_{k+1}, \ldots, \mu_K, \omega, x) 
& \;\propto\;
\exp\Biggl\{
-\frac{(\mu_k - \eta)^2}{2\tau^2}
\Biggr\} \prod_{i:c_i=k}
\exp\Biggl\{
-\frac{(x_i - \mu_k)^2}{2\sigma^2}
\Biggr\}
\\ & \;\propto\;
\exp\Biggl\{
-\frac{1}{2}
\Biggl[ m_k \frac{1}{\sigma^2} - 2 \frac{\mu_k \sum_{i:c_i=k} x_i}{\sigma^2} + \frac{\mu_k}{\tau^2} - 2 \frac{\mu_k\eta}{\tau^2} \Biggr] \Biggr\}
\\ & \;\propto\;
\exp\Biggl\{
-\frac{1}{2}
\Biggl[ \frac{m_k}{\sigma^2} + \frac{1}{\tau^2} \Biggr] \Biggl[ \mu_k - \frac{\sigma^2 \sum_{i:c_i=k} x_i + \frac{\eta}{\tau^2} }{\mu_k + \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2}} \Biggr] \Biggr\}

\end{aligned}
$$ {#eq-fullcond-mu_k}

which is just the kernel of a normal distribution with updated mean

$$
\eta_k^* = \frac{\frac{1}{\sigma^2}\sum_{i:c_i=k} x_i + \frac{\eta}{\tau^2}}{ \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2}u^2}
$$ {#eq-eta-k-star}

and updated standard deviation

$$
\tau_k^*=\Bigl[ \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2} \Bigr]^{-1/2}
$$ {#eq-post-muk}

finaly 

$$
\begin{aligned}
\mathbb{P}r(\sigma^2 | c, \mu, \omega, x) 
&\propto 
\Biggl(\frac{1}{\sigma^2}\Biggr)^{d+1}
\exp\Biggl\{
− q
\sigma^2
\Biggr\}
\Biggl(\frac{1}{\sigma^2}\Biggr)^{n/2}
\exp\Biggl\{
− \frac{1}{2\sigma^2}
\sum_{i=1}^{n}(x_i − \mu_{c_i})^2
\Biggr\}
\\&=
\Biggl(\frac{1}{\sigma^2}\Biggr)^{n/2+d+1}
\exp\Biggl\{
− \frac{1}{\sigma^2}
\Biggl[ \frac{1}{2} \sum_{i=1}^{n}(x_i − \mu_{c_i})^2
+ q
\Biggr]
\Biggr\}
\end{aligned}
$$

which is the kernel of another Inverse Gamma distribution with shape $d∗ =n/2 + d$ and rate parameter

$$
q^*=\tfrac12\sum_{i=1}^n(x_i-\mu_{c_i})^2 + q
$$ {#eq-post-sigma2}

### 3.3 general location and scale mixtures of p-variate gaus-
sian distributions

29. General $q$-variate Gaussian mixture

$$
\sigma^2 \;\sim\;\mathrm{InvGamma}\bigl(d^*,\,q^*\bigr),
\quad
d^*=\tfrac{n}{2}+d,
\quad
q^*=\tfrac12\sum_{i=1}^n(x_i-\mu_{c_i})^2 + q
$$ {#eq-post-sigma2}

$$
f(x)
=\sum_{k=1}^K
\omega_k\,
\frac{1}{(2\pi)^{q/2}\,\lvert\Sigma_k\rvert^{1/2}}\,
\exp\!\Bigl\{-\tfrac12(x-\mu_k)^\top\Sigma_k^{-1}(x-\mu_k)\Bigr\}
$$ {#eq-multivariate-mixture}

30. Posterior for multivariate $\mu\_k$

$$
\mu_k \;\sim\; N\bigl(b_k^*,\,B_k^*\bigr),
\quad
B_k^*=\bigl(B^{-1}+m_k\,\Sigma_k^{-1}\bigr)^{-1},
\quad
b_k^*=B_k^*\bigl(B^{-1}b + \Sigma_k^{-1}\sum_{i:c_i=k}x_i\bigr)
$$ {#eq-post-multivariate-mu}

31. Posterior for multivariate $\Sigma\_k$

$$
\Sigma_k \;\sim\;\mathrm{InvWishart}\bigl(\nu^*,\,S^*\bigr),
\quad
\nu^*=\nu + m_k,
\quad
S^*=S + \sum_{i:c_i=k}(x_i-\mu_k)(x_i-\mu_k)^\top
$$ {#eq-post-multivariate-sigma}



## 4. Applications of Mixture Models


### 4.1 density estimation


32. Kernel density estimator (general form)

$$
\tilde f(x)
=\frac{1}{n}\sum_{i=1}^n\frac{1}{h}\,
g \Bigl(\tfrac{\|x-x_i \| }{h}\Bigr)
$$ {#eq-kde-general}

33. Gaussian kernel density estimator

$$
\tilde f(x)
=\sum_{i=1}^n\frac{1}{n}\,
\tfrac{1}{\sqrt{2\pi\,}h}\,
\exp\!\Bigl\{-\tfrac{1}{2}\left(\dfrac{x-x_i}{h}\right)^2\Bigr\}
$$ {#eq-kde-gaussian}


In order to understand the relationship between kernel density estimation
and mixture models it is useful to contrast (8) with the density estimate
$$
\hat f(x) = \sum_{k=1}^K \hat{\omega}_k \frac{1}{\sqrt{2\pi} \hat{\sigma}} \exp\Biggl\{-\frac{1}{2\hat{\sigma}^2}(x - \hat{\mu}_k)^2\Biggr\}
$$

obtained by plugging-in the maximum likelihood estimates of the parameters, $\hat{\omega}_1, . . . , \hat{\omega}_K, \hat{\mu}_1, . . . , \hat{\mu}_K$ and $\hat{\sigma}$ of a location mixture of K univariate Gaussian distributions.

### 4.2 clustering (unsupervised classification

$$
f(x) = \sum_{k=1}^K \frac{1}{K} \left(\frac{1}{\sqrt{2\pi\sigma}} \right)^p \exp\left\{-\frac{1}{2\sigma^2} (x − \mu_k)^T (x − \mu_k)\right\}
$$ {#eq-clustering}




### 4.3 (supervised) classification

## 5. Practical Considerations



### 5.1 Ensuring numerical stability when computing class probabilities


$$
\begin{aligned}
\frac{z_k}{\sum_{l=1}^K z_l} &= \frac{\exp\{ \log z_k \}}{\sum_{l=1}^K \exp\left\{ \log z_l \right\}} &&  \text{(exp and log are inverse
functions)}\\
&= \frac{\exp\{ -b \}\exp\{ \log z_k \}}{\exp\{ -b \}\sum_{l=1}^K \exp\left\{ \log z_l \right\}} && \text{(multiply and divide by
the same quantity)} \ e^{-b} \\
&= \frac{\exp\left\{ \log z_k - b \right\}}{\sum_{l=1}^K \exp\left\{ \log z_l - b \right\}} \\
\end{aligned}
$$ {#eq-zk-sum}

Although (11) is valid for any b, it should be clear that some values will
work better than others for the purpose of avoiding a 0/0 calculation. In
particular, we are interested in choosing a value b that makes at least one of
the terms in the denominator different from zero after exponentiation. One
such choice is $b=\max_{l=1,\ldots,K} \log z_l$, which gives us

$$
\sum_{l=1}^K \exp\left\{ \log z_l - \max_{l=1,\ldots,K} \log z_l \right\} = 1 + \sum_{l:l \neq l^*} \exp\left\{ \log z_l - \max_{l=1,\ldots,K} \log z_l \right\}
$$

One key advantage of this choice is that all the terms in the sum are less or equal than one, which ensures that we do not overflow
when computing  $\exp\left\{ \log z_l - \max_{l=1,\ldots,K} \log z_l \right\}$.

### 5.2 numerical consequences of multimodality

no math in this section

### 5.3 selecting the number components


$$
\text{BIC} = -2 \ell(\hat{\theta}) + p \log(n)
$$

where $\ell(\hat{\theta})$ is the maximized log-likelihood, $p$ is number of free parameters, $n$ is sample size.

### 5.4 fully Bayesian inference on the number of components
$$
f(x) = \sum_{k=1}^K \omega_k g_k(x | \theta_k) + \sum_{k=K+1}^K 0g_k(x | \theta_k)
$$


$$
(\omega_1, \ldots, \omega_K) \sim Dirichlet( \frac{1}{K}, \ldots , \frac{1}{K} )
$$

where K is the number of components in the mixture model.

$$
\mathbb{E}(K^*)=\sum_{i=1}^m \frac{\alpha}{\alpha + i - 1} \approx \frac{\alpha \log(n + \alpha -1) }{\alpha}
$$ {#eq-expected-K}

where $\alpha$ is the concentration parameter of the Dirichlet process prior, $n$ is the number of observations, and $K^*$ is the number of components in the mixture model.

### 5.5 fully Bayesian inference on the partition structure



Extracting formulas from **Section 4.2 ("Clustering (unsupervised classification)") to the end** of *mixturemodels (2).pdf*:

---

## Section 4.2: Clustering (Unsupervised Classification) — Formulas

### 1. Hard assignment (mode assignment for EM):

$$
\hat{c}_i = \arg\max_k \hat{v}_{i,k}
$$

where $\hat{v}_{i,k}$ are the final iteration weights.

---

### 2. Posterior Probability of Cluster Assignment (Bayesian approach):

$$
\mathbb{P}r(c_i = k \mid x, \text{rest}) = v_{i,k}
$$

where $v_{i,k}$ is as defined in the EM/MCMC steps (probability that observation $i$ comes from component $k$).

---

## Section 4.3: (Supervised) Classification

### 3. Posterior probability of class membership (LDA/QDA/Mixture discriminant analysis):

$$
\mathbb{P}r(Y = k \mid X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
$$

where $\pi_k$ is prior class probability, $f_k(x)$ is the class-conditional density for class $k$.

---

### 4. For a mixture model:

$$
f_k(x) = \sum_{j=1}^{M_k} w_{k,j} g_{k,j}(x)
$$

where each class-conditional density can itself be a mixture (with weights $w_{k,j}$ and kernels $g_{k,j}(x)$).

---

### 5. Posterior probability for classification under mixture model:

$$
\mathbb{P}r(Y = k \mid X = x) = \frac{\pi_k \sum_{j=1}^{M_k} w_{k,j} g_{k,j}(x)}{\sum_{l=1}^K \pi_l \sum_{j=1}^{M_l} w_{l,j} g_{l,j}(x)}
$$

---

## Section 5: Practical Considerations

### 6. Log-sum-exp trick (for numerical stability):

$$
\log \left( \sum_{k=1}^K e^{a_k} \right) = a_{k^*} + \log \left( \sum_{k=1}^K e^{a_k - a_{k^*}} \right)
$$

where $a_{k^*} = \max\{a_1, \ldots, a_K\}$.

---

### 7. Bayesian Information Criterion (BIC):

$$
\text{BIC} = -2 \ell(\hat{\theta}) + p \log(n)
$$

where $\ell(\hat{\theta})$ is the maximized log-likelihood, $p$ is number of free parameters, $n$ is sample size.

---

### 8. Dirichlet process partition probability:

$$
\mathbb{P}r(c_1, \ldots, c_n) = \frac{\alpha^K \prod_{k=1}^K (n_k - 1)!}{\prod_{j=1}^n (\alpha + j - 1)}
$$

where $K$ is number of clusters, $n_k$ is number of points in cluster $k$, $\alpha$ is the concentration parameter.


