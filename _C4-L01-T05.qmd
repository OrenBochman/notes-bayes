<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/B9LHp/simulating-from-an-ar-1-process -->

I will now show you how to get samples from a simulated AR process using the arima.sim
function in R. 
So here, I'm just going to begin setting the random seed so that you can replicate the example that I'm running here. 
And what I'm going to do is I'm going to
set the number of observations or time points that I want to sample to
500 for these different processes. And I'm going to sample from
an autoregressive of order one with an AR coefficient 0.9 and
variance 1. So the arima.sim function is going to
take different kinds of parameters here, this is a more general function than
just sampling from an AR process. You can actually sample
from an autoregressive integrated moving average process. So is a process that has an AR, an
autoregressive part. Integrated part, meaning how many
times you difference the series and then a moving average part that
tells you also what order and what parameters you are going to have for
that process. We will only use this function
to simulate from AR processes. So here in the model, I specify
the list of components for the model, so usually the list is going to
contain the AR coefficients. The information about
the I part of the process, the integrated part of the process,
we don't have an integrated part here. And then also information about the moving
average component that we don't have here. And then you specify also what is the
standard deviation of the process, which is going to be just the square root of the
variance of the process that we defined. So I'm setting the variance to be 1,
then I'm setting the standard deviation of the process to be
the square root of the variance phi here. I'm setting the AR
coefficient to be 0.9 and then I just simulate 500 observations. And then I'm going to also show you
what happens if I were to simulate 500 observations from another AR1 process. Where now I'm going to keep
the same variance 1 but instead of having
a positive AR coefficient, I'm going to consider
a negative 0.9 AR coefficient. Just to show you how those processes
look very different in terms of their properties. So if I were to plot those two time
series, the first one that I plot here is that set of simulated observations
from an AR1 with AR coefficient 0.9. So you can see this kind of
almost random work behavior that is consistent with having
a high value of the AR coefficient. The process is still
because I'm using 0.9, the process is a stable process and
it's going to be stationary. If I were to use a phi value that
is above 1 or below minus 1, I would have an explosive process and
you would see that the function would just tell you there is
a problem with that process. The second AR process that we simulated
had AR coefficient minus 0.9, so that one is consistent
as we said before. You can see the behavior here is
very different just looking at something that has a negative
value there is this oscillatory behavior that is more quasi periodic. So I can now plot the true
auto correlation function and I'm going to plot for both processes. I'm going to show you also how the sample
ACF and the sample PACF look like. So here I'm going to set the maximum
lag to be 50 just to show you all the way to 50 lag and computing
first, the covariance of the process. So that's the autocovariance at lag zero. So the variance of the process is just the
v / 1 minus phi square, that's the structure. And then as you know,
the autocovariance is just has, you have to multiply that by the AR
coefficient to the power of the lag. So this is what this line is doing. So if I were to do this and
now plot that true ACF of the process, we can see what we learn because
the phi is smaller than one. We are going to have this exponential
decay as a function of the lag which is what this process does. In terms of just the second
process that we simulated with the minus 0.9 as the AR coefficient. We can compute again the true
autocovariance function, the true autocorrelation function. And it's going to look similar in
terms of the exponential behavior, the decay here the decay is exponential
as a function of the lag. But you can see the difference
between these two plots, is that here the exponential decay occurs,
with this oscillatory behavior. That has to do with the fact that
you have a negative coefficient. So now, if you look at how the ACF,
the sample ACF and the sample for each of the two processes for
the first process and the second process. So if you just now use
the ACF function and look at the data in there,
put the data in there. You can see that the sample ACF based
on those 500 observations that you simulated from each of the two processes
resembles the structure of the true ACF. So in here, you have that sort
of exponential decay that is consistent with the true
ACF of the process. And in here you have that
oscillatory behavior but also exponential decay that is consistent
with the minus 0.9 AR coefficient. You can then plot the PACF for
the sample PACF for the two processes. What we know here is that because these are simulated observations
from an AR1 process. What should happen with the partial
autocorrelation function is that any lag after the first
one should be zero or should be negligible in this case for
the sample PACF. So let's plot those two and
we can see that this is precisely what is happening with the sample
PACF based on those two data sets. You simulate for
the first case you have only the first lag is not negligible,
which is consistent with the fact that these are simulated
observations from an AR1. And then you have that is a positive
quantity that corresponds precisely actually should be close to
the coefficient 0.9 that you have here. For the minus 0.9, you have
a negative PACF coefficient at lag1, which should be close to that
negative 0.9 value of the phi and everything else is negligible. So you can see that for
these two examples, the sample PACF is also doing
what you expected to do.