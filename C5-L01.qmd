---
date: 2025-07-02
title: "Probability - M1L1"
subtitle: "Capstone Project: Bayesian Conjugate Analysis for Autogressive Time Series Models"
subject: "Bayesian Statistics"
description: "Capstone Project: Bayesian Conjugate Analysis for Autogressive Time Series Models"
categories:
  - Bayesian Statistics
  - Capstone Project
keywords:
  - Odds
  - Probability
  - Expectation
  - Ïƒ-Algebra
  - Probability Measure
  - Probability Space
  - Probability Paradigms
  - Coherence
  - Abraham de Moivre
  - Bruno de Finetti
  - Objectivity
  - Variance
  - Covariance
  - Correlation
---

::: {.callout-note collapse="true"}
## Learning Objectives

-   [x] Explain the procedure for sampling from the posterior distribution, and code in R to achieve it.
-   [x] Calculate the posterior distributions of model parameters.
-   [x] Describe the AR models under Bayesian conjugate setting. Specify the likelihood function and prior distribution.
:::

## Introduction ðŸŽ¥ {#sec-capstone-introduction} 

This is a capstone project for the Bayesian Statistics course. 
The instructor is a doctoral student Chi Joe Kao. 
We will do a capstone project.
We will also cover a new type of model - a mixture version of the autoregressive model from the previous course.
We will cover the Bayesian conjugate analysis for autoregressive time series models.

## Prerequisite skill checklist ðŸ“–


:::: {.callout-note collapse="true"}
## Prerequisite skill checklist

### Bayesian Statistics
- [x] Interpret and specify the components of Bayesian statistical models: likelihood, prior, posterior.
- [x] Explain the basics of sampling algorithms, including sampling from
standard distributions and using MCMC to sample from non-standard
posterior distributions.
- [x] Assess the performance of a statistical model and compare competing
models using posterior samples.
- [x] Coding in R to achieve the above tasks.

### Mixture Models
- [x] Define mixture model.
- [x] Explain the likelihood function associated with a random sample
from a mixture distribution.
- [x] Derive Markov chain Monte Carlo algorithms for fitting mixture models.
- [x] Coding in R to achieve the above tasks.

### Time Series Analysis
- [x] Define time series and stochastic processes (univariate, discrete-time, equally-spaced)
- [x] Define strong and weak stationarity
- [x] Define the auto-covariance function, the auto-correlation function
(ACF), and the partial autocorrelation function (PACF)
- [x] Definition of the general class of autoregressive processes of order p.
:::

## Read Data ðŸ“– â„›

### Earth Quake Data â„›

```{r}
#| label: lst-capstone-earthquake-data
## read data, you need to make sure the data file is in your current working directoryÂ 
earthquakes.dat <- read.delim("data/earthquakes.txt")
earthquakes.dat$Quakes=as.numeric(earthquakes.dat$Quakes)

y.dat=earthquakes.dat$Quakes[1:100] ## this is the training data
y.new=earthquakes.dat$Quakes[101:103] ## this is the test data
```

### Google Search Index Data â„›


```{r}
#| label: lst-capstone-google-search-index-covid
## read data, you need to make sure the data file is in your current working directoryÂ 
covid.dat <- read.delim("data/GoogleSearchIndex.txt")
covid.dat$Week=as.Date(as.character(covid.dat$Week),format = "%Y-%m-%d")

y.dat=covid.dat$covid[1:57] ## this is the training data
y.new=covid.dat$covid[58:60] ## this is the test data
```

## Model Formulation ðŸŽ¥

## Review: Useful Distributions ðŸ“– â„›

This is a review of three distributions related to the Normal distribution. 

- The multivariate normal distribution is used to model multiple correlated variables
- The inverse-gamma distribution is used as a prior for the variance for Normally distributed data when the mean is known and the variance is unknown.
- The multivariate Student-t distribution is a robust alternative to the multivariate normal distribution when the data has outliers or is not normally distributed.

### The multivariate normal distribution

A $k$-dimensional random vector $x=(x_1, \cdots, x_k)^T$ that follows a multivariate normal distribution with mean $\mu$ and variance-covariance matrix $\Sigma$ is denoted as $x \sim \mathcal{N}(\mu, \Sigma)$ or $p(x) = \mathcal{N}(x \mid \mu, \Sigma)$ has a density function given by:
$$
p(\mathbf{x})=(2\pi)^{-k/2}|\boldsymbol{\Sigma}|^{-1/2}\exp[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})]
$$ {#eq-multivariate-normal-pdf}

where $\mathbf{x}$ is a $k$-dimensional vector, $|\boldsymbol{\Sigma}|$ is the determinant of the covariance matrix, and $\boldsymbol{\Sigma}^{-1}$ is the inverse of the covariance matrix.

### The gamma and inverse-gamma distribution

A random variable $x$ that follows a gamma distribution with shape parameter $\alpha$ and inverse scale parameter $\beta$ , denoted as $x \sim \mathcal{G}(\alpha, \beta)$, or $\mathbb{P}r(x) = \mathcal{G}(x \mid \alpha, \beta)$, has a density of the form
$$
\mathbb{P}r(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x},\quad x>0
$$ {#eq-gamma-pdf}

where $\alpha$ is the shape parameter, $\beta$ is the rate parameter, and $\Gamma(\alpha)$ is the gamma function evaluated at $\alpha$. 

In addition, 
$$
\mathbb{E}[x]=\frac{\alpha}{\beta}  \qquad \mathbb{V}ar[x]=\frac{\alpha}{\beta^2}
$$

If $1/x \sim G(\alpha, \beta)$, then $x$ follows an inverse-gamma distribution $X\sim \mathcal{IG}(\alpha, \beta)$.
 or $p(x) = \mathcal{IG}(X\mid \alpha, \beta)$. with

$$
\mathbb{P}r(x)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{-(\alpha+1)}e^{-\beta/x},\quad x>0
$$ {#eq-inverse-gamma-pdf}

where $\alpha$ is the shape parameter, $\beta$ is the scale parameter, and $\Gamma(\alpha)$ is the gamma function evaluated at $\alpha$.    

in this case 

$$
\mathbb{E}[x]=\frac{\beta}{\alpha-1} \quad \text{for } \alpha > 1 \qquad
\mathbb{V}ar[x]=\frac{\beta^2}{(\alpha-1)^2(\alpha-2)} \quad \text{for } \alpha > 2
$$

### The multivariate Student-t distribution

A random vector $x$ of dimension $k$ follows a multivariate Student-t distribution with $\nu$ degrees of freedom, location $\boldsymbol{\mu}$, and scale matrix $\boldsymbol{\Sigma}$, denoted as $x \sim T_\nu(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, if its density is given by:

$$
\mathbb{P}r(\mathbf{x})=\frac{\Gamma(\frac{\nu+k}{2})}{\Gamma(\frac{\nu}{2})(\nu\pi)^{k/2}}|\boldsymbol{\Sigma}|^{-1/2}[1+\frac{1}{\nu}(\mathbf{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})]^{-(\nu+k)/2}
$$

$$
\mathbb{E}[x]=\mu \text{ for } \nu>1  \qquad \mathbb{V}ar[x]=\frac{\nu\Sigma}{\nuâˆ’2} \quad \text{for } \nu>2
$$

## Posterior Distribution Derivation ðŸ“– {#sec-capstone-posterior}

In this lesson, we derive the posterior distributions for $\boldsymbol{\phi}$ and $\nu$ for the following model:

$$
\mathbf{y}\sim \mathcal{N}(\mathbf{F}^\top \boldsymbol{\phi},\nu\mathbf{I}_n),\quad \boldsymbol{\phi}\sim \mathcal{N}(\mathbf{m}_0,\nu\mathbf{C}_0),\quad \nu\sim IG(\frac{n_0}{2},\frac{d_0}{2}) 
$$ {#eq-model-specification}

where :

- $\mathbf{y}$ is the observed data,
- $\mathbf{F}$ is the design matrix,
- $\boldsymbol{\phi}$ is the vector of parameters,
- $\nu$ is the scale parameter,
- $\mathbf{m}_0$ is the prior mean of $\boldsymbol{\phi}$,
- $\mathbf{C}_0$ is the prior covariance matrix of $\boldsymbol{\phi}$,
- $n_0$ and $d_0$ are the shape and scale parameters of the inverse-gamma prior for $\nu$.

i.e. we start with a Normal likelihood with a Normal prior for $\boldsymbol{\phi}$ and an inverse-gamma prior for $\nu$.

I recall that we already did this derivation in the first course by Herbert Lee.

 <!-- TODO: link the relevant section in the first course as it is unclear what this particular normal model means w.r.t. our knowledge of the mean and variance -->


Using Bayes theorem, we have :

$$
\begin{aligned} 
\mathbb{P}r(\boldsymbol{\phi},\nu\mid\mathbf{y}) & \propto \mathbb{P}r(\mathbf{y} \mid \boldsymbol{\phi},\nu)\mathbb{P}r(\boldsymbol{\phi}\mid\nu)\mathbb{P}r(\nu)\\ 
&\propto \nu^{-n/2}\exp \left(-\frac{(\mathbf{y}-\mathbf{F}^T\boldsymbol{\phi})^T(\mathbf{y}-\mathbf{F}^T\boldsymbol{\phi})}{2\nu}\right)\\ 
&\quad \times \nu^{-p/2}\exp\left(-\frac{(\boldsymbol{\phi}-\mathbf{m}_0)^T\mathbf{C}_0^{-1}(\boldsymbol{\phi}-\mathbf{m}_0)}{2\nu}\right)\\ 
&\quad \times \nu^{-(\frac{n_0}{2}+1)}\exp\left(-\frac{d_0}{2\nu}\right)\\ 
&\propto \nu^{-(\frac{n^*}{2}+1)}\exp\left(-\frac{d^*}{2\nu}\right)\\ 
&\quad \times \nu^{-p/2}\exp\left(-\frac{(\boldsymbol{\phi}-\mathbf{m})^T\mathbf{C}^{-1}(\boldsymbol{\phi}-\mathbf{m})}{2\nu}\right)\\ 
&\propto \mathbb{P}r(\nu\mid\mathbf{y})\mathbb{P}r(\boldsymbol{\phi}\mid\nu,\mathbf{y}) 
\end{aligned}
$$ {#eq-posterior-derivation-1}

where 

$$
\begin{aligned} 
\mathbf{e}&=\mathbf{y}-\mathbf{F}^T\mathbf{m}_0, 
&\mathbf{Q}&=\mathbf{F}^T\mathbf{C}_0\mathbf{F}+\mathbf{I}_n,\quad 
\mathbf{A}=\mathbf{C}_0\mathbf{F}\mathbf{Q}^{-1}\\
\mathbf{m}&=\mathbf{m}_0+\mathbf{A}\mathbf{e},\quad 
&\mathbf{C}&=\mathbf{C}_0-\mathbf{A}\mathbf{Q}\mathbf{A}^{T}\\ 
 n^* &=n+n_0,\quad 
 &d^*&=(y-\mathbf{F}^T\mathbf{m}_0)^T\mathbf{Q}^{-1}(y-\mathbf{F}^T\mathbf{m}_0)+d_0
\end{aligned} 
$$ {#eq-posterior-derivation}

Therefore, we have the posterior distribution of $\boldsymbol{\phi}$ and $\nu$ as:

$$
\begin{aligned} 
\mathbb{P}r(\boldsymbol{\phi},\nu \mid \mathbf{y}) &=\mathbb{P}r(\boldsymbol{\phi} \mid \nu,\mathbf{y})\mathbb{P}r(\nu\mid \mathbf{y}) \\
&=\mathcal{N}(\boldsymbol{\phi}\mid \mathbf{m},\nu\mathbf{C})\ \mathcal{IG}(\nu\mid \frac{n^*}{2},\frac{d^*}{2}) \end{aligned}
$$ {#eq-posterior-distribution}



To get the $s$^th^ sample of $(\boldsymbol{\phi}^{(s)},\nu^{(s)})$ from the joint posterior distribution of $\boldsymbol{\phi}$ and $\nu$, we first sample $\nu^{(s)}$ from $\mathcal{IG}(\frac{n^*}{2},\frac{d^*}{2})$, then sample $\boldsymbol{\phi}^{(s)}$ from $\mathcal{N}(\boldsymbol{m},\nu^{(s)}\boldsymbol{C})$.

**One can also obtain posterior samples** of $\boldsymbol{\phi}$ by directly sampling from its **marginal posterior distribution.** 
Integrate out $\nu$ from the joint posterior distribution, we have $\mathbb{P}r(\boldsymbol{\phi} \mid \mathbf{y})=T_{n^*}(\boldsymbol{m},\frac{d^*}{n^*}\boldsymbol{C})$.

Denote the $s$^th^ posterior sample of $\boldsymbol{\phi}$ and $\nu$ as $\boldsymbol{\phi}^{(s)}$ and $\nu^{(s)}$, we can obtain the corresponding $i$^th^ posterior sample of $\mathbf{y}^{(s)}$ by sampling from $\mathcal{N}(\mathbf{F}^\top \boldsymbol{\phi}^{(s)},\nu^{(s)})$. 
Therefore, we can obtain the posterior point and interval estimates for the time series.


## AR model fitting example

### Bayesian Conjugate Analysis of AR Model

#### Simulate Data

We give an example about fitting an $AR(2)$ model using simulated data. We simulate 200 observations from the following model:

$$
y_t=0.5y_{t-1}+0.4y_{t-2}+\epsilon_t,\quad \epsilon_t\sim N(0,1) 
$$ {#eq-ar2-model}

```{r}
#| label: fig-capstone-simulate-data
#| fig-cap: "Simulated AR(2) data"
#| 
## simulate data

phi1=0.5
phi2=0.4
v=1

set.seed(1)
y.sample=arima.sim(n=200,model=list(order=c(2,0,0),ar=c(phi1,phi2),sd=sqrt(v)))
plot.ts(y.sample,ylab=expression(italic(y)[italic(t)]),xlab=expression(italic(t)),
        main='')
```

Now, we perform a prior sensitivity analysis for the choice of hyperparameters in prior distribution. We choose three sets of prior hyperparameters and plot the posterior distribution for $\mathbb{P}r(\phi_1, \phi_2 \mid y_{1:n})$ below. The three sets of prior hyperparameters are :

$$
\begin{aligned} 
& (1) \quad \mathbf{m}_0=(0,0)^T,\mathbf{C}_0=I_2,n_0=2,d_0=2\\ 
& (2) \quad \mathbf{m}_0=(0,0)^T,\mathbf{C}_0=I_2,n_0=6,d_0=1\\ 
& (3) \quad \mathbf{m}_0=(-0.5,-0.5)^T,\mathbf{C}_0=I_2,n_0=6,d_0=1 
\end{aligned} 
$$

In the plot, the left, middle and right panel correspond to the first, second and last sets of hyperparameters, respectively.

```{r}
#| label: fig-capstone-prior-sensitivity
#| fig-cap: "Prior sensitivity analysis of AR coefficients"
## prior sensitivity analysis
## plot posterior distribution of phi_1 and phi_2 on a grid 

library(colorRamps)
library(leaflet)
library(fields)
library(mvtnorm)

## generate grid
coordinates_1=seq(-3,3,length.out = 100)
coordinates_2=seq(-3,3,length.out = 100)
coordinates=expand.grid(coordinates_1,coordinates_2)
coordinates=as.matrix(coordinates)

## set up
N=100
p=2  ## order of AR process
n.all=length(y.sample) ## T, total number of data

Y=matrix(y.sample[3:n.all],ncol=1)
Fmtx=matrix(c(y.sample[2:(n.all-1)],y.sample[1:(n.all-2)]),nrow=p,byrow=TRUE)
n=length(Y)

## function to compute parameters for the posterior distribution of phi_1 and phi_2
## the posterior distribution of phi_1 and phi_2 is a multivariate t distribution

cal_parameters=function(m0=matrix(c(0,0),nrow=2),C0=diag(2),n0,d0){
  e=Y-t(Fmtx)%*%m0
  Q=t(Fmtx)%*%C0%*%Fmtx+diag(n)
  Q.inv=chol2inv(chol(Q))  ## similar as solve, but more robust
  A=C0%*%Fmtx%*%Q.inv
  m=m0+A%*%e
  C=C0-A%*%Q%*%t(A)
  n.star=n+n0
  d.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0
  
  params=list()
  params[[1]]=n.star
  params[[2]]=d.star
  params[[3]]=m
  params[[4]]=C
  
  return(params)
}


## evaluate density at the grid points
get_density=function(param){
  location=param[[3]]
  scale=as.numeric(param[[2]]/param[[1]])*param[[4]]
  density=rep(0,N^2)
  
  for (i in 1:N^2) {
    xi=coordinates[i,]
    density[i]=dmvt(xi,delta=location,sigma=scale,df=param[[1]])
  }
  
  density_expand=matrix(density,nrow=N)
  return(density_expand)
}

## calculate density for three sets of hyperparameters
params1=cal_parameters(n0=2,d0=2)
params2=cal_parameters(n0=6,d0=1)
params3=cal_parameters(m0=matrix(c(-0.5,-0.5),nrow=2),n0=6,d0=1)

col.list=matlab.like2(N)
Z=list(get_density(params1),get_density(params2),get_density(params3))

op <- par(mfrow = c(1,3),
          oma = c(5,4,0,0) + 0.1,
          mar = c(4,4,0,0) + 0.2)
image(coordinates_1,coordinates_2,Z[[1]],col=col.list,
      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))
image(coordinates_1,coordinates_2,Z[[2]],col=col.list,
      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))
image(coordinates_1,coordinates_2,Z[[3]],col=col.list,
      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))
```

Since we change the value of hyperparameters but the posterior is almost the same, we can conclude that the posterior distribution of both AR coefficients and variance are robust w.r.t. the choice of hyperparameters.

Posterior Inference
We now sample 5000 sets of $(\phi_1, \phi_2, \nu)$  from their marginal posterior distributions and plot them. For prior hyperparameters, we take $m_0=(0,0)^\top$, $C_0=I_2$, $n_0=2$, and $d_0=2$.

```{r}
#| label: fig-capstone-posterior-sampling
#| fig-cap: "Posterior sampling of AR coefficients and variance"
m0=matrix(rep(0,p),ncol=1)
C0=diag(p)
n0=2
d0=2

e=Y-t(Fmtx)%*%m0
Q=t(Fmtx)%*%C0%*%Fmtx+diag(n)
Q.inv=chol2inv(chol(Q))
A=C0%*%Fmtx%*%Q.inv
m=m0+A%*%e
C=C0-A%*%Q%*%t(A)
n.star=n+n0
d.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0


n.sample=5000

nu.sample=rep(0,n.sample)
phi.sample=matrix(0,nrow=n.sample,ncol=p)

for (i in 1:n.sample) {
  set.seed(i)
  nu.new=1/rgamma(1,shape=n.star/2,rate=d.star/2)
  nu.sample[i]=nu.new
  phi.new=rmvnorm(1,mean=m,sigma=nu.new*C)
  phi.sample[i,]=phi.new
}

par(mfrow=c(1,3))
hist(phi.sample[,1],freq=FALSE,xlab=expression(phi[1]),main="",ylim=c(0,6.4))
lines(density(phi.sample[,1]),type='l',col='red')
hist(phi.sample[,2],freq=FALSE,xlab=expression(phi[2]),main="",ylim=c(0,6.4))
lines(density(phi.sample[,2]),type='l',col='red')
hist(nu.sample,freq=FALSE,xlab=expression(nu),main="")
lines(density(nu.sample),type='l',col='red')
```

### Model Checking by In-sample Point and Interval Estimation

To check whether the model fits well, we plot the posterior point and interval estimate for each point.


```{r}
#| label: lst-capstone-in-sample-predictions

## get in sample prediction
post.pred.y=function(s){
  
  beta.cur=matrix(phi.sample[s,],ncol=1)
  nu.cur=nu.sample[s]
  mu.y=t(Fmtx)%*%beta.cur
  sapply(1:length(mu.y), function(k){rnorm(1,mu.y[k],sqrt(nu.cur))})
  
  
}  

y.post.pred.sample=sapply(1:5000, post.pred.y)
```

```{r}
#| label: fig-capstone-result
#| fig-cap: "TS with prediction and credible interval"

## show the result
summary.vec95=function(vec){
  c(unname(quantile(vec,0.025)),mean(vec),unname(quantile(vec,0.975)))
}

summary.y=apply(y.post.pred.sample,MARGIN=1,summary.vec95)

plot(Y,type='b',xlab='Time',ylab='',ylim=c(-7,7),pch=16)
lines(summary.y[2,],type='b',col='grey',lty=2,pch=4)
lines(summary.y[1,],type='l',col='purple',lty=3)
lines(summary.y[3,],type='l',col='purple',lty=3)
legend("topright",legend=c('Truth','Mean','95% C.I.'),lty=1:3,col=c('black','grey','purple'),
       horiz = T,pch=c(16,4,NA))
```

## Prediction for AR Models ðŸŽ¥ {#sec-capstone-prediction}

![prediction](images/C5-L01-sl05.png){#fig-capstone-prediction .column-margin  group="slides" width="53mm"}
$AR(p)$ Model:

$$
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \epsilon_t, \qquad \epsilon_t\sim \mathcal{N}(0,\nu)
$$

where ($\epsilon_t$) follows a normal distribution with mean zero and variance ($\nu$).


- First Step Ahead Prediction, $t>T$ :
$$
y_{T+1} \sim \mathcal{N}(\phi_1 y_T + \ldots + \phi_p y_{T+1-p}, \nu)
$$

- General H-Step Ahead Prediction: 

$$
y_{T+S}^s \sim \mathcal{N}\left(\sum_{j=1}^{p} \phi_j^s y_{T+s-j}^s, \nu^{(s)}\right)
$$

where ($\phi_j^s$) and ($\nu^{(s)}$) are the posterior samples of the parameters.



## AR model prediction example ðŸ“–â„› {#sec-capstone-ar-prediction-example}


We now give an example for coding the prediction algorithm in R. Consider the simulated data in the previous class, an *AR(2)* model with 200 observations obtained from $y_t=0.5y_{tâˆ’1}+0.4y_{tâˆ’2}+\epsilon_t,\epsilon_t\sim \mathcal{N}(0,1)$. Before doing the prediction, we have to obtain posterior samples of model parameters 
$\phi$ and $\nu$, which has been taught in the previous class. Here we just copy the code from previous to obtain 5000 posterior samples of

```{r}
#| label: lst-capstone-ar-prediction-example
library(mvtnorm)

## simulate data

phi1<-0.5
phi2<-0.4
v<-1

set.seed(1)
y.sample=arima.sim(n=200,model=list(order=c(2,0,0),ar=c(phi1,phi2),sd=sqrt(v)))


## set up
N<-100
p<-2 # order of AR process
n.all=length(y.sample) ## T, total number of data


Y=matrix(y.sample[3:n.all],ncol=1)
Fmtx=matrix(c(y.sample[2:(n.all-1)],y.sample[1:(n.all-2)]),nrow=p,byrow=TRUE)
n=length(Y)


## posterior inference


## set the prior
m0=matrix(rep(0,p),ncol=1)
C0=diag(p)
n0=2
d0=2


## calculate parameters that will be reused in the loop
e=Y-t(Fmtx)%*%m0
Q=t(Fmtx)%*%C0%*%Fmtx+diag(n)
Q.inv=chol2inv(chol(Q))
A=C0%*%Fmtx%*%Q.inv
m=m0+A%*%e
C=C0-A%*%Q%*%t(A)
n.star=n+n0
d.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0


n.sample=5000

## store posterior samples
nu.sample=rep(0,n.sample)
phi.sample=matrix(0,nrow=n.sample,ncol=p)


for (i in 1:n.sample){
  set.seed(i)
Â  nu.new=1/rgamma(1,shape=n.star/2,rate=d.star/2)
Â  nu.sample[i]=nu.new
Â  phi.new=rmvnorm(1,mean=m,sigma=nu.new*C)
Â  phi.sample[i,]=phi.new
}

```

