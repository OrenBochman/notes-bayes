---
title: "Moore-Penrose Inversion & Cholesky Decomposition"
subtitle: "Appendix"
description: "This appendix explains the Moore-Penrose inversion and the Cholesky decomposition, which are methods for solving linear equations efficiently using linear algebra."
categories: 
    - "numerical linear algebra"
keywords: 
    - "Moore-Penrose inversion"
    - "Cholesky decomposition"
    - "linear equations"
    - "matrix factorization"
    - "numerical stability"
    - "linear algebra"
---

The Moore-Penrose inversion and the Cholesky decomposition are two important methods in linear algebra for solving linear equations efficiently. They are widely used in various applications, including statistics, machine learning, and numerical analysis.

## Moore-Penrose Inversion {#sec-moore-penrose}

The Moore-Penrose inversion is a method for computing the pseudoinverse of a matrix. The pseudoinverse is a generalization of the inverse of a matrix that can be used to solve linear equations when the matrix is rectangular, not-invertible or even singular.

::: {#def-moore-penrose-inverse-1}

## Definition of the Moore-Penrose Inverse 1

The Moore–Penrose inverse of the $m × n$ matrix $A$ is the $n × m$
matrix, denoted by $A^+$, which satisfies the conditions

$$
AA+ A = A
$$ {#eq-moore-penrose-1}

$$  
A^+AA^+ = A^+
$$ {#eq-moore-penrose-2}

$$
(AA^+ )' = AA^+ 
$$ {#eq-moore-penrose-3}

$$
(A^+A)' = A^+A
$$ {#eq-moore-penrose-4}

:::


An important features of the Moore–Penrose inverse, is that it is uniquely defined.


Corresponding to each m × n matrix A, one and only one n × m matrix $A^+$ exists satisfying conditions (@eq-moore-penrose-1)–(@eq-moore-penrose-4).

Definition @def-moore-penrose-inverse-1 is the definition of a generalized inverse given by Penrose (1955).

The following alternative definition, which we will find useful on some occasions, utilizes properties of the Moore–Penrose inverse that were first illustrated by Moore (1935).

::: {#def-moore-penrose-inverse-2}
## Definition of the Moore-Penrose Inverse 2

Let $A$ be an $m × n$ matrix. Then the Moore–Penrose inverse of $A$ is the unique $n × m$ matrix $A^+$ satisfying

$$
AA^+ = P_{R(A)}
$$ {#eq-moore-penrose-5}

$$
A^+ A = P_{R(A^+)}
$$ {#eq-moore-penrose-6}

where $P_{R(A)}$ and $P_{R(A^+)}$ are the projection matrices of the range spaces of $A$ and $A^+$, respectively.
:::


::: {#thm-properties-moore-penrose-inverse}
## Properties of the Moore-Penrose inverse 

Let A be an $m \times n$ matrix. Then:

a. $(αA)^+ = α^{-1} A^+ , \text{ if } \alpha \ne 0 \text{ is a scalar}$
b. $(A^T)^+ = (A^+)^T$ 
c. $(A^+)^+ = A$
d. $A^+ = A^{-1} ,\text{if A is square and nonsingular}$
e. $(A^T A)^+ = A^+ A^T$ and $(AA^T)^+ = A^T A^+$
f. $(AA^+)^+ = AA^+$ and $(A^+ A)^+ = A^+ A$
g. $A^+ = (A^T A)^+ A^T = A^T (AA^T)^+$
h. $A^+ = (A^T A)^{-1} A^T$ and $A^+ A = I_n , \text{ if } rank(A) = n$
i. $A^+ = A^T (AA^T)^{-1}$ and $AA^+ = I_m , \text{ if } rank(A) = m$
j. $A^+ = A^T$  if the columns of A are orthogonal, that is, $A^T A = I_n$
:::


::: {#thm-rank}
##  Rank of Moore-Penrose Inverse

For any $m \times n$ matrix $A$, $\text{rank}(A) = \text{rank}(A^+) = \text{rank}(AA^+) = \text{rank}(A^+ A)$.
:::


::: {#thm-symmetric-moore-penrose-inverse}
##  Symmetric Moore-Penrose Inverse


Let A be an m × m symmetric matrix. Then
a. $A^+$ is also symmetric,
b. $AA^+$ = $A^+A,
c. $A^+$ = $A$, if $A$ is idempotent.
:::


<!-- add about use in MLE -->
The Moore-Penrose inverse is particularly useful in maximum likelihood estimation (MLE) for linear models. In MLE, we often need to solve linear equations of the form $Ax = b$, where $A$ is the design matrix and $b$ is the response vector. If $A$ is not full rank or is not square, we can use the Moore-Penrose inverse to find a solution that minimizes the residual sum of squares.

In the context of MLE, the Moore-Penrose inverse allows us to obtain parameter estimates even when the design matrix is singular or when there are more predictors than observations. This is achieved by projecting the response vector onto the column space of the design matrix, leading to a solution that is consistent and has desirable statistical properties.

## Cholesky Decomposition {#sec-cholesky}

The Cholesky decomposition is a method for factorizing a positive definite matrix into the product of a lower triangular matrix and its transpose. It is particularly useful for solving systems of linear equations and for generating samples from multivariate normal distributions.

::: {#def-cholesky-decomposition}
## Definition of the Cholesky Decomposition

Let $A$ be a symmetric positive definite matrix. The Cholesky decomposition of $A$ is a factorization of the form:
$$
A = LL^T
$$ {#eq-cholesky-decomposition}
where $L$ is a lower triangular matrix with positive diagonal entries.

:::

The Cholesky decomposition is unique for a given positive definite matrix, and it can be computed efficiently using algorithms such as the Doolittle algorithm or the Crout algorithm.

The Cholesky decomposition has several important properties:

1. If $A$ is positive definite, then $L$ is unique and has positive diagonal entries.
2. The Cholesky decomposition can be used to solve linear systems of equations of the form $Ax = b$ by first solving $Ly = b$ for $y$, and then solving $L^Tx = y$.
3. The Cholesky decomposition can be used to generate samples from a multivariate normal distribution by transforming samples from a standard normal distribution using the Cholesky factor $L$.  

The Cholesky decomposition is widely used in various applications, including numerical optimization, Bayesian inference, and machine learning. It is particularly useful for solving linear systems efficiently and for generating samples from multivariate normal distributions.




## Kronecker Product and Hadamard Product {#sec-kron-hadamard}

The Kronecker product and Hadamard product are two important operations in linear algebra that are used to manipulate matrices in various ways.
These operations are particularly useful in applications such as signal processing, image processing, and machine learning, where they can be used as short hand notation for certain matrix operations used in many algorithms.

The Kronecker product is a matrix operation that takes two matrices and produces a block matrix by multiplying each element of the first matrix by the entire second matrix. 

The Hadamard product, on the other hand, is an element-wise multiplication of two matrices of the same dimensions. 

::: {#def-kron-product}
## Definition of the Kronecker Product
The Kronecker product of two matrices $A$ and $B$, denoted by $A \otimes B$, is defined as the block matrix formed by multiplying each element of $A$ by the entire matrix $B$. If $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix, then the Kronecker product $A \otimes B$ is an $(mp) \times (nq)$ matrix given by:

$$
A \otimes B = \begin{bmatrix}
a_{11}B & a_{12}B & \cdots & a_{1n}B \\
a_{21}B & a_{22}B & \cdots & a_{2n}B \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1}B & a_{m2}B & \cdots & a_{mn}B
\end{bmatrix}
$$ {#eq-kron-product}

where $a_{ij}$ are the elements of matrix $A$.
:::

some properties of the Kronecker product include:

::: {#thm-properties-kron-product}
## Properties of the Kronecker Product
Let $A$ be an $m \times n$ matrix and $B$ be a $p \times q$ matrix. Then:

a. $a \otimes A = A\otimes a$ for any scalar $a$ (scalar multiplication property)
b. $(aA) \otimes (bB) = ab(A \otimes B)$ for any scalars $a$ and $b$ (scalar multiplication property)
c. $(A \otimes B) \otimes C =  A \otimes (B\otimes C)$ (associative property)
d. $(A+B) \otimes C = (A \otimes C) + (B \otimes C)$  when $A,B,C$ are matrices of the same size (distributive property)
e. $A \otimes (B+C) = (A \otimes B) + (A \otimes C)$  when $A,B,C$ are matrices of the same size (distributive property)
f. $(A \otimes B)^{\prime} = A^{\prime} \otimes B^{\prime}$ (transpose property)
g. $(\mathbf{a} \otimes \mathbf{b})^{\prime} = \mathbf{a}^{\prime} \otimes \mathbf{b}^{\prime}$ (commutativity property for vectors)
h. $\text{det}(A \otimes B) = \text{det}(A)^{p} \cdot \text{det}(B)^{m}$ (determinant property)
i. $\text{rank}(A \otimes B) = \text{rank}(A) \cdot \text{rank}(B)$ (rank property)
j. $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$, if both $A$ and $B$ are invertible (inverse property)
k. $(A \otimes B)(C \otimes D) = (AC) \otimes (BD)$ (multiplication property)
;. $\text{tr}(A \otimes B) = \text{tr}(A) \cdot \text{tr}(B)$ (trace property)

:::

