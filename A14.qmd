---
title: "Moore-Penrose Inversion & Cholesky Decomposition"
subtitle: "Appendix"
description: "This appendix explains the Moore-Penrose inversion and the Cholesky decomposition, which are methods for solving linear equations efficiently using linear algebra."
categories: 
    - "Linear Algebra"
keywords: 
    - "Moore-Penrose inversion"
    - "Cholesky decomposition"
    - "linear equations"
    - "matrix factorization"
    - "numerical stability"
    - "linear algebra"
---

The Moore-Penrose inversion and the Cholesky decomposition are two important methods in linear algebra for solving linear equations efficiently. They are widely used in various applications, including statistics, machine learning, and numerical analysis.

## Moore-Penrose Inversion {#sec-moore-penrose}

The Moore-Penrose inversion is a method for computing the pseudoinverse of a matrix. The pseudoinverse is a generalization of the inverse of a matrix that can be used to solve linear equations when the matrix is rectangular, not-invertible or even singular.

::: {#def-moore-penrose-inverse-1}

## Definition of the Moore-Penrose Inverse 1

The Moore–Penrose inverse of the $m × n$ matrix $A$ is the $n × m$
matrix, denoted by $A^+$, which satisfies the conditions

$$
AA+ A = A
$$ {#eq-moore-penrose-1}

$$  
A^+AA^+ = A^+
$$ {#eq-moore-penrose-2}

$$
(AA^+ )' = AA^+ 
$$ {#eq-moore-penrose-3}

$$
(A^+A)' = A^+A
$$ {#eq-moore-penrose-4}

:::


An important features of the Moore–Penrose inverse, is that it is uniquely defined.


Corresponding to each m × n matrix A, one and only one n × m matrix $A^+$ exists satisfying conditions (@eq-moore-penrose-1)–(@eq-moore-penrose-4).

Definition @def-moore-penrose-inverse-1 is the definition of a generalized inverse given by Penrose (1955).

The following alternative definition, which we will find useful on some occasions, utilizes properties of the Moore–Penrose inverse that were first illustrated by Moore (1935).

::: {#def-moore-penrose-inverse-2}
## Definition of the Moore-Penrose Inverse 2

Let $A$ be an $m × n$ matrix. Then the Moore–Penrose inverse of $A$ is the unique $n × m$ matrix $A^+$ satisfying

$$
AA^+ = P_{R(A)}
$$ {#eq-moore-penrose-5}

$$
A^+ A = P_{R(A^+)}
$$ {#eq-moore-penrose-6}

where $P_{R(A)}$ and $P_{R(A^+)}$ are the projection matrices of the range spaces of $A$ and $A^+$, respectively.
:::


::: {#thm-properties-moore-penrose-inverse}
## Properties of the Moore-Penrose inverse 

Let A be an $m \times n$ matrix. Then:

a. $(αA)^+ = α^{-1} A^+ , \text{ if } \alpha \ne 0 \text{ is a scalar}$
b. $(A^T)^+ = (A^+)^T$ 
c. $(A^+)^+ = A$
d. $A^+ = A^{-1} ,\text{if A is square and nonsingular}$
e. $(A^T A)^+ = A^+ A^T$ and $(AA^T)^+ = A^T A^+$
f. $(AA^+)^+ = AA^+$ and $(A^+ A)^+ = A^+ A$
g. $A^+ = (A^T A)^+ A^T = A^T (AA^T)^+$
h. $A^+ = (A^T A)^{-1} A^T$ and $A^+ A = I_n , \text{ if } rank(A) = n$
i. $A^+ = A^T (AA^T)^{-1}$ and $AA^+ = I_m , \text{ if } rank(A) = m$
j. $A^+ = A^T$  if the columns of A are orthogonal, that is, $A^T A = I_n$
:::


::: {#thm-rank}
##  Rank of Moore-Penrose Inverse

For any $m \times n$ matrix $A$, $\text{rank}(A) = \text{rank}(A^+) = \text{rank}(AA^+) = \text{rank}(A^+ A)$.
:::


::: {#thm-symmetric-moore-penrose-inverse}
##  Symmetric Moore-Penrose Inverse


Let A be an m × m symmetric matrix. Then
a. $A^+$ is also symmetric,
b. $AA^+$ = $A^+A,
c. $A^+$ = $A$, if $A$ is idempotent.
:::


<!-- add about use in MLE -->
The Moore-Penrose inverse is particularly useful in maximum likelihood estimation (MLE) for linear models. In MLE, we often need to solve linear equations of the form $Ax = b$, where $A$ is the design matrix and $b$ is the response vector. If $A$ is not full rank or is not square, we can use the Moore-Penrose inverse to find a solution that minimizes the residual sum of squares.

In the context of MLE, the Moore-Penrose inverse allows us to obtain parameter estimates even when the design matrix is singular or when there are more predictors than observations. This is achieved by projecting the response vector onto the column space of the design matrix, leading to a solution that is consistent and has desirable statistical properties.

## Cholesky Decomposition {#sec-cholesky}

The Cholesky decomposition is a method for factorizing a positive definite matrix into the product of a lower triangular matrix and its transpose. It is particularly useful for solving systems of linear equations and for generating samples from multivariate normal distributions.

::: {#def-cholesky-decomposition}
## Definition of the Cholesky Decomposition

Let $A$ be a symmetric positive definite matrix. The Cholesky decomposition of $A$ is a factorization of the form:
$$
A = LL^T
$$ {#eq-cholesky-decomposition}
where $L$ is a lower triangular matrix with positive diagonal entries.

:::

The Cholesky decomposition is unique for a given positive definite matrix, and it can be computed efficiently using algorithms such as the Doolittle algorithm or the Crout algorithm.

The Cholesky decomposition has several important properties:

1. If $A$ is positive definite, then $L$ is unique and has positive diagonal entries.
2. The Cholesky decomposition can be used to solve linear systems of equations of the form $Ax = b$ by first solving $Ly = b$ for $y$, and then solving $L^Tx = y$.
3. The Cholesky decomposition can be used to generate samples from a multivariate normal distribution by transforming samples from a standard normal distribution using the Cholesky factor $L$.  

The Cholesky decomposition is widely used in various applications, including numerical optimization, Bayesian inference, and machine learning. It is particularly useful for solving linear systems efficiently and for generating samples from multivariate normal distributions.