---
title : 'The MCMC algorithm for Zero-Inflated Mixtures'
subtitle : 'Bayesian Statistics: Mixture Models'
categories:
  - Bayesian Statistics
keywords:
  - Mixture Models
  - Homework
---

## HW - Simulation of Poisson mixture model

In the lessons we mentioned that Zero-Inflated Poisson (ZIP) models arises naturally in biology when analyzing nest size data since many birds fail to mate or lay eggs. As such they will have zero eggs in their nests.  In this exercise, we will use the EM algorithm to fit a ZIP model to a dataset of nest sizes.

::: {.callout-note}
### Instructions

- A biologist is interest in characterizing the number of eggs laid by a particular bird species.  To do this, they sample  
n=300n, nests on a site in Southern California.  The observations are contained in the attached file data/nestsize.csv:

- generate a barplot with the empirical frequencies of all the integers included in the sample.

As you can see, the Poisson distribution underestimates the number of empty nests in the data, and overestimates the number of nests with either 1 or 2 eggs.  To address this, you are asked to modify the implementation of the EM algorithm contained in the Reading "Sample code for EM example 1" so that you can fit a mixture between a point mass at zero and a Poisson distribution (we call this a "zero-inflated Poisson" distribution):

$$
f(x) = w \delta_0(x) + (1-w) \frac{e^{-\lambda} \lambda^x}{x!}, \quad x \in \{0, 1, 2, \ldots\}
$$ {#eq-zip-mixture}

where $w$ is the weight associated with the point mass at zero, $\lambda$ is the parameter of the Poisson distribution, and $\delta_0(x)$ represents the degenerate distribution placing all of its mass at zero.  

- You then should run your algorithm with the data contained in nestsize.csv and report the values of the estimates that you obtained, rounded to two decimal places.
â€‹

:::

::: {.callout-note}
### Grading overview

The code you generate should follow the same structure as "Sample code for EM example 1".  Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect the fact that 

1. you provided a reasonable initial point for you algorithm,
2. the observation-specific weights $v_i,k$ are computed correctly (E step),
3. the formulas for the maximum of the Q functions are correct (M step),
4. the converge check is correct, and
5. the numerical values that you obtain are correct.  

To simplify the peer-review process, assume that component 1 corresponds to the point mass at zero, while component 2 corresponds to the Poisson distribution.

There are two things that make this problem more challenging than the ones we have used for illustrations so far:  

1. the two components in the mixture belong to different families, and 
2. each component has a very different support.  

keep these two circumstances in mind when working on your answer.

:::


```{r}
#| label: lbl-load-data
# Load the nest size data

rm(list=ls())
library(MCMCpack)
set.seed(81196)    # So that results are reproducible (same simulated data every time)

nestsize <- read.csv("data/nestsize.csv",header=FALSE)
colnames(nestsize) <- c("n")
x <- nestsize$n
n <- length(x) # Number of observations
# how many rows in the data
nrow(nestsize)
# how many zeros in x
sum(x==0)
# almost half of the data is zeros!

par(mfrow=c(1,1))
xx.true = seq(0, max(x), by=1)
hist(x, breaks=seq(0, max(x), by=1), freq=FALSE, xlab="Number of eggs", ylab="Density", main="Empirical distribution of nest sizes")
```


```{r}
#| label: lbl-zip-mix-1
#| warnings: true
#| errors: false

# MCMC algorithm for fitting a ZIP
# The actual MCMC algorithm starts here


## MCMC iterations of the sampler

iterations <- 6000
burn       <- 1000

## Initialize the parameters
cc         = rep(2, n)
cc[x == 0] = sample(1:2, sum(x == 0), replace = TRUE, prob = c(0.5, 0.5))
cc[x != 0] = 2

## Priors
aa     = c(1, 1)        # Uniform prior on w
lambda = mean(x[x > 0]) # Initial lambda from nonzero data
w      = 0.2            # fewer zeros

# Storing the samples
w.out      = rep(0, iterations)
cc.out     = array(0, dim=c(iterations, n))
lambda.out = array(0, dim=c(iterations, n))

#logpost    = rep(0, iterations)

# MCMC iterations

for (s in 1:iterations) {
  # Sample latent indicators c_i
  cc = numeric(n)
  # Full conditional for cc
  for(i in 1:n){
    v = rep(0,2)
    if(x[i]==0){
      v[1] = log(w)
      v[2] = log(1-w) + dpois(x[i], lambda, log=TRUE)
      v    = exp(v - max(v))/sum(exp(v - max(v)))
    }else{
      v[1] = 0
      v[2] = 1
    }
    cc[i] = sample(1:2, 1, replace=TRUE, prob=v)
  }
 
  # Sample the weights
  # Full conditional for w
  w = rbeta(1, 1+sum(cc==1), 1+n-sum(cc==1))  
  lambda = rgamma(1, sum(x[cc==2]) + 1, sum(cc==2) + 1)
  # Store samples
  w.out[s] =  w
  lambda.out[s]  = lambda
  cc.out[s,] = cc
}
```


```{r}
#| label: lbl-mle-estimates

# w
# lambda

# Posterior summaries

w.post = w.out[-(1:burn)]
lambda.post = lambda.out[-(1:burn)]
cc.post = cc.out[-(1:burn),]

cat("Posterior mean of w:", mean(w.post), "\n")
cat("Posterior mean of lambda:", mean(lambda.post), "\n")
```