<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/EIciI/smoothing-and-forecasting -->

### Smoothing  {.unnumbered}

> We know, we now discuss the smoothing equations for the case of the normal dynamic linear model. When we are assuming that both the variance at the observation level is known and the covariance matrix at the system level is also known.

### the NDLM we will be inferring {.unnumbered}

> Recall we have two equations here, we have the observation equation, where $y_t$ is modeled as $F_t'\theta_t + \text{noise}$ the noise is $\mathcal{N}(0,\nu_t)$. And we're assuming that the vt is given. We are also assuming that we know Ft for all t. And then in the evolution equation we have $\theta_t= G_t \theta(t-1) + noise$. And then again, the assumption for the $w_t$ is here is that they are normally distributed with mean zero, and these variants co variance matrix, capital $W_t$. So we can summarize the model in terms of $F_t$, $G_t$, $Vt$ and $W_t$, that are given for all t. We have discussed the filtering equations. 

### Recall what is Filtering? {.unnumbered}

> So the process for obtaining the distributions of $\theta_t \mid \mathcal{D}_t$, as we collect observations over time is called filtering. 

### Recall what is Smoothing? {.unnumbered}

> Now we will discuss what happens when we do smoothing, meaning when we revisit the distributions of $\theta_t$, given now that we have received a set of observations. 

### Filtering illustrated {.unnumbered}

> So Just to illustrate the process, we have here, $\theta_0$,$\theta_1$ all way up to $\theta_4$. And we can assume just for the sake of the example, that we are going to receive three observations. So we are going to proceed with the filtering, and then once we receive the last observation at time three, we're going to go backwards and we're going to revisit the distributions for the state parameters. 
>
> So just to remind you how the filtering works, we move forward, before we receive any observations. In the NDLM, when we have all the variances known. The *conjugate prior* distribution is a $\mathcal{N}(m_0,C_0)$, and this is specified by the user, before collecting any observations.
>
> We can then use the structure of the model, meaning the system equation and the observation equation to obtain the distribution of $\theta_t \mid \mathcal{D}_0$. Before observing the first $y$. This gives us first the distribution of $\theta_t$, $\theta_1 \mid \mathcal{D}_0$, which is $\mathcal{N}(a_1, R_1)$. And then we can also get the one step ahead forecast distribution for $y_1 \mid  \mathcal{D}_0$, which is a $\mathcal{N}(f_1, q_1)$. And we have discussed how to obtain these moments using the filtering equations. 
>
> Then we received the first observation, and the first observation can allows us to update the distribution of \theta1. So we obtain now the distribution of $\theta1 \mid y_1$, and whatever information we have at $\mathcal{D}_0$. So this gives us $\mathcal{N}(m_1, C_1)$. And using again the structure of the model, we can get the prior distribution for $\theta_2$ given the one and that's a $\mathcal{N}(a_2, R_2)$. And then the one step ahead forecast distribution now for $y_2 \mid \mathcal{D}_1$ and that's a $\mathcal{N}(f_2, q_2)$. So we can receive $y_2$ update the distribution of \theta2 and we can continue this process, now get the priors at $T=3$. And then once we get the observation at $T=3$, we update the distribution. And we can continue like this with the prior for $\theta_4$ and so on. Let's say that we stop here, at $T=3$. 

> And [now we are interested in answering the question. Well, what is the distribution for example of $\theta_2$ given that, now, I obtain not only $y_1$ and $y_2$, but also $y_3$. I want to revisit that distribution using all that information. Same thing for say, the distribution of $\theta_0 \mid D_0, y_1, y_2, y_3$. So that's what it's called smoothing.]{.mark} 
>
> So the smoothing equations, allow us to obtain those distributions. So just to talk a little bit about the notation again, in the normal dynamic linear model where $v_t$ and $w_t$ are known for all t's. We have that this is a normal, so the notation here, the $T >t$, here. So we're looking at the distribution of $\theta_t$, now in the past and that one follows a normal distribution with mean aT(t-T). So the notation here for the subscript T means that I'm conditioning on all the information I have to T. And then the variance covariance matrix is given by this, RT(t-T). So this is just going to indicate how many steps I'm going to go backwards as you will see in the example.

> So we have some recursions in the same way that we have the filtering equations. Now we have the smoothing equations. And for these smoothing equations we have that the mean. You can see here, that whenever you're computing a particular step t- T, you're going to need a quantity that you computed in the previous step, t-T+1. So you're going to need that, is a recursion, but you're also going to need mt and and at+1. So those are quantities that you computed using the filtering equations. So in order to get the smoothing equations, you first have to proceed with the filtering. Similarly for RT(t-T), you have also that depends on something you previously obtained. And then you also have the Ct, the Rt+1 and so on. So those quantities you computed when you were updating the filtering equations. The recursion begins with aT(0) meaning that you are not going to go backwards any points in time. So that is precisely the mean is going to be whatever you computed with the filtering equations of up to T, that's mT. And then RT(0) is going to be CT. So just to again illustrate how this would work in the example, if we start here right? If we condition, so the first step would be to compute again to initialize using the distribution of \theta3 given D3. And that is a normal with mean a3(0) and variance covariance matrix R3(0), But those are precisely m3 and C3 respectively. Then we go backwards one step. And if we want to look at what is the distribution of $\theta^2$, now conditional on D3. That's a normal with mean a3(-1) and variance covariance matrix R3(-1). So if you look at the equations down here, you will see that, in order to compute a3 (-1), and R3(-1). You're going to need m2,C2, a3,R3 and then what you computed here these moments in the previous step, a3(0) and R3(0). Then you obtain that distribution and you can now look at the distribution of \theta1 given D3, that's the normal a3(-2), R3(-2). And once again, to compute these moments, you're going to need $m1,C1,a2,R2$ and then you're going to need a3(-1),R3(-1). And you can continue all the way down to \theta0 given D3 using these recursions. So the smoothing equations allow us to, just compute all these distributions. And the important equations work basically because of the linear and Gaussian structure in the normal dynamic linear model. 

### Forecasting

> In a similar way, we can compute the forecasting distributions. Now we are going to be looking forward, and in the case of forecasting, we are interested in the distribution of $\theta(t+h)$ given $D_t$. And now h is a positive lag. So here we assume that is $hâ‰¥0$. So we are going to have the recursion is a $N(a_t(h), R_t(h))$. The mean is $a_t(h)$ and we are going to use the structure of the model to obtain these recursions, again. So here we are using the system equation, and the moment at(h) depends on what you computed at $a_t(h-1)$ the previous lag, times $G_{t+h}$. And then, would you initialize the recursion with $a_t(0)=m_t$. 

> Similarly, for the covariance matrix h steps ahead, you're going to have a recursion that depends on $Rt(h-1)$. And then you're going to need to input also $G_{t+h}$ and $W_{t+h}$. To initialize, the recursion with $Rt(0)= Ct$. So you can see that in order to compute these moments, you're going to need mt and Ct to start with. And then you're also going to have to input all the G's and the W's for the number of steps ahead that you require.

> Similarly, you can compute the distribution, the h steps ahead distribution of y_t+h given Dt. And that one also follows a normal, with mean $f_t(h)$, $q_t(h)$. And now we also have a recursion here, $ft(h)$ depends on $at(h)$ and as we said, $a_t(h)$ depends on $a_t(h-1)$ and so on. And $q_t(h)$ is just given by these equations. So once again, you have to have access to $F_{t+h}$ for all the $h$, a number of steps ahead that you are trying to compute this distribution. And then you also have to provide the observational variance for every h value. So that you get $v_{t+h}$. So this is specified in the modeling framework as well. If you want proceed with the forecasting distributions.

