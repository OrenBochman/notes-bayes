<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="dcterms.date" content="2024-10-22">
<meta name="keywords" content="time series, notation, bibliography, R code">
<meta name="description" content="The AR(1) process, Stationarity, ACF, PACF, Differencing, and Smoothing">

<title>88&nbsp; Week 0: Introductions to time series analysis and the AR(1) process – Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C4-L01.html" rel="next">
<link href="./C3-L09-Ex4.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(images/banner_deep.jpg);
background-size: cover;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C4-L00.html"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Week 0: Introductions to time series analysis and the AR(1) process</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Week 0: Introductions to time series analysis and the AR(1) process</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Time Series Analysis</p>
                  <div>
        <div class="description">
          The AR(1) process, Stationarity, ACF, PACF, Differencing, and Smoothing
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Coursera</div>
                <div class="quarto-category">notes</div>
                <div class="quarto-category">Bayesian Statistics</div>
                <div class="quarto-category">Autoregressive Models</div>
                <div class="quarto-category">Time Series</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 22, 2024</p>
      </div>
    </div>
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>time series, notation, bibliography, R code</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">About</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">M1L1 - Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Homework on paradigms of probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">M1L2 - Bayes’ Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">M1L3 - Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Homework on Random Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Homework on Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">M2L4 - Frequentist Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Homework - Frequentist MLE</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">M2L5 - Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">M3L6 - Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Homework on Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">M3L8 - Poisson Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Homework on Poisson Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Honors Quiz - Beta Bernoulli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Homework exponential data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">M4L10 - Normally distributed Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Homework Normal data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">M4L11 - Non-Informative Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Homework Alternative Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">M4L12 - Brief Review of Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">—</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Honors Homework On Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">M1L1 - Statistical Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">M1L3 - Monte Carlo estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">M2L4 - Metropolis-Hastings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">M2L5 - Gibbs sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">HW - Gibbs-Sampling algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">M2L5 - Assessing Convergence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Honnors Homework on M-H algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">M3L7 - Linear regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">HW on Linear Regression Model Part 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">HW - Deviance information criterion (DIC)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">M3L8 - ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">HW on ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">HW+ - Multiple Factor ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">M3L9 - Logistic regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">M4L10 - Poisson regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Homework on Poisson regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">M4L11 -Hierarchical modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">M4L12 - Capstone Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">M1L1 - Definitions of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">M1L2 - Likelihood functions for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">HW - The likelihood function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">HW - Identifiability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">HW - The likelihood function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">HW - Simulating from a Mixture Model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">HW+ - Sim mixture of exponential distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">M2L3 - The EM algorithm for Mixture models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">M4L1 - MCMC for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">M4L5 - Density Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">M4L6 - Clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">M4L7 -Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Bayesian Mixture Models for Classification of Banknotes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">M5L8 - Computational Considerations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">M5L9 - Determining the number of components</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">M5HW2 - Bayesian Information Criteria (BIC)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">M5HW2 - Estimating the number of components in Bayesian settings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">M5HW3 - Estimating the partition structure in Bayesian models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">M5HW4 - BIC for zero-inflated mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L00.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Week 0: Introductions to time series analysis and the AR(1) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Introductions to Time Series analysis &amp; the AR(1) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(p) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Time Series Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Bayes by backprop</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#course-card" id="toc-course-card" class="nav-link active" data-scroll-target="#course-card"><span class="header-section-number">88.1</span> Course Card</a></li>
  <li><a href="#overview-of-the-course" id="toc-overview-of-the-course" class="nav-link" data-scroll-target="#overview-of-the-course"><span class="header-section-number">88.2</span> Overview of the course</a>
  <ul class="collapse">
  <li><a href="#mathematical-review" id="toc-mathematical-review" class="nav-link" data-scroll-target="#mathematical-review"><span class="header-section-number">88.2.1</span> Mathematical Review</a></li>
  <li><a href="#complex-numbers-review" id="toc-complex-numbers-review" class="nav-link" data-scroll-target="#complex-numbers-review"><span class="header-section-number">88.2.2</span> Complex Numbers (Review)</a></li>
  <li><a href="#eigenvalues-eigenvectors-the-characteristic-polynomials-and-unit-roots" id="toc-eigenvalues-eigenvectors-the-characteristic-polynomials-and-unit-roots" class="nav-link" data-scroll-target="#eigenvalues-eigenvectors-the-characteristic-polynomials-and-unit-roots"><span class="header-section-number">88.2.3</span> Eigenvalues, Eigenvectors the characteristic polynomials and Unit roots</a></li>
  <li><a href="#spectral-analysis-1898" id="toc-spectral-analysis-1898" class="nav-link" data-scroll-target="#spectral-analysis-1898"><span class="header-section-number">88.2.4</span> Spectral analysis (1898)</a></li>
  <li><a href="#yule-walker-equations-1932" id="toc-yule-walker-equations-1932" class="nav-link" data-scroll-target="#yule-walker-equations-1932"><span class="header-section-number">88.2.5</span> Yule-Walker Equations (1932)</a></li>
  <li><a href="#durbin-levinson-recursion-off-course-reading" id="toc-durbin-levinson-recursion-off-course-reading" class="nav-link" data-scroll-target="#durbin-levinson-recursion-off-course-reading"><span class="header-section-number">88.2.6</span> Durbin-Levinson recursion (Off-Course Reading)</a></li>
  <li><a href="#durbin-levinson-and-the-yule-walker-equations-off-course-reading" id="toc-durbin-levinson-and-the-yule-walker-equations-off-course-reading" class="nav-link" data-scroll-target="#durbin-levinson-and-the-yule-walker-equations-off-course-reading"><span class="header-section-number">88.2.7</span> Durbin-Levinson and the Yule-Walker equations (Off-Course Reading)</a></li>
  <li><a href="#wolds-theorem---extra-curricular-circa-1939" id="toc-wolds-theorem---extra-curricular-circa-1939" class="nav-link" data-scroll-target="#wolds-theorem---extra-curricular-circa-1939"><span class="header-section-number">88.2.8</span> Wold’s theorem - (extra curricular) circa 1939</a></li>
  </ul></li>
  <li><a href="#kalman-filter-1960" id="toc-kalman-filter-1960" class="nav-link" data-scroll-target="#kalman-filter-1960"><span class="header-section-number">88.3</span> Kalman Filter (1960)</a>
  <ul class="collapse">
  <li><a href="#box-jenkins-method-1970" id="toc-box-jenkins-method-1970" class="nav-link" data-scroll-target="#box-jenkins-method-1970"><span class="header-section-number">88.3.1</span> Box Jenkins Method (1970)</a></li>
  </ul></li>
  <li><a href="#bayesian-time-series-bibliography" id="toc-bayesian-time-series-bibliography" class="nav-link" data-scroll-target="#bayesian-time-series-bibliography"><span class="header-section-number">88.4</span> Bayesian Time Series Bibliography</a>
  <ul class="collapse">
  <li><a href="#time-series-modeling-computation-and-inference" id="toc-time-series-modeling-computation-and-inference" class="nav-link" data-scroll-target="#time-series-modeling-computation-and-inference"><span class="header-section-number">88.4.1</span> Time Series: Modeling, Computation, and Inference</a></li>
  <li><a href="#bayesian-forecasting-and-dynamic-models" id="toc-bayesian-forecasting-and-dynamic-models" class="nav-link" data-scroll-target="#bayesian-forecasting-and-dynamic-models"><span class="header-section-number">88.4.2</span> Bayesian Forecasting and Dynamic Models</a></li>
  <li><a href="#practical-time-series-analysis" id="toc-practical-time-series-analysis" class="nav-link" data-scroll-target="#practical-time-series-analysis"><span class="header-section-number">88.4.3</span> Practical Time Series Analysis</a></li>
  <li><a href="#bayesian-modeling-and-computation-in-python" id="toc-bayesian-modeling-and-computation-in-python" class="nav-link" data-scroll-target="#bayesian-modeling-and-computation-in-python"><span class="header-section-number">88.4.4</span> Bayesian Modeling and Computation in Python</a></li>
  <li><a href="#bayesian-data-analysis" id="toc-bayesian-data-analysis" class="nav-link" data-scroll-target="#bayesian-data-analysis"><span class="header-section-number">88.4.5</span> Bayesian Data Analysis</a></li>
  <li><a href="#introductory-time-series-with-r-c.f.-cowpertwait2009introductory" id="toc-introductory-time-series-with-r-c.f.-cowpertwait2009introductory" class="nav-link" data-scroll-target="#introductory-time-series-with-r-c.f.-cowpertwait2009introductory"><span class="header-section-number">88.4.6</span> Introductory Time Series with R c.f. <span class="citation" data-cites="cowpertwait2009introductory">(Cowpertwait and Metcalfe 2009)</span></a></li>
  <li><a href="#analysis-of-integrated-and-cointegrated-time-series-with-r-c.f." id="toc-analysis-of-integrated-and-cointegrated-time-series-with-r-c.f." class="nav-link" data-scroll-target="#analysis-of-integrated-and-cointegrated-time-series-with-r-c.f."><span class="header-section-number">88.4.7</span> Analysis of Integrated and Cointegrated Time Series with R c.f.</a></li>
  <li><a href="#bayesian-analysis-of-time-series-by-lyle-d.-broemeling" id="toc-bayesian-analysis-of-time-series-by-lyle-d.-broemeling" class="nav-link" data-scroll-target="#bayesian-analysis-of-time-series-by-lyle-d.-broemeling"><span class="header-section-number">88.4.8</span> Bayesian Analysis of Time Series by Lyle D. Broemeling</a></li>
  <li><a href="#bayesian-inference-for-stochastic-processes-by-lyle-d.-broemeling" id="toc-bayesian-inference-for-stochastic-processes-by-lyle-d.-broemeling" class="nav-link" data-scroll-target="#bayesian-inference-for-stochastic-processes-by-lyle-d.-broemeling"><span class="header-section-number">88.4.9</span> Bayesian Inference for Stochastic Processes by Lyle D. Broemeling</a></li>
  <li><a href="#dynamic-time-series-models-using-r-inla-an-applied-perspective" id="toc-dynamic-time-series-models-using-r-inla-an-applied-perspective" class="nav-link" data-scroll-target="#dynamic-time-series-models-using-r-inla-an-applied-perspective"><span class="header-section-number">88.4.10</span> Dynamic Time Series Models using R-INLA: An Applied Perspective</a></li>
  <li><a href="#statistics-for-spatio-temporal-data" id="toc-statistics-for-spatio-temporal-data" class="nav-link" data-scroll-target="#statistics-for-spatio-temporal-data"><span class="header-section-number">88.4.11</span> Statistics for Spatio-Temporal Data</a></li>
  </ul></li>
  <li><a href="#bayesian-analysis-of-stochastic-process-models" id="toc-bayesian-analysis-of-stochastic-process-models" class="nav-link" data-scroll-target="#bayesian-analysis-of-stochastic-process-models"><span class="header-section-number">88.5</span> Bayesian Analysis of Stochastic Process Models</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<p>I decided to migrate some material that is auxiliary to the course:</p>
<ol type="1">
<li>An overview of the course.</li>
<li>A review of some mathematical and statistical results used in the course.</li>
<li>A bibliography of books I found useful in the course.</li>
<li>A Feynman notebook for the course - is now in a separate notebook.</li>
</ol>
<section id="course-card" class="level2" data-number="88.1">
<h2 data-number="88.1" class="anchored" data-anchor-id="course-card"><span class="header-section-number">88.1</span> Course Card</h2>
<ul>
<li><strong>Course:</strong> <a href="https://www.coursera.org/learn/bayesian-statistics-time-series">Bayesian Statistics: Time Series</a></li>
<li><strong>Offered by:</strong> University of California, Santa Cruz</li>
<li><strong>Instructor:</strong> Raquel Prado</li>
<li><strong>Certificate:</strong> Yes</li>
<li><strong>Level:</strong> Graduate</li>
<li><strong>Commitment:</strong> 4 weeks of study, 3-4 hours/week</li>
</ul>
</section>
<section id="overview-of-the-course" class="level2 page-columns page-full" data-number="88.2">
<h2 data-number="88.2" class="anchored" data-anchor-id="overview-of-the-course"><span class="header-section-number">88.2</span> Overview of the course</h2>
<p>This course seems very similar to classic basic time series course without the Bayesian part. (AR, MA, ARMA, ARIMA, SARIMA, DLM etc.)</p>
<p>One of the questions I had when I started this course was what is the difference between a Bayesian approach to time series analysis and a classical approach. The following is a summary of what I found:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Are we Being Bayesian ?
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Bayesian approach presents primarily in:</p>
<ol type="1">
<li>Sections on Bayesian inference where we do inference on the parameters of the models.</li>
<li>Bayesian prediction unlike an MLE prediction is a distribution of predictions not just a point estimate, and therefore is useful for quantifying uncertainty.</li>
<li>We also cover some material on model selection - this again is where the Bayesian approach to optimization presents more powerful tools than the classical approach.</li>
<li>When we want to quantify the uncertainty in our model we have four sources of uncertainty:
<ol type="1">
<li>Uncertainty due to using the correct model (structure).
<ul>
<li>I consider this is an epistemic uncertainty -</li>
<li>One could reduce it by collecting more data, then applying the Bayesian model selection to choose the best model.</li>
</ul></li>
<li>Uncertainty due to the estimation of the model parameters. This is an epistemic uncertainty - we can reduce it by collecting more data reducing the plausible intervals for these parameters under the bayesian approach.</li>
<li>Uncertainty due to random shocks <span class="math inline">\epsilon_t</span>. for the period being predicted. This is an aleatory uncertainty.</li>
<li>Uncertainty in the forecasted values <span class="math inline">X_{t+h}</span> Items 2-3 can be quantified using a plausible interval in the Bayesian approach and as we predict further into the future the interval will grow.</li>
</ol></li>
<li>Model selection is a big part of the Bayesian approach. We can use the DIC, WAIC, and LOO to compare models.</li>
</ol>
</div>
</div>
<ul>
<li>The book by Professor Prado is very comprehensive and covers plenty of additional models and references lots of recent research. These including VAR, VARMA models, Kalman filters, SMC/Particle filters, etc. These are useful for the <em>continuous control</em> flavours of RL. But you will need to learn it on your own.</li>
<li>In the capstone project that is the next course in the specialization the teacher adds another layer of sophistication by introducing mixtures of TS models.</li>
<li>However unlike some courses I took we dive deep enough and get sufficient examples to understand how to put all the bits together into more sophisticated time series models.</li>
</ul>
<section id="mathematical-review" class="level3" data-number="88.2.1">
<h3 data-number="88.2.1" class="anchored" data-anchor-id="mathematical-review"><span class="header-section-number">88.2.1</span> Mathematical Review</h3>
<ul>
<li><p>There is a issues with mathematics most of the results and techniques are so rarely useful that students will soon forget most but a few very useful results. Having a good memory is a great asset in mathematics but is rarely enough. I like to review some mathematical results from my undergraduate days every five years or so. This helps me keep many of the results fresh in my mind and also makes reading new mathematics easier. Fundamentals in mathematics can fo a very long way. This is material from topology, determinants and solving linear equations, numerical methods for decomposing matrices, and so on. Definitions of certain groups.</p></li>
<li><p>One reason this and other Bayesian courses and books can be challenging and even overwhelming is that they can use lots of mathematics. This can range from high school material like complex numbers and quadratics formulas to intermediate results like finding root of characteristic polynomials, eigenvalues, Topelitz matrices, jordan forms, and advanced topics like the Durbin-Levinson recursion and certain results from functional analysis theory.</p></li>
</ul>
<p>Note that I have not even touched on probability and statistics in that list.</p>
<p>Rather than complain I see this as an opportunity to review/learn some mathematics and statistics that can be useful to a data scientist. During my last sting in Data science I often was able to write formulas but more often then not felt that I lacked sufficient mathematical tools to manipulate them to get the kind of results I wanted. Rather then learning lots of mathematics I wanted to find the most practical and useful results for wrangling maths. When I was a physics undergraduate these might be trigonometric identities, completing the square, being familiar with many integrals and <a href="https://en.wikipedia.org/wiki/Taylor_series">Taylor</a> or Maclaurin series approximations and a few useful inequalities occasionally we use l’Hopital’s rule. Familiarity with some ODEs was also greatly beneficial as these come up in many physical models. Later on hermitian and unitary matrices, fourier expansions, spectral theory, and some results from functional analysis were useful.</p>
<p>For statistics we have the variants of the law of large numbers and the central limit theorem, convergence theorems, manipulations of the normal distribution, linear properties of expectation can get you along way. But you have to remember lots of definitions and there are lots of results and theorems that seem to be stepping stones to other results rather than any practical use.</p>
<p>On the other hand conjugacy of certain distributions as demonstrated by Herbert Lee and other instructors in this specialization are often very challenging. Charts of Convergence of distributions to other distributions under certain conditions are neat but. There is <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding’s inequality</a> and the <a href="https://en.wikipedia.org/wiki/Markov%27s_inequality">Markov’s inequality</a> which can be useful but like most results in mathematics I never had a where they might be used. Then there are certain results - convergence of Markov chains, doubly stochastic matrices. De Finetti’s theorem in statistics.</p>
<p>I have found that the more I learn the more I can understand and appreciate the material.</p>
<ul>
<li>The autoregressive process gives rise to Toeplitz matrices which can be solved using the Durbin-Levinson recursion mentioned many times in the course.</li>
<li>Durbin-Levinson recursion - is an advanced topic not covered in Numerical Analysis courses or Algebra courses I took.</li>
<li>To use it with time series we also need to understand the Yule-Walker equations.</li>
<li>ar(p) require some linear algebra concepts like eigenvalues and Eigenvectors, and characteristic polynomials.</li>
<li>The AR(p) the Wold decomposition theorem to get to the infinite order moving average representation and this is not a result I recall learning in my functional analysis course. We also use some complex numbers and Fourier analysis and spectral density functions.</li>
</ul>
<p>Summarize some of the extra curricular material I found useful in the course.</p>
<ul class="task-list">
<li><label><input type="checkbox" checked="">Complex numbers</label></li>
<li><label><input type="checkbox">Eigenvalues, Eigenvectors and characteristic polynomials</label></li>
<li><label><input type="checkbox" checked="">Durbin-Levinson recursion</label></li>
<li><label><input type="checkbox" checked="">Yule-Walker equations</label></li>
<li><label><input type="checkbox">Wiener process (Random walk)</label></li>
<li><label><input type="checkbox">Brownian motion (Continuous Random walk with drift)</label></li>
<li><label><input type="checkbox">Markov Chains ()</label></li>
<li><label><input type="checkbox">Martingales ()</label></li>
<li><label><input type="checkbox">Stopping theorem</label></li>
<li><label><input type="checkbox" checked="">Kalman filter</label></li>
<li><label><input type="checkbox">Wold’s theorem</label></li>
<li><label><input type="checkbox">De Finetti’s theorem</label></li>
<li><label><input type="checkbox">Cholesky decomposition</label></li>
</ul>
</section>
<section id="complex-numbers-review" class="level3 page-columns page-full" data-number="88.2.2">
<h3 data-number="88.2.2" class="anchored" data-anchor-id="complex-numbers-review"><span class="header-section-number">88.2.2</span> Complex Numbers (Review)</h3>
<p>When we wish to find the roots of real valued polynomials we will often encounter complex numbers. In this course such polynomials arise naturally in the characteristic polynomials of AR(p) processes.</p>
<p>We will need the polar form of complex numbers to represent some variants of AR(p) process.</p>
<p>The numbers in the Complex field <span class="math inline">z \in \mathbb{C}</span> numbers are numbers that can be expressed in the form <span class="math inline">z = a + bi</span>, where <span class="math inline">a,b\in\mathbb{R}</span> and <span class="math inline">i</span> is the imaginary unit. The imaginary unit <span class="math inline">i</span> is defined as the square root of -1. Complex numbers can be added, subtracted, multiplied, and divided just like real numbers.</p>
<div class="page-columns page-full"><p>The complex conjugate  of a complex number <span class="math inline">z = a + bi</span> is denoted by <span class="math inline">\bar{z} = a - bi</span>. The magnitude of a complex number <span class="math inline">z = a + bi</span> is denoted by <span class="math inline">|z| = \sqrt{a^2 + b^2}</span>. This is sometimes called the modulus of the complex number in this course. The argument of a complex number <span class="math inline">z = a + bi</span> is denoted by <span class="math inline">\text{arg}(z) = \tan^{-1}(b/a)</span>. The polar form of a complex number is given by <span class="math inline">z = r e^{i \theta}</span>, where <span class="math inline">r = |z|</span> and <span class="math inline">\theta = \text{arg}(z)</span>.</p><div class="no-row-height column-margin column-container"><span class="">complex conjugate</span></div></div>
<p>The polar form of a complex number is given by:</p>
<p><span id="eq-polar-form"><span class="math display">
\begin{aligned}
z &amp;= \mid z\mid e^{i \theta} \\
  &amp;= r (\cos(\theta) + i \sin(\theta))
\end{aligned}
\tag{88.1}</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">|z|</span> is the magnitude of the complex number, i.e.&nbsp;the distance from the origin to the point in the complex plane.</li>
<li><span class="math inline">\theta</span> is the angle of the complex number.</li>
</ul>
<p>I think we will also need the unit roots.</p>
</section>
<section id="eigenvalues-eigenvectors-the-characteristic-polynomials-and-unit-roots" class="level3" data-number="88.2.3">
<h3 data-number="88.2.3" class="anchored" data-anchor-id="eigenvalues-eigenvectors-the-characteristic-polynomials-and-unit-roots"><span class="header-section-number">88.2.3</span> Eigenvalues, Eigenvectors the characteristic polynomials and Unit roots</h3>
<p>The Eigenvalues of a matrix are the roots of the characteristic polynomial of the matrix. The characteristic polynomial of a matrix A is defined as:</p>
<p><span class="math display">
\begin{aligned}
\text{det}(A - \lambda I) = 0
\end{aligned}
</span></p>
<p>where <span class="math inline">\lambda</span> is the Eigenvalue and <span class="math inline">I</span> is the identity matrix. The eigenvectors of a matrix are the vectors that satisfy the equation:</p>
<p><span class="math display">
\begin{aligned}
A v = \lambda v
\end{aligned}
</span></p>
<p>where <span class="math inline">v</span> is the eigenvector and <span class="math inline">\lambda</span> is the eigenvalue. The eigenvalues and eigenvectors of a matrix are used in many applications in mathematics and physics, including the diagonalization of matrices, the solution of differential equations, and the analysis of dynamical systems.</p>
<section id="unit-roots" class="level4" data-number="88.2.3.1">
<h4 data-number="88.2.3.1" class="anchored" data-anchor-id="unit-roots"><span class="header-section-number">88.2.3.1</span> Unit Roots</h4>
<p>A unit root is a root of the characteristic polynomial of an autoregressive model that is equal to 1. The presence of a unit root in an autoregressive model indicates that the model is not stationary. The unit root test is a statistical test that is used to determine whether a time series is stationary or non-stationary. The unit root test is based on the null hypothesis that the time series has a unit root, and the alternative hypothesis that the time series is stationary. The unit root test is used to determine whether a time series is stationary or non-stationary, and is an important tool in time series analysis.</p>
</section>
</section>
<section id="spectral-analysis-1898" class="level3" data-number="88.2.4">
<h3 data-number="88.2.4" class="anchored" data-anchor-id="spectral-analysis-1898"><span class="header-section-number">88.2.4</span> Spectral analysis (1898)</h3>
<p>The power spectrum of a signal is the squared absolute value of its Fourier transform. If it is estimated from the discrete Fourier transform it is also called periodogram. Usually estimated using the a fast Fourier transform (FFT) algorithm.</p>
</section>
<section id="yule-walker-equations-1932" class="level3" data-number="88.2.5">
<h3 data-number="88.2.5" class="anchored" data-anchor-id="yule-walker-equations-1932"><span class="header-section-number">88.2.5</span> Yule-Walker Equations (1932)</h3>
</section>
<section id="durbin-levinson-recursion-off-course-reading" class="level3" data-number="88.2.6">
<h3 data-number="88.2.6" class="anchored" data-anchor-id="durbin-levinson-recursion-off-course-reading"><span class="header-section-number">88.2.6</span> Durbin-Levinson recursion (Off-Course Reading)</h3>
<p>Like me, you might be curious about the Durbin-Levinson recursion mentioned above. This is not covered in the course, and turned out to be an enigma wrapped in a mystery.</p>
<p>I present my finding in the note below - much of it is due to <span class="citation" data-cites="enwiki-LevinsonRecursion">(<a href="#ref-enwiki-LevinsonRecursion" role="doc-biblioref">Wikipedia contributors 2024b</a>)</span> and <span class="citation" data-cites="enwiki-YuleWalkerEquations">(<a href="#ref-enwiki-YuleWalkerEquations" role="doc-biblioref">Wikipedia contributors 2024a</a>)</span></p>
<p>In <span class="citation" data-cites="yule1927periodicities">(<a href="#ref-yule1927periodicities" role="doc-biblioref">Yule 1927</a>)</span> and <span class="citation" data-cites="walker1931periodicity">(<a href="#ref-walker1931periodicity" role="doc-biblioref">Walker 1931</a>)</span>, Yule and Walker proposed a method for estimating the parameters of an autoregressive model. The method is based on the Yule-Walker equations which are a set of linear equations that can be used to estimate the parameters of an autoregressive model.</p>
<p>Due to the autoregressive nature of the model, the equations are take a special form called a Toeplitz matrix. However at the time they probably had to use the numerically unstable Gauss-Jordan elimination to solve these equations which is <span class="math inline">O(n^3)</span> in time complexity.</p>
<p>A decade or two later in <span class="citation" data-cites="Levinson1947Wiener">(<a href="#ref-Levinson1947Wiener" role="doc-biblioref">Levinson 1946</a>)</span> and <span class="citation" data-cites="Durbin1960Fitting">(<a href="#ref-Durbin1960Fitting" role="doc-biblioref">Durbin 1960</a>)</span> the authors came up for with a weakly stable yet more efficient algorithm for solving these autocorrelated system of equations which requires only <span class="math inline">O(n^2)</span> in time complexity. Later their work was further refined in <span class="citation" data-cites="Trench1964ToeplitzMI">(<a href="#ref-Trench1964ToeplitzMI" role="doc-biblioref">Trench 1964</a>)</span> and <span class="citation" data-cites="Zohar1969ToeplitzMI">(<a href="#ref-Zohar1969ToeplitzMI" role="doc-biblioref">Zohar 1969</a>)</span> to just <span class="math inline">3\times n^2</span> multiplication. A cursory search reveals that Toeplitz matrix inversion is still an area of active research with papers covering parallel algorithms and stability studies. Not surprising as man of the more interesting deep learning models, including LLMs are autoregressive.</p>
<p>So the <a href="https://en.wikipedia.org/wiki/Levinson_recursion">Durbin-Levinson recursion</a> is just an elegant bit of linear algebra for solving the <a href="https://w.wiki/9gVB#Estimation_of_AR_parameters">Yule-Walker equations</a> more efficiently.</p>
<p>Here is what I dug up:</p>
</section>
<section id="durbin-levinson-and-the-yule-walker-equations-off-course-reading" class="level3" data-number="88.2.7">
<h3 data-number="88.2.7" class="anchored" data-anchor-id="durbin-levinson-and-the-yule-walker-equations-off-course-reading"><span class="header-section-number">88.2.7</span> Durbin-Levinson and the Yule-Walker equations (Off-Course Reading)</h3>
<p>The Durbin-Levinson recursion is a method in linear algebra for computing the solution to an equation involving a <em>Toeplitz matrix</em> AKA a <em>diagonal-constant matrix</em> where descending diagonals are constant. The recursion runs in <span class="math inline">O(n^2)</span> time rather then <span class="math inline">O(n^3)</span> time required by Gauss-Jordan elimination.</p>
<p>The recursion can be used to compute the coefficients of the autoregressive model of a stationary time series. It is based on the <a href="https://w.wiki/9gVB#Estimation_of_AR_parameters">Yule-Walker equations</a> and is used to compute the PACF of a time series.</p>
<p>The Yule-Walker equations can be stated as follows for an AR(p) process:</p>
<p><span id="eq-yule-walker"><span class="math display">
\gamma_m = \sum_{k=1}^p \phi_k \gamma_{m-k} + \sigma_\epsilon^2\delta_{m,0} \qquad \text{(Yule-Walker equations)}
\tag{88.2}</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\gamma_m</span> is the autocovariance function of the time series,</li>
<li><span class="math inline">\phi_k</span> are the AR coefficients,</li>
<li><span class="math inline">\sigma_\epsilon^2</span> is the variance of the white noise process, and</li>
<li><span class="math inline">\delta_{m,0}</span> is the Kronecker delta function.</li>
</ul>
<p>when <span class="math inline">m=0</span> the equation simplifies to:</p>
<p><span id="eq-yule-walker-m-0"><span class="math display">
\gamma_0 = \sum_{k=1}^p \phi_k \gamma_{-k} + \sigma_\epsilon^2 \qquad \text{(Yule-Walker equations for m=0)}
\tag{88.3}</span></span></p>
<p>for <span class="math inline">m &gt; 0</span> the equation simplifies to:</p>
<p><span class="math display"> \begin{bmatrix}
    \gamma_1 \newline
    \gamma_2 \newline
    \gamma_3 \newline
    \vdots \newline
    \gamma_p \newline
\end{bmatrix} =  \begin{bmatrix}
    \gamma_0     &amp; \gamma_{-1}  &amp; \gamma_{-2}  &amp; \cdots \newline
    \gamma_1     &amp; \gamma_0     &amp; \gamma_{-1}  &amp; \cdots \newline
    \gamma_2     &amp; \gamma_1     &amp; \gamma_0     &amp; \cdots \newline
    \vdots       &amp; \vdots       &amp; \vdots       &amp; \ddots \newline
    \gamma_{p-1} &amp; \gamma_{p-2} &amp; \gamma_{p-3} &amp; \cdots \newline
\end{bmatrix}  \begin{bmatrix}
    \phi_{1} \newline
    \phi_{2} \newline
    \phi_{3} \newline
    \vdots \newline
    \phi_{p} \newline
\end{bmatrix}
</span></p>
<p>and since this matrix is Toeplitz, we can use Durbin-Levinson recursion to efficiently solve the system for <span class="math inline">\phi_k \forall k</span>.</p>
<p>Once <span class="math inline">\{\phi_m ; m=1,2, \dots ,p \}</span> are known, we can consider m=0 and solved for <span class="math inline">\sigma_\epsilon^2</span> by substituting the <span class="math inline">\phi_k</span> into <a href="#eq-yule-walker-m-0" class="quarto-xref">Equation&nbsp;<span>88.3</span></a> Yule-Walker equations.</p>
<p>Of course the Durbin-Levinson recursion is not the last word on solving this system of equations. There are today numerous improvements which are both faster and more numerically stable.</p>
<p>The Yule-Walker equations are a set of p linear equations in the p unknowns <span class="math inline">\phi_1, \phi_2, \ldots, \phi_p</span> that can be used to estimate the parameters of an autoregressive model of order p.&nbsp;The Yule-Walker equations are derived by setting the sample autocorrelation function equal to the theoretical autocorrelation function of an AR(p) model and then solving for the unknown parameters. The Yule-Walker equations are given by:</p>
<p><span class="math display">
\begin{aligned}
\gamma(0) &amp; = \phi_1 \gamma(1) + \phi_2 \gamma(2) + \ldots + \phi_p \gamma(p) \\
\gamma(1) &amp; = \phi_1 \gamma(0) + \phi_2 \gamma(1) + \ldots + \phi_p \gamma(p-1) \\
\gamma(2) &amp; = \phi_1 \gamma(1) + \phi_2 \gamma(0) + \ldots + \phi_p \gamma(p-2) \\
\vdots \\
\gamma(p) &amp; = \phi_1 \gamma(p-1) + \phi_2 \gamma(p-2) + \ldots + \phi_p \gamma(0) \\
\end{aligned}
</span></p>
<p>where <span class="math inline">\gamma(k)</span> is the sample autocorrelation function at lag k. The Yule-Walker equations can be solved using matrix algebra to obtain the estimates of the AR parameters <span class="math inline">\phi_1, \phi_2, \ldots, \phi_p</span>.</p>
</section>
<section id="wolds-theorem---extra-curricular-circa-1939" class="level3" data-number="88.2.8">
<h3 data-number="88.2.8" class="anchored" data-anchor-id="wolds-theorem---extra-curricular-circa-1939"><span class="header-section-number">88.2.8</span> Wold’s theorem - (extra curricular) circa 1939</h3>
<p>In the 1920 Yule and <a href="">Eugen Slutsky</a> were researching time series and they came up with two different ways to represent a time series.</p>
<ul>
<li><p>Yule’s researches led to the notion of the autoregressive scheme. <span id="eq-yule-autoregressive-scheme"><span class="math display">
\begin{aligned}
Y_{t} &amp; = \sum _{j=1}^{p} \phi _{j} Y_{t-j} + u_{t}
\end{aligned}
\tag{88.4}</span></span></p></li>
<li><p>Slutsky’s researches led to the notion of a moving average scheme. <span id="eq-slutsky-moving-average-scheme"><span class="math display">
\begin{aligned}
Y_{t} &amp; =\sum _{j=0}^{q} \theta _{j} u_{t-j}
\end{aligned}
\tag{88.5}</span></span></p></li>
</ul>
<p>we can use the two schemes together and get the ARMA(p,q) model:</p>
<p><span id="eq-arma-scheme"><span class="math display">
\begin{aligned}
Y_{t} &amp; = \sum _{j=1}^{p} \phi _{j} Y_{t-j} + u_{t} + \sum _{j=0}^{q} \theta _{j} u_{t-j}
\end{aligned}
\tag{88.6}</span></span></p>
<p>where:</p>
<p>The following is extracted from: the wikipedia at https://en.wikipedia.org/wiki/Wold%27s_theorem</p>
<p>Wold’s decomposition AKA called the Wold representation theorem states that:</p>
<blockquote class="blockquote">
<p>Every covariance-stationary time series <span class="math inline">Y_{t}</span> can be written as the sum of two time series, one deterministic and one stochastic.</p>
</blockquote>
<p>Formally:</p>
<p><span class="math display">
\begin{aligned}
Y_{t} &amp; =\sum _{j=0}^{\infty }  \underbrace{b_{j}\epsilon _{t-j}}_{\text{stochastic}} + \underbrace{\eta _{t}}_{\text{deterministic}} \\
&amp;= \sum _{j=0}^{\infty } b_{j}\epsilon _{t-j} + \phi_{j} y_{t-j}
\end{aligned}
</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">{Y_{t}}</span> is the time series being considered,</li>
<li><span class="math inline">{\epsilon _{t}}</span> is an white noise sequence called <strong>innovation process</strong> that acts as an input to the linear filter <span class="math inline">{\{b_{j}\}}</span>.</li>
<li><span class="math inline">{b}</span> is the possibly infinite vector of moving average weights (coefficients or parameters)</li>
<li><span class="math inline">{\eta _{t}}</span> is a “deterministic” time series, in the sense that it is completely determined as a linear combination of its past values It may include “deterministic terms” like sine/cosine waves of <span class="math inline">{t}</span>, but it is a stochastic process and it is also covariance-stationary, it cannot be an arbitrary deterministic process that violates stationarity.</li>
</ul>
<p>The moving average coefficients have these properties:</p>
<ol type="1">
<li>Stable, that is, square summable <span class="math inline">\sum _{j=1}^{\infty } \mid b_{j}|^{2} &lt; \infty</span></li>
<li>Causal (i.e.&nbsp;there are no terms with j &lt; 0)</li>
<li>Minimum delay</li>
<li>Constant (<span class="math inline">b_j</span> independent of t)</li>
<li>It is conventional to define <span class="math inline">b_0=1</span></li>
</ol>
<p>Any stationary process has this seemingly special representation. Not only is the existence of such a simple linear and exact representation remarkable, but even more so is the special nature of the moving average model.</p>
<p>This result is used without stating its name in the course when we are show the AR(p) representation in terms of moving averages.</p>
</section>
</section>
<section id="kalman-filter-1960" class="level2" data-number="88.3">
<h2 data-number="88.3" class="anchored" data-anchor-id="kalman-filter-1960"><span class="header-section-number">88.3</span> Kalman Filter (1960)</h2>
<p><span id="eq-kalman-filter"><span class="math display">
\begin{aligned}
x_{t} &amp; = F_{t} x_{t-1} + G_{t} u_{t} + w_{t} &amp;&amp; \text{(transition equation)} \\
y_{t} &amp; = H_{t} x_{t} + v_{t} &amp;&amp; \text{(observation equation)}
\end{aligned}
\tag{88.7}</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">x_{t}</span> is the state vector at time t,</li>
<li><span class="math inline">F_{t}</span> is the state transition matrix,</li>
<li><span class="math inline">G_{t}</span> is the control input matrix,</li>
<li><span class="math inline">u_{t}</span> is the control vector,</li>
<li><span class="math inline">w_{t}</span> is the process noise vector,</li>
<li><span class="math inline">y_{t}</span> is the observation vector at time t,</li>
<li><span class="math inline">H_{t}</span> is the observation matrix,</li>
<li><span class="math inline">v_{t}</span> is the observation noise vector.</li>
</ul>
<p>The Kalman filter is a recursive algorithm that estimates the state of a linear dynamic system from a series of noisy observations. The Kalman filter is based on a linear dynamical system model that is defined by two equations: the state transition equation and the observation equation. The state transition equation describes how the state of the system evolves over time, while the observation equation describes how the observations are generated from the state of the system. The Kalman filter uses these two equations to estimate the state of the system at each time step, based on the observations received up to that time step. This could be implemented in real time in the 1960s and was used in the Apollo missions.</p>
<p>The Extended Kalman Filter (EKF) is an extension of the Kalman filter that can be used to estimate the state of a nonlinear dynamic system. The EKF linearizes the nonlinear system model at each time step and then applies the Kalman filter to the linearized system. The EKF is an approximation to the true nonlinear system, and its accuracy depends on how well the linearized system approximates the true system.</p>
<section id="box-jenkins-method-1970" class="level3" data-number="88.3.1">
<h3 data-number="88.3.1" class="anchored" data-anchor-id="box-jenkins-method-1970"><span class="header-section-number">88.3.1</span> Box Jenkins Method (1970)</h3>
<p>see <a href="https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method">Box Jenkins Method</a></p>
<p>A five step process for identifying, selecting and assessing ARMA (and similar) models.</p>
<ul>
<li>There are three courses on Stochastic Processes on MIT OCW that I found useful:
<ul>
<li><a href="https://ocw.mit.edu/courses/18-445-introduction-to-stochastic-processes-spring-2015/">Introduction to Stochastic Processes</a></li>
<li><a href="https://ocw.mit.edu/courses/6-262-discrete-stochastic-processes-spring-2011/">Discrete Stochastic Processes</a></li>
<li>has lecture videos and notes</li>
<li>poisson processes</li>
<li><a href="https://ocw.mit.edu/courses/15-070j-advanced-stochastic-processes-fall-2013/">Advanced Stochastic Processes</a></li>
<li>martingales</li>
<li>ito calculus</li>
</ul></li>
</ul>
</section>
</section>
<section id="bayesian-time-series-bibliography" class="level2 page-columns page-full" data-number="88.4">
<h2 data-number="88.4" class="anchored" data-anchor-id="bayesian-time-series-bibliography"><span class="header-section-number">88.4</span> Bayesian Time Series Bibliography</h2>
<p>We start with some books from the course, I collected here both the recommended books and some others that I found useful.</p>
<section id="time-series-modeling-computation-and-inference" class="level3 page-columns page-full" data-number="88.4.1">
<h3 data-number="88.4.1" class="anchored" data-anchor-id="time-series-modeling-computation-and-inference"><span class="header-section-number">88.4.1</span> Time Series: Modeling, Computation, and Inference</h3>
<p>c.f. <span class="citation" data-cites="prado2023time">(<a href="#ref-prado2023time" role="doc-biblioref">Prado, Ferreira, and West 2023</a>)</span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/prado2023time.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Time Series: Modeling, Computation, and Inference"><img src="images/prado2023time.jpg" class="img-fluid figure-img" width="150" alt="Time Series: Modeling, Computation, and Inference"></a></p>
<figcaption>Time Series: Modeling, Computation, and Inference</figcaption>
</figure>
</div></div><ul>
<li>Title:Time Series: Modeling, Computation, and Inference</li>
<li>ISBN:9781032040042, 1032040041</li>
<li>Page count:452</li>
<li>Published:September 2023</li>
<li>Format:Paperback</li>
<li>Publisher:CRC Press</li>
<li>Authors: Raquel Prado, Marco A. R. Ferreira, Mike West</li>
</ul>
<p><span class="citation" data-cites="prado2023time">(<a href="#ref-prado2023time" role="doc-biblioref">Prado, Ferreira, and West 2023</a>)</span> <strong>“Time Series: Modeling, Computation, and Inference”</strong> by course instructor Raquel Prado. This book, now in its second edition is a comprehensive introduction to time series analysis and covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</p>
<p>While learning this course I found some of the material harder to follow than I expected. The books helped to clarify definitions and so on however the book is<br>
rather comprehensive and mathematically advanced unlike some other books on statistics.</p>
<p>The teacher frequently point out that many aspects of Times series and are beyond the scope of the course. Yet this book covers much more ground like unequally spaced time series and vector valued time series.</p>
<p>For example we look at EKG data which the authors have been working on for years. However we look at it in this course in terms of a univariate time series while in reality EKG is usually sampled at 12 sites simultaneously yielding a multi-variate time series.</p>
<p>Once this course is done I will probably want to dive deeper into the subject and try to devote more time to other models in the book.</p>
<hr>
</section>
<section id="bayesian-forecasting-and-dynamic-models" class="level3 page-columns page-full" data-number="88.4.2">
<h3 data-number="88.4.2" class="anchored" data-anchor-id="bayesian-forecasting-and-dynamic-models"><span class="header-section-number">88.4.2</span> Bayesian Forecasting and Dynamic Models</h3>
<p>c.f. <span class="citation" data-cites="west2013bayesian">(<a href="#ref-west2013bayesian" role="doc-biblioref">West and Harrison 2013</a>)</span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/west2013bayesian.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Bayesian Forecasting and Dynamic Models"><img src="images/west2013bayesian.png" class="img-fluid figure-img" width="150" alt="Bayesian Forecasting and Dynamic Models"></a></p>
<figcaption>Bayesian Forecasting and Dynamic Models</figcaption>
</figure>
</div></div><ul>
<li>Title:Bayesian Forecasting and Dynamic Models</li>
<li>ISBN:9781475770971, 1475770979</li>
<li>Page count:682</li>
<li>Published:March 17, 2013</li>
<li>Format:Paperback</li>
<li>Publisher:Springer New York</li>
<li>Author:Mike West, Jeff Harrison</li>
</ul>
<p><span class="citation" data-cites="west2013bayesian">(<a href="#ref-west2013bayesian" role="doc-biblioref">West and Harrison 2013</a>)</span> <strong>“Bayesian Forecasting and Dynamic Models”</strong> by Mike West and Jeff Harrison. This book is a classic text on Bayesian statistics and covers a wide range of topics in Bayesian forecasting and dynamic models. The following is the description from the publisher:</p>
<blockquote class="blockquote">
<p>The use of dynamic models in the forecasting of time series data has a long history, with the development of autoregressive integrated moving average (ARIMA) models and state space models. However, the use of Bayesian methods in the development of dynamic models is a relatively recent development. This book provides a comprehensive introduction to the use of Bayesian methods in the development of dynamic models for forecasting time series data. The book covers a wide range of topics, including the use of dynamic models in the analysis of time series data, the use of Bayesian methods in the development of dynamic models, and the use of dynamic models in the forecasting of time series data.</p>
</blockquote>
<ul>
<li>Audience: The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</li>
</ul>
<!--
In this book we are concerned with Bayesian learning and forecast ing in dynamic environments. We describe the structure and theory of classes of dynamic models, and their uses in Bayesian forecasting. The principles, models and methods of Bayesian forecasting have been developed extensively during the last twenty years. This development has involved thorough investigation of mathematical and statistical aspects of forecasting models and related techniques. With this has come experience with application in a variety of areas in commercial and industrial, scientific and socio-economic fields. In deed much of the technical development has been driven by the needs of forecasting practitioners. As a result, there now exists a relatively complete statistical and mathematical framework, although much of this is either not properly documented or not easily accessible. Our primary goals in writing this book have been to present our view of this approach to modelling and forecasting, and to provide a reasonably complete text for advanced university students and research workers. The text is primarily intended for advanced undergraduate and postgraduate students in statistics and mathematics. In line with this objective we present thorough discussion of mathematical and statistical features of Bayesian analyses of dynamic models, with illustrations, examples and exercises in each Chapter.
-->
<hr>
</section>
<section id="practical-time-series-analysis" class="level3 page-columns page-full" data-number="88.4.3">
<h3 data-number="88.4.3" class="anchored" data-anchor-id="practical-time-series-analysis"><span class="header-section-number">88.4.3</span> Practical Time Series Analysis</h3>
<p>c.f. <span class="citation" data-cites="nielsen2019practical">(<a href="#ref-nielsen2019practical" role="doc-biblioref">Nielsen 2019</a>)</span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/nielsen2019practical.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Practical Times Series Analysis"><img src="images/nielsen2019practical.jpg" class="img-fluid figure-img" width="150" alt="Practical Times Series Analysis"></a></p>
<figcaption>Practical Times Series Analysis</figcaption>
</figure>
</div></div><ul>
<li><p>Title:Practical Time Series Analysis: Prediction with Statistics and Machine Learning</p></li>
<li><p>ISBN:1492041602, 9781492041603</p></li>
<li><p>Page count:504</p></li>
<li><p>Published:2019</p></li>
<li><p>Format:Paperback</p></li>
<li><p>Publisher:O’Reilly Media, Inc.</p></li>
<li><p><span class="citation" data-cites="nielsen2019practical">(<a href="#ref-nielsen2019practical" role="doc-biblioref">Nielsen 2019</a>)</span> “Practical Time Series Analysis: Prediction with Statistics and Machine Learning” by Aileen Nielsen. Is a good resource for parctionars getting started with time series analysis. I also recommend any videos by Aileen Nielsen on the subject.</p></li>
</ul>
<blockquote class="blockquote">
<p>Practical Times Series Analysis by Aileen Nielsen is a good book for beginners. It is a practical guide to time series analysis and covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for beginners in statistics, computer science, and related fields.</p>
<p>Time series data analysis is increasingly important due to the massive production of such data through the internet of things, the digitalization of healthcare, and the rise of smart cities. As continuous monitoring and data collection become more common, the need for competent time series analysis with both statistical and machine learning techniques will increase.</p>
<p>Covering innovations in time series data analysis and use cases from the real world, this practical guide will help you solve the most common data engineering and analysis challenges in time series, using both traditional statistical and modern machine learning techniques. Author Aileen Nielsen offers an accessible, well-rounded introduction to time series in both R and Python that will have data scientists, software engineers, and researchers up and running quickly.</p>
<p>You’ll get the guidance you need to confidently:</p>
<ul>
<li>Find and wrangle time series data</li>
<li>Undertake exploratory time series data analysis</li>
<li>Store temporal data</li>
<li>Simulate time series data</li>
<li>Generate and select features for a time series</li>
<li>Measure error</li>
<li>Forecast and classify time series with machine or deep learning</li>
<li>Evaluate accuracy and performance</li>
</ul>
</blockquote>
<hr>
<section id="machine-learning-a-bayesian-and-optimization-perspective-by-sergios-theodoridis." class="level4 page-columns page-full" data-number="88.4.3.1">
<h4 data-number="88.4.3.1" class="anchored" data-anchor-id="machine-learning-a-bayesian-and-optimization-perspective-by-sergios-theodoridis."><span class="header-section-number">88.4.3.1</span> “Machine Learning: A Bayesian and Optimization Perspective” by Sergios Theodoridis.</h4>
<p>c.f. <span class="citation" data-cites="theodoridis2015ML">(<a href="#ref-theodoridis2015ML" role="doc-biblioref">Theodoridis 2015</a>)</span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/theodoridis2015ML.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Machine Learning: A Bayesian and Optimization Perspective"><img src="images/theodoridis2015ML.png" class="img-fluid figure-img" width="150" alt="Machine Learning: A Bayesian and Optimization Perspective"></a></p>
<figcaption>Machine Learning: A Bayesian and Optimization Perspective</figcaption>
</figure>
</div></div><ul>
<li>Title:Machine Learning: A Bayesian and Optimization Perspective</li>
<li>ISBN:0128015225, 9780128015223</li>
<li>Page count:1062</li>
<li>Published:2015</li>
<li>Format:Hardcover</li>
<li>Publisher:Academic Press</li>
<li>Authors: Sergios Theodoridis</li>
</ul>
<p>I came across this book while looking into the Durban-Levinson recursion and the Yule-Walker equations. So far I haven’t had time to read it but it looks like a good book on machine learning. The following is the description from the publisher:</p>
<blockquote class="blockquote">
<p>This tutorial text gives a unifying perspective on machine learning by covering both probabilistic and deterministic approaches -which are based on optimization techniques - together with the Bayesian inference approach, whose essence lies in the use of a hierarchy of probabilistic models. The book presents the major machine learning methods as they have been developed in different disciplines, such as statistics, statistical and adaptive signal processing and computer science. Focusing on the physical reasoning behind the mathematics, all the various methods and techniques are explained in depth, supported by examples and problems, giving an invaluable resource to the student and researcher for understanding and applying machine learning concepts.</p>
<p>The book builds carefully from the basic classical methods to the most recent trends, with chapters written to be as self-contained as possible, making the text suitable for different courses: pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models.</p>
<ul>
<li>All major classical techniques: Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods.</li>
<li>The latest trends: Sparsity, convex analysis and optimization, online distributed algorithms, learning in RKH spaces, Bayesian inference, graphical and hidden Markov models, particle filtering, deep learning, dictionary learning and latent variables modeling.</li>
<li>Case studies - protein folding prediction, optical character recognition, text authorship identification, fMRI data analysis, change point detection, hyperspectral image unmixing, target localization, channel equalization and echo cancellation, show how the theory can be applied.</li>
<li>MATLAB code for all the main algorithms are available on an accompanying website, enabling the reader to experiment with the code.</li>
</ul>
</blockquote>
<hr>
</section>
<section id="statistical-analysis-in-climate-research" class="level4 page-columns page-full" data-number="88.4.3.2">
<h4 data-number="88.4.3.2" class="anchored" data-anchor-id="statistical-analysis-in-climate-research"><span class="header-section-number">88.4.3.2</span> Statistical Analysis in Climate Research</h4>
<p>c.f.<span class="citation" data-cites="von2002statistical">(<a href="#ref-von2002statistical" role="doc-biblioref">Storch and Zwiers 2002</a>)</span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/von2002statistical.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Statistical Analysis in Climate Research"><img src="images/von2002statistical.jpg" class="img-fluid figure-img" width="150" alt="Statistical Analysis in Climate Research"></a></p>
<figcaption>Statistical Analysis in Climate Research</figcaption>
</figure>
</div></div><ul>
<li>Title:Statistical Analysis in Climate Research</li>
<li>ISBN:1139425099, 9781139425094</li>
<li>Page count:484</li>
<li>Published:2002</li>
<li>Format:Paperback</li>
<li>Publisher:Cambridge University Press</li>
<li>Authors: Hans von Storch, Francis W. Zwiers</li>
</ul>
<p>I came across this book while looking into the Durban-Levinson recursion and the Yule-Walker equations. So far I haven’t had time to read it but it looks promising. Here is the description from the publisher:</p>
<blockquote class="blockquote">
<p>Climatology is, to a large degree, the study of the statistics of our climate. The powerful tools of mathematical statistics therefore find wide application in climatological research. The purpose of this book is to help the climatologist understand the basic precepts of the statistician’s art and to provide some of the background needed to apply statistical methodology correctly and usefully. The book is self contained: introductory material, standard advanced techniques, and the specialised techniques used specifically by climatologists are all contained within this one source. There are a wealth of real-world examples drawn from the climate literature to demonstrate the need, power and pitfalls of statistical analysis in climate research. Suitable for graduate courses on statistics for climatic, atmospheric and oceanic science, this book will also be valuable as a reference source for researchers in climatology, meteorology, atmospheric science, and oceanography.</p>
<ul>
<li><p>Hans von Storch is Director of the Institute of Hydrophysics of the GKSS Research Centre in Geesthacht, Germany and a Professor at the Meteorological Institute of the University of Hamburg.</p></li>
<li><p>Francis W. Zwiers is Chief of the Canadian Centre for Climate Modelling and Analysis, Atmospheric Environment Service, Victoria, Canada, and an Adjunct Professor at the Department of Mathematicw and Statistics of the University of Victoria.</p></li>
</ul>
</blockquote>
<hr>
</section>
</section>
<section id="bayesian-modeling-and-computation-in-python" class="level3 page-columns page-full" data-number="88.4.4">
<h3 data-number="88.4.4" class="anchored" data-anchor-id="bayesian-modeling-and-computation-in-python"><span class="header-section-number">88.4.4</span> Bayesian Modeling and Computation in Python</h3>
<p>c.f. <span class="citation" data-cites="BMCP2021">(<a href="#ref-BMCP2021" role="doc-biblioref">Martin, Kumar, and Lao 2021</a>)</span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/BMCP2021.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Bayesian Modeling and Computation in Python"><img src="images/BMCP2021.jpg" class="img-fluid figure-img" width="150" alt="Bayesian Modeling and Computation in Python"></a></p>
<figcaption>Bayesian Modeling and Computation in Python</figcaption>
</figure>
</div></div><p>This is a great resource for translating what we learned to Python. The book is available at <a href="https://bayesiancomputationbook.com/">Bayesian Modeling and Computation in Python</a></p>
<p>I found the chapter on state space modeling and the Kalman filter particularly useful. The book is a great resource for translating what we learned in the course to Python. The book is suitable for undergraduate students in statistics, computer science, and related fields.</p>
<hr>
</section>
<section id="bayesian-data-analysis" class="level3 page-columns page-full" data-number="88.4.5">
<h3 data-number="88.4.5" class="anchored" data-anchor-id="bayesian-data-analysis"><span class="header-section-number">88.4.5</span> Bayesian Data Analysis</h3>
<p>c.f. <span class="citation" data-cites="gelman2013bayesian">(<a href="#ref-gelman2013bayesian" role="doc-biblioref">Gelman et al. 2013</a>)</span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="gelman2013bayesian.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Bayesian Data Analysis"><img src="gelman2013bayesian.jpg" class="img-fluid figure-img" width="150" alt="Bayesian Data Analysis"></a></p>
<figcaption>Bayesian Data Analysis</figcaption>
</figure>
</div></div><ul>
<li>Title:Bayesian Data Analysis</li>
<li>ISBN:1439840954, 9781439840955</li>
<li>Page count:675</li>
<li>Published:2013</li>
<li>Format:Hardcover</li>
<li>Publisher:Chapman and Hall/CRC</li>
<li>Authors: Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, Donald B. Rubin</li>
</ul>
<p><span class="citation" data-cites="gelman2013bayesian">(<a href="#ref-gelman2013bayesian" role="doc-biblioref">Gelman et al. 2013</a>)</span> “Bayesian Data Analysis” is probably the most famous book on Bayesian statistics. This book is a classic text on Bayesian statistics and covers a wide range of topics in Bayesian data analysis. Although this is not a time series book, the authors have been intersted in the domain of political election prediction and have used time series data in their research and some of that is covered in the book’s examples.</p>
<ul>
<li>Audience: The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</li>
<li>An electronic version of the third eddition book is available at <a href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis</a></li>
</ul>
<hr>
</section>
<section id="introductory-time-series-with-r-c.f.-cowpertwait2009introductory" class="level3 page-columns page-full" data-number="88.4.6">
<h3 data-number="88.4.6" class="anchored" data-anchor-id="introductory-time-series-with-r-c.f.-cowpertwait2009introductory"><span class="header-section-number">88.4.6</span> Introductory Time Series with R c.f. <span class="citation" data-cites="cowpertwait2009introductory">(<a href="#ref-cowpertwait2009introductory" role="doc-biblioref">Cowpertwait and Metcalfe 2009</a>)</span></h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/cowpertwait2009introductory.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Introductory Time Series with R"><img src="images/cowpertwait2009introductory.jpg" class="img-fluid figure-img" width="150" alt="Introductory Time Series with R"></a></p>
<figcaption>Introductory Time Series with R</figcaption>
</figure>
</div></div><p><span class="citation" data-cites="cowpertwait2009introductory">(<a href="#ref-cowpertwait2009introductory" role="doc-biblioref">Cowpertwait and Metcalfe 2009</a>)</span> “Introductory Time Series with R” by Cowpertwait and Metcalfe, and the second is</p>
<blockquote class="blockquote">
<p>Yearly global mean temperature and ocean levels, daily share prices, and the signals transmitted back to Earth by the Voyager space craft are all examples of sequential observations over time known as time series. This book gives you a step-by-step introduction to analysing time series using the open source software R. Each time series model is motivated with practical applications, and is defined in mathematical notation. Once the model has been introduced it is used to generate synthetic data, using R code, and these generated data are then used to estimate its parameters. This sequence enhances understanding of both the time series model and the R function used to fit the model to data. Finally, the model is used to analyse observed data taken from a practical application. By using R, the whole procedure can be reproduced by the reader.</p>
<p>All the data sets used in the book are available on the website at <a href="http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/">datasets</a></p>
<p>The book is written for undergraduate students of mathematics, economics, business and finance, geography, engineering and related disciplines, and postgraduate students who may need to analyse time series as part of their taught programme or their research.</p>
<ul>
<li><p>Paul Cowpertwait is an associate professor in mathematical sciences (analytics) at Auckland University of Technology with a substantial research record in both the theory and applications of time series and stochastic models.</p></li>
<li><p>Andrew Metcalfe is an associate professor in the School of Mathematical Sciences at the University of Adelaide, and an author of six statistics text books and numerous research papers. Both authors have extensive experience of teaching time series to students at all levels.</p></li>
</ul>
</blockquote>
<hr>
</section>
<section id="analysis-of-integrated-and-cointegrated-time-series-with-r-c.f." class="level3 page-columns page-full" data-number="88.4.7">
<h3 data-number="88.4.7" class="anchored" data-anchor-id="analysis-of-integrated-and-cointegrated-time-series-with-r-c.f."><span class="header-section-number">88.4.7</span> Analysis of Integrated and Cointegrated Time Series with R c.f.</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/pfaff2008analysis.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Analysis of Integrated and Cointegrated Time Series with R"><img src="images/pfaff2008analysis.jpg" class="img-fluid figure-img" style="vertical-align: top;" width="150" alt="Analysis of Integrated and Cointegrated Time Series with R"></a></p>
<figcaption>Analysis of Integrated and Cointegrated Time Series with R</figcaption>
</figure>
</div></div><p><span class="citation" data-cites="pfaff2008analysis">(<a href="#ref-pfaff2008analysis" role="doc-biblioref">Pfaff 2008</a>)</span> “Analysis of Integrated and Cointegrated Time Series with R” by Bernhard Pfaff. Its been a long time since I read this book and rather than do it an injustice I direct you to the review by Dirk Eddelbuettel in the Journal of Statistical Software is avaoilable at <a href="https://dirk.eddelbuettel.com/papers/pfaff_urca.pdf">review</a>. Or the book’s website at <a href="https://www.springer.com/gp/book/9780387759661">Analysis of Integrated and Cointegrated Time Series with R</a>.</p>
<blockquote class="blockquote">
<p>The analysis of integrated and co-integrated time series can be considered as the main methodology employed in applied econometrics. This book not only introduces the reader to this topic but enables him to conduct the various unit root tests and co-integration methods on his own by utilizing the free statistical programming environment R. The book encompasses seasonal unit roots, fractional integration, coping with structural breaks, and multivariate time series models. The book is enriched by numerous programming examples to artificial and real data so that it is ideally suited as an accompanying text book to computer lab classes.</p>
<p>The second edition adds a discussion of vector auto-regressive, structural vector auto-regressive, and structural vector error-correction models.</p>
</blockquote>
</section>
<section id="bayesian-analysis-of-time-series-by-lyle-d.-broemeling" class="level3" data-number="88.4.8">
<h3 data-number="88.4.8" class="anchored" data-anchor-id="bayesian-analysis-of-time-series-by-lyle-d.-broemeling"><span class="header-section-number">88.4.8</span> Bayesian Analysis of Time Series by Lyle D. Broemeling</h3>
<p><span class="citation" data-cites="broemeling2011bayesian">(<a href="#ref-broemeling2011bayesian" role="doc-biblioref">Broemeling 2019</a>)</span></p>
<ul>
<li>covers pretty much the material in the course.</li>
<li>uses winbugs and R</li>
<li>models considered include
<ul>
<li>white noise</li>
<li>Wiener process (random walk)</li>
<li>AR(p)</li>
<li>ARMA(p,q)</li>
<li>ARIMA</li>
<li>Regression</li>
<li>Regression with MA and Seasonal effects</li>
<li>DLM</li>
<li>TAR</li>
</ul></li>
</ul>
</section>
<section id="bayesian-inference-for-stochastic-processes-by-lyle-d.-broemeling" class="level3 page-columns page-full" data-number="88.4.9">
<h3 data-number="88.4.9" class="anchored" data-anchor-id="bayesian-inference-for-stochastic-processes-by-lyle-d.-broemeling"><span class="header-section-number">88.4.9</span> Bayesian Inference for Stochastic Processes by Lyle D. Broemeling</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/broemeling2018bayesian.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Bayesian Inference for Stochastic Processes by Lyle D. Broemeling"><img src="images/broemeling2018bayesian.jpg" class="img-fluid figure-img" style="vertical-align: top;" width="150" alt="Bayesian Inference for Stochastic Processes by Lyle D. Broemeling"></a></p>
<figcaption>Bayesian Inference for Stochastic Processes by Lyle D. Broemeling</figcaption>
</figure>
</div></div><ul>
<li>The code for R and WinBUGS is available at <a href="http://www.lbroemeling.com/">code</a></li>
<li>IT is based on WinBUGS which is a bit dated but still useful.</li>
<li>This books seems a bit dated but it covers a lot of the material in the course.</li>
</ul>
</section>
<section id="dynamic-time-series-models-using-r-inla-an-applied-perspective" class="level3 page-columns page-full" data-number="88.4.10">
<h3 data-number="88.4.10" class="anchored" data-anchor-id="dynamic-time-series-models-using-r-inla-an-applied-perspective"><span class="header-section-number">88.4.10</span> Dynamic Time Series Models using R-INLA: An Applied Perspective</h3>
<p><span class="citation" data-cites="ravishanker2022dynamic">(<a href="#ref-ravishanker2022dynamic" role="doc-biblioref">Ravishanker, Raman, and Soyer 2022</a>)</span> is a new book that covers the use of the R-INLA package for fitting dynamic time series models. The book is available online <a href="https://ramanbala.github.io/dynamic-time-series-models-R-INLA/">gitbook</a></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="ravishanker2022dynamic.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Dynamic Time Series Models using R-INLA: An Applied Perspective"><img src="ravishanker2022dynamic.jpg" class="img-fluid figure-img" style="vertical-align: top;" width="150" alt="Dynamic Time Series Models using R-INLA: An Applied Perspective"></a></p>
<figcaption>Dynamic Time Series Models using R-INLA: An Applied Perspective</figcaption>
</figure>
</div></div><p>This is a very interesting book which covers a new approach to fitting time series models using the R-INLA package. INLA stands for Integrated Nested Laplace Approximation and is a method for fitting Bayesian models that is faster than MCMC. The book covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</p>
</section>
<section id="statistics-for-spatio-temporal-data" class="level3 page-columns page-full" data-number="88.4.11">
<h3 data-number="88.4.11" class="anchored" data-anchor-id="statistics-for-spatio-temporal-data"><span class="header-section-number">88.4.11</span> Statistics for Spatio-Temporal Data</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/cressie2011statistics.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Statistics for Spatio-Temporal Data"><img src="images/cressie2011statistics.jpg" class="img-fluid figure-img" style="vertical-align: top;" width="150" alt="Statistics for Spatio-Temporal Data"></a></p>
<figcaption>Statistics for Spatio-Temporal Data</figcaption>
</figure>
</div></div><p><span class="citation" data-cites="cressie2011statistics">(<a href="#ref-cressie2011statistics" role="doc-biblioref">Cressie and Wikle 2011</a>)</span> is a book I came across when I tried to understand the NDLM model. NLDMs have a two level hierarcial form and it seems possible to extend this formulation will non-normaly distributed shocks and possibly non linear relation. In this book the authors take an interesting approch of not only looking at NDLM as a heirarchical model but they also extend the time series model into a spatio-temporal model.</p>
<p>This book is a comprehensive introduction to the analysis of spatio-temporal data and covers a wide range of topics in spatio-temporal statistics. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</p>
</section>
</section>
<section id="bayesian-analysis-of-stochastic-process-models" class="level2 page-columns page-full" data-number="88.5">
<h2 data-number="88.5" class="anchored" data-anchor-id="bayesian-analysis-of-stochastic-process-models"><span class="header-section-number">88.5</span> Bayesian Analysis of Stochastic Process Models</h2>
<p>c.f. <span class="citation" data-cites="rios2012bayesian">(<a href="#ref-rios2012bayesian" role="doc-biblioref">Rios Insua, Ruggeri, and Wiper 2012</a>)</span></p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/rios2012bayesian.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Bayesian Analysis of Stochastic Process Models"><img src="images/rios2012bayesian.jpg" class="img-fluid figure-img" style="vertical-align: top;" width="150" alt="Bayesian Analysis of Stochastic Process Models"></a></p>
<figcaption>Bayesian Analysis of Stochastic Process Models</figcaption>
</figure>
</div></div><p>David Rios Insua, Fabrizio Ruggeri, Michael P. Wiper</p>
<p>This book is a comprehensive introduction to the analysis of stochastic process models using Bayesian methods. The book covers a wide range of topics in stochastic process modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</p>
<p>There are also a number of books on NDLM that I’ve come accross:</p>
<ul>
<li><p><a href="https://mjlaine.github.io/dlm/dlmtut.html#org04d2ab8">Dynamic linear model tutorial</a> matlab</p></li>
<li><p><a href="https://www.cambridge.org/core/books/forecasting-structural-time-series-models-and-the-kalman-filter/CE5E112570A56960601760E786A5E631">Forecasting, structural time series and the Kalman filter</a> by Andrew C. Harvey</p></li>
<li><p><a href="">Dynamic Linear Models with R</a> by Giovanni Petris Sonia Petrone Patrizia Campagnoli</p></li>
<li><p><a href="https://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198523543.001.0001/acprof-9780198523543">Time Series Analysis by State Space Methods</a> by J. Durbin and S.J. Koopman</p></li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-broemeling2011bayesian" class="csl-entry" role="listitem">
Broemeling, Lyle D. 2019. <em>Bayesian Analysis of Time Series</em>. CRC Press.
</div>
<div id="ref-cowpertwait2009introductory" class="csl-entry" role="listitem">
Cowpertwait, P. S. P., and A. V. Metcalfe. 2009. <em>Introductory Time Series with r</em>. Use r! Springer New York. <a href="https://books.google.co.il/books?id=QFiZGQmvRUQC">https://books.google.co.il/books?id=QFiZGQmvRUQC</a>.
</div>
<div id="ref-cressie2011statistics" class="csl-entry" role="listitem">
Cressie, N., and C. K. Wikle. 2011. <em>Statistics for Spatio-Temporal Data</em>. CourseSmart Series. Wiley. <a href="https://books.google.co.il/books?id=-kOC6D0DiNYC">https://books.google.co.il/books?id=-kOC6D0DiNYC</a>.
</div>
<div id="ref-Durbin1960Fitting" class="csl-entry" role="listitem">
Durbin, J. 1960. <span>“The Fitting of Time-Series Models.”</span> <em>Revue de l’Institut International de Statistique / Review of the International Statistical Institute</em> 28 (3): 233–44. <a href="http://www.jstor.org/stable/1401322">http://www.jstor.org/stable/1401322</a>.
</div>
<div id="ref-gelman2013bayesian" class="csl-entry" role="listitem">
Gelman, A., J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. 2013. <em>Bayesian Data Analysis, Third Edition</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis. <a href="https://books.google.co.il/books?id=ZXL6AQAAQBAJ">https://books.google.co.il/books?id=ZXL6AQAAQBAJ</a>.
</div>
<div id="ref-Levinson1947Wiener" class="csl-entry" role="listitem">
Levinson, Norman. 1946. <span>“The Wiener (Root Mean Square) Error Criterion in Filter Design and Prediction.”</span> <em>Journal of Mathematics and Physics</em> 25 (1-4): 261–78. https://doi.org/<a href="https://doi.org/10.1002/sapm1946251261">https://doi.org/10.1002/sapm1946251261</a>.
</div>
<div id="ref-BMCP2021" class="csl-entry" role="listitem">
Martin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. <em><span class="nocase">Bayesian Modeling and Computation in Python</span></em>. <span>Boca Raton</span>.
</div>
<div id="ref-nielsen2019practical" class="csl-entry" role="listitem">
Nielsen, A. 2019. <em>Practical Time Series Analysis: Prediction with Statistics and Machine Learning</em>. O’Reilly Media. <a href="https://books.google.co.il/books?id=xNOwDwAAQBAJ">https://books.google.co.il/books?id=xNOwDwAAQBAJ</a>.
</div>
<div id="ref-pfaff2008analysis" class="csl-entry" role="listitem">
Pfaff, B. 2008. <em>Analysis of Integrated and Cointegrated Time Series with r</em>. Use r! Springer New York. <a href="https://books.google.co.il/books?id=ca5MkRbF3fYC">https://books.google.co.il/books?id=ca5MkRbF3fYC</a>.
</div>
<div id="ref-prado2023time" class="csl-entry" role="listitem">
Prado, R., M. A. R. Ferreira, and M. West. 2023. <em>Time Series: Modeling, Computation, and Inference</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press. <a href="https://books.google.co.il/books?id=pZ6lzgEACAAJ">https://books.google.co.il/books?id=pZ6lzgEACAAJ</a>.
</div>
<div id="ref-ravishanker2022dynamic" class="csl-entry" role="listitem">
Ravishanker, N., B. Raman, and R. Soyer. 2022. <em>Dynamic Time Series Models Using r-INLA: An Applied Perspective</em>. CRC Press. <a href="https://books.google.co.il/books?id=e6h6EAAAQBAJ">https://books.google.co.il/books?id=e6h6EAAAQBAJ</a>.
</div>
<div id="ref-rios2012bayesian" class="csl-entry" role="listitem">
Rios Insua, David, Fabrizio Ruggeri, and Michael P Wiper. 2012. <em>Bayesian Analysis of Stochastic Process Models</em>. John Wiley &amp; Sons.
</div>
<div id="ref-von2002statistical" class="csl-entry" role="listitem">
Storch, H. von, and F. W. Zwiers. 2002. <em>Statistical Analysis in Climate Research</em>. Cambridge University Press. <a href="https://books.google.co.il/books?id=bs8hAwAAQBAJ">https://books.google.co.il/books?id=bs8hAwAAQBAJ</a>.
</div>
<div id="ref-theodoridis2015ML" class="csl-entry" role="listitem">
Theodoridis, S. 2015. <em>Machine Learning: A Bayesian and Optimization Perspective</em>. Elsevier Science. <a href="https://books.google.co.il/books?id=hxQRogEACAAJ">https://books.google.co.il/books?id=hxQRogEACAAJ</a>.
</div>
<div id="ref-Trench1964ToeplitzMI" class="csl-entry" role="listitem">
Trench, William F. 1964. <span>“An Algorithm for the Inversion of Finite Toeplitz Matrices.”</span> <em>Journal of the Society for Industrial and Applied Mathematics</em> 12 (3): 515–22. <a href="http://ramanujan.math.trinity.edu/wtrench/research/papers/TRENCH_RP_6.PDF">http://ramanujan.math.trinity.edu/wtrench/research/papers/TRENCH_RP_6.PDF</a>.
</div>
<div id="ref-walker1931periodicity" class="csl-entry" role="listitem">
Walker, Gilbert Thomas. 1931. <span>“On Periodicity in Series of Related Terms.”</span> <em>Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character</em> 131 (818): 518–32. <a href="https://doi.org/10.1098/rspa.1931.0069">https://doi.org/10.1098/rspa.1931.0069</a>.
</div>
<div id="ref-west2013bayesian" class="csl-entry" role="listitem">
West, M., and J. Harrison. 2013. <em>Bayesian Forecasting and Dynamic Models</em>. Springer Series in Statistics. Springer New York. <a href="https://books.google.co.il/books?id=NmfaBwAAQBAJ">https://books.google.co.il/books?id=NmfaBwAAQBAJ</a>.
</div>
<div id="ref-enwiki-YuleWalkerEquations" class="csl-entry" role="listitem">
Wikipedia contributors. 2024a. <span>“Autoregressive Model — <span>Wikipedia</span><span>,</span> the Free Encyclopedia.”</span> <a href="https://en.wikipedia.org/w/index.php?title=Autoregressive_model&amp;oldid=1233171855#Estimation_of_AR_parameters" class="uri">https://en.wikipedia.org/w/index.php?title=Autoregressive_model&amp;oldid=1233171855#Estimation_of_AR_parameters</a>.
</div>
<div id="ref-enwiki-LevinsonRecursion" class="csl-entry" role="listitem">
———. 2024b. <span>“Levinson Recursion — <span>Wikipedia</span><span>,</span> the Free Encyclopedia.”</span> <a href="https://en.wikipedia.org/w/index.php?title=Levinson_recursion&amp;oldid=1229942891" class="uri">https://en.wikipedia.org/w/index.php?title=Levinson_recursion&amp;oldid=1229942891</a>.
</div>
<div id="ref-yule1927periodicities" class="csl-entry" role="listitem">
Yule, George Udny. 1927. <span>“VII. On a Method of Investigating Periodicities Disturbed Series, with Special Reference to Wolfer’s Sunspot Numbers.”</span> <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em> 226 (636-646): 267–98. <a href="https://doi.org/10.1098/rsta.1927.0007">https://doi.org/10.1098/rsta.1927.0007</a>.
</div>
<div id="ref-Zohar1969ToeplitzMI" class="csl-entry" role="listitem">
Zohar, Shalhav. 1969. <span>“Toeplitz Matrix Inversion: The Algorithm of w. F. Trench.”</span> <em>J. ACM</em> 16: 592–601. <a href="https://api.semanticscholar.org/CorpusID:3115290">https://api.semanticscholar.org/CorpusID:3115290</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C3-L09-Ex4.html" class="pagination-link" aria-label="M5HW4 - BIC for zero-inflated mixtures">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">M5HW4 - BIC for zero-inflated mixtures</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C4-L01.html" class="pagination-link" aria-label="Introductions to Time Series analysis &amp; the AR(1) process">
        <span class="nav-page-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Introductions to Time Series analysis &amp; the AR(1) process</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2024-10-22</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Week 0: Introductions to time series analysis and the AR(1) process"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> Time Series Analysis</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "The AR(1) process, Stationarity, ACF, PACF, Differencing, and Smoothing"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> </span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - Coursera </span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - notes</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - Bayesian Statistics</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - Autoregressive Models</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - Time Series</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span><span class="co"> </span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">  - time series</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">  - notation</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">  - bibliography   </span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">  - R code</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Oren Bochman</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> course-banner.png</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="an">fig-caption:</span><span class="co"> Notes about ... Bayesian Statistics</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="an">title-block-banner:</span><span class="co"> images/banner_deep.jpg</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">#bibliography: references.bib</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">    html: </span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">        code-fold: true</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">        css: styles.css</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>I decided to migrate some material that is auxiliary to the course:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>An overview of the course.</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A review of some mathematical and statistical results used in the course.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A bibliography of books I found useful in the course.</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A Feynman notebook for the course - is now in a separate notebook.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="fu">## Course Card</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>**Course:** <span class="co">[</span><span class="ot">Bayesian Statistics: Time Series</span><span class="co">](https://www.coursera.org/learn/bayesian-statistics-time-series)</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>**Offered by:** University of California, Santa Cruz</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>**Instructor:** Raquel Prado</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>**Certificate:** Yes</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>**Level:** Graduate</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">-  </span>**Commitment:** 4 weeks of study, 3-4 hours/week</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overview of the course</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>This course seems very similar to classic basic time series course without the Bayesian part. (AR, MA, ARMA, ARIMA, SARIMA, DLM etc.)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>One of the questions I had when I started this course was what is the difference between a Bayesian approach to time series analysis and a classical approach. The following is a summary of what I found:</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="fu">### Are we Being Bayesian ?</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>The Bayesian approach presents primarily in:</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sections on Bayesian inference  where we do inference on the parameters of the models.</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Bayesian prediction unlike an MLE prediction is a distribution of predictions not just a point estimate, and therefore is useful for quantifying uncertainty.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We also cover some material on model selection - this again is where the Bayesian approach to optimization presents more powerful tools than the classical approach.</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>When we want to quantify the uncertainty in our model we have four sources of uncertainty:</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="ss">   1. </span>Uncertainty due to using the correct model (structure).</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="ss">      - </span>I consider this is an epistemic uncertainty - </span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="ss">      - </span>One could reduce it by collecting more data, then applying the Bayesian model selection to choose the best model.</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="ss">   1. </span>Uncertainty due to the estimation of the model parameters. This is an epistemic uncertainty - we can reduce it by collecting more data reducing the plausible intervals for these parameters under the bayesian approach.</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="ss">   1. </span>Uncertainty due to random shocks $\epsilon_t$. for the period being predicted. This is an aleatory uncertainty.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="ss">   1.  </span>Uncertainty in the forecasted values $X_{t+h}$</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>   Items 2-3 can be quantified using a plausible interval in the Bayesian approach and as we predict further into the future the interval will grow. </span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Model selection is a big part of the Bayesian approach. We can use the DIC, WAIC, and LOO to compare models.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The book by Professor Prado is very comprehensive and covers plenty of additional models and references lots of recent research. These including VAR, VARMA models, Kalman filters, SMC/Particle filters, etc. These are useful for the *continuous control* flavours of RL. But you will need to learn it on your own. </span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the capstone project that is the next course in the specialization the teacher adds another layer of sophistication by introducing mixtures of TS models.</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>However unlike some courses I took we dive deep enough and get sufficient examples to understand how to put all the bits together into more sophisticated time series models. </span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mathematical Review </span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There is a issues with mathematics most of the results and techniques are so rarely useful that students will soon forget most but a few very useful results. Having a good memory is a great asset in mathematics but is rarely enough. I like to review some mathematical results from my undergraduate days every five years or so. This helps me keep many of the results fresh in my mind and also makes reading new mathematics easier. Fundamentals in mathematics can fo a very long way.</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>This is material from topology, determinants and solving linear equations, numerical methods for decomposing matrices, and so on. Definitions of certain groups. </span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>One reason this and other Bayesian courses and books can be challenging and even overwhelming is that they can use lots of mathematics. This can range from high school material like complex numbers and quadratics formulas to intermediate results like finding root of characteristic polynomials, eigenvalues, Topelitz matrices, jordan forms, and advanced topics like the Durbin-Levinson recursion and certain results from functional analysis  theory. </span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>Note that I have not even touched on probability and statistics in that list. </span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>Rather than complain I see this as an opportunity to review/learn some mathematics and statistics that can be useful to a data scientist. During my last sting in Data science I often was able to write formulas but more often then not felt that I lacked sufficient mathematical tools to manipulate them to get the kind of results I wanted. Rather then learning lots of mathematics I wanted to find the most practical and useful results for wrangling maths. When I was a physics undergraduate these might be trigonometric identities, completing the square, being familiar with many integrals and <span class="co">[</span><span class="ot">Taylor</span><span class="co">](https://en.wikipedia.org/wiki/Taylor_series)</span> or Maclaurin series approximations and a few useful inequalities occasionally we use l'Hopital's rule. Familiarity with some ODEs was also greatly beneficial as these come up in many physical models. Later on hermitian and unitary matrices, fourier expansions, spectral theory, and some results from functional analysis were useful.</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>For statistics we have the variants of the law of large numbers and the central limit theorem, convergence theorems, manipulations of the normal distribution, linear properties of expectation can get you along way. But you have to remember lots of definitions and there are lots of results and theorems that seem to be stepping stones to other results rather than any practical use.</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>On the other hand conjugacy of certain distributions as demonstrated by Herbert Lee and other instructors in this specialization are often very challenging. Charts of Convergence of distributions to other distributions under certain conditions are neat but. There is <span class="co">[</span><span class="ot">Hoeffding's inequality</span><span class="co">](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality)</span> and the <span class="co">[</span><span class="ot">Markov's inequality</span><span class="co">](https://en.wikipedia.org/wiki/Markov%27s_inequality)</span> which can be useful but like most results in mathematics I never had a where they might be used. Then there are certain results - convergence of Markov chains, doubly stochastic matrices. De Finetti's theorem in statistics.</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>I have found that the more I learn the more I can understand and appreciate the material.</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The autoregressive process gives rise to Toeplitz matrices which can be solved using the Durbin-Levinson recursion mentioned many times in the course.</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Durbin-Levinson recursion - is an advanced topic not covered in Numerical Analysis courses or Algebra courses I took.</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>To use it with time series we also need to understand the Yule-Walker equations.</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>ar(p) require some linear algebra concepts like eigenvalues and Eigenvectors, and characteristic polynomials. </span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The AR(p) the Wold decomposition theorem to get to the infinite order moving average representation and this is not a result I recall learning in my functional analysis course. </span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>  We also use some complex numbers and Fourier analysis and spectral density functions.</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>Summarize some of the extra curricular material I found useful in the course.</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Complex numbers</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> Eigenvalues, Eigenvectors and characteristic polynomials</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Durbin-Levinson recursion</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Yule-Walker equations</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> Wiener process (Random walk)</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> Brownian motion (Continuous Random walk with drift)</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> Markov Chains ()</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> Martingales ()</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> Stopping theorem</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Kalman filter</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> Wold's theorem</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> De Finetti's theorem</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[ ]</span> Cholesky decomposition</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="fu">### Complex Numbers (Review)</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>When we wish to find the roots of real valued polynomials we will often encounter complex numbers. In this course such polynomials arise naturally in the characteristic polynomials of AR(p) processes. </span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>We will need the polar form of complex numbers to represent some variants of AR(p) process.</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>The numbers in the Complex field $z \in \mathbb{C}$ numbers are numbers that can be expressed in the form $z = a + bi$, where $a,b\in\mathbb{R}$ and $i$ is the imaginary unit. The imaginary unit $i$ is defined as the square root of -1. Complex numbers can be added, subtracted, multiplied, and divided just like real numbers. </span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>The complex conjugate <span class="co">[</span><span class="ot">complex conjugate</span><span class="co">]</span>{.column-margin} of a complex number $z = a + bi$ is denoted by $\bar{z} = a - bi$. The magnitude of a complex number $z = a + bi$ is denoted by $|z| = \sqrt{a^2 + b^2}$. This is sometimes called the modulus of the complex number in this course. The argument of a complex number $z = a + bi$ is denoted by $\text{arg}(z) = \tan^{-1}(b/a)$. The polar form of a complex number is given by $z = r e^{i \theta}$, where $r = |z|$ and $\theta = \text{arg}(z)$.</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>The polar form of a complex number is given by:</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>z &amp;= \mid z\mid e^{i \theta} <span class="sc">\\</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>  &amp;= r (\cos(\theta) + i \sin(\theta))</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>$$ {#eq-polar-form}</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$|z|$ is the magnitude of the complex number, i.e. the distance from the origin to the point in the complex plane. </span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta$ is the angle of the complex number. </span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>I think we will also need the unit roots. </span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a><span class="fu">### Eigenvalues, Eigenvectors the characteristic polynomials and Unit roots</span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>The Eigenvalues of a matrix are the roots of the characteristic polynomial of the matrix. The characteristic polynomial of a matrix A is defined as:</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>\text{det}(A - \lambda I) = 0</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>where $\lambda$ is the Eigenvalue and $I$ is the identity matrix. The eigenvectors of a matrix are the vectors that satisfy the equation:</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>A v = \lambda v</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>where $v$ is the eigenvector and $\lambda$ is the eigenvalue. The eigenvalues and eigenvectors of a matrix are used in many applications in mathematics and physics, including the diagonalization of matrices, the solution of differential equations, and the analysis of dynamical systems.</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Unit Roots</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>A unit root is a root of the characteristic polynomial of an autoregressive model that is equal to 1. The presence of a unit root in an autoregressive model indicates that the model is not stationary. The unit root test is a statistical test that is used to determine whether a time series is stationary or non-stationary. The unit root test is based on the null hypothesis that the time series has a unit root, and the alternative hypothesis that the time series is stationary. The unit root test is used to determine whether a time series is stationary or non-stationary, and is an important tool in time series analysis.</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="fu">### Spectral analysis (1898)</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>The power spectrum of a signal is the squared absolute value of its Fourier transform. </span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>If it is estimated from the discrete Fourier transform it is also called periodogram. </span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>Usually estimated using the a fast Fourier transform (FFT) algorithm.</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="fu">### Yule-Walker Equations (1932)</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a><span class="fu">### Durbin-Levinson recursion (Off-Course Reading)</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>Like me, you might be curious about the Durbin-Levinson recursion mentioned above. This is not covered in the course, and turned out to be an enigma wrapped in a mystery.</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>I present my finding in the note below - much of it is due to <span class="co">[</span><span class="ot">@enwiki-LevinsonRecursion</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@enwiki-YuleWalkerEquations</span><span class="co">]</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@yule1927periodicities</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@walker1931periodicity</span><span class="co">]</span>, Yule and Walker proposed a method for estimating the parameters of an autoregressive model. The method is based on the Yule-Walker equations which are a set of linear equations that can be used to estimate the parameters of an autoregressive model.</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>Due to the autoregressive nature of the model, the equations are take a special form called a Toeplitz matrix. However at the time they probably had to use the numerically unstable Gauss-Jordan elimination to solve these equations which is $O(n^3)$ in time complexity.</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>A decade or two later in <span class="co">[</span><span class="ot">@Levinson1947Wiener</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@Durbin1960Fitting</span><span class="co">]</span> the authors came up for with a weakly stable yet more efficient algorithm for solving these autocorrelated system of equations which requires only $O(n^2)$ in time complexity. Later their work was further refined in <span class="co">[</span><span class="ot">@Trench1964ToeplitzMI</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@Zohar1969ToeplitzMI</span><span class="co">]</span> to just $3\times n^2$ multiplication. A cursory search reveals that Toeplitz matrix inversion is still an area of active research with papers covering parallel algorithms and stability studies. Not surprising as man of the more interesting deep learning models, including LLMs are autoregressive.</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>So the <span class="co">[</span><span class="ot">Durbin-Levinson recursion</span><span class="co">](https://en.wikipedia.org/wiki/Levinson_recursion)</span> is just an elegant bit of linear algebra for solving the <span class="co">[</span><span class="ot">Yule-Walker equations</span><span class="co">](https://w.wiki/9gVB#Estimation_of_AR_parameters)</span> more efficiently.</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>Here is what I dug up:</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="fu">### Durbin-Levinson and the Yule-Walker equations (Off-Course Reading)</span></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>The Durbin-Levinson recursion is a method in linear algebra for computing the solution to an equation involving a *Toeplitz matrix* AKA a *diagonal-constant matrix* where descending diagonals are constant. The recursion runs in $O(n^2)$ time rather then $O(n^3)$ time required by Gauss-Jordan elimination.</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>The recursion can be used to compute the coefficients of the autoregressive model of a stationary time series. It is based on the <span class="co">[</span><span class="ot">Yule-Walker equations</span><span class="co">](https://w.wiki/9gVB#Estimation_of_AR_parameters)</span> and is used to compute the PACF of a time series.</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>The Yule-Walker equations can be stated as follows for an AR(p) process:</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>\gamma_m = \sum_{k=1}^p \phi_k \gamma_{m-k} + \sigma_\epsilon^2\delta_{m,0} \qquad \text{(Yule-Walker equations)}</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>$$ {#eq-yule-walker}</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\gamma_m$ is the autocovariance function of the time series,</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\phi_k$ are the AR coefficients,</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\sigma_\epsilon^2$ is the variance of the white noise process, and</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\delta_{m,0}$ is the Kronecker delta function.</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>when $m=0$ the equation simplifies to:</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>\gamma_0 = \sum_{k=1}^p \phi_k \gamma_{-k} + \sigma_\epsilon^2 \qquad \text{(Yule-Walker equations for m=0)}</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>$$ {#eq-yule-walker-m-0}</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>for $m &gt; 0$ the equation simplifies to:</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>$$ \begin{bmatrix}</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>    \gamma_1 \newline</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>    \gamma_2 \newline</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>    \gamma_3 \newline</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>    \vdots \newline</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>    \gamma_p \newline</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a> \end{bmatrix} =  \begin{bmatrix}</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>    \gamma_0     &amp; \gamma_{-1}  &amp; \gamma_{-2}  &amp; \cdots \newline</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>    \gamma_1     &amp; \gamma_0     &amp; \gamma_{-1}  &amp; \cdots \newline</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>    \gamma_2     &amp; \gamma_1     &amp; \gamma_0     &amp; \cdots \newline</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>    \vdots       &amp; \vdots       &amp; \vdots       &amp; \ddots \newline</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>    \gamma_{p-1} &amp; \gamma_{p-2} &amp; \gamma_{p-3} &amp; \cdots \newline</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a> \end{bmatrix}  \begin{bmatrix}</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>    \phi_{1} \newline</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>    \phi_{2} \newline</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>    \phi_{3} \newline</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>    \vdots \newline</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>    \phi_{p} \newline</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a> \end{bmatrix}</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>and since this matrix is Toeplitz, we can use Durbin-Levinson recursion to efficiently solve the system for $\phi_k \forall k$.</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>Once $<span class="sc">\{</span>\phi_m ; m=1,2, \dots ,p <span class="sc">\}</span>$ are known, we can consider m=0 and solved for $\sigma_\epsilon^2$ by substituting the $\phi_k$ into @eq-yule-walker-m-0 Yule-Walker equations.</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>Of course the Durbin-Levinson recursion is not the last word on solving this system of equations. There are today numerous improvements which are both faster and more numerically stable.</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>The Yule-Walker equations are a set of p linear equations in the p unknowns $\phi_1, \phi_2, \ldots, \phi_p$ that can be used to estimate the parameters of an autoregressive model of order p. The Yule-Walker equations are derived by setting the sample autocorrelation function equal to the theoretical autocorrelation function of an AR(p) model and then solving for the unknown parameters. The Yule-Walker equations are given by:</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>\gamma(0) &amp; = \phi_1 \gamma(1) + \phi_2 \gamma(2) + \ldots + \phi_p \gamma(p) <span class="sc">\\</span></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>\gamma(1) &amp; = \phi_1 \gamma(0) + \phi_2 \gamma(1) + \ldots + \phi_p \gamma(p-1) <span class="sc">\\</span></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>\gamma(2) &amp; = \phi_1 \gamma(1) + \phi_2 \gamma(0) + \ldots + \phi_p \gamma(p-2) <span class="sc">\\</span></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>\gamma(p) &amp; = \phi_1 \gamma(p-1) + \phi_2 \gamma(p-2) + \ldots + \phi_p \gamma(0) <span class="sc">\\</span></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>where $\gamma(k)$ is the sample autocorrelation function at lag k. The Yule-Walker equations can be solved using matrix algebra to obtain the estimates of the AR parameters $\phi_1, \phi_2, \ldots, \phi_p$.</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a><span class="fu">### Wold's theorem - (extra curricular) circa 1939</span></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>In the 1920 Yule and <span class="co">[</span><span class="ot">Eugen Slutsky</span><span class="co">]()</span> were researching time series and they came up with two different ways to represent a time series.</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Yule’s researches led to the notion of the autoregressive scheme.</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>Y_{t} &amp; = \sum _{j=1}^{p} \phi _{j} Y_{t-j} + u_{t}</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>$$ {#eq-yule-autoregressive-scheme}</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Slutsky’s researches led to the notion of a moving average scheme.</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>Y_{t} &amp; =\sum _{j=0}^{q} \theta _{j} u_{t-j}</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>$$ {#eq-slutsky-moving-average-scheme}</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>we can use the two schemes together and get the ARMA(p,q) model:</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>Y_{t} &amp; = \sum _{j=1}^{p} \phi _{j} Y_{t-j} + u_{t} + \sum _{j=0}^{q} \theta _{j} u_{t-j}</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>$$ {#eq-arma-scheme}</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>The following is extracted from: the wikipedia at https://en.wikipedia.org/wiki/Wold%27s_theorem</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>Wold's decomposition AKA called the Wold representation theorem states that:</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Every covariance-stationary time series $Y_{t}$ can be written as the sum of two time series, one deterministic and one stochastic.</span></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>Formally:</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>Y_{t} &amp; =\sum _{j=0}^{\infty }  \underbrace{b_{j}\epsilon _{t-j}}_{\text{stochastic}} + \underbrace{\eta _{t}}_{\text{deterministic}} <span class="sc">\\</span></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>&amp;= \sum _{j=0}^{\infty } b_{j}\epsilon _{t-j} + \phi_{j} y_{t-j} </span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${Y_{t}}$ is the time series being considered,</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\epsilon _{t}}$ is an white noise sequence called **innovation process** that acts as an input to the linear filter ${\{b_{j}<span class="sc">\}</span>}$.</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${b}$ is the possibly infinite vector of moving average weights (coefficients or parameters)</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\eta _{t}}$ is a "deterministic" time series, in the sense that it is completely determined as a linear combination of its past values It may include "deterministic terms" like sine/cosine waves of ${t}$, but it is a stochastic process and it is also covariance-stationary, it cannot be an arbitrary deterministic process that violates stationarity.</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>The moving average coefficients have these properties:</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Stable, that is, square summable $\sum _{j=1}^{\infty } \mid b_{j}|^{2} &lt; \infty$</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Causal (i.e. there are no terms with j &lt; 0)</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Minimum delay</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Constant ($b_j$ independent of t)</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>It is conventional to define $b_0=1$</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>Any stationary process has this seemingly special representation. Not only is the existence of such a simple linear and exact representation remarkable, but even more so is the special nature of the moving average model. </span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>This result is used without stating its name in the course when we are show the AR(p) representation in terms of moving averages.</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a><span class="fu">## Kalman Filter (1960)</span></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>x_{t} &amp; = F_{t} x_{t-1} + G_{t} u_{t} + w_{t} &amp;&amp; \text{(transition equation)} <span class="sc">\\</span></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>y_{t} &amp; = H_{t} x_{t} + v_{t} &amp;&amp; \text{(observation equation)}</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>$$ {#eq-kalman-filter}</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$x_{t}$ is the state vector at time t,</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$F_{t}$ is the state transition matrix,</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$G_{t}$ is the control input matrix,</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$u_{t}$ is the control vector,</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$w_{t}$ is the process noise vector,</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y_{t}$ is the observation vector at time t,</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$H_{t}$ is the observation matrix,</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$v_{t}$ is the observation noise vector.</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>The Kalman filter is a recursive algorithm that estimates the state of a linear dynamic system from a series of noisy observations. The Kalman filter is based on a linear dynamical system model that is defined by two equations: the state transition equation and the observation equation. The state transition equation describes how the state of the system evolves over time, while the observation equation describes how the observations are generated from the state of the system. The Kalman filter uses these two equations to estimate the state of the system at each time step, based on the observations received up to that time step.</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>This could be implemented  in real time in the 1960s and was used in the Apollo missions.</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>The Extended Kalman Filter (EKF) is an extension of the Kalman filter that can be used to estimate the state of a nonlinear dynamic system. The EKF linearizes the nonlinear system model at each time step and then applies the Kalman filter to the linearized system. The EKF is an approximation to the true nonlinear system, and its accuracy depends on how well the linearized system approximates the true system.</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a><span class="fu">### Box Jenkins Method (1970)</span></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>see <span class="co">[</span><span class="ot">Box Jenkins Method</span><span class="co">](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method)</span></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a>A five step process for identifying, selecting and assessing ARMA (and similar) models.</span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There are three courses on Stochastic Processes on MIT OCW that I found useful:</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">Introduction to Stochastic Processes</span><span class="co">](https://ocw.mit.edu/courses/18-445-introduction-to-stochastic-processes-spring-2015/)</span></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">Discrete Stochastic Processes</span><span class="co">](https://ocw.mit.edu/courses/6-262-discrete-stochastic-processes-spring-2011/)</span></span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>has lecture videos and notes</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>poisson processes</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">Advanced Stochastic Processes</span><span class="co">](https://ocw.mit.edu/courses/15-070j-advanced-stochastic-processes-fall-2013/)</span></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>martingales</span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>ito calculus</span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian Time Series Bibliography</span></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a>We start with some books from the course, I collected here both the recommended books and some others that I found useful.</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a><span class="fu">### Time Series: Modeling, Computation, and Inference </span></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>c.f. <span class="co">[</span><span class="ot">@prado2023time</span><span class="co">]</span></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a><span class="al">![Time Series: Modeling, Computation, and Inference](images/prado2023time.jpg)</span>{.column-margin width="150px"}</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Title:Time Series: Modeling, Computation, and Inference</span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ISBN:9781032040042, 1032040041</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Page count:452</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Published:September 2023</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Format:Paperback</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publisher:CRC Press</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Authors: Raquel Prado, Marco A. R. Ferreira, Mike West</span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@prado2023time</span><span class="co">]</span> **"Time Series: Modeling, Computation, and Inference"** by course instructor Raquel Prado. This book, now in its second edition is a comprehensive introduction to time series analysis and covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. </span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>While learning this course I found some of the material harder to follow than I expected. The books helped to clarify definitions and so on however the book is  </span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>rather comprehensive and mathematically advanced unlike some other books on statistics. </span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>The teacher frequently point out that many aspects of Times series and are beyond the scope of the course. Yet this book covers much more ground like unequally spaced time series and vector valued time series. </span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a>For example we look at EKG data which the authors have been working on for years. However we look at it in this course in terms of a univariate time series while in reality EKG is usually sampled at 12 sites simultaneously yielding a multi-variate time series.</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a>Once this course is done I will probably want to dive deeper into the subject and try to devote more time to other models in the book.</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">hr</span><span class="dt">&gt;</span></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Forecasting and Dynamic Models </span></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a>c.f. <span class="co">[</span><span class="ot">@west2013bayesian</span><span class="co">]</span></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a><span class="al">![Bayesian Forecasting and Dynamic Models](images/west2013bayesian.png)</span>{.column-margin width="150px"}</span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Title:Bayesian Forecasting and Dynamic Models</span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ISBN:9781475770971, 1475770979</span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Page count:682</span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Published:March 17, 2013</span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Format:Paperback</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publisher:Springer New York</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Author:Mike West, Jeff Harrison</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@west2013bayesian</span><span class="co">]</span> **"Bayesian Forecasting and Dynamic Models"** by Mike West and Jeff Harrison. This book is a classic text on Bayesian statistics and covers a wide range of topics in Bayesian forecasting and dynamic models. The following is the description from the publisher:</span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The use of dynamic models in the forecasting of time series data has a long history, with the development of autoregressive integrated moving average (ARIMA) models and state space models. However, the use of Bayesian methods in the development of dynamic models is a relatively recent development. This book provides a comprehensive introduction to the use of Bayesian methods in the development of dynamic models for forecasting time series data. The book covers a wide range of topics, including the use of dynamic models in the analysis of time series data, the use of Bayesian methods in the development of dynamic models, and the use of dynamic models in the forecasting of time series data. </span></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Audience: The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a><span class="co">In this book we are concerned with Bayesian learning and forecast ing in dynamic environments. We describe the structure and theory of classes of dynamic models, and their uses in Bayesian forecasting. The principles, models and methods of Bayesian forecasting have been developed extensively during the last twenty years. This development has involved thorough investigation of mathematical and statistical aspects of forecasting models and related techniques. With this has come experience with application in a variety of areas in commercial and industrial, scientific and socio-economic fields. In deed much of the technical development has been driven by the needs of forecasting practitioners. As a result, there now exists a relatively complete statistical and mathematical framework, although much of this is either not properly documented or not easily accessible. Our primary goals in writing this book have been to present our view of this approach to modelling and forecasting, and to provide a reasonably complete text for advanced university students and research workers. The text is primarily intended for advanced undergraduate and postgraduate students in statistics and mathematics. In line with this objective we present thorough discussion of mathematical and statistical features of Bayesian analyses of dynamic models, with illustrations, examples and exercises in each Chapter.</span></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">hr</span><span class="dt">&gt;</span></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Time Series Analysis </span></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>c.f. <span class="co">[</span><span class="ot">@nielsen2019practical</span><span class="co">]</span></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a><span class="al">![Practical Times Series Analysis](images/nielsen2019practical.jpg)</span>{.column-margin width="150px"}</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Title:Practical Time Series Analysis: Prediction with Statistics and Machine Learning</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ISBN:1492041602, 9781492041603</span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Page count:504</span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Published:2019</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Format:Paperback</span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publisher:O'Reilly Media, Inc.</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">@nielsen2019practical</span><span class="co">]</span> "Practical Time Series Analysis: Prediction with Statistics and Machine Learning" by Aileen Nielsen. Is a good resource for parctionars getting started with time series analysis. I also recommend any videos by Aileen Nielsen on the subject.</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Practical Times Series Analysis by Aileen Nielsen is a good book for beginners. It is a practical guide to time series analysis and covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for beginners in statistics, computer science, and related fields.</span></span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Time series data analysis is increasingly important due to the massive production of such data through the internet of things, the digitalization of healthcare, and the rise of smart cities. As continuous monitoring and data collection become more common, the need for competent time series analysis with both statistical and machine learning techniques will increase.</span></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Covering innovations in time series data analysis and use cases from the real world, this practical guide will help you solve the most common data engineering and analysis challenges in time series, using both traditional statistical and modern machine learning techniques. Author Aileen Nielsen offers an accessible, well-rounded introduction to time series in both R and Python that will have data scientists, software engineers, and researchers up and running quickly.</span></span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; You’ll get the guidance you need to confidently:</span></span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Find and wrangle time series data</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Undertake exploratory time series data analysis</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Store temporal data</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simulate time series data</span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generate and select features for a time series</span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Measure error</span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Forecast and classify time series with machine or deep learning</span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluate accuracy and performance</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">hr</span><span class="dt">&gt;</span></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a><span class="fu">#### "Machine Learning: A Bayesian and Optimization Perspective" by Sergios Theodoridis. </span></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>c.f. <span class="co">[</span><span class="ot">@theodoridis2015ML</span><span class="co">]</span></span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a><span class="al">![Machine Learning: A Bayesian and Optimization Perspective](images/theodoridis2015ML.png)</span>{.column-margin width="150px"}</span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Title:Machine Learning: A Bayesian and Optimization Perspective</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ISBN:0128015225, 9780128015223</span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Page count:1062</span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Published:2015</span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Format:Hardcover</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publisher:Academic Press</span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Authors: Sergios Theodoridis</span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a> I came across this book while looking into the Durban-Levinson recursion and the Yule-Walker equations. So far I haven't had time to read it but it looks like a good book on machine learning. The following is the description from the publisher:</span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This tutorial text gives a unifying perspective on machine learning by covering both probabilistic and deterministic approaches -which are based on optimization techniques - together with the Bayesian inference approach, whose essence lies in the use of a hierarchy of probabilistic models. The book presents the major machine learning methods as they have been developed in different disciplines, such as statistics, statistical and adaptive signal processing and computer science. Focusing on the physical reasoning behind the mathematics, all the various methods and techniques are explained in depth, supported by examples and problems, giving an invaluable resource to the student and researcher for understanding and applying machine learning concepts.</span></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a><span class="at">The book builds carefully from the basic classical methods to the most recent trends, with chapters written to be as self-contained as possible, making the text suitable for different courses: pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models.</span></span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>All major classical techniques: Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods.</span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The latest trends: Sparsity, convex analysis and optimization, online distributed algorithms, learning in RKH spaces, Bayesian inference, graphical and hidden Markov models, particle filtering, deep learning, dictionary learning and latent variables modeling.</span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Case studies - protein folding prediction, optical character recognition, text authorship identification, fMRI data analysis, change point detection, hyperspectral image unmixing, target localization, channel equalization and echo cancellation, show how the theory can be applied.</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MATLAB code for all the main algorithms are available on an accompanying website, enabling the reader to experiment with the code.</span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">hr</span><span class="dt">&gt;</span></span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Statistical Analysis in Climate Research </span></span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a>c.f.<span class="co">[</span><span class="ot">@von2002statistical</span><span class="co">]</span></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a><span class="al">![Statistical Analysis in Climate Research](images/von2002statistical.jpg)</span>{.column-margin width="150px" }</span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Title:Statistical Analysis in Climate Research</span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ISBN:1139425099, 9781139425094</span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Page count:484</span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Published:2002</span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Format:Paperback</span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publisher:Cambridge University Press</span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Authors: Hans von Storch, Francis W. Zwiers</span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>I came across this book while looking into the Durban-Levinson recursion and the Yule-Walker equations. So far I haven’t had time to read it but it looks promising. Here is the description from the publisher:</span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Climatology is, to a large degree, the study of the statistics of our climate. The powerful tools of mathematical statistics therefore find wide application in climatological research. The purpose of this book is to help the climatologist understand the basic precepts of the statistician's art and to provide some of the background needed to apply statistical methodology correctly and usefully. The book is self contained: introductory material, standard advanced techniques, and the specialised techniques used specifically by climatologists are all contained within this one source. There are a wealth of real-world examples drawn from the climate literature to demonstrate the need, power and pitfalls of statistical analysis in climate research. Suitable for graduate courses on statistics for climatic, atmospheric and oceanic science, this book will also be valuable as a reference source for researchers in climatology, meteorology, atmospheric science, and oceanography.</span></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hans von Storch is Director of the Institute of Hydrophysics of the GKSS Research Centre in Geesthacht, Germany and a Professor at the Meteorological Institute of the University of Hamburg.</span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Francis W. Zwiers is Chief of the Canadian Centre for Climate Modelling and Analysis, Atmospheric Environment Service, Victoria, Canada, and an Adjunct Professor at the Department of Mathematicw and Statistics of the University of Victoria.</span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">hr</span><span class="dt">&gt;</span></span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Modeling and Computation in Python</span></span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a>c.f. <span class="co">[</span><span class="ot">@BMCP2021</span><span class="co">]</span></span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a><span class="al">![Bayesian Modeling and Computation in Python](images/BMCP2021.jpg)</span>{.column-margin width="150px" }</span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a>This is a great resource for translating what we  learned to Python.</span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a> The book is available at <span class="co">[</span><span class="ot">Bayesian Modeling and Computation in Python</span><span class="co">](https://bayesiancomputationbook.com/)</span></span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a> I found the chapter on state space modeling and the Kalman filter particularly useful. The book is a great resource for translating what we learned in the course to Python. The book is suitable for undergraduate students in statistics, computer science, and related fields.</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">hr</span><span class="dt">&gt;</span></span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Data Analysis </span></span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a>c.f. <span class="co">[</span><span class="ot">@gelman2013bayesian</span><span class="co">]</span></span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a><span class="al">![Bayesian Data Analysis](gelman2013bayesian.jpg)</span>{.column-margin width="150px" }</span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Title:Bayesian Data Analysis</span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ISBN:1439840954, 9781439840955</span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Page count:675</span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Published:2013</span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Format:Hardcover</span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Publisher:Chapman and Hall/CRC</span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Authors: Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, Donald B. Rubin</span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@gelman2013bayesian</span><span class="co">]</span> "Bayesian Data Analysis" is probably the most famous book on Bayesian statistics. </span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a>This book is a classic text on Bayesian statistics and covers a wide range of topics in Bayesian data analysis. </span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a>Although this is not a time series book, the authors have been intersted in the domain of political election prediction and have used time series data in their research and some of that is covered in the book's examples.</span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Audience: The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>An electronic version of the third eddition book is available at <span class="co">[</span><span class="ot">Bayesian Data Analysis</span><span class="co">](http://www.stat.columbia.edu/~gelman/book/)</span></span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">hr</span><span class="dt">&gt;</span></span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a><span class="fu">### Introductory Time Series with R c.f. [@cowpertwait2009introductory]</span></span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a><span class="al">![Introductory Time Series with R](images/cowpertwait2009introductory.jpg)</span>{.column-margin width="150px" }</span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@cowpertwait2009introductory</span><span class="co">]</span></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a>"Introductory Time Series with R" by Cowpertwait and Metcalfe, and the second is </span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Yearly global mean temperature and ocean levels, daily share prices, and the signals transmitted back to Earth by the Voyager space craft are all examples of sequential observations over time known as time series. This book gives you a step-by-step introduction to analysing time series using the open source software R. Each time series model is motivated with practical applications, and is defined in mathematical notation. Once the model has been introduced it is used to generate synthetic data, using R code, and these generated data are then used to estimate its parameters. This sequence enhances understanding of both the time series model and the R function used to fit the model to data. Finally, the model is used to analyse observed data taken from a practical application. By using R, the whole procedure can be reproduced by the reader. </span></span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;All the data sets used in the book are available on the website  at </span><span class="co">[</span><span class="ot">datasets</span><span class="co">](http://staff.elena.aut.ac.nz/Paul-Cowpertwait/ts/)</span></span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The book is written for undergraduate students of mathematics, economics, business and finance, geography, engineering and related disciplines, and postgraduate students who may need to analyse time series as part of their taught programme or their research.</span></span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Paul Cowpertwait is an associate professor in mathematical sciences (analytics) at Auckland University of Technology with a substantial research record in both the theory and applications of time series and stochastic models.</span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Andrew Metcalfe is an associate professor in the School of Mathematical Sciences at the University of Adelaide, and an author of six statistics text books and numerous research papers. Both authors have extensive experience of teaching time series to students at all levels.</span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">hr</span><span class="dt">&gt;</span></span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a><span class="fu">### Analysis of Integrated and Cointegrated Time Series with R c.f. </span></span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a><span class="al">![Analysis of Integrated and Cointegrated Time Series with R](images/pfaff2008analysis.jpg)</span>{.column-margin width="150px" style="vertical-align: top;"  }</span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@pfaff2008analysis</span><span class="co">]</span> "Analysis of Integrated and Cointegrated Time Series with R" by Bernhard Pfaff. Its been a long time since I read this book and rather than do it an injustice I direct you to  the review by Dirk Eddelbuettel in the Journal of Statistical Software is avaoilable at <span class="co">[</span><span class="ot">review</span><span class="co">](https://dirk.eddelbuettel.com/papers/pfaff_urca.pdf)</span>. Or the book's website at <span class="co">[</span><span class="ot">Analysis of Integrated and Cointegrated Time Series with R</span><span class="co">](https://www.springer.com/gp/book/9780387759661)</span>.</span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The analysis of integrated and co-integrated time series can be considered as the main methodology employed in applied econometrics. This book not only introduces the reader to this topic but enables him to conduct the various unit root tests and co-integration methods on his own by utilizing the free statistical programming environment R. The book encompasses seasonal unit roots, fractional integration, coping with structural breaks, and multivariate time series models. The book is enriched by numerous programming examples to artificial and real data so that it is ideally suited as an accompanying text book to computer lab classes.</span></span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The second edition adds a discussion of vector auto-regressive, structural vector auto-regressive, and structural vector error-correction models.</span></span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Bayesian Analysis of Time Series by Lyle D. Broemeling</span></span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@broemeling2011bayesian</span><span class="co">]</span> </span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>covers pretty much the material in the course.</span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>uses winbugs and R</span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>models considered include</span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>white noise</span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Wiener process (random walk)</span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>AR(p)</span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>ARMA(p,q)</span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>ARIMA</span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Regression</span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Regression with MA and Seasonal effects</span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>DLM</span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>TAR</span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Inference for Stochastic Processes by Lyle D. Broemeling</span></span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a><span class="al">![Bayesian Inference for Stochastic Processes by Lyle D. Broemeling](images/broemeling2018bayesian.jpg)</span>{.column-margin width="150px" style="vertical-align: top;"  }</span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The code for R and WinBUGS is available at <span class="co">[</span><span class="ot">code</span><span class="co">](http://www.lbroemeling.com/)</span></span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>IT is based on WinBUGS which is a bit dated but still useful.</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This books seems a bit dated but it covers a lot of the material in the course.</span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dynamic Time Series Models using R-INLA: An Applied Perspective</span></span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@ravishanker2022dynamic</span><span class="co">]</span> is a new book that covers the use of the R-INLA package for fitting dynamic time series models. The book is available online <span class="co">[</span><span class="ot">gitbook</span><span class="co">](https://ramanbala.github.io/dynamic-time-series-models-R-INLA/)</span></span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a> <span class="al">![Dynamic Time Series Models using R-INLA: An Applied Perspective](ravishanker2022dynamic.jpg)</span>{.column-margin width="150px" style="vertical-align: top;"  }</span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a>This is a very interesting book which covers a new approach to fitting time series models using the R-INLA package. INLA stands for Integrated Nested Laplace Approximation and is a method for fitting Bayesian models that is faster than MCMC. The book covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. </span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistics for Spatio-Temporal Data</span></span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a> <span class="al">![Statistics for Spatio-Temporal Data](images/cressie2011statistics.jpg)</span>{.column-margin width="150px" style="vertical-align: top;"  }</span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@cressie2011statistics</span><span class="co">]</span> is a book I came across when I tried to understand the NDLM model. NLDMs have a two level hierarcial form and it seems possible to extend this formulation will non-normaly distributed shocks and possibly non linear relation. In this book the authors take an interesting approch of not only looking at NDLM as a heirarchical model but they also extend the time series model into a spatio-temporal model. </span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a>This book is a comprehensive introduction to the analysis of spatio-temporal data and covers a wide range of topics in spatio-temporal statistics. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian Analysis of Stochastic Process Models </span></span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a>c.f. <span class="co">[</span><span class="ot">@rios2012bayesian</span><span class="co">]</span></span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a><span class="al">![Bayesian Analysis of Stochastic Process Models](images/rios2012bayesian.jpg)</span>{.column-margin width="150px" style="vertical-align: top;"  }</span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>David Rios Insua, Fabrizio Ruggeri, Michael P. Wiper</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a>This book is a comprehensive introduction to the analysis of stochastic process models using Bayesian methods. The book covers a wide range of topics in stochastic process modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.</span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a>There are also a number of books on NDLM that I've come accross:</span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Dynamic linear model tutorial</span><span class="co">](https://mjlaine.github.io/dlm/dlmtut.html#org04d2ab8)</span> matlab</span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Forecasting, structural time series and the Kalman filter</span><span class="co">](https://www.cambridge.org/core/books/forecasting-structural-time-series-models-and-the-kalman-filter/CE5E112570A56960601760E786A5E631)</span>  by Andrew C. Harvey</span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Dynamic Linear Models with R</span><span class="co">]()</span> by Giovanni Petris Sonia Petrone Patrizia Campagnoli</span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Time Series Analysis by State Space Methods</span><span class="co">](https://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198523543.001.0001/acprof-9780198523543)</span> by J. Durbin and S.J. Koopman</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>