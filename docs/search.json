[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "Preface\nThese are my notes on Bayesian Statistics. They are based on online courses I took after I worked as a Data Scientist for a number of years. Over this time I have often felt that if i was better at statistics I could overcome limitations in the data with mathematics, build even better models, and perhaps most importantly make better inferences using model I have already developed.\nI tried to review the main result in statistic and probability theory, and I started to get better results and so I decided to take some classes. I often felt that the material was introductory and simplistic. Real world problems are monsters and the examples are usually trivial in comparison. The issues raised in class are rarely the troubles I saw at work. But I do believe deep down that the two are somehow related. And indeed as I covered more material certain aspects began to connect.\nI noticed that like in real classes the teachers made mistakes, their motivation was not always clear and that they skipped steps or reffered to certain results. On the other hand each teacher offered different insights into this complicated area of data analysis.\nI therefore have a number main areas of Focus:\n\nWhat are the questions one should ask\nWhat are the explicit details of each examples or problem.\nWhat is the mathematical representation of the model for these\nWhat is the code representation for this in R and in Python.\nCan I find or create diagrams to make interpretation of probabilities clearer.\nCan I annotate the main equations to break them down.\nCan I keep track of the most useful Mathematical results in Probability and Statistics?\n\nI tried to keep these handy as appendices to keep the main material less cluttered. However as time goes by these keep expanding.\n\nCan I learn to communicate my intuitions of these concepts to laymen and to colleages\n\nSome courses have discussion prompts, but I tried to make the most of these, and included them in these notes.\nI realized that while I often get excited about a new paper or technique my colleges are smart and talented people and quickly ask questions that are difficult to answer. These question often indicate gaps of knowledge which can dampen the enthusiasm for implementing these new ideas.\nI found that good communicators can overcome these issues more readily and connect what they already know.\n\n\nI have often found exercises rather easy to solve and so I often breezed through them with little notice. This time I resolved to make the most of these as opportunities to get better at using and manipulating probabilities and posterior distributions. So although I could get around 75% in each exercises based on intuition I took the extra effort to understand what is realty going on here mathematically.\nAt work one of the main challenges is making the problem conform to a simple model. This can be even more challenging when the goal is a latent (unobserved) variable or when you are considering a synergy of multiple effects and you have seen and unseen confounds. In many cases it is unclear how to proceed based on the simple examples we see in these classes. However I am now able to look at the problems with more critical point of view. Also I see great advantages of a quick expositions to many new simple models. In Bayesian hierarchical framework each can become a link to adding just a little more complexity, integrating new types of prior information and so on. So I view these as jumping boards for overcoming my next challenges.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction to Bayesian Statistics",
    "section": "",
    "text": "2 Introduction\nWhat is covered is:\nWith this goal in mind, the content is divided into the following three main sections (courses).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#why-inference",
    "href": "intro.html#why-inference",
    "title": "1  Introduction to Bayesian Statistics",
    "section": "2.1 Why Inference?",
    "text": "2.1 Why Inference?\nThe purpose of the set of courses is to focus on Inferential Statistics as opposed to Descriptive Statistics.\nAll the samples in the group that we are interested in learning about make up a population. Populations can be described by parameters such as the mean and variance since they represent all of the data. Often, we do not have access to all the data in our population and have to sample from the population. The metrics of mean and variance computed from these samples are not called parameters but statistics of the data.\n\n2.1.1 Descriptive Statistics\nThis is used to summarize the data so that we have a quantitative way to understand data. This allows to understand and visualize data qualitatively. We can draw conclusions about the nature of the data. Descriptive statistics is applied to a population and hence can provide measures such as the mean and variance of the data. They do not allow us to make predictions about data that we have not analyzed.\n\n\n2.1.2 Inferential Statistics\nInferential Statistics allow us to make generalizations about the population from the samples. This process of sampling introduces errors as this is never a perfect representation of the underlying data. The statistics thus computed are supposed to be an estimate of the true population parameters. It allows you to form a distribution of the population from the sampled data by accounting for the errors in the sampling, thereby allowing you to make predictions about data that is not yet seen or sampled.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#how-is-inference-different-from-prediction",
    "href": "intro.html#how-is-inference-different-from-prediction",
    "title": "1  Introduction to Bayesian Statistics",
    "section": "2.2 How is Inference different from Prediction?",
    "text": "2.2 How is Inference different from Prediction?\nReference\n\n2.2.1 Prediction\nIf you happen to come from a background in Machine Learning, you are probably used to making predictions. This is exactly what it sounds like, you use a model to make predictions on unseen data. The predictive process involves the following steps\n\nCreate the model\nSelect the best model using performance metrics such as accuracy, F1 scores on out-of-sample data\nMake predictions on new data\n\n\n\n2.2.2 Inference\nIn Inference, you are trying to model a distribution and understand the process that generates the data. This involves the following steps\n\nCreate the model, usually involves some prior understanding of the data generation process\nSelect the model using goodness-of-fit measures such as such as residual analysis, deviance, AIC scores etc.\nPerform inference by generating distributions that describe the data, or the data generation process",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "1  Introduction to Bayesian Statistics",
    "section": "2.3 References",
    "text": "2.3 References\n\n(Casella and Berger 2002) e-book solutions\n(Spanos 2019)\n(Hobbs and Hooten 2015) ebook website\n(VanderPlas 2016) ebook notebooks\n(Bishop 2006) ebook website\n\n\n\n\n\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer (India) Private Limited. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nCasella, G., and R. L. Berger. 2002. Statistical Inference. Duxbury Advanced Series in Statistics and Decision Sciences. Thomson Learning. http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&Roger%20L.Berger--Statistical%20Inference.pdf.\n\n\nHobbs, N. Thompson, and Mevin B. Hooten. 2015. Bayesian Models: A Statistical Primer for Ecologists. STU - Student edition. Princeton University Press. http://www.jstor.org/stable/j.ctt1dr36kz.\n\n\nSpanos, A. 2019. Probability Theory and Statistical Inference. Cambridge University Press. https://books.google.co.il/books?id=9nCiDwAAQBAJ.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. 1st ed. O’Reilly Media, Inc. https://jakevdp.github.io/PythonDataScienceHandbook/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "C1-L01.html",
    "href": "C1-L01.html",
    "title": "2  Probability - M1L1",
    "section": "",
    "text": "2.1 Introduction\nIn these notes I supplement the course material with my own notes for establishing an axiomatic foundation for probability. These were ostentatiously omitted from the specialization material, but alluded to in many places as a form of hand waving for introducing results. I found their absence increasingly irksome that I decided to add them to my notes. In reality probability theory is a beautiful part of Mathematics and bring together many results from analysis, topology, functional analysis and integration theory. I hope that adding this material will make your journey easier and not more challenging. In the future I hope to dive a little deeper, as progressing with the specialization has uncovered additional topics in probability theory that might be useful to review. When this happens, I will move most of these extras into thier own appendecies.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-rules-of-probability-odds--expectation",
    "href": "C1-L01.html#sec-rules-of-probability-odds--expectation",
    "title": "2  Probability - M1L1",
    "section": "2.2 Rules of Probability, Odds & Expectation",
    "text": "2.2 Rules of Probability, Odds & Expectation\n\nBackground reading: This reviews the rules of probability, odds, and expectation.\n\nFor all its discussion of different paradigms of probability, the course lacks a rigorous definition of probability.\n\nDefinition 2.1 (Sample Space and Sample Point)  Sample space \\Omega\n\n\\Omega = \\{ \\forall w \\mid w \\text { is an outcome of an experiment} \\} \\ne \\emptyset\n\\tag{2.1}\nthen \\Omega is called a sample space.\nSince\n\n\\Omega \\ne \\emptyset \\implies  \\exists\\ \\omega \\in \\Omega\n\\tag{2.2}\nthen \\omega is called a sample point  Sample point \\omega\n\n\nDefinition 2.2 (Event)  Event A\n\n\\Omega \\ne \\emptyset \\implies  \\exists \\mathcal{F} \\subset 2^\\Omega \\implies \\exists A\\in F\n\\tag{2.3}\nLet \\mathcal{F} denote a family of subsets of a sample space \\Omega, and A any such subset. Then A is called an event\n\n\nDefinition 2.3 (Elementary Event) An event composed of a single point \\omega is called an elementary event.\n\n\nDefinition 2.4 (Outcome) We say that event A happened if when conducting the experiment we got an outcome \\omega and \\omega\\in A.\n\n\nDefinition 2.5 (Certain Event) \\Omega is called the certain event.\n\n\nDefinition 2.6 (Impossible Event) \\emptyset is called the impossible event.\n\n\nDefinition 2.7 (σ-Algebra) A family of events \\mathcal{F} with the following properties:\n\n\\Omega is the universal set\n\n\\Omega \\in \\mathcal{F}\n  \\tag{2.4}\n\\mathcal{F} is closed under complement operation:\n\n\\forall A \\in \\mathcal{F} \\implies A^c \\in \\mathcal{F}\n\\tag{2.5}\n\\mathcal{F} is closed under countable unions:\n\n\\exists A_i \\in \\mathcal{F} \\quad  i \\in \\mathcal{N} \\implies \\bigcup_{n=1}^\\infty {A_i} \\in \\mathcal{F}\n\\tag{2.6}\n\nis called a \\sigma-algebra or a \\sigma-field .\n\nsome properties of \\sigma-algebra\n\nhttps://math.stackexchange.com/questions/1330649/difference-between-topology-and-sigma-algebra-axioms\nAn epsilon of room: pages from year three of a mathematical blog section 2.7\n\n\nDefinition 2.8 (Probability Measure) if \\Omega is a sample space Definition 2.1 and \\mathcal{F} a \\sigma-algebra Definition 2.7 for \\Omega then a function P: \\mathcal{f} \\to [0,1] with the following properties:\n\nTotal measure of the sample space is 1: \n     \\mathbb{P}r(\\Omega)=1\n  \\tag{2.7}\ncountably additive for pairwise disjoint countable collections of events: is called a probability measure over \\mathcal{F}. \n\\forall E_{ i \\in \\mathbb{N} } \\quad   \\mathbb{P}r(\\bigcup_{n\\in \\mathbb{N} }{E_n})=\\sum_{n\\in \\mathbb{N} } \\mathbb{P}r(E_n)\n\\tag{2.8}\nthen P is a probability measure over \\mathcal{F}\n\n\n\nDefinition 2.9 (Probability Space) If \\Omega is a sample space Definition 2.1 and \\mathcal{F} a \\sigma-algebra Definition 2.7 for \\Omega, and P a probability measure (Definition 2.8) for \\mathcal{F} then the ordered set &lt;\\Omega,\\mathcal{F},P &gt; is called a probability space\n\n\n\n\n\n\n\n\nFigure 2.1: Illustration of a probability space by Ziggystar\n\n\n\n\n\n\n\n\nTipNotation\n\n\n\nsometimes \\mathcal{F} is replaced with \\Sigma. for the \\sigma-algebra like in the figure below",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-properties-of-probability-measures",
    "href": "C1-L01.html#sec-properties-of-probability-measures",
    "title": "2  Probability - M1L1",
    "section": "2.3 Properties of Probability Measures",
    "text": "2.3 Properties of Probability Measures\nThe probability of the null event is 0.\n\n\\mathbb{P}r(\\emptyset) = 0\n\\tag{2.9}\nProbabilities of all possible events (the space of all possible outcomes) must sum to one.\n\n\\mathbb{P}r(\\Omega) = 1\n\\tag{2.10}\n\nA\\cap B = \\emptyset \\implies \\mathbb{P}r(A \\cup B) = \\mathbb{P}r(A)+\\mathbb{P}r(B)\n\\tag{2.11}\n\n\n\\mathbb{P}r(A^c) =1-\\mathbb{P}r(A) \\qquad \\forall A\\in\\Omega\n\\tag{2.12}\n\nif A is an event in \\Omega then A^C is in Omega and since they are mutually exclusive by Equation 2.10\nIf S is the certain event in class C \\Omega then\nFor every event X in class \\Omega\n\n1 \\ge \\mathbb{P}r(X) \\ge 0 \\qquad \\forall X \\in \\Omega \\qquad \\text{(P1)} \\qquad\n\\tag{2.13}\nProbabilities add to one:\n\n\\sum_{i\\in \\Omega} \\mathbb{P}r(X=i)=1 \\qquad \\text{(P2)}\\qquad\n\\tag{2.14}\nThe complement of an event A is A^c\n\n\\mathbb{P}r(S) = 1\n\\tag{2.15}\nIf events A_\\lambda are mutually exclusive (only one event may happen):\n\n\\mathbb{P}r(A_1 \\cup A_2) = \\mathbb{P}r(A_1) + \\mathbb{P}r(A_2) - \\mathbb{P}r(A_1\\cap A_1)\n\\tag{2.16}\n\n\\mathbb{P}r(\\bigcup_{\\lambda\\in \\Omega} A_\\lambda)=\\sum_{\\lambda \\in \\Omega} \\mathbb{P}r(A_\\lambda)\n\\tag{2.17}\nif {B_i} is a finite or countably infinite partition of a sample space \\Omega then\n\n\\mathbb{P}r(A) = {\\sum_{i=1}^{N} \\mathbb{P}r(A \\cap B_i)}= {\\sum_{i=1}^{N} \\mathbb{P}r(A|B_i)\\mathbb{P}r(B_i)}\n\\tag{2.18}\n\n2.3.1 Odds\n\nC-3PO: Sir, the possibility of successfully navigating an asteroid field is approximately 3,720 to 1!  Han Solo: Never tell me the odds! — Star Wars Episode V: The Empire Strikes Back\n\nAnother way to think about probabilities is using odds Equation 2.19. Odds are more intuitive when we are thinking about the risk of an event happening or not happening. and when we consider the risk associated with uncertainty odds are a handy way of considering the risks.\n\nDefinition 2.10 (Odds Definitions) the odds of an event A are:\n\n\\mathcal{O}(A)  = \\frac{\\mathbb{P}r(A)}{\\mathbb{P}r(A^c)} = \\frac{ \\mathbb{P}r(A)}{1-\\mathbb{P}r(A)}\n\\tag{2.19}\n\nIt is also possible to convert odds to probabilities Equation 2.20\n\nTheorem 2.1 (Probability from odds) \n\\mathbb{P}r(A) = \\frac{ \\mathcal{O}(A)} {1+ \\mathcal{O}(A)}\n\\tag{2.20}\n\n\nProof. \n\\begin{aligned}\n& & \\mathcal{O}(A)  &= \\frac{\\mathbb{P}r(A)}{1-\\mathbb{P}r(A)} && \\text{(odds definition)} \\\\\n   &\\implies & \\mathbb{P}r(A) &= \\mathcal{O}(A) (1-\\mathbb{P}r(A))  && (\\times \\text{ denominator}) \\\\\n   &\\implies &  \\mathbb{P}r(A) &= \\mathcal{O}(A) - \\mathcal{O}(A) \\mathbb{P}r(A) && \\text{(expand)} \\\\\n   &\\implies &  \\mathbb{P}r(A)(1+ \\mathcal{O}(A)) &= \\mathcal{O}(A) && \\text{(collect)}   \\\\\n   &\\implies & \\mathbb{P}r(A) &= \\frac{ \\mathcal{O}(A)} {1+ \\mathcal{O}(A)} && \\blacksquare  \n\\end{aligned}\n\n\nIf we are at the races and thinking about each horse a horse what we may care about is if it will win or lose. In such a case the odds can summarize the ratio of past successes and failures to win. Odds seem to be in line with a frequentist view summarizing ratios of success to failure. In reality, the other horses have odds as well and we may want to consider the probability of winning given the other horses in the race, and perhaps other parameters, like the track type, length of the race, jockey, and perhaps some hot tips. So let us not get ahead of ourselves\n\n\n\n\n\n\nTipData Scientist - insights.\n\n\n\nMany of these formulas are rather tedious. But, once you start to work on a data science project you will often discover that there are some problems with the data and because of that you cannot use your favorite algorithm. Or worse when you do the results are not very useful. It is at this point that the ability to think back to first principles will be very fruitful. The more of this material you can recall, the more the dots will connect, and your ability will translate into models of increasing sophistication. Luckily, the rules of probability are logical. So it is fairly easy to remember or even derive if you take some time to understand them.\nI realize that figuring out which results are more useful is easier in hindsight. And one of the reasons I am taking these courses is to annotate in my note the results I think to be most useful.\n\n\n\n\n2.3.2 Expectation\nThe expectation of a random variable (RV) X is the weighted average of the outcomes it can take weighted by their probabilities.\n\nDefinition 2.11 (Expectation for a discrete RV) \n\\mathbb{E}(x) = \\sum^N_{i=1} x_i \\times \\mathbb{P}r(X=x_i)\n\\tag{2.21}\n\n\nDefinition 2.12 (Expectation for a continuous RV) \n\\mathbb{E}(x) = \\int_{\\Omega} x \\mathbb{P}r(X=x) dx\n\\tag{2.22}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-probability-paradigms",
    "href": "C1-L01.html#sec-probability-paradigms",
    "title": "2  Probability - M1L1",
    "section": "2.4 Probability Paradigms",
    "text": "2.4 Probability Paradigms\n\n\n\n\n\n\n\nFigure 2.2: Probability Paradigms\n\n\nWe start by looking at probability as defined or interpreted under three paradigms. Probability is at its root a logical and scientific approach to formalizing and modeling uncertainty.\nThe three paradigms are:\n\nDefinition 2.13 (Classical Probability) Deals primarily with cases where probabilities are distributed equally, like with dice and cards.\n\n\n\n\n\n\n\n\nFigure 2.3: Abraham De Moivre\n\n\n\n\n\n\n\n\nTipBiographical note on Abraham de Moivre\n\n\n\n\n\n\nThe Probability of an Event is greater or less, according to the number of chances by which it may happen, compared with the whole number of chances by which it may either happen or fail. — (Moivre 1718)\n\nAbraham de Moivre (1667-1754) was a prominent French mathematician known for his significant contributions to the field of probability and his work on the foundations of Bayesian statistics. His research and writings played a crucial role in establishing the mathematical principles of probability theory and laid the groundwork for future advancements in the field.\nDe Moivre is best known for his work on the theory of probability. He made significant advancements in understanding the Binomial distribution and its application to games of chance and coin tossing. In his influential book, “The Doctrine of Chances” (1718), he presented a comprehensive treatise on probability theory, providing mathematical explanations for various phenomena such as the law of large numbers and the central limit theorem. His book became a standard reference in the field and greatly influenced subsequent research on probability.\nFurthermore, de Moivre’s work laid the foundation for Bayesian statistics, although the term “Bayesian” was not coined until many years after his death. He developed a formula known as de Moivre’s theorem, which establishes a connection between the normal distribution and the binomial distribution. This theorem became a fundamental tool in probability theory and enabled the calculation of probabilities for large sample sizes. It provided a bridge between frequentist and Bayesian approaches, allowing for the estimation of parameters and the quantification of uncertainty.\n\nAnd thus in all cases it will be found, that although Chance produces irregularities, still the Odds will be infinitely great, that in process of Time, those Irregularities will bear no proportion to the recurrency of that Order which naturally results from Original Design. (Moivre 1718)\n\nHe was an active participant in scientific societies and maintained correspondence with renowned mathematicians of his time, including Isaac Newton and James Stirling. His work played a crucial role in disseminating mathematical knowledge and promoting the study of probability theory across Europe. De Moivre’s research and writings laid the groundwork for the development of probability theory and Bayesian statistics. His ideas and formulas continue to be foundational in the field, and his contributions have had a lasting impact on mathematics, statistics, and the broader scientific community.\nHis work remains an essential reference for researchers and serves as a testament to his profound understanding of probability and statistics.\n\nFurther, the same Arguments which explode the Notion of Luck, may, on the other side, be useful in some cases to establish a due comparison between Chance and Design: We may imagine Chance and Design to be, as it were, in Competition with each other, for the production of some sorts of Events, and many calculate what Probability there is, that those Events should be rather be owing to the one than to the other. (Moivre 1718)\n\n\n\n\n\nDefinition 2.14 (Frequentist Probability) Defines probabilities using long-run limits of frequencies from repeated independent sampling generated by a hypothetical infinite sequence of experiments from a population\nFrequentist probability or frequentism is an interpretation of probability; it defines an event’s probability as the limit of its relative frequency in many trials AKA long-run probability. Probabilities can be found, in principle, by a repeatable objective process and are thus ideally devoid of opinion. The continued use of frequentist methods in scientific inference, however, has been called into question.\n\nSince in reality we cannot repeat most experiments many times.\n“by definition, scientific researchers do not possess sufficient knowledge about the relevant and irrelevant aspects of their tests and populations to be sure that their replications will be equivalent to one another” - Mark Rubin 2020\n\n\n\nDefinition 2.15 (Bayesian Probability) Defines probability starting with a subjective view of the problem called a prior and updates it as evidence comes in using Bayes Rule.\n\nThe lesson and assignments test these views with examples - but the division is rather artificial to me. Not that it does not exist, but rather different authors on the subject treat it differently.\n\n\n\n\n\n\n\nVideo 2.1: Interview with Dennis Lindley, a pioneer of Bayesian statistics, discussing the history and philosophy of Bayesian methods, and his contributions to the field. He emphasizes the importance of subjective probability and the role of prior beliefs in statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-bayesian-probability-and-coherence",
    "href": "C1-L01.html#sec-bayesian-probability-and-coherence",
    "title": "2  Probability - M1L1",
    "section": "2.5 Bayesian Probability and Coherence",
    "text": "2.5 Bayesian Probability and Coherence\n\n\n\n\n\n\n\nFigure 2.4: Coherence\n\n\nA notion of a fair bet - one which we would take either way for the same reward.\n\ncoherence following the rules of statistics\nincoherence or Dutch book one would be guaranteed to lose money.\n\n\n\n\n\n\n\n\nFigure 2.5: Bruno de Finetti\n\n\n\n\n\n\n\n\nTipBiographical note on Bruno de Finetti\n\n\n\n\n\n\nFrom the subjective standpoint, no assertion is possible without a priori opinion, but the variety of possible opinions makes problems depending on different opinions interesting.\n\nBruno de Finetti 1906-1985 was born in Innsbruck (Austria) to an Italian family. He studied mathematics at the University of Trieste, where he developed a keen interest in probability theory and its applications.\nAfter completing his doctoral studies in 1928, de Finetti embarked on a distinguished academic career. His first research work dealt with mathematical biology and was published, in 1926 when he was still an undergraduate. After graduation and up to 1931, he worked in the mathematical office of the Central Italian Agency for Statistics. From 1931-46, de Finetti worked in Trieste at Assicurazioni Generali, one of the most important insurance companies in Italy. In the same period, he lectured at the University of Trieste and the University of Padua.\nOne of de Finetti’s most significant contributions was his development of the theory of subjective probability, also known as the Bayesian interpretation of probability. He developed his ideas independently of F. P. Ramsey who also published on this (Ramsey 1926)\nIn his seminal work, (Finetti 1937), he proposed that probability should be interpreted as a personal measure of belief or degree of uncertainty rather than as a frequency or long-run proportion. This subjective approach allowed for the incorporation of prior information and updating of beliefs in light of new data, forming the basis of Bayesian inference.\n\nProbabilistic reasoning – always to be understood as subjective – merely stems from our being uncertain about something. (Finetti 2017 § preface)\n\nIt is impossible to summarize in a few paragraphs the scientific activity of de Finetti in the different fields of mathematics (probability), measure theory, analysis, geometry, mathematics of finance, economics, the social sciences, teaching, computer science, and biomathematics or to describe his generous and complex personality as a scientist and a humanitarian. De Finetti discussed his own life in a book edited by Gani (1982). See also the article by Lindley (1989).\n\nMy thesis, paradoxically, and a little provocatively, but nonetheless genuinely, is simply this :  PROBABILITY DOES NOT EXIST.  … Probability, too, if regarded as something endowed with some kind of objective existence, is no less a misleading misconception, an illusory attempt to exteriorize or materialize our true probabilistic beliefs. (Finetti 2017 § preface page x)\n\nde Finetti was a brilliant statistician but his books and papers have garnered a reputation of being challenging to read both in the original Italian, French and English translation. The above quote embodies his radical point of view which he challenged other statisticians to rethink their views.\nWhat I think he meant is that meant primarily was that probabilities unlike physical quantities cannot be measured in the objective sense. de Fineti was well versed with quantum mechanics, where physical quantities like the position and speed of an electron are interpreted primarily through probabilities in a wave equation, to include a discussion in the start of his second volume.\nA large part of this course is that we are inferring parameters - which are often probabilities.\nAnother milestone result by de Finetti is his theorem\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\nRepresenting uncertainty with probability: Don’t use any outside information on this question, just determine probabilities subjectively. The country of Chile is divided into 15 administrative regions. The size of the country is 756,096 square kilometers. How big do you think the region of Atacama is? Let:\n\nA_1 be the event: Atacama is less than 10,000 km^2.\nA_2 be the event: Atacama is between 10,000 and 50,000 km^2\nA_3 be the event: Atacama is between 50,000 and 100,000 km^2\nA_4 be the event: Atacama is more than 100,000 km^2 Assign probabilities to A_1 \\ldots A_4\n\n\n\n\n\n\n\n\n\nEvent\nMin km^2\nMax km^2\nP\n\n\n\n\nA_1\n0\n10k\n\\frac{1}{4}\n\n\nA_2\n10k\n50k\n\\frac{1}{4}\n\n\nA_3\n50k\n100k\n\\frac{1}{4}\n\n\nA_4\n100k\n\n\\frac{1}{4}\n\n\n\n\nWhat do I know at this point?\n\nThe expected area for the region is \\frac{750,000}{15}=50,000\\ km^2 .\n\nWhat Do I believe?\n\nI believe that the administrative regions have significantly different sizes\nfrom my familiarity with some other countries.\nAs I don’t know if Atacama is large or small my best bet is to assign equal probabilities to each event.\n\n\n\n\n\n\n\n\nNoteMore information 1\n\n\n\nAtacama is the fourth largest of 15 regions. Using this information, I revised my probabilities as follows:\n\n\n\n\n\n\n\nEvent\nMin km^2\nMax km^2\nP\n\n\n\n\nA_1\n\n10k\n\\frac{1}{16}\n\n\nA_2\n10k\n50k\n\\frac{3}{16}\n\n\nA_3\n50k\n100k\n\\frac{6}{16}\n\n\nA_4\n100k\n\n\\frac{6}{16}\n\n\n\n\nWhat do I know?\n\nThe expected area is \\frac{750,000}{15}=50,000\\ km^2 .\nI know that Atacama is the Fourth largest.\n\nWhat Do I believe?\n\nI believe that the administrative regions have significantly different sizes - from my familiarity with some other countries.\n\nHow do I revise my guesstimate?\n\nIf the region sizes are equally sized I should gamble mostly on A_2 and A_3 .\nBut I think there are a few large regions and many smaller ones.\nAlso I know that 11 regions are smaller than Atacama and that 3 three are larger.\nNone of the events can yet be ruled out.\nA_1 seems extremely unlikely as it necessitates the top three regions account for almost all of the area of the country. \\frac{750,000 - 14 * 10,000}{3} = 203,333.3 that’s about 4 times the average for each state.\nA_2 is fairly unlikely to require the top three regions to account for \\frac{(750,000-14*20000)}{3}=170,000 each that’s more than 3 times the average.\n\n\n\n\n\n\n\n\nNoteMore information 2\n\n\n\nThe smallest region is the capital region, Santiago Metropolitan, which has an area of 15,403 km^2. Using this information, I revised my probabilities as follows:\n\n\n\n\n\n\n\nEvent\nMin km^2\nMax km^2\nP\n\n\n\n\nA_1\n\n10k\n0\n\n\nA_2\n10k\n50k\n\\frac{1}{8}\n\n\nA_3\n50k\n100k\n\\frac{4}{8}\n\n\nA_4\n100k\n\n\\frac{3}{8}\n\n\n\nWhat do I know?\n\nThe expected area is \\frac{750,000}{15}=50,000 \\quad km^2\n\\mathbb{P}r(A_1)=0 since the smallest region is $ 15,403 km^2$.\nI believe that the administrative regions have significantly different sizes - from my familiarity with some other countries.\nI know that Atacama is the Fourth largest.\n\nIf the region sizes are equally sized I should gamble mostly on A_2 and A_3.\nBut I think there are a few large regions and many smaller ones.\nAlso I know that 11 regions are smaller than Atacama and that 3 three are larger.\nNone of the events can yet be ruled out. But A1 and A2 are now very unlikely as they would require the top three regions to account for almost all of the area of the country.\n\n\n\n\n\n\n\n\nNoteMore information 3\n\n\n\nThe third largest region is Aysén del General Carlos Ibáñez del Campo, which has an area of 108,494 km^2.\nUsing this information, I revised my probabilities as follows:\n\n\n\n\n\n\n\nEvent\nMin km^2\nMax km^2\nP\n\n\n\n\nA_1\n\n10K\n0\n\n\nA_2\n10k\n50K\n\\frac{1}{8}\n\n\nA_3\n50k\n100K\n\\frac{6}{8}\n\n\nA_4\n100k\n\n\\frac{1}{8}\n\n\n\n\nThe expected area is \\frac{750,000}{15}=50,000 \\quad km^2\n\\mathbb{P}r(A1)=0 since the smallest region is $15,403 km^2 $ .\nI believe that the administrative regions have significantly different sizes - from my familiarity with some other countries.\nI know that Atacama is the Fourth largest.\n\nIf the region sizes are equally sized I should gamble mostly on A_2 and A_3 .\nBut I think there are a few large regions and many smaller ones.\nAlso I know that 11 regions are smaller than Atacama and that 3 three are larger.\nNone of the events can yet be ruled out. But A1 and A2 are now very unlikely as they would require the top three regions to account for almost all of the area of the country.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-discussions-objectivity",
    "href": "C1-L01.html#sec-discussions-objectivity",
    "title": "2  Probability - M1L1",
    "section": "2.6 Discussions: Objectivity",
    "text": "2.6 Discussions: Objectivity\n\n\n\n\n\n\nTipDiscussion: Objectivity\n\n\n\nIn what ways could the frequentist paradigm be considered objective? In what ways could the Bayesian paradigm be considered objective? Identify ways in which each paradigm might be considered subjective.\n\nFrequentist:\n\nThe orthodox approach is statisticians should establish an objective statistical methodology and field researchers should then use it to solve their problems. This leads to following flow charts for analysis and tests without fully understanding the model and how it works. At best one makes mistakes due to misunderstanding. But we can see that there is a systematic gaming of this methodology using p-hacking, multiple hypotheses, and hiding failed experiments leading to the publication of outrageously good results, which then cannot be replicated.\nThe analysis is done on data that is supposedly sampled from a population. But the same data may belong to different populations (the city, the country, etc) each with different statistics. We should assume the same long-run frequencies would converge to different to each one of these statistics if we repeat the experiment enough times.\nThe sample size, or how long we run the experiment is a tricky decision to make in advance and without prior knowledge. And if we do not decide in advance, but periodically as the data comes in. It turns out that this can completely change the outcomes of the experiment - even if both approaches have the same data.\nThe choice of H_0 and H_1 is often subjective and each hypothesis can lead to yet another.\nThe choice of the confidence level 95%, 99%, etc. used for statistical significance is subjective.\nIf an effect size is considered large is subjective and depends on the field one studies.\n\nBayesian:\n\nthe prior should be highly informative and therefore subjective. But it can be\nuninformative and hence more objective.\nit can be difficult to decide what impact the prior should have on the posterior. Ideally, we can quantify the effective sample size for the prior data and we can understand how much information each contributes to the posterior.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-expected-values",
    "href": "C1-L01.html#sec-expected-values",
    "title": "2  Probability - M1L1",
    "section": "2.7 Expected values",
    "text": "2.7 Expected values\nThe expectation of an RV is a measure of its central tendency.\nThe expected value, also known as the expectation or mean, of a random variable X is denoted \\mathbb{E}[X]. It is the weighted average of all values X could take, weighted by their probabilities.\n\n\n\n\n\n\nTip\n\n\n\n\nI looked this up and found the following answer, see Autolatry (2015).\nThe RV X is a function whereas the Expectation is a Functional (a mapping from a function to a number). Mathematicians adopt the use of square brackets for functionals.\nSee Wikipedia contributors (2023) for more information on what a Functional is.\n\n\nWhy Square Brackets for Expectation\n2.7.1 Expectation of a discrete random variable\nIf X is a discrete-valued random variable then its expectation is defined by(?eq-expectation-discrete-RV)\n\n\\mathbb{E}[X]=\\sum^N_{i=1} x_i \\cdot \\mathbb{P}r(X=x_i) = \\sum^N_{i=1} x_i \\cdot f(x)\n\\tag{2.23}\nwhere f(x) is the probability mass function (PMF) of X.\n\n\n2.7.2 Expectation of a continuous random variable\nIf X is a continuous random variable then its expectation is defined by(?eq-expectation-continuous-RV)\n\n\\mathbb{E}[X]=\\int_{-\\infty}^{\\infty} x \\cdot f(x) dx\n\\tag{2.24}\nwhile the mean is an important descriptive statistic for central tendencies, we often prefer the median which is robust to outliers, and pick the mode as a representative if we need a value in the data set.\n\n\n2.7.3 Properties of Expectation\nSum and integral are linear operators so the Expectation is also a linear operator\n\n\\mathbb{E}[c]= c\n\\tag{2.25}\n\n\\mathbb{E}[aX+bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\n\\tag{2.26}\n\n\\mathbb{E}[g[X]]  = \\int{g(x)f(x)dx}\n\\tag{2.27}\nwhere g[X] is a function of the random variable X.\nIf X & Y are independent\n\n\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]\n\\tag{2.28}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-variance",
    "href": "C1-L01.html#sec-variance",
    "title": "2  Probability - M1L1",
    "section": "2.8 Variance",
    "text": "2.8 Variance\nVariance is the dispersion of a distribution about the mean.\n\nDefinition 2.16 For a discrete random variable, the Variance is defined using (Equation 2.29)\n\n\\mathbb{V}ar(X)=\\sum^N_{i=1} (x_i-\\mu)^2 \\mathbb{P}r(X=x_i)\n\\tag{2.29}\n\n\nDefinition 2.17 For a continuous random variable, the Variance is defined using (Equation 2.30)\n\n\\mathbb{V}ar[X]=\\int_{- \\infty}^{\\infty} (x-\\mu)^2 f(x)dx\n\\tag{2.30}\n\n\n2.8.1 Properties of Variance\n\n\\mathbb{V}ar[c] = 0\n\\tag{2.31}\nif X and Y are independent then\n\n\\mathbb{V}ar[aX+by] = a^2\\mathbb{V}ar[X] +b^2\\mathbb{V}ar[Y]\n\\tag{2.32}\notherwise\n\n\\mathbb{V}ar[aX+by] = a^2\\mathbb{V}ar[X] +b^2\\mathbb{V}ar[Y] + 2ab\\mathbb{C}ov(X,Y)\n\\tag{2.33}\nwhere \\mathbb{C}ov(X,Y) is the covariance of X and Y.\nHere is one of the most useful identities (Equation 2.34) for wrangling with variance using the expectation of X and X^2.\n\n\\begin{aligned}\n    \\mathbb{V}ar[X] &= \\mathbb{E}[(X- \\mathbb{E}[X])^2]\n    \\\\&= \\mathbb{E}[X^2] − (\\mathbb{E}[X])^2\n\\end{aligned}\n\\tag{2.34}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-covariance",
    "href": "C1-L01.html#sec-covariance",
    "title": "2  Probability - M1L1",
    "section": "2.9 Covariance",
    "text": "2.9 Covariance\nCovariance is a measure of the joint variability of two random variables. It indicates the direction of the linear relationship between the variables.\nIf X and Y are two random variables, the covariance of X and Y is defined as:\n\n\\begin{aligned}\n\\mathrm{Cov}(X,Y) &= \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n\\\\ &= \\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y]\n\\end{aligned}\n\\tag{2.35}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01.html#sec-correlation",
    "href": "C1-L01.html#sec-correlation",
    "title": "2  Probability - M1L1",
    "section": "2.10 Correlation",
    "text": "2.10 Correlation\nCorrelation is a standardized measure of the linear relationship between two random variables. It is a dimensionless quantity that ranges from -1 to 1.\nThe correlation coefficient \\rho_{XY} is defined as the covariance of X and Y divided by the product of their standard deviations:\n\n\\rho_{XY} = \\frac{\\mathrm{Cov}(X,Y)}{\\sigma_X\\sigma_Y}\n\\tag{2.36}\n\n\n\n\n\n\nAutolatry. 2015. “Why Square Brackets for Expectation.” Mathematics Stack Exchange. https://math.stackexchange.com/q/1302543.\n\n\nFinetti, Bruno de. 1937. “La Prévision: Ses Lois Logiques, Ses Sources Subjectives.” Annales de l’Institut Henri Poincaré 7 (1): 1–68.\n\n\n———. 2017. “Theory of Probability.” Edited by Antonio Machí and Adrian Smith. Wiley Series in Probability and Statistics, January. https://doi.org/10.1002/9781119286387.\n\n\nMoivre, Abraham De. 1718. The Doctrine of Chances. H. Woodfall. https://tellingstorieswithdata.com.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” In The Foundations of Mathematics and Other Logical Essays, edited by R. B. Braithwaite, 156–98. McMaster University Archive for the History of Economic Thought. https://EconPapers.repec.org/RePEc:hay:hetcha:ramsey1926.\n\n\nWikipedia contributors. 2023. “Functional (Mathematics) — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Functional_(mathematics)&oldid=1148699341.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability - M1L1</span>"
    ]
  },
  {
    "objectID": "C1-L01-Ex1.html",
    "href": "C1-L01-Ex1.html",
    "title": "3  Paradigms of probability - M1L1HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Paradigms of probability - M1L1HW1</span>"
    ]
  },
  {
    "objectID": "C1-L02.html",
    "href": "C1-L02.html",
    "title": "4  Bayes’ Theorem - M1L2",
    "section": "",
    "text": "4.1 Bayes’ Theorem",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes' Theorem - M1L2</span>"
    ]
  },
  {
    "objectID": "C1-L02.html#sec-conditional-probability",
    "href": "C1-L02.html#sec-conditional-probability",
    "title": "4  Bayes’ Theorem - M1L2",
    "section": "4.2 Conditional Probability",
    "text": "4.2 Conditional Probability\n\n\n\n\n\n\n\nFigure 4.1: conditional probability\n\n\n\n\\mathbb{P}r(A \\mid B)=\\frac{\\mathbb{P}r(A \\cap B)}{\\mathbb{P}r(B)}\n\\tag{4.1}\nindependence\n\n\\mathbb{P}r(A \\mid B) = \\mathbb{P}r(A) \\implies \\mathbb{P}r(A \\cap B) = \\mathbb{P}r(A)\\mathbb{P}r(B)\n\\tag{4.2}\n\n4.2.1 Conditional Probability Example - Female CS Student\nSuppose there are 30 students, 9 of whom are female. Of the 30 students, 12 are computer science majors. 4 of those 12 computer science majors are female. We want to estimate what is the probability of a student being female given that she is a computer science major We start by writing the above in the language of probability by converting frequencies to probabilities. We start with the marginal. First, the probability of a student being female from the data given above.\n\n\\mathbb{P}r(\\text{Female}) = \\frac{9}{30} = \\frac{3}{10}\n\nNext, we estimate the probability of a student being a computer science major again just using the data given above.\n\n\\mathbb{P}r(CS) = \\frac{12}{30} = \\frac{2}{5}\n\nNext, we can estimate the joint probability, i.e. the probability of being female and being a CS major. Again we have been given the numbers in the data above.\n\n\\mathbb{P}r(F\\cap CS) = \\frac{4}{30} = \\frac{2}{15}\n\nFinally, we can use the definition of conditional probability and substitute the above\n\n\\mathbb{P}r(F \\mid CS) = \\frac{\\mathbb{P}r(F \\cap CS)}{\\mathbb{P}r(CS)} = \\frac{2/15}{2/5} = \\frac{1}{3}\n\\tag{4.3}\n\nAn intuitive way to think about a conditional probability is that we’re looking at a sub-segment of the original population, and asking a probability question within that segment\n\n\\mathbb{P}r(F \\mid CS^c) = \\frac{\\mathbb{P}r(F\\cap CS^c)}{ \\mathbb{P}r (CS^c)} = \\frac{5/30}{18/30} = \\frac{5}{18}\n\\tag{4.4}\nThe concept of independence is when one event does not depend on another.\n\nA \\perp \\!\\!\\! \\perp B \\iff \\mathbb{P}r(A \\mid B) = \\mathbb{P}r(A)\n\nIt doesn’t matter that B occurred.\nIf two events are independent then the following is true:\n\nA \\perp \\!\\!\\! \\perp B \\iff \\mathbb{P}r(A\\cap B) = \\mathbb{P}r(A)\\mathbb{P}r(B)\n\\tag{4.5}\nThis can be derived from the conditional probability equation.\n\n4.2.2 Inverting Conditional Probabilities\nIf we don’t know \\mathbb{P}r(A \\mid B) but we do know the inverse probability \\mathbb{P}r(B \\mid A) is. We can then rewrite \\mathbb{P}r(A \\mid B) in terms of \\mathbb{P}r(B \\mid A)\n\n\\mathbb{P}r(A \\mid B) = \\frac{\\mathbb{P}r(B \\mid A)\\mathbb{P}r(A)}{\\mathbb{P}r(B \\mid A)\\mathbb{P}r(A) + \\mathbb{P}r(B \\mid A^c)\\mathbb{P}r(A^c)}\n\\tag{4.6}\n\n\n4.2.3 Conditional Probability Example - ELISA HIV test\n\n\n\n\n\n\n\nVideo 4.1\n\n\nLet’s look at an example of an early test for HIV antibodies known as the ELISA test. - The test has a true positive rate of 0.977. - It has a true negative rate of 0.926. - The incidence of HIV in North America is .0026.\nNow we want to know the probability of an individual having the disease given that they tested positive \\mathbb{P}r(HIV | +).\nThis is the inverse probability of the true positive, so we will need to use Bayes’ theorem.\nWe start by encoding the above using mathematical notation, so we know what to substitute into Bayes’ theorem.\nThe true positive rate is:\n\n\\mathbb{P}r(+ \\mid HIV) = 0.977\n\nThe true negative rate is:\n\n\\mathbb{P}r(- \\mid NO\\_HIV) = 0.926\n\nThe probability of someone in North America having this disease was\n\n\\mathbb{P}r(HIV) = .0026\n\nwhat we want is: \\mathbb{P}r(HIV \\mid +)\n\n\\begin{aligned}\n\\mathbb{P}r(HIV \\mid +) &= \\frac{\\mathbb{P}r(+ \\mid HIV)\\mathbb{P}r(HIV)}{\\mathbb{P}r(+ \\mid HIV)\\mathbb{P}r(HIV) + \\mathbb{P}r(+ \\mid NO\\_HIV){\\mathbb{P}r(NO\\_HIV)}}  \\\\\n&= \\frac{(.977)(.0026)}{(.977)(.0026) + (1-.977)(1-.0026)}  \\\\\n&=  0.033\n\\end{aligned}\n\\tag{4.7}\nThis is a bit of a surprise - although the test has 90% + true and false accuracy - taking it once is only valid 3% of the time. How is this possible?\nWhat happens in Bayes law is that we are updating probabilities. And since we started with such a low probability of .0026, Bayesian updating only brings it up to 0.03.\n\n\\begin{aligned}\n\\mathbb{P}r(A \\mid B) = \\frac{\\mathbb{P}r(B \\mid A_1){(A_1)}}{\\sum_{i=1}^{n}{\\mathbb{P}r(B \\mid A_i)}\\mathbb{P}r(A_i)} \\end{aligned}\n\\tag{4.8}\n\nNote: (McElreath (2015)) discusses how this can be presented less surprisingly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes' Theorem - M1L2</span>"
    ]
  },
  {
    "objectID": "C1-L02.html#sec-bayes-theorem-video",
    "href": "C1-L02.html#sec-bayes-theorem-video",
    "title": "4  Bayes’ Theorem - M1L2",
    "section": "4.3 Bayes’ theorem 🎥",
    "text": "4.3 Bayes’ theorem 🎥\n\n\n\n\n\n\n\nVideo 4.2: Bayes Theorem in Probability with Examples\n\n\n\n\n\n\n\n\nFigure 4.2: Bayes theorem\n\n\n\nHere are a few formulations of Bayes’ theorem. We denote H for our hypothesis and E as our evidence i.e. the data!\nWe start by using the definition of conditional probability:\n\n\\mathbb{P}r(A \\mid B) = \\frac{ \\mathbb{P}r(A \\cap B)}{\\mathbb{P}r(B)} \\quad \\text{(conditional probability)}\n\n\n\\begin{aligned}\n{\\color{orange} \\overbrace{\\color{orange} \\mathbb{P}r(H|E)}^{\\text{Posterior}}} &= \\frac{  {\\color{pink} \\overbrace{\\color{pink} \\mathbb{P}r(H \\cap E)}^{\\text{Joint}}}  } {  {\\color{green} \\underbrace{{\\color{green} \\mathbb{P}r(\\text{E})}}_{\\text{Marginal Evidence}}} } \\\\\n&= \\frac{  {\\color{red} \\overbrace{\\color{red} P (\\text{H})}^{\\text{Prior}}} \\cdot  {\\color{blue} \\overbrace{\\color{blue} P (E \\mid H)}^{\\text{Likelihood}}} } { {\\color{green} \\underbrace{{\\color{green} \\mathbb{P}r(E)}}_{\\text{Marginal Evidence}}} } \\\\\n&= \\frac{  {\\color{red} \\overbrace{\\color{red} P (H)}^{\\text{Prior}}} \\cdot {\\color{blue} \\overbrace{\\color{blue} P (E \\mid H)}^{\\text{Likelihood}}} }{  {\\color{green} \\underbrace{\\color{green} \\mathbb{P}r(E \\mid H) \\mathbb{P}r(H) + \\mathbb{P}r(E \\mid H^c) \\mathbb{P}r(H^c)  }_{\\text{Marginal Evidence}}}}\n\\end{aligned}\n\nWe can extend Bayes theorem to cases with multiple mutually exclusive events:\n\n\n\n\n\n\n\nFigure 4.3: mutually exclusive events\n\n\nif H_1 \\ldots H_n are mutually exclusive events that sum to 1:\n\n\\begin{aligned} \\mathbb{P}r(H_1 \\mid E)\n  & = \\frac{\\mathbb{P}r(E \\mid H)\\mathbb{P}r(H_1)}{\\mathbb{P}r(E \\mid H_1)\\mathbb{P}r(H_1) +\\ldots  + \\mathbb{P}r(E \\mid H_n)\\mathbb{P}r(H_N)} \\\\\n  & = \\frac{\\mathbb{P}r(E \\mid H)\\mathbb{P}r(H_1)}{\\sum_{i=1}^{N} \\mathbb{P}r(E \\mid H_i)\\mathbb{P}r(H_i)} \\end{aligned}\n\nwhere we used the law of total probability in the denominator\nif \\{B_i\\} is a finite or countably finite partition of a sample space then\n\n\\mathbb{P}r(A) = {\\sum_{i=1}^{N} \\mathbb{P}r(A \\cup B_i)}= {\\sum_{i=1}^{N} \\mathbb{P}r(A \\mid B_i)\\mathbb{P}r(B_i)}\n\n\n{\\color{orange} P (\\text{H} \\mid \\text{E})} = \\frac {{\\color{red} \\mathbb{P}r(\\text{H})} \\times {\\color{blue}\\mathbb{P}r(\\text{E} \\mid \\text{H})}} {\\color{gray} {\\mathbb{P}r(\\text{E})}}\n\n\n{\\color{orange} \\overbrace{\\color{orange} P (\\text{Unknown} \\mid \\text{Data})}^{\\text{Posterior}}} = \\frac {{\\color{red} \\overbrace{\\color{red} P (\\text{Unknown})}^{\\text{Prior}}} \\times {\\color{blue} \\overbrace{\\color{blue} P (\\text{Data} \\mid \\text{Unknown})}^{\\text{Likelihood}}}} {{\\color{green} \\underbrace{{\\color{green} \\mathbb{P}r(\\text{E})}}_{\\text{Average likelihood}}}}\n\nThe following is a video explaining Bayes law.\n\n\n\n\n\n\n\nVideo 4.3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes' Theorem - M1L2</span>"
    ]
  },
  {
    "objectID": "C1-L02.html#bayes-theorem-for-continuous-distributions",
    "href": "C1-L02.html#bayes-theorem-for-continuous-distributions",
    "title": "4  Bayes’ Theorem - M1L2",
    "section": "4.4 Bayes’ Theorem for continuous distributions",
    "text": "4.4 Bayes’ Theorem for continuous distributions\nWhen dealing with a continuous random variable \\theta, we can write the conditional density for \\theta given y as:\n\nf(\\theta \\mid y) =\\frac{f(y\\mid\\theta)f(\\theta)}{\\int f(y\\mid\\theta) f(\\theta) d\\theta }\n\\tag{4.9}\nThis expression does the same thing that the versions of Bayes’ theorem from Lesson 2 do. Because \\theta is continuous, we integrate over all possible values of \\theta in the denominator rather than take the sum over these values. The continuous version of Bayes’ theorem will play a central role from Lesson 5 on.\n\n\n\n\n\n\n\nFigure 4.4: Rev. Thomas Bayes by Mark Riehl\n\n\n\n\n\n\n\n\nTipHistorical Note on The Reverend Thomas Bayes\n\n\n\nBayes Rule is due to Thomas Bayes (1701-1761) who was an English statistician, philosopher and Presbyterian minister. Although Bayes never published what would become his most famous accomplishment; his notes were edited and published posthumously by Richard Price.\n\n\n\n\n\n\n\n\nMcElreath, Richard. 2015. Statistical Rethinking, a Course in r and Stan.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Bayes' Theorem - M1L2</span>"
    ]
  },
  {
    "objectID": "C1-L02-Ex1.html",
    "href": "C1-L02-Ex1.html",
    "title": "5  Conditional Probability and Bayes’ Law - M1L2HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Bayes' Law - M1L2HW1</span>"
    ]
  },
  {
    "objectID": "C1-L02-Ex2.html",
    "href": "C1-L02-Ex2.html",
    "title": "6  Probability and Bayes’ Theorem - M1L2HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probability and Bayes' Theorem - M1L2HW2</span>"
    ]
  },
  {
    "objectID": "C1-L03.html",
    "href": "C1-L03.html",
    "title": "7  Distributions - M1L3",
    "section": "",
    "text": "7.1 Distributions",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distributions - M1L3</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-bernoulli--binomial-distribution",
    "href": "C1-L03.html#sec-the-bernoulli--binomial-distribution",
    "title": "7  Distributions - M1L3",
    "section": "7.2 The Bernoulli & Binomial Distribution",
    "text": "7.2 The Bernoulli & Binomial Distribution\n\n\n\n\n\n\n\nFigure 7.1: Bernoulli and Binomial Distributions\n\n\nThese two distributions are built on a trial of a coin toss (possibly biased).\n\nWe use the Bernoulli distribution to model a random variable for the probability of such a coin toss trial.\nWe use the Binomial distribution to model a random variable for the probability of getting k heads in N independent trials.\n\n\n7.2.1 The Bernoulli Distribution\nArises when modeling events with two possible outcomes, Success and Failure for a coin toss these can be Heads and Tails\n\nX \\sim \\mathrm{Bernoulli}(p) =\n\\begin{cases}\n   \\mathbb{P}r(X=1) = p & \\text{success} \\\\\n   \\mathbb{P}r(X=0)=1-p & \\text{failure}\n\\end{cases}\n\\tag{7.1}\nWhere parameter p is the probability of getting heads.\nThe probability for the two events is:\nNotation:\n\nwe use (Roman) p if its value is known.\n\nwe use (Greek) \\theta when its value is unknown.\n\nThis is a probability mass function since it is discrete. But we call it a Probability Density Function (PDF) in the measure-theoretic sense.\n\nf(X=x\\mid p) = p^x(1-p)^x \\mathbb{I}_{[0,1]}(x)\n\\tag{7.2}\n\n\\mathbb{E}(x)= p\n\\tag{7.3}\n\n\\text{Var}(x)= \\mathbb{P}r(1-p)\n\\tag{7.4}\n\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=0.30, var=0.21, skew=0.87, kurt=-1.24\n\nx = np.arange(bernoulli.ppf(0.01, p),\n              bernoulli.ppf(0.99, p))\nax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\n\nrv = bernoulli(p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\nBernoulli distribution\n\n\n\n## Generate random numbers\nr = bernoulli.rvs(p, size=10)\nr\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])\n\n\n\n\n\n\n\n\n\nFigure 7.2: Jacob Bernoulli\n\n\n\n\n\n\n\n\nTipBiographical note on Jacob Bernoulli\n\n\n\n\nIt seems that to make a correct conjecture about any event whatever, it is necessary to calculate exactly the number of possible cases and then to determine how much more likely it is that one case will occur than another. (Bernoulli 1713)\n\nThe Bernoulli distribution as well as The Binomial distribution are due to Jacob Bernoulli (1655-1705) who was a prominent mathematicians in the Bernoulli family. He discovered the fundamental mathematical constant e. However, his most important contribution was in the field of probability, where he derived the first version of the law of large numbers.\nfor a fuller biography see\n\n\n\n\n7.2.2 The Binomial Distribution\n\n\\overbrace{\\underbrace{\\fbox{0}\\ \\ldots \\fbox{0}}_{N_0}\\ \\underbrace{\\fbox{1}\\ \\ldots \\fbox{1}}_{N_1}}^N\n\\tag{7.5}\nThe Binomial distribution models counts of successes in independent Bernoulli trials . It arises when we need to consider the summing N independent and identically distributed Bernoulli RV with the same probability of success \\theta.\n\n\n\n\n\n\nTipConditions\n\n\n\n\nDiscrete data\nTwo possible outcomes for each trial\nEach trial is independent\nThe probability of success/failure is the same in each trial\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial reparams mindmap\n\n\n\n\n\nX \\sim \\mathrm{Bin}(n,p)\n\\tag{7.6}\nthe probability function\n\nf(X=x \\mid \\theta) = {n \\choose x} \\theta^x(1-\\theta)^{n-x}\n\\tag{7.7}\n\n\\mathcal{L}(\\theta)=\\prod_{i=1}^{n} {n\\choose x_i}  \\theta ^ {x_i} (1− \\theta) ^ {(n−x_i)}\n\\tag{7.8}\n\n\\begin{aligned}\n\\ell( \\theta) &= \\log \\mathcal{L}( \\theta) \\\\\n              &= \\sum_{i=1}^n \\left[\\log {n\\choose x_i} + x_i \\log  \\theta + (n-x_i)\\log (1- \\theta) \\right]\n\\end{aligned}\n\\tag{7.9}\n\n\\mathbb{E}[X]= N \\times  \\theta\n\\tag{7.10}\n\n\\mathbb{V}ar[X]=N \\cdot \\theta \\cdot (1-\\theta)\n\\tag{7.11}\n\n\\mathbb{H}(X) = \\frac{1}{2}\\log_2 \\left (2\\pi n \\theta(1 - \\theta)\\right) + O(\\frac{1}{n})\n\\tag{7.12}\n\n\\mathcal{I}(\\theta)=\\frac{n}{ \\theta \\cdot (1- \\theta)}\n\\tag{7.13}\n\n7.2.2.1 Relationships\n\n\n\n\n\n\n\nFigure 7.3: binomial distribution relations\n\n\nThe Binomial Distribution is related to:\n\nthe Geometric distribution,\nThe Multinomial distribution with two categories is the binomial.\nthe Poisson distribution distribution. If X \\sim \\mathrm{Binomial}(n, p) rv and Y \\sim \\mathrm{Poisson}(np) distribution then \\mathbb{P}r(X = n) \\approx \\mathbb{P}r(Y = n) for large n and small np.\nthe Bernoulli distribution If X \\sim \\mathrm{Binomial}(n, p) RV with n = 1, X \\sim Bernoulli(p) RV.\nthe Normal distribution If X \\sim \\mathrm{Binomial}(n, p) RV and Y \\sim Normal(\\mu=np,\\sigma=n\\mathbb{P}r(1-p)) then for integers j and k, \\mathbb{P}r(j \\le X \\le k) \\approx \\mathbb{P}r(j – {1 \\over 2} \\le Y \\le k + {1 \\over 2}). The approximation is better when p ≈ 0.5 and when n is large. For more information, see normal approximation to binomial\nHypergeometric: The difference between a binomial distribution and a hypergeometric distribution is the difference between sampling with replacement and sampling without replacement. As the population size increases relative to the sample size, the difference becomes negligible. So If X \\sim Binomial(n, p) RV and Y \\sim HyperGeometric(N,a,b) then\n\n\n\\lim_{n\\to \\infty} X = Y\n\n\nimport numpy as np\nfrom scipy.stats import binom\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\nn, p = 5, 0.4\nmean, var, skew, kurt = binom.stats(n, p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=2.00, var=1.20, skew=0.18, kurt=-0.37\n\nx = np.arange(binom.ppf(0.01, n, p), binom.ppf(0.99, n, p))\nax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\nax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\nrv = binom(n, p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n## generate random numbers\nr = binom.rvs(n, p, size=10)\nr\n\narray([0, 1, 2, 2, 4, 2, 1, 2, 2, 1])\n\n\n\n\n\n7.2.3 The Discrete Uniform Distribution\n\nX \\sim U[0,1]\n\\tag{7.14}\n\n    f(x)=\n    \\begin{cases}\n      1, & \\text{if}\\ x \\in [0,1] \\\\\n      0, & \\text{otherwise}\n    \\end{cases}\n    = \\mathbb{I}_{\\{0 \\le x \\le 1\\}}(x)\n\\tag{7.15}\n\nimport numpy as np\nfrom scipy.stats import uniform\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = uniform.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=0.50, var=0.08, skew=0.00, kurt=-1.20\n\n# we use ppf to get the domain from a range of (0.01,0.99)\nx = np.linspace(uniform.ppf(0.01), uniform.ppf(0.99), 100)\nax.plot(x, uniform.pdf(x), 'r-', lw=5, alpha=0.6, label='uniform pdf')\nrv = uniform()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\n## generate random numbers\nr = uniform.rvs(size=1000)\n\n# And compare the histogram:\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n(array([1.12566475, 0.96012582, 1.02634139, 1.10359289, 0.89391024,\n       1.00426953, 1.23602404, 1.02634139, 0.73940724, 1.03737732,\n       0.88287431]), array([0.00182179, 0.09243492, 0.18304804, 0.27366116, 0.36427429,\n       0.45488741, 0.54550054, 0.63611366, 0.72672679, 0.81733991,\n       0.90795304, 0.99856616]), [&lt;matplotlib.patches.Polygon object at 0x74f59798ca00&gt;])\n\nax.set_xlim([x[0], x[-1]])\n\n(0.01, 0.99)\n\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.2.4 The Continuous Uniform Distribution\n\nX \\sim \\mathrm{Uniform}[\\theta_1,\\theta_2]\n\\tag{7.16}\n\nf(x)= \\frac{1}{\\theta_2-\\theta_1} \\mathbb{I}_{\\{\\theta_1 \\le x \\le \\theta_2\\}}(x)\n\\tag{7.17}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distributions - M1L3</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-normal-z-t-distributions",
    "href": "C1-L03.html#sec-the-normal-z-t-distributions",
    "title": "7  Distributions - M1L3",
    "section": "7.3 The Normal, Z, t Distributions",
    "text": "7.3 The Normal, Z, t Distributions\n The normal, AKA Gaussian distribution is one of the most important distributions in statistics.\nIt arises as the limiting distribution of sums (and averages) of random variables. This is due to the Section 104.1. Because of this property, the normal distribution is often used to model the “errors,” or unexplained variations of individual observations in regression models.\n\n7.3.1 The Standard Normal distribution\n The standard normal distribution is given by\n\n\\mathcal{Z} \\sim \\mathcal{N}[1,0]\n\\tag{7.18}\n\nf(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}}\n\\tag{7.19}\n\n\\mathbb{E}[\\mathcal{Z}] = 0\n\\tag{7.20}\n\n\\mathbb{V}ar[\\mathcal{Z}]= 1\n\\tag{7.21}\n\n\n7.3.2 The Normal distribution\n Now consider X = \\sigma \\mathcal{Z}+\\mu where \\sigma &gt; 0 and \\mu is any real constant. Then \\mathbb{E}(X) = \\mathbb{E}(\\sigma \\mathcal{Z}+\\mu) = \\sigma \\mathbb{E}(\\mathcal{Z}) + \\mu = \\sigma \\times 0 + \\mu = \\mu and Var(X) = Var(\\sigma^2 + \\mu) = \\sigma^2 Var(\\mathcal{Z}) + 0 = \\sigma^2 \\cdot 1 = \\sigma^2\nThen, X follows a normal distribution with mean \\mu and variance \\sigma^2 (standard deviation \\sigma) denoted as\n\nX \\sim \\mathcal{N}[\\mu,\\sigma^2]\n\\tag{7.22}\n\nf(x\\mid \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}  e^{-\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}(x-\\mu)^2}\n\\tag{7.23}\n\n\\mathbb{E}[x]= \\mu\n\\tag{7.24}\n\n\\mathbb{V}ar[x]= \\sigma^2\n\\tag{7.25}\n\nThe normal distribution is symmetric about the mean \\mu and is often described as a bell-shaped curve.\nAlthough X can take on any real value (positive or negative), more than 99% of the probability mass is concentrated within three standard deviations of the mean.\n\nThe normal distribution has several desirable properties.\nOne is that if X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2_1) and X_2 \\sim \\mathcal{N}(\\mu_2, \\sigma^2_2) are independent, then X_1+X_2 \\sim \\mathcal{N}(\\mu_1+\\mu_2, \\sigma^2_1+\\sigma^2_2).\nConsequently, if we take the average of n Independent and Identically Distributed (IID) normal random variables we have\n\n\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})\n\\tag{7.26}\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = norm.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=0.00, var=1.00, skew=0.00, kurt=0.00\n\nx = np.linspace(norm.ppf(0.01),\n                norm.ppf(0.99), 100)\nax.plot(x, norm.pdf(x),\n       'r-', lw=5, alpha=0.6, label='norm pdf')\n\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nr = norm.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n(array([0.00726787, 0.00363393, 0.01816967, 0.03633934, 0.05814294,\n       0.14172341, 0.19259849, 0.21440209, 0.3016165 , 0.31978617,\n       0.3343219 , 0.40700058, 0.38883091, 0.28708076, 0.30888437,\n       0.27617896, 0.11265195, 0.08721441, 0.07631261, 0.0327054 ,\n       0.01453573, 0.00726787, 0.00726787]), array([-3.13879616, -2.86361227, -2.58842839, -2.3132445 , -2.03806062,\n       -1.76287673, -1.48769285, -1.21250896, -0.93732508, -0.66214119,\n       -0.3869573 , -0.11177342,  0.16341047,  0.43859435,  0.71377824,\n        0.98896212,  1.26414601,  1.5393299 ,  1.81451378,  2.08969767,\n        2.36488155,  2.64006544,  2.91524932,  3.19043321]), [&lt;matplotlib.patches.Polygon object at 0x74f58fc931c0&gt;])\n\nax.set_xlim([x[0], x[-1]])\n\n(-2.3263478740408408, 2.3263478740408408)\n\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.3.3 The t-Distribution\n If we have normal data, we can use (Equation 101.32) to help us estimate the mean \\mu. Reversing the transformation from the previous section, we get\n\n\\frac {\\hat X - \\mu}{\\sigma / \\sqrt(n)} \\sim N(0, 1)\n\\tag{7.27}\nHowever, we may not know the value of \\sigma. If we estimate it from data, we can replace it with S = \\sqrt{\\sum_i \\frac{(X_i-\\hat X)^2}{n-1}}, the sample standard deviation. This causes the expression (Equation 101.33) to no longer be distributed as a Standard Normal; but as a standard t-distribution with ν = n − 1 degrees of freedom\n\nX \\sim t[\\nu]\n\\tag{7.28}\n\nf(t\\mid\\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}}\\left (1 + \\frac{t^2}{\\nu}\\right)^{-(\\frac{\\nu+1}{2})}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{7.29}\n\n\\text{where }\\Gamma(w)=\\int_{0}^{\\infty}t^{w-1}e^{-t}\\mathrm{d}t \\text{ is the gamma function}\n\n\nf(t\\mid\\nu)={\\frac {1}{{\\sqrt {\\nu }}\\,\\mathrm {B} ({\\frac {1}{2}},{\\frac {\\nu }{2}})}}\\left(1+{\\frac {t^{2}}{\\nu }}\\right)^{-(\\nu +1)/2}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{7.30}\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the beta function}\n\n\n\\mathbb{E}[Y] = 0 \\qquad \\text{ if } \\nu &gt; 1\n\\tag{7.31}\n\n\\mathbb{V}ar[Y] = \\frac{\\nu}{\\nu - 2} \\qquad \\text{ if } \\nu &gt; 2\n\\tag{7.32}\nThe t distribution is symmetric and resembles the Normal Distribution but with thicker tails. As the degrees of freedom increase, the t distribution looks more and more like the standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distributions - M1L3</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#the-exponential-distribution",
    "href": "C1-L03.html#the-exponential-distribution",
    "title": "7  Distributions - M1L3",
    "section": "7.4 The Exponential Distribution",
    "text": "7.4 The Exponential Distribution\n The Exponential distribution models the waiting time between events for events with a rate \\lambda. Those events, typically, come from a Poisson process.\nThe Exponential distribution is often used to model the waiting time between random events. Indeed, if the waiting times between successive events are independent then they form an \\exp(r(\\lambda) distribution. Then for any fixed time window of length t, the number of events occurring in that window will follow a Poisson distribution with mean t\\lambda.\n\nX \\sim Exp[\\lambda]\n\\tag{7.33}\n\nf(x \\mid \\lambda) = \\frac{1}{\\lambda} e^{- \\frac{x}{\\lambda}}(x)\\mathbb{I}_{\\lambda\\in\\mathbb{R}^+ } \\mathbb{I}_{x\\in\\mathbb{R}^+_0 } \\quad \\text{(PDF)}\n\\tag{7.34}\n\n\\mathbb{E}(x)= \\lambda\n\\tag{7.35}\n\n\\mathbb{V}ar[X]= \\lambda^2\n\\tag{7.36}\n\nimport numpy as np\nfrom scipy.stats import expon\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = expon.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=1.00, var=1.00, skew=2.00, kurt=6.00\n\nx = np.linspace(expon.ppf(0.01), expon.ppf(0.99), 100)\nax.plot(x, expon.pdf(x), 'r-', lw=5, alpha=0.6, label='expon pdf')\n\nrv = expon()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\nr = expon.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n(array([0.9700008 , 0.79990588, 0.56545071, 0.39075862, 0.33099553,\n       0.29421825, 0.23905233, 0.19767789, 0.16549777, 0.11033184,\n       0.11492901, 0.08734604, 0.05516592, 0.04137444, 0.05976308,\n       0.02758296, 0.03218012, 0.00919432, 0.01838864, 0.01838864,\n       0.00919432, 0.00459716, 0.01838864, 0.        , 0.00459716,\n       0.        , 0.00919432, 0.00459716, 0.        , 0.00459716,\n       0.01379148]), array([1.76061227e-04, 2.17701654e-01, 4.35227247e-01, 6.52752840e-01,\n       8.70278433e-01, 1.08780403e+00, 1.30532962e+00, 1.52285521e+00,\n       1.74038081e+00, 1.95790640e+00, 2.17543199e+00, 2.39295758e+00,\n       2.61048318e+00, 2.82800877e+00, 3.04553436e+00, 3.26305996e+00,\n       3.48058555e+00, 3.69811114e+00, 3.91563673e+00, 4.13316233e+00,\n       4.35068792e+00, 4.56821351e+00, 4.78573911e+00, 5.00326470e+00,\n       5.22079029e+00, 5.43831589e+00, 5.65584148e+00, 5.87336707e+00,\n       6.09089266e+00, 6.30841826e+00, 6.52594385e+00, 6.74346944e+00]), [&lt;matplotlib.patches.Polygon object at 0x74f58fb40f10&gt;])\n\nax.set_xlim([x[0], x[-1]])\n\n(0.010050335853501442, 4.605170185988091)\n\nax.legend(loc='best', frameon=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distributions - M1L3</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-geometric-distribution",
    "href": "C1-L03.html#sec-the-geometric-distribution",
    "title": "7  Distributions - M1L3",
    "section": "8.1 The Geometric Distribution",
    "text": "8.1 The Geometric Distribution\n The Geometric distribution arises when we want to know “What is the number of Bernoulli trials required to get the first success?”, i.e., the number of Bernoulli events until a success is observed, such as the probability of getting the first head when flipping a coin. It takes values on the positive integers starting with one (since at least one trial is needed to observe a success).\n\nX \\sim \\mathrm{Geo}(p)\n\\tag{8.1}\n\n\\mathbb{P}r(X = x\\mid p) = \\mathbb{P}r(1-p)^{x-1} \\qquad \\forall x \\in N;\\quad 0\\le p \\le 1\n\\tag{8.2}\n\n\\mathbb{E}[X] = \\frac{1}{p}\n\\tag{8.3}\n\n\\mathbb{V}ar[X]=\\frac{1-p}{p^2}\n\\tag{8.4}\n\n\\mathbb{M}_X[t] = \\frac{pe^t}{1-(1-p)e^t} \\qquad t &lt; -log(1-p)\n\\tag{8.5}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distributions - M1L3</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-multinomial-distribution",
    "href": "C1-L03.html#sec-the-multinomial-distribution",
    "title": "7  Distributions - M1L3",
    "section": "8.2 The Multinomial Distribution",
    "text": "8.2 The Multinomial Distribution\n Another generalization of the Bernoulli distribution and the Binomial distribution is the Multinomial distribution, which sums the successes of Bernoulli trials when there are n different possible outcomes. Suppose we have n trials and there are k different possible outcomes that occur with probabilities p_1, \\ldots, p_k. For example, we are rolling a six-sided die that might be loaded so that the sides are not equally likely, then n is the total number of rolls, k = 6, p_1 is the probability of rolling a one, and we denote by x_1, \\ldots, x_6 a possible outcome for the number of times we observe rolls of each of one through six, where\n\nX \\sim \\mathrm{Multinomial}(p_1,...p_k)\n\n\nP (X = x \\mid p_1,\\ldots,p_k) = \\frac{n!}{x_1! \\cdot \\cdot \\cdot x_k! } \\prod_i p_i^{x_i}",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distributions - M1L3</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#sec-the-poisson-distribution",
    "href": "C1-L03.html#sec-the-poisson-distribution",
    "title": "7  Distributions - M1L3",
    "section": "8.3 The Poisson Distribution",
    "text": "8.3 The Poisson Distribution\nThe Poisson distribution arises when modeling count data. The parameter \\lambda &gt; 0 is the rate at which we expect to observe the thing we are counting. We write this as X \\sim \\mathrm{Poisson}(\\lambda)\n\n\\mathbb{P}r(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{−\\lambda}}{x!} \\qquad \\forall x \\in \\mathbb{N}_0 \\qquad \\text{PDF}\n\\tag{8.6}\n\n\\mathbb{E}[X] = \\lambda \\qquad \\text{Expectation}\n\\tag{8.7}\n\n\\mathbb{V}ar[X] = \\lambda \\qquad \\text{Variance}\n\\tag{8.8}\n\n\\mathbb{M}_X(t) = \\exp[\\lambda(e^t-1)] \\qquad \\text{Moment Generating fn.}\n\\tag{8.9}\n\n\\mathcal{I}_X(t) = \\frac{1}{\\lambda}\n\\tag{8.10}\n\n8.3.1 Relations\n\n\n\n\n\n\n\nFigure 8.1: Relations of the Poisson distribution\n\n\n A Poisson process is a process wherein events occur on average at rate \\mathbb{E}, events occur one at a time, and events occur independently of each other.\n\n\n\n\n\n\n\nFigure 8.2: Siméon Denis Poisson\n\n\n\n\n\n\n\n\nTipBiographical Note on The Siméon Denis Poisson\n\n\n\nThe Poisson distribution is due to Baron Siméon Denis Poisson (1781-1840) see (Poisson 2019, 205–7) was a French mathematician and physicist who worked on statistics, complex analysis, partial differential equations, the calculus of variations, analytical mechanics, electricity and magnetism, thermodynamics, elasticity, and fluid mechanics.\nfor a fuller biography see",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distributions - M1L3</span>"
    ]
  },
  {
    "objectID": "C1-L03.html#hypergeometric-distribution",
    "href": "C1-L03.html#hypergeometric-distribution",
    "title": "7  Distributions - M1L3",
    "section": "8.4 Hypergeometric Distribution",
    "text": "8.4 Hypergeometric Distribution\n\nConsider an urn with a white balls and b black balls. Draw N balls from this urn without replacement. The number white balls drawn, n is Hypergeometrically distributed.\n\nX \\sim \\mathrm{Hypergeometric}(n \\mid N,a,b)\n\n\n\\mathrm{Hypergeometric}(n\\mid N,a,b) = \\frac{\\normalsize{\\binom{a}{n} \\binom{b}{N - n}}} {\\normalsize{\\binom{a + b}{N}}} \\quad \\text{(PDF)}\n\\tag{8.11}\n\n\\mathbb{E}[X]=N\\frac{a}{a+b} \\qquad \\text{(expectation)}\n\\tag{8.12}\n\n\\mathbb{V}ar[X]=N\\,\\frac{ab}{(a + b)^2}\\,\\frac{a+b-N}{a+b-1} \\qquad \\text{(variance)}\n\\tag{8.13}\n\n\n\n\n\n\nBernoulli, J. 1713. Ars Conjectandi [the Art of Conjecturing]. Impensis Thurnisiorum. https://books.google.co.il/books?id=Ba5DAAAAcAAJ.\n\n\nPoisson, S. -D. 2019. “English Translation of Poisson’s \"Recherches Sur La Probabilité Des Jugements En Matière Criminelle Et En Matière Civile\" / \"Researches into the Probabilities of Judgements in Criminal and Civil Cases\".” https://arxiv.org/abs/1902.02782.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distributions - M1L3</span>"
    ]
  },
  {
    "objectID": "C1-L03-Ex1.html",
    "href": "C1-L03-Ex1.html",
    "title": "8  Random Variables - M1L3HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Random Variables - M1L3HW1</span>"
    ]
  },
  {
    "objectID": "C1-L03-Ex2.html",
    "href": "C1-L03-Ex2.html",
    "title": "9  Homework on Distributions - M1L3HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Homework on Distributions - M1L3HW2</span>"
    ]
  },
  {
    "objectID": "C1-L04.html",
    "href": "C1-L04.html",
    "title": "10  Frequentist Inference - M2L4",
    "section": "",
    "text": "10.1 Confidence Intervals\nBefore delving into Bayesian inference in the next module, in this module we will review inference in the frequentist approach. Much of the material was developed by R. A. Fischer in the last century. Some of the central ideas and tools of this approach include:\nOne point of interest is how much of this work is based on the law of large numbers, central limit theorem and the empirical rule, three related key results in probability theory.\nHowever the second point to stress is that the frequentist paradigm is fraught with practical as well as philosophical challenges which are handled better to some extent within the Bayesian paradigm.\nIn particular, the frequentist paradigm does not allow us to make probability statements about parameters, which is a key feature of the Bayesian approach.\nA brief review of the frequentist approach to inference will be useful for contrasting with the Bayesian approach. (Kruschke 2011) Chapter 2 suggests that CI provides the basis for a Bayesian workflow and that the rest of the text fills in the missing pieces.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Frequentist Inference - M2L4</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-confidence-intervals",
    "href": "C1-L04.html#sec-confidence-intervals",
    "title": "10  Frequentist Inference - M2L4",
    "section": "",
    "text": "Figure 10.1: frequentist approach to confidence intervals\n\n\n\n\n\n\n\n\n\nImportantFrequentist paradigm\n\n\n\nUnder the frequentist paradigm, one views the data as a random sample from some larger, potentially hypothetical population. We can then make probability statements i.e. long-run frequency statements based on this larger population.\n\n\n\nExample 10.1 (Coin Flip Example - Central Limit Theorem) Suppose we flip a coin 100 times. And we get 44 heads and 56 tails. We can view these 100 flips as a random sample from a much larger infinite hypothetical population of flips from this coin. We can say that each flip is X_i an RV which follows a Bernoulli distribution with some probability p. In this case p is unknown, but we can assume it is fixed since we are using a specific physical coin.\n\nX_i \\sim B(p)\n\\tag{10.1}\nWe ask :\n\nWhat is our best estimate of p the probability of getting a head?\nHow confident are we in the estimate of p?\n\nTo estimate p we will apply the Central limit theorem c.f. Theorem 104.1 which states that the mean of a large number of IID RV with mean \\mu and variance \\sigma^2 is approximately N(\\mu,\\sigma^2).\n\n\\sum^{100}_{i=1} X_i\\mathrel{\\dot \\sim } \\mathcal{N}(100 \\enspace p, 100 \\enspace \\mathbb{P}r(1-p))\n\\tag{10.2}\n Given that this is a Normal distribution, we can use the empirical rule often called the 68-95-99.7 rule see (Wikipedia contributors 2023), that says 95% of the time we will get a result is in within 1.96 standard deviations of the mean. This is referred to as a Confidence Interval or (CI).\n\n95\\% \\: \\text{CI}= 100 \\: \\hat{p} \\pm 1.96\\sqrt{100 \\: \\hat{p}(1-\\hat{p})}\n\\tag{10.3}\nSince we observed 44 heads we can estimate \\hat{p} as\n\n\\hat p = \\frac{44}{100} = .44\n\\tag{10.4}\nThis answers our first questions. Now we want to quantify our uncertainty.\n\\begin{aligned}\n95\\% \\: \\text{CI for 100 tosses} &= 100 \\: (.44) \\pm 1.96\\sqrt{100(0.44)(1-0.44)} \\\\\n&= 44 \\pm 1.96\\sqrt{(44) (0.56)} \\\\\n&= 44 \\pm 1.96\\sqrt{23.64} \\\\\n&= (34.27,53.73) \\end{aligned}\n\\tag{10.5}\nWe can be 95% confident that 100\\times \\hat{p} \\in [34.3,53.7] We can say that we’re 95% confident that the true probability p \\in (.343, .537).\nIf one were to ask do I think this coin is Fair ? This is a reasonable hypothesis, since 0.5 \\in [.343,.537].\nBut we can also step back and say what does this interval mean when we say we’re 95% confident? Under the frequentist paradigm, we have to think back to our infinite hypothetical sequence of events, were we to repeat this trial an arbitrarily large number of times and each time create a confidence interval, then on average 95% of the intervals we make will contain the true value of p. This makes senses as a long run frequency explanation.\nOn the other hand, we might want to know something about this particular interval. Does this interval contain the true p. What’s the probability that this interval contains a true p? Well, we don’t know for this particular interval. But under the frequentist paradigm, we’re assuming that there is a fixed right answer for p. Either p is in that interval or it’s not in that interval. And so technically, from a frequentist perspective, the probability that p is in this interval is either 0 or 1. This is not a particularly satisfying explanation. In the other hand when we get to the Bayesian approach we will be able to compute an interval and actually say there is probably a p is in this interval is 95% based on a random interpretation of an unknown parameter.\n\n\n\n\n\n\n\nTip\n\n\n\nIn this example of flipping a coin 100 times, observing 44 heads resulted in the following 95% confidence interval for p: (.343, .537). From this, we concluded that it is plausible that the coin may be fair because p=.5 is in the interval.\nSuppose instead that we flipped the coin 100,000 times, observing 44,000 heads (the same percentage of heads as before). Then using the method just presented, the 95% confidence interval for p is (.437, .443). Is it still reasonable to conclude that this is a fair coin with 95% confidence?\nNo Because 0.5 \\not\\in (.437, .443), we must conclude that p=.5 is not a plausible value for the population mean . Observing 100,000 flips increases the power of the experiment, leading to a more precise estimate with a narrower CI, due to the law of large numbers.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Frequentist Inference - M2L4</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-likelihood-function-and-MLE",
    "href": "C1-L04.html#sec-likelihood-function-and-MLE",
    "title": "10  Frequentist Inference - M2L4",
    "section": "10.2 Likelihood function and MLE",
    "text": "10.2 Likelihood function and MLE\n\n\n\n\n\n\n\nFigure 10.2: Likelihood fn and MLE\n\n\n\nExample 10.2 (Heart Attack Patients - MLE) Consider a hospital where 400 patients are admitted over a month for heart attacks, and a month later 72 of them have died and 328 of them have survived.\nwhat’s our estimate of the mortality rate?\n\n\n\n\n\n\nWarningReference Population\n\n\n\nUnder the frequentist paradigm, we must first establish our reference population. This is the cornerstone of our thinking as we are considering how the sample parameter approximates the population statistic. What do we think our reference population is here?\n\nRef Pop 1: Heart attack patients in the region.\nRef Pop 2: Heart attack patients that are admitted to this hospital, but over a longer period.\nRef Pop 3: The people in the region who might have a heart attack and might get admitted to this hospital.\n\nBoth Ref Pop 1 and Ref Pop 2 seem like viable options. Unfortunately, in our data is not a random sample drawn from either. We could pretend they are and move on, or we could also try to think harder about what our data is sampled from, perhaps Ref Pop 3.\nThis is an odd hypothetical situation, and so there are some philosophical issues with the setup of this whole problem within the frequentist paradigm\n\n\n\nY_i \\sim \\mathrm{Bernoulli}(p)\n\\tag{10.6}\nSince this is a Bernoulli trial we need to specify what we interpret as the success . In this case, the success is a mortality.\n\n\\mathbb{P}r(Y_i=1) = \\theta\n\\tag{10.7}\nThe PDF for the dataset can be written in vector form. \\mathbb{P}r(\\vec{Y}=\\vec{y} \\mid \\theta) is the Probability of all the Y’s take some value little y given a value of theta.\n\n\\begin{aligned}\n\\mathbb{P}r(\\vec{Y}=\\vec{y} \\mid \\theta) &= \\mathbb{P}r(Y_1=y,\\dots,Y_n=y_n \\mid \\theta) && \\text{(joint probability)}\\\\\n&= \\mathbb{P}r(Y_1=y_1 \\mid \\theta) \\cdots \\mathbb{P}r(Y_n=y_n \\mid \\theta)            && \\text {(independence)}\\\\\n&= \\prod^n_{i=1} \\mathbb{P}r(Y_i=y_i \\mid \\theta)                            && \\text {(product notation)}\\\\\n&= \\prod^n_{i=1} \\theta^{y_i} (1-\\theta)^{1-y_i}                   && \\text {(Bernoulli PMF)}\n\\end{aligned}\n\\tag{10.8}\nWe now cal the expression for \\mathbb{P}r(\\vec{Y}=\\vec{y} \\mid \\theta) above the likelihood function L(\\theta \\mid \\vec{y} ):\n  \n\\mathcal{L}(\\theta\\mid\\vec{y}) = \\prod^n_{i=1} \\theta^{y_i} (1-\\theta)^{1-y_i}\n\\tag{10.9}\nRecall that we want to find the mortality rate parameter \\theta for our Sample \\vec Y.\nSince it is a probability, it has a range of values from 0 to 1. One way to estimate it is that there should be one value that maximizes (Equation 10.9). It makes the data the most likely to occur for the particular data we observed. This is referred to as the maximum likelihood estimate (MLE). \n\n\\mathop{\\mathrm{MLE}}(\\hat \\theta) = \\mathop{\\mathrm{argmax}} \\; \\mathcal{L}(\\theta\\mid y)\n\\tag{10.10}\nAlthough we are trying to find the \\theta that maximizes the likelihood, in practice, it’s usually easier to maximize the natural logarithm of the likelihood, commonly referred to as the log-likelihood.\n\n\\begin{aligned}\n\\mathcal{L}(\\theta) &= \\log(L(\\theta|\\vec{y}))  && \\\\\n                    &= \\log(\\prod^n_{i=1} {\\theta^{y_i} (1-\\theta)^{1-y_i}})       && \\text{subst. likelihood} \\\\\n                    &= \\sum^n_{i=1}{ \\log(\\theta^{y_i}) + \\log(1-\\theta)^{1-y_i}}  && \\text{log product rule} \\\\\n                    &= \\sum^n_{i=1}{ y_i \\log(\\theta) + (1-y_i) \\log(1-\\theta)}     && \\text{log power rule}\\\\\n                    &= \\log(\\theta) \\sum^n_{i=1}{  y_i + \\log(1-\\theta)} \\sum^n_{i=1}{  (1-y_i) }& & \\text{extracting logs}\n\\end{aligned}\n\\tag{10.11}\nWhat is the interpretation of the MLE of \\theta in the context of the heart attack example?\nIf \\hat \\theta is the MLE for \\theta, the 30-day mortality rate, then all possible values of θ produce a lower value of the likelihood than \\hat \\theta.\nTo calculate the MLE one should differentiate \\mathcal{L}(\\theta) w.r.t. \\theta and then set it equal to 0.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Frequentist Inference - M2L4</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-computing-the-MLE",
    "href": "C1-L04.html#sec-computing-the-MLE",
    "title": "10  Frequentist Inference - M2L4",
    "section": "10.3 Computing the MLE",
    "text": "10.3 Computing the MLE\n\n\n\n\n\n\n\nFigure 10.3: Computing the MLE\n\n\n\n\\begin{aligned}\n   && \\mathcal{L}'(\\theta)=& \\frac{1}{\\theta}\\sum_{i=1}^n y_i-\\frac{1}{1-\\theta}\\sum_{i=1}^n 1-y_i \\stackrel{\\text{set}}{=}0  \\text { set derivative to 0}\n\\\\ & \\implies   & \\frac{1}{\\hat \\theta}\\sum_{i=1}^n y_i & = \\frac{1}{1- \\hat \\theta}\\sum_{i=1}^n 1 - y_i\n\\\\ & \\implies   & (1 -\\hat \\theta) \\sum_{i=1}^n{y_i}    &= \\hat\\theta \\sum_{i=1}^n {1-y_i}  \n\\\\ & \\implies   & 1 \\sum_{i=1}^n{y_i} - \\cancel{ \\hat \\theta \\sum_{i=1}^{n}{y_i}} &= \\hat\\theta \\sum_{i=1}^n {1} - \\cancel{\\hat\\theta \\sum_{i=1}^n {y_i}}  \n\\\\ & \\implies   & \\sum_{i=1}^n{y_i}  &= \\hat\\theta N\n\\\\ & \\implies   &  \\hat \\theta &= \\frac{1}{N} \\sum_{i=1}^n y_i  = \\hat p = \\frac{72}{400}=.18\n\\end{aligned}\n\nMaximum Likelihood Estimates (MLEs) possess the following favorable properties:\n\nUnbiased - Thus given sufficient data the MLE will converge to the true value. As a consequence, MLEs are asymptotically unbiased. As we will see in the examples they can still be biased in finite samples.\nconsistent - One important property of maximum likelihood is that it produces consistent estimates.\ninvariant - The invariance principle states that the MLE is invariant against reparameterization.\n\nusing the Central Limit theorem (see Theorem 104.1).\n\n\\hat \\theta \\pm 1.96\\sqrt\\frac{\\hat \\theta(1-\\hat \\theta)}{n}\n\n\n\\hat \\theta \\simeq \\mathcal{N}(\\theta,\\frac{1}{\\mathcal{I} (\\hat \\theta)})\n\nwhere \\mathcal{I} is the Fischer information which for the Bernoulli distribution is:\n\n\\mathcal{I}( \\hat \\theta) = \\frac{1}{\\theta(1-\\theta)}\n\nNote: The Fischer information is a measure of how much information about theta is in each data point!\n\n\n\n\n\n\nTipExplainable AI (XAI) & Fischer information\n\n\n\nIn XAI we use discuss local and global explanations.\n\nGlobal explanations explain a black box model’s predictions based on each feature, via its parameters.\nLocal explanations explain the prediction of a specific datum from its features.\n\nsince Fischer information quantifies the information in a data point on a parameter we should be able to use it to produce local and perhaps even global explanations for Bayesian models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Frequentist Inference - M2L4</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-computing-the-MLE-examples",
    "href": "C1-L04.html#sec-computing-the-MLE-examples",
    "title": "10  Frequentist Inference - M2L4",
    "section": "10.4 Computing the MLE: examples",
    "text": "10.4 Computing the MLE: examples\nSome more examples of maximum likelihood estimators.\n\n10.4.1 Computing the MLE for Exponential RV\n\n\n\n\n\n\n\nFigure 10.4: computing the MLE for Exponential RV\n\n\nLet’s say X_i are exponential distributed\n\nX_i \\sim \\mathrm{Exp}(\\lambda)\n\nLet’s say the data is independent and identically distributed, therefore making the overall density function\n\n\\begin{aligned}\n  f(x \\mid \\lambda) &= \\prod_{i = 1}^n{\\lambda e^{-\\lambda x_i}} && \\text {(simplifying)} \\\\\n                    &= \\lambda^ne^{-\\lambda \\sum{x_i}}\n\\end{aligned}\n\\tag{10.12}\nNow the likelihood function is\n\n\\mathcal{L}(\\lambda \\mid x)=\\lambda^ne^{-\\lambda \\sum{x_i}}\n\\tag{10.13}\nthe log likelihood is\n\n\\ell(\\lambda) = n\\ln{\\lambda} - \\lambda\\sum_i{x_i}\n\\tag{10.14}\nTaking the derivative\n\n\\begin{aligned}\n  \\ell^\\prime(\\lambda) &= \\frac{n}{\\lambda} - \\sum_i{x_i} \\stackrel{\\text{set}}{=}0 && \\text {(set derivative = 0)} \\\\\n  \\implies \\frac{n}{\\hat{\\lambda}} &= \\sum_i{x_i} && \\text{(rearranging)}\n\\end{aligned}\n\\tag{10.15}\n\n\\hat{\\lambda} = \\frac{n}{\\sum_i{x_i}} = \\frac{1}{\\bar{x}}\n\\tag{10.16}\n\n\n10.4.2 Computing the MLE for Uniform RV\n\n\n\n\n\n\n\nFigure 10.5: computing the MLE for Uniform RV\n\n\n\nX_i \\sim \\mathcal{U}[0, \\theta]\n\\tag{10.17}\n\nf(x \\mid \\theta) = \\prod_{i = 1}^n{\\frac{1}{\\theta}\\mathbb{I}_{0 \\le x_i \\le \\theta}}\n\\tag{10.18}\nCombining all the indicator functions, for this to be a 1, each of these has to be true. These are going to be true if all the observations are bigger than 0, as in the minimum of the x is bigger than or equal to 0. The maximum of the x’s is also less than or equal to \\theta.\n\n\\mathcal{L}(\\theta|x) = \\theta^{-1} \\mathbb{I}_{0\\le min(x_i) \\le max(x_i) \\le \\theta}\n\\tag{10.19}\n\n\\mathcal{L}^\\prime(\\theta) = -n\\theta^{-(n + 1)}\\mathbb{I}_{0 \\le min(x_i) \\le max(x_i)\\le \\theta}\n\\tag{10.20}\nWe ask, can we set this equal to zero and solve for \\theta? It turns out, this is not equal to zero for any \\theta positive value. We need \\theta to be strictly larger than zero. But for \\theta positive, this will always be negative. The derivative is negative, that says this is a decreasing function. Therefore this function will be maximized when we pick \\theta as small as possible. What’s the smallest possible value of \\theta we can pick? Well we need in particular for \\theta to be larger than all of the X_i. And so, the maximum likelihood estimate is the maximum of X_i\n\n\\hat{\\theta} = max(x_i)\n\\tag{10.21}",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Frequentist Inference - M2L4</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-cumulative-distribution-function",
    "href": "C1-L04.html#sec-cumulative-distribution-function",
    "title": "10  Frequentist Inference - M2L4",
    "section": "10.5 Cumulative Distribution Function",
    "text": "10.5 Cumulative Distribution Function\nThe cumulative distribution function (CDF) exists for every distribution. We define it as F(x) = \\mathbb{P}r(X \\le x) for random variable X.\nIf X is discrete-valued, then the CDF is computed with summation F(x) = \\sum_{t = -\\infty}^x {f(t)}. where f(t) = \\mathbb{P}r(X = t) is the probability mass function (PMF) which we’ve already seen.\nIf X is continuous, the CDF is computed with an integral F(x) = \\int_{-\\infty}^x{f(t)dt}\nThe CDF is convenient for calculating probabilities of intervals. Let a and b be any real numbers with a &lt; b. Then the probability that X falls between a and b is equal to \\mathbb{P}r(a &lt; X &lt; b) = \\mathbb{P}r(X \\le b) - \\mathbb{P}r(X \\le a) = F(b) - F(a)\n\n\n\n\n\n\nFigure 10.6: Illustration of using the CDF to calculate the probability of an interval for continuous random variable X. Probability values are represented with shaded regions in the graphs.\n\n\n\n\nExample 10.3 (CDF example 1) Suppose X \\sim \\mathrm{Binomial}(5, 0.6). Then\n\n  \\begin{aligned}\n  F(1) &= \\mathbb{P}r(X \\le 1)\n\\\\ &= \\sum_{−∞}^1 f(t)\n\\\\ &= \\sum_{t=−∞}^{-1} 0 + \\sum_{t=0}^1 {5 \\choose t} 0.6^t (1 − 0.6)^{5−t}\n\\\\ &= {5 \\choose 0} 0.6^0 (1 − 0.6)5−0 +{5 \\choose 1} 0.6^1 (1 − 0.6)^{5−1}\n\\\\ &= (0.4)^5 + 5(0.6)(0.4)^4\n\\\\ &≈ 0.087\n\\end{aligned}\n\\tag{10.22}\n\n\nExample 10.4 (CDF example 1) Example: Suppose Y ∼ Exp(1). Then\n\n\\begin{aligned}\nF(2) &= \\mathbb{P}r(Y \\le 2)\n\\\\ &= \\int^{2}_{−∞} e^{−t}\\mathbb{I}_{(t≥0)} dt\n\\\\ &= \\int^{2}_{0} e^{−t}dt\n\\\\ &= −e^{−t}|^2_0\n\\\\ &= −(e^{−2} − e^0)\n\\\\ &= 1−e^{−2}\n\\\\ &\\approx 0.865\n\\end{aligned}\n\\tag{10.23}",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Frequentist Inference - M2L4</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-quantile-function",
    "href": "C1-L04.html#sec-quantile-function",
    "title": "10  Frequentist Inference - M2L4",
    "section": "10.6 Quantile Function",
    "text": "10.6 Quantile Function\nThe CDF takes a value for a random variable and returns a probability. Suppose instead we start with a number between 0 and 1, which we call p, and we wish to find a value x so that \\mathbb{P}r(X \\le x) = p. The value x which satisfies this equation is called the p quantile. (or 100p percentile) of the distribution of X.\n\nExample 10.5 (Quantile Function example 1) In a standardized test, the 97th percentile of scores among all test-takers is 23. Then 23 is the score you must achieve on the test in order to score higher than 97% of all test-takers. We could equivalently call q = 23 the .97 quantile of the distribution of test scores.\n\n\nExample 10.6 (Quantile Function example 2) The middle 50% of probability mass for a continuous random variable is found between the .25 and .75 quantiles of its distribution. If Z \\sim N(0, 1), then the .25 quantile is −0.674 and the .75 quantile is 0.674. Therefore, \\mathbb{P}r(−0.674 &lt;Z &lt;0.674) = 0.5.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Frequentist Inference - M2L4</span>"
    ]
  },
  {
    "objectID": "C1-L04.html#sec-plotting-the-likelihood-function-in-r",
    "href": "C1-L04.html#sec-plotting-the-likelihood-function-in-r",
    "title": "10  Frequentist Inference - M2L4",
    "section": "11.1 Plotting the likelihood function in R",
    "text": "11.1 Plotting the likelihood function in R\nGoing back to the hospital example\n\nlikelihood = function(n, y, theta) {\n  return(theta^y * (1 - theta)^(n - y))\n}\ntheta = seq(from = 0.01, to = 0.99, by = 0.01)\nplot(theta, likelihood(400, 72, theta))\n\n\n\n\n\n\n\n\nYou can also do this with log likelihoods. This is typically more numerically stable to compute\n\nloglike = function(n, y, theta) {\n  return(y * log(theta) + (n - y) * log(1 - theta))\n}\nplot(theta, loglike(400, 72, theta))\n\n\n\n\n\n\n\n\nHaving these plotted as points makes it difficult to see, let’s plot it as lines\n\nplot(theta, loglike(400, 72, theta), type = \"l\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Frequentist Inference - M2L4</span>"
    ]
  },
  {
    "objectID": "C1-L04-Ex1.html",
    "href": "C1-L04-Ex1.html",
    "title": "11  Frequentist MLE - M2L3HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Frequentist MLE - M2L3HW1</span>"
    ]
  },
  {
    "objectID": "C1-L05.html",
    "href": "C1-L05.html",
    "title": "12  Bayesian Inference - M2L5",
    "section": "",
    "text": "12.1 Inference example: frequentist\nWe’ll start by defining the unknown parameter \\theta, this is either that the coin is fair or it’s a loaded coin.\n\\theta = \\{\\text{fair},\\ \\text{loaded}\\} \\qquad \\text{(parameter)}\n\\tag{12.1}\nwe get to flip it five times but we do not know what kind of coin it is\nX \\sim Bin(5, \\theta) \\qquad \\text{(model)}\n\\tag{12.2}\neach value of theta gives us a competing binomial likelihood:\nf(x\\ mid\\theta) = \\begin{cases}\n      {5 \\choose x}(\\frac{1}{2})^5            & \\theta = \\text{fair}\n\\\\    {5 \\choose x} (.7)^x (.3)^{5 - x}       & \\theta = \\text{loaded}\n   \\end{cases} \\qquad \\text{(likelihood)}\n\\tag{12.3}\nWe can also rewrite the likelihood f(x \\mid \\theta) using indicator functions\nf(x\\mid\\theta) = {5\\choose x}(.5)^5\\mathbb{I}_{\\{\\theta= \\text{fair}\\}} + {5 \\choose x}(.7)^x(.3)^{5 - x}\\mathbb{I}_{\\{\\theta = \\text{loaded}\\}} \\qquad \\text{(likelihood)}\n\\tag{12.4}\nIn this case, we observed that x = 2\nf(\\theta \\mid x = 2) = \\begin{cases}\n    0.3125 & \\theta = \\text{fair} \\\\\n    0.1323 & \\theta = \\text{loaded}\n\\end{cases} \\qquad \\text{(sub. x=2)}\n\\tag{12.5}\n\\therefore \\hat{\\theta} = \\text{fair} MLE\n\\tag{12.6}\nThat’s a good point estimate, but then how do we answer the question, how sure are you?\nThis is not a question that’s easily answered in the frequentest paradigm. Another question is that we might like to know what is the probability that theta equals fair, give, we observe two heads.\n\\mathbb{P}r(\\theta = \\text{fair} \\mid x = 2) = ?\n\\tag{12.7}\nIn the frequentest paradigm, the coin is a physical quantity. It’s a fixed coin, and therefore it has a fixed probability of coining up heads. It is either the fair coin, or it’s the loaded coin.\n\\mathbb{P}r(\\theta = \\text{fair}) = \\{0,1\\}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - M2L5</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-inference-example-frequentist",
    "href": "C1-L05.html#sec-inference-example-frequentist",
    "title": "12  Bayesian Inference - M2L5",
    "section": "",
    "text": "Figure 12.1: coin probability inference\n\n\n\nExample 12.1 (Two Coin Example) Suppose your brother has a coin that you know to be loaded so that it comes up heads 70% of the time. He then comes to you with some coin, you’re not sure which one and he wants to make a bet with you. Betting money that it’s going to come up heads.\nYou’re not sure if it’s the loaded coin or if it’s just a fair one. So he gives you a chance to flip it 5 times to check it out.\nYou flip it five times and get 2 heads and 3 tails.\nWhich coin do you think it is and how sure are you about that?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - M2L5</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-bayesian-approach-to-the-problem",
    "href": "C1-L05.html#sec-bayesian-approach-to-the-problem",
    "title": "12  Bayesian Inference - M2L5",
    "section": "12.2 Bayesian Approach to the Problem",
    "text": "12.2 Bayesian Approach to the Problem\n\n\n\n\n\n\n\nFigure 12.2: Bayesian coin probability inference\n\n\nAn advantage of the Bayesian approach is that it allows you to easily incorporate prior information when you know something in advance of looking at the data. This is difficult to do under the frequentist paradigm.\nIn this case, we’re talking about your brother. You probably know him pretty well. So suppose you think that before you’ve looked at the coin, there’s a 60% probability that this is the loaded coin.\nIn this case, we put this into our prior. Our prior belief is that the probability the coin is loaded is 0.6. We can update our prior beliefs with the data to get our posterior beliefs, and we can do this using the Bayes theorem.\n\n\\begin{aligned}\n  \\mathbb{P}r(\\text{loaded}) &= 0.6\\ && \\text{(prior)}\n\\\\ f(\\theta \\mid x) &= \\frac{f(x \\mid \\theta)f(\\theta)}{\\sum_\\theta{f(x \\mid \\theta)f(\\theta)}} && \\text{(Bayes)}\n\\\\ f(\\theta\\mid x=2)&= \\frac{{5\\choose x} \\left [(\\frac{1}{2})^5(1-0.6)\\ \\mathbb{I}_{(\\theta = \\text{fair})} + (.7)^x (.3)^{5-x}(.6)\\ \\mathbb{I}_{(\\theta = \\text{loaded})}  \\right] } {{5\\choose x} \\left [(\\frac{1}{2})^5(.4) + (.7)^x (.3)^{5-x}(0.6)  \\right] }&& \\text{(sub. x=2)}\n\\\\ &= \\frac{0.0125\\ \\mathbb{I}_{(\\theta = \\text{fair})}  + 0.0079\\ \\mathbb{I}_{(\\theta = \\text{loaded})} }{0.0125+0.0079}&& \\text{(normalize)}\n\\\\ &= \\textbf{0.612}\\ \\mathbb{I}_{(\\theta=\\text{fair})} + 0.388\\ \\mathbb{I}_{(\\theta = \\text{loaded})} && \\text{(MLE)}\n\\end{aligned}\n\\tag{12.8}\nAs you can see in the calculation Equation 12.8, we have the likelihood times the prior in the numerator, and a normalizing constant in the denominator. When we divide the two, we’ll get an answer that adds up to 1. These numbers match exactly in this case because it’s a very simple problem.\nThis is a concept that we will revisit — what’s in the denominator here is always a normalizing constant.\n\n\\mathbb{P}r(\\theta = loaded \\mid x = 2) = 0.388\n\nThis here updates our beliefs after seeing some data about what the probability might be.\nWe can also examine what would happen under different choices of prior.\n\n\\mathbb{P}r(\\theta = loaded) = \\frac{1}{2} \\implies \\mathbb{P}r(\\theta = loaded \\mid x = 2) = 0.297\n\n\n\\mathbb{P}r(\\theta = loaded) = 0.9 \\implies \\mathbb{P}r(\\theta = loaded \\mid x = 2) = 0.792\n\nIn this case, the Bayesian approach is inherently subjective. It represents your perspective, and this is an important part of the paradigm. If you have a different perspective, you will get different answers, and that’s okay. It’s all done in a mathematically vigorous framework, and it’s all mathematically consistent and coherent.\nAnd in the end, we get interpretable results.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - M2L5</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-continuous-version-of-bayes-theorem",
    "href": "C1-L05.html#sec-continuous-version-of-bayes-theorem",
    "title": "12  Bayesian Inference - M2L5",
    "section": "12.3 Continuous version of Bayes’ theorem",
    "text": "12.3 Continuous version of Bayes’ theorem\n\n\n\n\n\n\n\nFigure 12.3: Continuous version of Bayes’ theorem",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - M2L5</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-posterior-intervals",
    "href": "C1-L05.html#sec-posterior-intervals",
    "title": "12  Bayesian Inference - M2L5",
    "section": "12.4 Posterior Intervals",
    "text": "12.4 Posterior Intervals\n\n\n\n\n\n\n\nFigure 12.4: Posterior Intervals",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - M2L5</span>"
    ]
  },
  {
    "objectID": "C1-L05.html#sec-discussion-cis",
    "href": "C1-L05.html#sec-discussion-cis",
    "title": "12  Bayesian Inference - M2L5",
    "section": "12.5 Discussion CIs",
    "text": "12.5 Discussion CIs\n\nFrequentist confidence intervals have the interpretation that “If you were to repeat many times the process of collecting data and computing a 95% confidence interval, then on average about 95% of those intervals would contain the true parameter value; however, once you observe data and compute an interval the true value is either in the interval or it is not, but you can’t tell which.”\nBayesian credible intervals have the interpretation that “Your posterior probability that the parameter is in a 95% credible interval is 95%.”\nBayesian intervals treat their bounds as fixed and the estimated parameter as a random variable.\nFrequentist confidence intervals treat their bounds as random variables and the parameter as a fixed value.\n\n\n\n\n\n\n\nNoteDiscussion Under what circumstances would you prefer a frequentist CI or a Bayesian CI?\n\n\n\n12.5.1 Focusing on Bayesian / Frequentist paradigms\n\nA Frequentist CI might be preferred if:\n\nI had plenty of data to support a frequentist construction of frequentist CI and\nI was doing research and refining or refuting a result that has been established using frequentist hypothesis testing.\n\nI would want to show that for H_1 against some null hypothesis, H_0 the parameters have a certain p-value for some effect.\nParticularly when we are interested in the inference and are less interested in using the value of the parameter.\n\nI cannot justify introducing some subjective priors.\n\nA Bayesian CI might be better if:\n\nMy dataset is too small.\nWhat I care about is the parameter’s value and less about hypothesis testing\nI need an estimate of uncertainty for the parameter for the inference it is used in.\nI had subjective reasons to introduce a prior:\n\nI know about constraints\nI have access to expert knowledge\n\nI wish to introduce pooling between groups, to share information for reducing uncertainty.\nMy results are in a Bayesian-oriented domain.\n\n\n\n\n\n\n\n\n\n\nNoteDiscussion: Under what circumstances would you prefer a frequentist CI or a Bayesian CI?\n\n\n\n12.5.2 Focusing on the CI choices\nLet’s point out that this is what we call a loaded question, as it has a bias against the frequentist approach by stating one of its shortcomings when it is still possible to get a point estimate for the parameter and compare it to the CI. Typically one will have already done it say using regression before considering the CI.\nNext, we have all the standard reasons for choosing between the Frequentist and the Bayesian paradigms. I could list them but I don’t think that is the real point of this question, but rather what would we prefer if both were viable options and why?\nCIs are primarily a tool for understanding uncertainties about parameters that encode effects. In the parametric Bayesian approach we are learning the distribution of our parameters so they have uncertainties baked into them. In the Frequentist approach, we look for the least squares point estimates for our parameters and consider using the CI to approximate the long-run uncertainty due to sampling.\nFrequentist CI might be preferable if I am worried about Aletoric uncertainty due to sampling i.e. to what degree can I be certain my experimental outcomes are not due to chance? I would feel this way since I am a classical physicist or a botanist studying a predominately deterministic effect and I see errors in estimating the parameters as artifacts of sampling that can be made smaller till the parameters will converge with the true population statistics and the error will become vanishingly small.\nGiven that I did my best to get a good data sample I just need to check how sure to decide the cardinal question do I publish or do I perish? I need to decide that the result is due to the effect and not due to some conspiracy bad samples.\nBayesian CIs are just a result of using Bayesian analysis which is a requirement to investigate what are predominately random effects that are the domain of quantum physicists, an ecologist, or a geneticist. Since almost everything I study is predominantly random and I need random variables and Bayes law to get to my results. I also need to report confidence intervals for my work when I publish - but if one is a Bayesian, one will use a Bayesian credible interval?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Bayesian Inference - M2L5</span>"
    ]
  },
  {
    "objectID": "C1-L05-Ex1.html",
    "href": "C1-L05-Ex1.html",
    "title": "13  Homework on Likelihoods and MLEs - M2L5HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Homework on Likelihoods and MLEs - M2L5HW1</span>"
    ]
  },
  {
    "objectID": "C1-L05-Ex2.html",
    "href": "C1-L05-Ex2.html",
    "title": "14  Homework on Bayesian Inference - M2L5HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Homework on Bayesian Inference - M2L5HW2</span>"
    ]
  },
  {
    "objectID": "C1-L06.html",
    "href": "C1-L06.html",
    "title": "15  Priors - M3L6",
    "section": "",
    "text": "15.1 Priors and prior predictive distributions\nIn this section, we will delve more deeply into choices of Priors and how they influence Bayesian CI by developing the prior predictive (Definition 15.1) and posterior predictive (Definition 15.2) intervals.\nTheoretically, we’re defining a cumulative distribution function for the parameter\n\\mathbb{P}r(\\theta \\le c) \\qquad \\forall c \\in \\mathbb{R}\nWe need to do this for an infinite number of possible sets but it isn’t practical to do, and it would be very difficult to do it coherently so that all the probabilities were consistent. Therefore in practice, we tend to work with a convenient family that is flexible enough for members to represent our beliefs.\nGenerally if one has enough data, the information in the data will overwhelm the information in the prior. This makes it seem like the prior is less important in terms of the form and substance of the posterior. Once the prior is overwhelmed, any reasonable choice of prior will lead to approximately the same posterior. This is a point where the Bayesian approach should converge to the frequentist and can be shown to be more or less objective.\nOn the other hand choices of priors can be important because even with masses of data, groups and items can be distributed very sparsely in which case priors can have a lasting impact on the posteriors. Secondly, we can decide to pick priors that have a long-lasting impact on operating as regularizing constraints within our models. In such cases, the impact of the prior can be significant.\nOne of our guiding questions will be to consider how much information the prior and the data contribute to the posterior. We will consider the effective sample size of different priors.\nFinally, a bad choice of priors can lead to specific issues.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Priors - M3L6</span>"
    ]
  },
  {
    "objectID": "C1-L06.html#sec-priors-and-prior-predictive-distributions",
    "href": "C1-L06.html#sec-priors-and-prior-predictive-distributions",
    "title": "15  Priors - M3L6",
    "section": "",
    "text": "ImportantChoosing a prior\n\n\n\nHow should we choose a prior?\n\nOur prior needs to represent our perspectives, beliefs, and our uncertainties.\nIt should encode any constraints on the data or parameters.\n\nage is positive and less than 120\n\nIt can regularize the data\nIt could encode expert knowledge we have elicited from domain experts.\nIt should prefer informative priors over uninformative ones.\n\n\n\n\n\n\n\n\n\n\n\nExample 15.1 (Example of Bad Prior) Suppose we chose a prior that says the probability of \\mathbb{P}r(\\theta = \\frac{1}{2}) = \\delta( \\frac{1}{2})= 1\nAnd thus, the probability of \\theta equaling any other value is 0. If we do this, our data won’t make a difference since we only put a probability of 1 at a single point.\n\nf(\\theta \\mid y) \\propto f(y \\mid\\theta)f(\\theta) = f(\\theta) = \\delta(\\theta)\n\\tag{15.1}\n\n\n\n\n\n\n\nCautionAvoid priors that assign 0 or 1\n\n\n\n\nEvents with a prior probability of 0 will always have a posterior probability of 0 because f(\\theta)=0 in (Equation 15.1) the product will and therefore the posterior be 0\nEvents with a prior probability of 1, will always have a posterior probability of 1. This is a little harder to see. In this case f(\\theta^c)=0 in (Equation 15.1) so that the posterior will again be zero elsewhere.\n\n\n\n\nIt is good practice to avoid assigning a probability of 0 or 1 to any event that has already occurred or is already known not to occur.\nIf the priors avoid 0 and 1 values the information within the data will eventually overwhelm the information within the prior.\n\n\n15.1.1 Calibration - making priors precise\n\n\nQ. How do we calibrate our prior probability to reality?\nCalibration of predictive intervals is a useful concept in terms of choosing priors. If we make an interval where we’re saying we predict 95% of new data points will occur in this interval. It would be good if, in reality, 95% of new data points did fall in that interval. This is a frequentist concept but this is important for practical statistical purposes so that our results reflect reality.\n\n\n\n\n\n\n\nFigure 15.1: Prior Predictive Distribution\n\n\nWe can compute a predictive interval. This is an interval such that 95% of new observations are expected to fall into it. It’s an interval for the data rather than an interval for \\theta\n\nDefinition 15.1 (Prior Predictive Distribution) The prior predictive distribution expresses our uncertainty about a parameter, i.e. the distribution of its possible values before we observe any data.\n\n\\begin{aligned}\nf(y) &= \\int{f(y \\mid\\theta)f(\\theta)d\\theta} &&\\text {by Bayes theorem}\n\\\\&= \\int{f(y, \\theta)d\\theta} && \\text{the joint probability}\n\\end{aligned}\n\\tag{15.2}\n\nf(y,\\theta) is the joint density of y and \\theta.\nIf we are integrating out \\theta, we will end up with a marginalized probability distribution of the data.\nHowever, we may well decide to not integrate out \\theta completely, so we will end up with a predictive interval.\nBut no data y has been observed, so this is the prior predictive before any data is observed.\nIt is used in prior predictive checks to assess whether the choice of prior distribution captures our prior beliefs.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Priors - M3L6</span>"
    ]
  },
  {
    "objectID": "C1-L06.html#sec-prior-predictive-binomial-example",
    "href": "C1-L06.html#sec-prior-predictive-binomial-example",
    "title": "15  Priors - M3L6",
    "section": "15.2 Prior Predictive: Binomial Example",
    "text": "15.2 Prior Predictive: Binomial Example\n\n\n\n\n\n\n\nFigure 15.2: Prior Predictive Distribution Binomial Example\n\n\nSuppose we’re going to flip a coin 10 times and count the number of heads we see. But we are thinking about this in advance of actually doing it, and we are interested in the predictive distribution\n\n\nQ. How many heads do we predict we’re going to see?\nQ. What’s the probability that it shows up heads?\nSo, we’ll need to choose a prior.\n\nN=10 \\qquad \\text {number of coin flips}\n\nWhere Y_i represents individual coin flips. with Head being a success\n\nY \\sim \\text{Bernoulli}(\\theta)\n\nOur data is the count of successes (heads) in N flips.\n\nX = \\sum_{i=0}^N Y_i \\qquad\n\nIf we think that all possible coins or all possible probabilities are equally likely, then we can put a prior for \\theta that’s flat over the interval from 0 to 1. That is the Uniform prior (Equation 7.17):\n\nf(\\theta)=\\mathbb{I}_{[0 \\le \\theta \\le 1]}\n\nThe predictive probability is a binomial likelihood times the prior = 1\n\nf(x) = \\int f(x \\mid\\theta) f(\\theta) d\\theta = \\int_0^1 \\frac{10!}{x!(10-x)!} \\theta^x(1-\\theta)^{10-x}(1) d \\theta\n\nNote that because we’re interested in X at the end, it’s important that we distinguish between a Binomial density and a Bernoulli density. Here we just care about the total count rather than the exact ordering which would be Bernoulli.\nFor most of the analyses, we’re doing, where we’re interested in \\theta rather than x, the binomial and the Bernoulli are interchangeable because the part in here that depends on \\theta is the same.\nTo solve this integral let us recall that:\n\nn! =\\Gamma(n+1)\n\\tag{15.3}\nand\n\nZ \\sim \\text{Beta}(\\alpha,\\beta)\n\nThe PDF for the beta distribution is given as:\n\nf(z)= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} z^{\\alpha−1}(1−z)^{\\beta−1}I_{(0 &lt; z &lt;1)}\n\nwhere \\alpha&gt;0 and \\beta&gt;0.\n\n\\begin{aligned}\n  f(x) &= \\int f(x \\mid\\theta) f(\\theta) d\\theta && \\text {prior predictive dfn}\n\\\\ &= \\int_0^1 \\frac{10!}{x!(10-x)!} \\theta^x(1-\\theta)^{10-x}( \\mathbb{I_{[0,1]}}) d \\theta && \\text {subst. Binomial, } \\mathbb{I_{[0,1]}}\n\\\\ &= \\int_0^1 \\frac{\\Gamma(11)}{\\Gamma(x+1)\\Gamma(11-x)} \\theta^{(x+1)-1}(1-\\theta)^{(11-x)-1}(1) d \\theta && \\text {convert to Beta(x+1,11-x), }\n\\\\ &=\\frac{\\Gamma(11)}{\\Gamma(12)}\n\\cancel{\n  \\int_0^1 \\frac{\\Gamma(12)}{\\Gamma(x+1)\\Gamma(11-x)}\\theta^{(x+1)-1}(1-\\theta)^{(11-x)-1}(1)d \\theta\n} && \\text {integrating PDF=1 }\n\\\\ &=\\frac{\\Gamma(11)}{\\Gamma(12)} \\times 1\n= \\frac{10!}{11!}\n=\\frac{1}{11} && \\forall x \\in \\{1,2,\\dots,10\\}\n\\end{aligned}\n\nThus we see that if we start with a uniform prior, we then end up with a discrete uniform predictive density for X. If all possible \\theta probabilities are equally likely, then all possible sums X outcomes are equally likely.\nThe integral above is a beta density, all integrals of valid beta densities equal one.\n\nf(x) = \\frac{\\Gamma(11)}{\\Gamma(12)} = \\frac{10!}{11!} = \\frac{1}{11}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Priors - M3L6</span>"
    ]
  },
  {
    "objectID": "C1-L06.html#posterior-predictive-distribution",
    "href": "C1-L06.html#posterior-predictive-distribution",
    "title": "15  Priors - M3L6",
    "section": "15.3 Posterior Predictive Distribution",
    "text": "15.3 Posterior Predictive Distribution\n\n\n\n\nPosterior Predictive Distribution\n\nWhat about after we’ve observed data? What’s our posterior predictive distribution?\nGoing from the previous example, let us observe after one flip that we got a head.\nWe want to ask, what’s our predictive distribution for the second flip, given we saw a head on the first flip?",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Priors - M3L6</span>"
    ]
  },
  {
    "objectID": "C1-L06.html#posterior-predictive-distribution-1",
    "href": "C1-L06.html#posterior-predictive-distribution-1",
    "title": "15  Priors - M3L6",
    "section": "15.4 Posterior Predictive Distribution",
    "text": "15.4 Posterior Predictive Distribution\nThe posterior predictive distribution is produced analogously to the posterior predictive distribution by marginalizing the posterior with respect to the parameter.\n\nDefinition 15.2 (Posterior Predictive Distribution) \n\\begin{aligned}\nf(y_2 \\mid y_1) &= \\text{likelihood}\\times \\text{posterior} \\\\\n&= \\int{f(y_2 \\mid \\theta,y_1) \\; f(\\theta \\mid y_1)}d\\theta\n\\end{aligned}\n\\tag{15.4}\n\n\n\n\n\n\n\nTipMarginalizing distribution\n\n\n\nSuppose we have an experiment with events based on two RVs: - (C) a coin toss - (D) and a dice toss. And we call this event X = \\mathbb{P}r(C,D) = \\mathbb{P}r(C) \\times \\mathbb{P}r(D)\n\n\n\nC D\n1\n2\n3\n4\n5\n6\n\\mathbb{P}r(C)\n\n\n\n\nH\n1/12\n1/12\n1/12\n1/12\n1/12\n1/12\n6/12\n\n\nT\n1/12\n1/12\n1/12\n1/12\n1/12\n1/12\n6/12\n\n\n\\mathbb{P}r(D)\n2/12\n2/12\n2/12\n2/12\n2/12\n2/12\n1\n\n\n\nWe can recover the \\mathbb{P}r(C) coin’s distribution or the dice distribution \\mathbb{P}r(D) by marginalization. \\mathbb{P}r(X) This is done by summing over the row or columns.\nThe marginal distribution let us subset a joint distribution. The marginal distribution has removed the uncertainty due to a parameter.\nwe use three terms interchangeably :\n\nmarginalizing the posterior w.r.t. \\theta\nintegrating/summing over \\theta\nintegrating \\theta out\n\nThe first is the real idea, the others are the techniques being used to do it. For a predictive distribution we may want to marginalize all the parameters so we end up with the RV we wish to predict.\n\n\nWe’re going to assume that Y_2 is independent of Y_1. Therefore,\n\nf(y_2 \\mid y_1) = \\int{f(y_2 \\mid \\theta)f(\\theta \\mid y_1)d\\theta}\n\nSuppose we’re thinking of a uniform distribution for \\theta and we observe the first flip is a “head”. What do we predict for the second flip?\nThis is no longer going to be a uniform distribution like it was before because we have some data. We’re going to think it’s more likely that we’re going to get a second head. We think this because since we observed a head \\theta is now likely to be at least \\frac{1}{2} possibly larger.\n\nf(y_2 \\mid Y_1 = 1) = \\int_0^1{\\theta^{y_2}(1-\\theta)^{1-y_2}2\\theta d\\theta}\n\n\nf(y_2 \\mid Y_1 = 1) = \\int_0^1{2\\theta^{y_2 + 1}(1-\\theta)^{1-y_2}d\\theta}\n\nWe could work this out in a more general form, but in this case, Y_2 has to take the value 0 or 1. The next flip is either going to be heads or tails so it’s easier to just plop in a particular example.\n\n\\mathbb{P}r(Y_2 = 1 \\mid Y_1 = 1) = \\int_\\theta^1 {2 \\theta^2 d \\theta} = \\frac{2}{3}\n\n\n\\mathbb{P}r(Y_2 = 0 \\mid Y_1 = 1) = 1 - \\mathbb{P}r(Y_2 = 1 \\mid Y_1 = 1) = 1 - \\frac{2}{3} = \\frac{1}{3}\n\nWe can see here that the posterior is a combination of the information in the prior and the information in the data. In this case, our prior is like having two data points, one head and one tail.\nSaying we have a uniform prior for \\theta is equivalent in an information sense to saying “we have observed one ‘Head’ and one ‘Tail’”.\nSo then when we observe one head, it’s like we now have seen two heads and one tail. So our predictive distribution for the second flip says if we have two heads and one tail, then we have a \\frac{2}{3} probability of getting another head and a \\frac{1}{3} probability of getting another tail.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Priors - M3L6</span>"
    ]
  },
  {
    "objectID": "C1-L06-Ex1.html",
    "href": "C1-L06-Ex1.html",
    "title": "16  Homework Posterior Probabilities - M3L6HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Homework Posterior Probabilities - M3L6HW1</span>"
    ]
  },
  {
    "objectID": "C1-L07.html",
    "href": "C1-L07.html",
    "title": "17  M3L7 - Binomial Data",
    "section": "",
    "text": "17.1 Bernoulli/Binomial likelihood with a uniform prior\nWhen we use a uniform prior for a Bernoulli likelihood, we get a beta posterior.\nThe Bernoulli likelihood of \\vec Y \\mid \\theta is\n{\\color{green}f(\\vec Y \\mid \\theta) = {\\theta^{\\sum{y_i}}(1-\\theta)^{n - \\sum{y_i}}}} \\qquad \\text{Bernoulli Likelihood}\nOur prior for \\theta is just a Uniform distribution\n{\\color{red}f(\\theta) = I_{\\{0 \\le \\theta \\le 1\\}} }\\qquad \\text {Uniform prior}\nThus our posterior for \\theta is \n\\begin{aligned}\nf(\\theta \\mid y) & = \\frac{f(y \\mid \\theta) f(\\theta)}{\\int f(y \\mid \\theta)f(\\theta) \\, d\\theta} & \\text{Bayes law} \\\\\n& = \\frac{\\theta^{\\sum{y_i}} (1 - \\theta)^{n - \\sum{y_i}} \\mathbb{I}_{\\{0 \\le \\theta \\le 1\\}}}{\\int_0^1 \\theta^{\\sum{y_i}}(1 - \\theta)^{n - \\sum{y_i}} \\mathbb{I}_{\\{0 \\le \\theta \\le 1\\}} \\, d\\theta} & \\text{subst. Likelihood \\& Prior} \\\\\n& = \\frac{\\theta^{\\sum{y_i}} (1-\\theta)^{n - \\sum{y_i}} \\mathbb{I}_{\\{0 \\le \\theta \\le 1\\}}}{\\frac{\\Gamma(\\sum{y_i} + 1)\\Gamma(n - \\sum{y_i} + 1)}{\\Gamma(n + 2)} \\cancel{\\int_0^1 \\frac{\\Gamma(n + 2)}{\\Gamma(\\sum{y_i} + 1) \\Gamma(n - \\sum{y_i} + 1)} \\theta^{\\sum{y_i}} (1 - \\theta)^{n - \\sum{y_i}} \\, d\\theta}} & \\text{Beta PDF integrates to 1} \\\\\n& = \\frac{\\Gamma(n + 2)}{\\Gamma(\\sum{y_i}+ 1) \\Gamma(n - \\sum{y_i}+ 1)} \\theta^{\\sum{y_i}}(1 - \\theta)^{n - \\sum{y_i}} \\mathbb{I}_{\\{0 \\le \\theta \\le 1\\}} & \\text{simplifying} \\\\\n& = \\mathrm{Beta} \\left (\\sum{y_i} + 1, n - \\sum{y_i} + 1 \\right )\n\\end{aligned}\nWhere we used a trick of recognizing the denominator as a Beta distribution (Equation 101.7) we then manipulate it to take the exact form of Beta. We can then cancel it since the Beta density integrates to 1, we can simplify this as From here we can see that the posterior follows a Beta distribution\n\\theta \\mid y \\sim \\mathrm{Beta}\\left (\\sum{y_i} + 1, n - \\sum{y_i} + 1 \\right )",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>M3L7 - Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07.html#bernoullibinomial-likelihood-with-a-uniform-prior",
    "href": "C1-L07.html#bernoullibinomial-likelihood-with-a-uniform-prior",
    "title": "17  M3L7 - Binomial Data",
    "section": "",
    "text": "Figure 17.1: Binomial likelihood with a Uniform prior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.2: R.A. Fisher\n\n\n\n\n\n\n\n\nTipHistorical Note on Sir Ronald Aylmer Fisher FRS\n\n\n\nR.A. Fisher’s objection to the Bayesian approach is that “The theory of inverse probability is founded upon an error, and must be wholly rejected” (Fisher 1925) was specifically referring to this example of a”Binomial with a Uniform prior”. The gist of it is that the posterior depends on the parametrization of the prior.(Aldrich 2008). Sir Harold Jeffreys FRS who corresponded with Fisher went on to develop his eponymous priors which were invariant to reparametrization. Which we will consider in Section 26.2",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>M3L7 - Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07.html#sec-conjugate-beta-priors",
    "href": "C1-L07.html#sec-conjugate-beta-priors",
    "title": "17  M3L7 - Binomial Data",
    "section": "17.2 Conjugate Priors",
    "text": "17.2 Conjugate Priors\n\n\n\n\n\n\n\nFigure 17.3: Conjugate Priors\n\n\nThe Uniform distribution is \\mathrm{Beta}(1, 1)\nAny beta distribution is conjugate for the Bernoulli distribution. Any beta prior will give a beta posterior.\n\nf(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta -1}\\mathbb{I}_{\\{\\theta \\le \\theta \\le 1\\}}\n\n\nf(\\theta \\mid y) \\propto f(y \\mid \\theta)f(\\theta) = \\theta^{\\sum{y_i}}(1-\\theta)^{n - \\sum{y_i}}\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\\mathbb{I}_{\\{\\theta \\le \\theta \\le 1\\}}\n\n\nf(y \\mid\\theta)f(\\theta) \\propto \\theta^{\\alpha + \\sum{y_i}-1}(1-\\theta)^{\\beta + n - \\sum{y_i} - 1}\n\nThus we see that this is a beta distribution\n\n\\theta \\mid y \\sim \\mathrm{Beta}(\\alpha + \\sum{y_i}, \\beta + n - \\sum{y_i})\n\nWhen \\alpha and \\beta are one like in the uniform distribution, then we get the same result as earlier.\nThis whole process where we choose a particular form of prior that works with a likelihood is called using a conjugate family.\nA family of distributions is referred to as conjugate if when you use a member of that family as a prior, you get another member of that family as your posterior.\nThe beta distribution is conjugate for the Bernoulli distribution. It’s also conjugate for the binomial distribution. The only difference in the binomial likelihood is that there is a combinatorics term. Since that does not depend on \\theta, we get the same posterior.\n We often use conjugate priors because they make life much simpler, sticking to conjugate families allows us to get closed-form solutions easily.\nIf the family is flexible enough, then you can find a member of that family that closely represents your beliefs.\n\nthe Uniform distribution can be written as the \\mathrm{Beta}(1,1) prior.\nAny Beta prior will result in a Beta posterior.\nBeta is conjugate for Binomial and for Bernoulli",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>M3L7 - Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07.html#sec-effective-sample-size",
    "href": "C1-L07.html#sec-effective-sample-size",
    "title": "17  M3L7 - Binomial Data",
    "section": "17.3 Posterior mean and effective sample size",
    "text": "17.3 Posterior mean and effective sample size\n\n\n\n\n\n\n\nFigure 17.4: Effective Sample Size\n\n\nReturning to the Beta posterior model it is clear how both the prior and the data contribute to the posterior.\nFor a prior \\mathrm{Beta}(\\alpha,\\beta) we can say that the effective sample size of the prior is\n\n\\alpha + \\beta \\qquad \\text {(ESS)}\n\\tag{17.1}\nRecall that the expected value or mean of a Beta distribution is \\frac{\\alpha}{\\alpha + \\beta}\nTherefore we can derive the posterior mean as\n\n\\begin{aligned}\n   posterior_{mean} &= \\frac{\\alpha + \\sum{y_i}}{\\alpha + \\sum{y_i}+\\beta + n - \\sum{y_i}}\n\\\\                  &= \\frac{\\alpha+\\sum{y_i}}{\\alpha + \\beta + n}\n\\\\                  &= \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\frac{\\alpha}{\\alpha + \\beta} + \\frac{n}{\\alpha + \\beta + n}\\frac{\\sum{y_i}}{n}\n\\\\ &= (\\text{prior weight} \\times \\text{prior mean}) + (\\text{data weight} \\times \\text{data mean})\n\\end{aligned}\n\\tag{17.2}\ni.e. The posterior mean is a weighted average of the prior mean and the data mean.\nThis effective sample size gives you an idea of how much data you would need to make sure that your prior does not have much influence on your posterior.\nIf \\alpha + \\beta is small compared to n then the posterior will largely just be driven by the data. If \\alpha + \\beta is large relative to n then the posterior will be largely driven by the prior.\nWe can make a 95% credible interval using our posterior distribution for \\theta . We can find an interval that has 95 \\% probability of containing \\theta.\n Using Bayesian Statistics we can do sequential analysis by doing a sequential update every time we get new data. We can get a new posterior, and we just use our previous Posterior as a Prior for doing another update using Bayes’ theorem.\n\n\n\n\n\n\n\nFigure 17.5: ESS algorithms\n\n\n\nfor a Beta prior, its effective sample size is a + b\nif n &gt;&gt; \\alpha+\\beta the posterior will be predominantly determined by the prior\nif n &lt;&lt; \\alpha+\\beta the posterior will be predominantly determined by the data\nthe idea of an effective sample size of the prior is a useful concept to work with.\n(Wiesenfarth and Calderazzo 2020)\n\nEffective Sample Size (ESS)\nEffective Current Sample size (ECSS)\nwith (Morita, Thall, and Müller 2008) on the left and ECSS on the right\n\n\n\n\n\n\n\n\n\nFigure 17.6: 200 year meteorological record\n\n\n\n\n\n\n\n\nFigure 17.7: 800K year ice core record\n\n\n\n\n\n\n\n\nFigure 17.8: 5M year deep sea record\n\n\n\n\n\nExercise 17.1 (Discussion on Prior elicitation)  \n\nSuppose we are interested in global temperatures, and that we have a summary measure that represents the average global temperature for each year. Now we could ask “What is the probability that next year will have a higher average global temperature than this year?” What would be your choice of prior and why? Be specific about the distribution and its parameters. You may use any other information that you want to bring into this problem.\n\n\n\n\n\n\n\n\nTipResponse\n\n\n\n\n\nIt is possible to get historical estimates using:\n\nMeteorological and satellites for the last 200 years. Figure 17.6\nIce cores for the last 800,000 years Figure 17.7\nDeep sea sediment oxygen 18 isotope fractation for the last 5 million years. Figure 17.8\n\nor yearly temperature data from 1850 till today based on meteorological readings. We can also consider Greenland ice core data covering 800,000 years.\nOne simple way is to model the yearly temperature as a random walk\ni.e. Each year is a Bernoulli trial where success is the temperature getting warmer. We can then use the historical data since 1800 to estimate theta the probability that we get warmer.\nI suppose we can use a Binomial prior with parameters for alpha the count of years the temperature increased and N for the total number of years and p the probability the a given year is hotter than the previous.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>M3L7 - Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07.html#data-analysis-example-in-r",
    "href": "C1-L07.html#data-analysis-example-in-r",
    "title": "17  M3L7 - Binomial Data",
    "section": "17.4 Data Analysis Example in R",
    "text": "17.4 Data Analysis Example in R\nSuppose we’re giving two students a multiple-choice exam with 40 questions, where each question has four choices. We don’t know how much the students have studied for this exam, but we think that they’ll do better than just guessing randomly\n\nWhat are the parameters of interest?\n\nThe parameters of interests are \\theta_1 = true the probability that the first student will answer a question correctly, \\theta_2 = true the probability that the second student will answer a question correctly.\n\nWhat is our likelihood?\n\nThe likelihood is \\mathrm{Binomial}(40, \\theta) if we assume that each question is independent and that the probability a student gets each question right is the same for all questions for that student.\n\nWhat prior should we use?\n\nThe Conjugate Prior is a Beta Distribution. We can plot the density with dbeta\n\ntheta = seq(from = 0, to = 1, by = 0.1)\n# Uniform\nplot(theta, dbeta(theta, 1, 1), type = 'l')\n\n\n\n\n\n\n\n# Prior mean 2/3\nplot(theta, dbeta(theta, 4, 2), type = 'l')\n\n\n\n\n\n\n\n# Prior mean 2/3 but higher effect size (more concentrated at mean)\nplot(theta, dbeta(theta, 8, 4), type = 'l')\n\n\n\n\n\n\n\n\n\nWhat are the prior probabilities \\mathbb{P}r(\\theta &gt; 0.25)? \\mathbb{P}r(\\theta &gt; 0.5)? \\mathbb{P}r(\\theta &gt; 0.8)?\n\n\n1 - pbeta(0.25, 8, 4)\n\n[1] 0.9988117\n\n#[1] 0.998117\n1 - pbeta(0.5, 8, 4)\n\n[1] 0.8867188\n\n#[1] 0.8867188\n1 - pbeta(0.8, 8, 4)\n\n[1] 0.1611392\n\n#[1] 0.16113392\n\n\nSuppose the first student gets 33 questions right. What is the posterior distribution for \\theta_1 ? \\mathbb{P}r(\\theta &gt; 0.25) ? \\mathbb{P}r(\\theta &gt; 0.5) ? \\mathbb{P}r(\\theta &gt; 0.8) ? What is the 95% posterior credible interval for \\theta_1?\n\n\n\\text{Posterior} \\sim \\mathrm{Beta}(8 + 33, 4 + 40 - 33) = \\mathrm{Beta}(41, 11)\n\nWith a posterior mean of \\frac{41}{41+11} = \\frac{41}{52}\nWe can plot the posterior distribution with the prior\n\nplot(theta, dbeta(theta, 41, 11), type = 'l')\nlines(theta, dbeta(theta, 8 ,4), lty = 2) #Dashed line for prior\n\n\n\n\n\n\n\n\nPosterior probabilities\n\n1 - pbeta(0.25, 41, 11)\n\n[1] 1\n\n#[1] 1\n1 - pbeta(0.5, 41, 11)\n\n[1] 0.9999926\n\n#[1] 0.9999926\n1 - pbeta(0.8, 41, 11)\n\n[1] 0.4444044\n\n#[1] 0.4444044\n\nEqual-tailed 95% credible interval\n\nqbeta(0.025, 41, 11)\n\n[1] 0.6688426\n\n#[1] 0.6688426\nqbeta(0.975, 41, 11)\n\n[1] 0.8871094\n\n#[1] 0.8871094\n\n95% confidence that \\theta_1 is between 0.67 and 0.89\n\nSuppose the second student gets 24 questions right. What is the posterior distribution for \\theta_2? \\mathbb{P}r(\\theta &gt; 0.25)? \\mathbb{P}r(\\theta &gt; 0.5)? \\mathbb{P}r(\\theta &gt; 0.8)? What is the 95% posterior credible interval for \\theta_2\n\n\n\\text{Posterior} \\sim \\mathrm{Beta}(8 + 24, 4 + 40 - 24) = \\mathrm{Beta}(32, 20)\n\nWith a posterior mean of \\frac{32}{32+20} = \\frac{32}{52}\nWe can plot the posterior distribution with the prior\n\nplot(theta, dbeta(theta, 32, 20), type = 'l')\nlines(theta, dbeta(theta, 8 ,4), lty = 2) #Dashed line for prior\n\n\n\n\n\n\n\n\nPosterior probabilities\n\n1 - pbeta(0.25, 32, 20)\n\n[1] 1\n\n#[1] 1\n1 - pbeta(0.5, 32, 20)\n\n[1] 0.9540427\n\n#[1] 0.9540427\n1 - pbeta(0.8, 32, 20)\n\n[1] 0.00124819\n\n#[1] 0.00124819\n\nEqual-tailed 95% credible interval\n\nqbeta(0.025, 32, 20)\n\n[1] 0.4808022\n\n#[1] 0.4808022\nqbeta(0.975, 32, 20)\n\n[1] 0.7415564\n\n#[1] 0.7415564\n\n95% confidence that \\theta_1 is between 0.48 and 0.74\n\nWhat is the posterior probability that \\theta_1 &gt; \\theta_2?\n\ni.e., that the first student has a better chance of getting a question right than the second student?\nEstimate by simulation: draw 1,000 samples from each and see how often we observe \\theta_1 &gt; \\theta_2\n\ntheta1 = rbeta(100000, 41, 11)\ntheta2 = rbeta(100000, 32, 20)\nmean(theta1 &gt; theta2)\n\n[1] 0.97517\n\n#[1] 0.975\n\n\n\n\n\n\n\nAldrich, John. 2008. “R. A. Fisher on Bayes and Bayes’ Theorem.” Bayesian Analysis 3 (March). https://doi.org/10.1214/08-BA306.\n\n\nFisher, R. A. 1925. Statistical Methods for Research Workers. 1st ed. Edinburgh Oliver & Boyd.\n\n\nMorita, Satoshi, Peter F Thall, and Peter Müller. 2008. “Determining the Effective Sample Size of a Parametric Prior.” Biometrics 64 (2): 595–602.\n\n\nWiesenfarth, Manuel, and Silvia Calderazzo. 2020. “Quantification of Prior Impact in Terms of Effective Current Sample Size.” Biometrics 76 (1): 326–36. https://doi.org/https://doi.org/10.1111/biom.13124.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>M3L7 - Binomial Data</span>"
    ]
  },
  {
    "objectID": "C1-L07-Ex1.html",
    "href": "C1-L07-Ex1.html",
    "title": "18  Homework on Priors - M2L7HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Homework on Priors - M2L7HW1</span>"
    ]
  },
  {
    "objectID": "C1-L08.html",
    "href": "C1-L08.html",
    "title": "19  Poisson Data - M3L8",
    "section": "",
    "text": "Figure 19.1: Poisson likelihood with a Gamma prior\n\n\n\n\n19.0.1 Poisson - Chocolate Chip Cookie\nIn mass-produced chocolate chip cookies, they make a large amount of dough; mix in a large number of chips; then chunk out the individual cookies. In this process, the number of chips per cookie approximately follows a Poisson distribution.\nIf we were to assume that chips have no volume, then this would be exactly a Poisson process and follow exactly a Poisson distribution. In practice, since chips are not that small, so they follow approximately a Poisson distribution for the number of chips per cookie.\n\n\n\nY_i \\sim \\mathrm{Poisson}(\\lambda)\n\\tag{19.1}\n The likelihood of the data is given by the Poisson distribution.What is the likelihood of the data?\n\n\\begin{aligned}\n{\\color{red}f(y \\mid \\lambda) = \\frac{\\lambda^{\\sum{y_i}}e^{-n\\lambda}}{\\prod_{i = 1}^n{y_i!}}} \\quad \\forall (\\lambda &gt; 0) && \\text{ Poisson Likelihood }\n\\end{aligned}\n\n It would be convenient if we could put a conjugate prior. What distribution looks like \\lambda raised to a power and e raised to a negative power?What type of prior should we put on \\lambda ?\nFor this, we’re going to use a Gamma prior.\n\n\\begin{aligned} \\lambda &\\sim \\mathrm{Gamma}(\\alpha, \\beta) && \\text{Gamma Prior} \\\\\n\\color{green}{ f(\\lambda)} &= \\color{green}{\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha - 1}e^{-\\beta\\lambda}} && \\text{subst. Gamma PDF} \\end{aligned}\n\\tag{19.2}\n We can use Bayes theorem to find the posterior.What is the posterior?\n\n\\begin{aligned} {\\color{blue}f(\\lambda \\mid y)} &\\propto \\color{red}{ f(y \\mid \\lambda)} \\color{green}{ f(\\lambda)} && \\text{Bayes without the denominator} \\\\\n&\\propto \\color{red}{\\lambda^{\\sum{y_i}}e^{-n\\lambda}}\\color{green}{\\lambda^{\\alpha - 1}e^{-\\beta \\lambda} } && \\text{subst. Likelihood and Prior} \\\\\n& \\propto { \\color{blue} \\lambda^{\\alpha + \\sum{y_i} - 1}e^{-(\\beta + n)\\lambda} } && \\text{collecting terms} \\\\\n& \\propto { \\color{blue} \\mathrm{Gamma}(\\alpha + \\sum{y_i}, \\beta + n)}\n\\end{aligned}\n\\tag{19.3}\n The posterior is a Gamma distribution with parameters \\alpha + \\sum{y_i} and \\beta + n.What is the posterior distribution?\nThus we can see that the posterior is a Gamma Distribution\n\n\\lambda \\mid y \\sim \\mathrm{Gamma}(\\alpha + \\sum{y_i}, \\beta + n)\n\\tag{19.4}\n The posterior mean of a Gamma distribution is given byWhat is the posterior mean?\nThe mean of Gamma under this parameterization is: \\frac{\\alpha}{\\beta}\nThe posterior mean is going to be\n\n\\begin{aligned}\n{\\color{blue}\\mu_{\\lambda}} &= \\frac{\\alpha + \\sum{y_i}}{\\beta + n} && \\text{(Posterior Mean)} \\\\\nposterior_{\\mu}\n&= \\frac{\\alpha + \\sum{y_i}}{\\beta + n} \\\\\n&= \\frac{\\beta}{\\beta + n}\\frac{\\alpha}{\\beta} + \\frac{n}{\\beta + n}\\frac{\\sum{y_i}}{n} \\\\\n& \\propto  \\beta \\cdot \\mu_\\text{prior} + n\\cdot \\mu_\\text{data}\n\\end{aligned}\n\\tag{19.5}\n The posterior variance of a Gamma distribution is given byWhat is the posterior variance?\nAs you can see here the posterior mean of the Gamma distribution is also the weighted average of the prior mean and the data mean.\nTherefore, the effective sample size (ESS) of the Gamma prior is \\beta\n\n\n\n\n\n\nTipPrior Elicitation of Gamma Hyper-parameters\n\n\n\nHere are two strategies for choose the hyper-parameters \\alpha and \\beta\n\nAn informative prior with a prior mean guess of \\mu=\\frac{a}{b} e.g. what is the average number of chips per cookie?\n\nNext we need another piece of knowledge to pinpoint both parameters.\nCan you estimate the error for the mean? I.e. what do you think the standard deviation is? Since for the Gamma prior\nWhat is the effective sample size \\text{ESS}=\\beta ?\nHow many units of information do you think we have in our prior v.s. our data points ? \\sigma = \\frac{ \\sqrt{\\alpha} }{\\beta}\n\nA vague prior refers to one that’s relatively flat across much of the space.\n\nFor a Gamma prior we can choose \\Gamma(\\epsilon, \\epsilon) where \\epsilon is small and strictly positive. This would create a distribution with a \\mu = 1 and a huge \\sigma stretching across the whole space. And the effective sample size will also be \\epsilon Hence the posterior will be largely driven by the data and very little by the prior.\n\n\n\n\nThe first strategy with a mean and an ESS will be used in numerous models going forward so it is best to remember these two strategies!",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Poisson Data - M3L8</span>"
    ]
  },
  {
    "objectID": "C1-L08-Ex1.html",
    "href": "C1-L08-Ex1.html",
    "title": "20  Homework on Poisson Data - M3L8HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Homework on Poisson Data - M3L8HW1</span>"
    ]
  },
  {
    "objectID": "C1-L08-Ex2.html",
    "href": "C1-L08-Ex2.html",
    "title": "21  Beta Bernoulli - M3L8HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Beta Bernoulli - M3L8HW2</span>"
    ]
  },
  {
    "objectID": "C1-L09.html",
    "href": "C1-L09.html",
    "title": "22  M4L9 - Exponential Data",
    "section": "",
    "text": "22.1 Exponential Data\nY \\sim \\mathrm{Exp}(\\lambda)\n\\tag{22.1}\nYour waiting time has a prior expectation of \\frac{1}{\\lambda}\nThe Gamma distribution is conjugate for an Exponential likelihood. We need to specify a prior, or a particular Gamma in this case. If we think that the buses come on average every ten minutes, that’s a rate of one over ten.\nprior_{\\mu} = \\frac{1}{10}\nThus, we’ll want to specify a gamma distribution so that the first parameter divided by the second parameter is {1 \\over 10}\nWe can now think about our variability. Perhaps you specify\n\\mathrm{Gamma}(100, 1000)\nThis will indeed have a prior mean of {1 \\over 10} and it’ll have a standard deviation of {1 \\over 100}. If you want to have a rough estimate of our mean plus or minus two standard deviations then we have the following\n0.1 \\pm 0.02\nSuppose that we wait for 12 minutes and a bus arrives. Now you want to update your posterior for \\lambda about how often this bus will arrive.\nf(\\lambda \\mid y) \\propto f(y\\mid \\lambda)f(\\lambda)\nf(\\lambda \\mid y) \\propto \\lambda e^{-\\lambda y}\\lambda^{\\alpha - 1}e^{-\\beta \\lambda}\nf(\\lambda \\mid y)  \\propto \\lambda^{(\\alpha + 1) - 1}e^{-(\\beta + y)\\lambda}\n\\lambda \\mid y \\sim \\mathrm{Gamma}(\\alpha + 1, \\beta + y)\nPlugging in our particular prior gives us a posterior for \\lambda which is\n\\lambda \\mid y \\sim \\mathrm{Gamma}(101, 1012)\nThus our posterior mean is going to be \\frac{101}{1012} = 0.0998.\nThis one observation does not contain a lot of data under this likelihood. When the bus comes and it takes 12 minutes instead of 10, it barely shifts our posterior mean up.\nOne data point does not have a big impact here.\nSuppose\nY_1, \\ldots, Y_n \\stackrel{iid}\\sim Exp(\\lambda)=\\lambda e^{-\\lambda x}\\mathbb{I}_{x\\ge0}\nwith mean\n\\mathbb{E}[Y]=\\frac{1}{\\lambda}\nand assume a\nf(\\lambda)= \\mathrm{Gamma}(\\alpha, \\beta) \\qquad (\\text{prior for }\\lambda)\nThe likelihood is then:\nf(y \\mid \\lambda) = \\prod \\lambda e^{-\\lambda x}\\mathbb{I}_{x\\ge0} =  \\lambda ^ n e^{− \\lambda \\sum y_i}\\cdot1\nand we can follow the same steps from the lesson to obtain the posterior distribution (try to derive it yourself):\n\\lambda \\mid y ∼ \\mathrm{Gamma}(\\alpha + n, \\beta + \\sum y_i)\nWhat is the prior effective sample size (ess) in this model?\nIt might be helpful to think about a related problems…",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>M4L9 - Exponential Data</span>"
    ]
  },
  {
    "objectID": "C1-L09.html#sec-exponential-data",
    "href": "C1-L09.html#sec-exponential-data",
    "title": "22  M4L9 - Exponential Data",
    "section": "",
    "text": "Figure 22.1: Exponential likelihood with a Gamma prior\n\n\n Suppose you’re waiting for a bus that you think comes on average once every 10 minutes, but you’re not sure exactly how often it comes.Time between buses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 22.1 We can generalize the result from the lesson to more than one data point.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution. The data sample size n is added to \\alpha to update the first parameter. Thus \\alpha can be interpreted as the sample size equivalent in the prior.\n\n\n\nWe are waiting at a bus stop with 1 bus line, the information at the bus stop say that the bus comes on average every 10 minutes at this time. How long do we expect to wait for the bus?\nwhat if we have waited for k minutes and the bus has not arrived yet? How long do we expect to wait for the bus?\nWhile we are waiting more people arrive at the bus stop. You notice the bus stop features a digital counter and a display with long term mean E and V variance of the number of people at the bus stop. Can we use this information to get a better estimate of our bus arrival time?\nIf we wait at a bus stop with K different bus lines each with the same lambda, and we see a L people waiting. Can we get a better estimate of our bus arrival time?\nWhat if more people come. And we know the mean and variance of the people waiting at the bus stop?\nWhat if a different bus line arrives and the number of people waiting is now M?\nWhat if each bus line has a different lambda, but we know the mean and variance of the people waiting at the bus stop?",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>M4L9 - Exponential Data</span>"
    ]
  },
  {
    "objectID": "C1-L09-Ex1.html",
    "href": "C1-L09-Ex1.html",
    "title": "23  Homework on Exponential Data - M4L9HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Homework on Exponential Data - M4L9HW1</span>"
    ]
  },
  {
    "objectID": "C1-L10.html",
    "href": "C1-L10.html",
    "title": "24  Normally distributed Data - M4L10",
    "section": "",
    "text": "24.1 Normal Likelihood with known variance\nNormally distributed data is not that common. However, modeling using a normal RV is second to none.(Hoff 2009, 75). The CLT is the primary reason that the normal is a good approximation if there are enough IID samples. We will look at two types of conjugate normal priors, and in the next unit we will consider two more uninformative priors for Normally distributed data.\nCharles Zaiontz provides pro types of conjugate priors for normally distributed data:\nIn each case, the unknown refer to population statistics. Since we are able to estimate sample parameters such as the mean and variance quite easily. A key question to consider is how well does our posterior distribution of the parameter representative of the unknown population statistic?\nIdeally, I will update the notes below with proofs of conjugate, prior and posterior and marginal distribution.\nSome of the proofs are in here as well\nSee (Hoff 2009, sec. 5.2)\nLet’s suppose the standard deviation or variance \\sigma^2 is known and we’re only interested in learning about the mean. This is a situation that often arises in monitoring industrial production processes.\nX_i \\stackrel{iid}\\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\tag{24.1}\nIt turns out that the Normal distribution is conjugate for itself when looking for the mean parameter\nPrior\n\\mu \\sim \\mathcal{N}(m_0,S_0^2)\n\\tag{24.2}\nBy Bayes rule:\nf(\\mu \\mid x ) \\propto f(x \\mid \\mu)f(\\mu)\n\\mu \\mid x \\sim \\mathcal{N} \\left (\\frac{\\frac{n\\bar{x}}{\\sigma_0^2} + \\frac{m_0}{s_0^2} }{\\frac{n}{\\sigma_0^2} +\\frac{1}{s_0^2}}, \\frac{1}{\\frac{n}{\\sigma_0^2} + \\frac{1}{s_0^2}}\\right )\n\\tag{24.3}\nwhere:\nLet’s look at the posterior mean\n\\begin{aligned}\nposterior_{\\mu} &= \\frac{\n          \\frac{n}{\\sigma_0^2}}\n       {\\frac{n}{\\sigma_0^2}s + \\frac{1}{s_0^2}}\\bar{x} +     \n          \\frac{ \\frac{1}{s_0^2} }{ \\frac{n}{\\sigma_0^2} + \\frac{1}{s_0^2}\n        }m \\\\\n&= \\frac{n}{n + \\frac{\\sigma_0^2}{s_0^2} }\\bar{x} + \\frac{ \\frac{\\sigma_0^2}{s_0^2} }{n + \\frac{\\sigma_0^2}{s_0^2}}m\n\\end{aligned}\n\\tag{24.4}\nThus we see, that the posterior mean is a weighted average of the prior mean and the data mean. And indeed that the effective sample size for this prior is the ratio of the variance for the data to the variance in the prior.\nPrior\\ ESS= \\frac{\\sigma_0^2}{s_0^2}\n\\tag{24.5}\nThis makes sense, because the larger the variance of the prior, the less information that’s in it.\nThe marginal distribution for Y is\n\\mathcal{N}(m_0, s_0^2 + \\sigma^2)\n\\tag{24.6}",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normally distributed Data - M4L10</span>"
    ]
  },
  {
    "objectID": "C1-L10.html#sec-normal-likelihood-with-unknown-mean",
    "href": "C1-L10.html#sec-normal-likelihood-with-unknown-mean",
    "title": "24  Normally distributed Data - M4L10",
    "section": "",
    "text": "Figure 24.1: Normal likelihood with variance known\n\n\n\n\n\n\n\n\n\n\n\n\n\nn is the sample size\n\\bar{x}=\\frac{1}{n}\\sum x_i is the sample mean\n\\sigma =\\frac{1}{n} \\sum (x_i-\\bar{x})^2 is the sample variance\nindexing parameters with 0 seems to be a convention that they are from the prior:\ns_0 is the prior variance\nm_0 is the prior mean\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nshould we use n-1 in the sample variance?\n\n\n\n\n\n\n\n\n24.1.1 Prior (and posterior) predictive distribution\nThe prior (and posterior) predictive distribution for data is particularly simple in the conjugate normal model .\nIf \ny \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^2)\n and \n\\theta \\sim \\mathcal{N}(m, s_0^2)\n\nthen the marginal distribution for Y, obtained as\n\n\\int f(y,\\theta) d\\theta = \\mathcal{N}(m_0,s_0^2)\n\\tag{24.7}\n\nExample 24.1 Suppose your data are normally distributed with \\mu=\\theta and \\sigma^2=1.\n\ny \\mid \\theta \\sim \\mathcal{N}(\\theta,1)\n\nYou select a normal prior for \\theta with mean 0 and variance 2.\n\n\\theta \\sim \\mathcal{N}(0, 2)\n\nThen the prior predictive distribution for one data point would be N(0, a). What is the value of a?\nSince, m_0 =0, and s^2_0=2 and \\sigma^2=1, the predictive distribution is N(0,2+1).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normally distributed Data - M4L10</span>"
    ]
  },
  {
    "objectID": "C1-L10.html#sec-normal-likelihood-with-expectation-and-variance-unknown",
    "href": "C1-L10.html#sec-normal-likelihood-with-expectation-and-variance-unknown",
    "title": "24  Normally distributed Data - M4L10",
    "section": "24.2 Normal likelihood with expectation and variance unknown",
    "text": "24.2 Normal likelihood with expectation and variance unknown\n\n\n\n\n\n\n\nFigure 24.2: Normal likelihood with a unknown variance\n\n\n\n\n\n\n\n\nTipChallenging\n\n\n\nThis section is challenging.\n\nThe updating derivation is skipped,\nthe posterior\nupdating rule values are introduced without motivations and explanation.\nThe model is also the most complicated in the course, the note at the end says this can be extended hierarchically if we want to specify hyper priors for m, w and \\beta\nOther text discuss this case using a inverse chi squared distribution\n\nIf we can understand the model the homework is going to make sense. Also this is probably the level needed for the other courses in the specialization.\nIt can help to review some of the books:\n\nSee (Hoff 2009, sec. 5.3) which has some R examples.\nSee (Gelman et al. 2013, sec. 5)\n\n\n\nIf both \\mu and \\sigma^2 are unknown, we can specify a conjugate prior in a hierarchical fashion.\n\nX_i \\mid \\mu, \\sigma^2 \\stackrel{iid}\\sim \\mathcal{N}(\\mu, \\sigma^2) \\qquad \\text{(the data given the params) }\n\n\nThis is the level 1 hierarchically model - X_i model our observations.\nWe state on the left, that the RV X is conditioned on the \\mu and \\sigma^2.\nBut the variables \\mu and \\sigma^2 are unknown population statistics which we will need to infer from the data. We can call them latent variables.\n\nNext we add a prior from \\mu conditional on the value for \\sigma^2\n\n\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(m, \\frac{\\sigma^2}{w}) \\qquad \\text{(prior of the mean conditioned on the variance)}\n\nwhere:\n\nw is going to be the ratio of \\sigma^2 and some variance for the Normal distribution. This is the effective sample size of the prior.\nWhy is the mean conditioned on the variance. We can have a model where they are independent too?\nlater on (in the homework) we are told that w can express the confidence in the prior.\nI think this means that Since this is a knowledge of m, i.e. giving w a weight of 1/10 expresses that we value\n\nPerhaps this is due to CLT ?\nThis is level 2 of the model\n\nFinally, the last step is to specify a prior for \\sigma^2. The conjugate prior here is an inverse gamma distribution with parameters \\alpha and \\beta.\n\n\\sigma^2 \\sim \\mathrm{Gamma}^{-1}(\\alpha, \\beta)  \\qquad \\text{prior of the variance}\n\nAfter many calculations… we get the posterior distribution\n\n\\sigma^2 \\mid x \\sim \\mathrm{Gamma}^{-1}(\\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2}\\sum_{i = 1}^n{(x-\\bar{x}^2 + \\frac{nw}{2(n+2)}(\\bar{x} - m)^2)})\n\\tag{24.8}\n\n\\mu \\mid \\sigma^2,x \\sim \\mathcal{N}(\\frac{n\\bar{x}+wm}{n+w}, \\frac{\\sigma^2}{n + w})\n\\tag{24.9}\nWhere the posterior mean can be written as the weighted average of the prior mean and the data mean.\n\n\\frac{n\\bar{x}+wm}{n+w} = \\frac{w}{n + w}m + \\frac{n}{n + w}\\bar{x} \\qquad \\text{post. mean}\n\\tag{24.10}\nIn some cases, we only care about \\mu. We want some inference on \\mu and we may want it such that it does not depend on \\sigma^2. We can marginalize that \\sigma^2 integrating it out. The posterior for \\mu marginally follows a t distribution.\n\n\\mu \\mid x \\sim t\n\nSimilarly, the posterior predictive distribution also is a t distribution.\nFinally, note that we can extend this in various directions, this can be extended to the multivariate normal case that requires matrix vector notations and can be extended hierarchically if we want to specify priors for m, w, \\beta\n\n\n\n\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Springer New York. https://doi.org/10.1007/978-0-387-92407-6.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normally distributed Data - M4L10</span>"
    ]
  },
  {
    "objectID": "C1-L10-Ex1.html",
    "href": "C1-L10-Ex1.html",
    "title": "25  Homework on Normal Data - M4L10HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Homework on Normal Data - M4L10HW1</span>"
    ]
  },
  {
    "objectID": "C1-L11.html",
    "href": "C1-L11.html",
    "title": "26  Non-Informative Priors - M4L11",
    "section": "",
    "text": "26.1 Non-Informative Priors\nWe’ve seen examples of choosing priors that contain a significant amount of information. We’ve also seen some examples of choosing priors where we’re attempting to not put too much information in to keep them vague.\nAnother approach is referred to as objective Bayesian statistics or inference where we explicitly try to minimize the amount of information that goes into the prior.\nThis is an attempt to have the data have maximum influence on the posterior\nLet’s go back to coin flipping\nY_i \\sim B(\\theta)\nHow do we minimize our prior information in \\theta? One obvious intuitive approach is to say that all values of \\theta are equally likely. So we could have a prior for \\theta which follows a uniform distribution on the interval [0, 1]\nSaying all values of \\theta are equally likely seems like it would have no information in it.\nRecall however, that a Uniform(0, 1) is the same as Beta(1, 1)\nThe effective sample size of a beta prior is the sum of its two parameters. So in this case, it has an effective sample size of 2. This is equivalent to data, with one head and one tail already in it.\nSo this is not a completely non-informative prior.\nWe could think about a prior that has less information. For example \\mathrm{Beta}(\\tfrac{1}{2}, \\tfrac{1}{2}), this would have half as much information with an effective sample size of one.\nWe can take this even further. Think about something like \\mathrm{Beta}(0.001, 0.001) This would have much less information, with the effective sample size fairly close to zero. In this case, the data would determine the posterior and there would be very little influence from the prior.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Non-Informative Priors - M4L11</span>"
    ]
  },
  {
    "objectID": "C1-L11.html#sec-non-informative-priors",
    "href": "C1-L11.html#sec-non-informative-priors",
    "title": "26  Non-Informative Priors - M4L11",
    "section": "",
    "text": "Figure 26.1: Non-Informative Priors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n26.1.1 Improper priors\n Can we go even further? We can think of the limiting case. Let’s think of Beta(0,0), what would that look like?\n\nf(\\theta) \\propto \\theta^{-1}(1-\\theta)^{-1}\n\nThis is not a proper density. If you integrate this over (0,1), you’ll get an infinite integral, so it’s not a true density in the sense of it not integrating to 1.\nThere’s no way to normalize it, since it has an infinite integral. This is what we refer to as an improper prior.\nIt’s improper in the sense that it doesn’t have a proper density. But it’s not necessarily improper in the sense that we can’t use it. If we collect data, we use this prior and as long as we observe one head and one tail, or at least one success and one failure. Then we can get a posterior\n\nf(\\theta\\mid y) \\propto \\theta^{y-1}(1-\\theta)^{n-y-1} \\sim Beta(y, n-y)\n\nWith a posterior mean of \\frac{y}{n} =\\hat{\\theta}, which you should recognize as the maximum likelihood estimate. So by using this improper prior, we get a posterior which gives us point estimates exactly the same as the frequentist approach. \nBut in this case, we can also think of having a full posterior. From this, we can make interval statements, and probability statements, and we can actually find an interval and say that there’s 95\\% probability that \\theta is in this interval. This is not something you can do under the frequentist approach even though we may get the same exact interval.\n\n\n26.1.2 Statements about improper priors\nImproper priors are okay as long as the posterior itself is proper. There may be some mathematical things that need to be checked and you may need to have certain restrictions on the data. In this case, we needed to make sure that we observed at least one head and one tail to get a proper posterior.\nBut as long as the posterior is proper, we can go forwards and do Bayesian inference even with an improper prior.\nThe second point is that for many problems there does exist a prior, typically an improper prior that will lead to the same point estimates as you would get under the frequentist paradigm. So we can get very similar results, results that are fully dependent on the data, under the Bayesian approach.\nBut in this case, we can also continue to have a posterior and make posterior interval estimates and talk about the posterior probabilities of the parameter.\n\n\n26.1.3 Normal Case\nAnother example is thinking about the normal case.\n\nY_i \\stackrel{iid}\\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\tag{26.1}\nLet’s start off by assuming that \\sigma^2 is known and we’ll just focus on the mean \\mu.\n We can think about a vague prior like before and say that:\n\n\\mu \\sim N(0, 1000000^2)\n\\tag{26.2}\nThis would just spread things out across the real line. That would be a fairly non-informative prior covering a lot of possibilities. We can then think about taking the limit, what happens if we let the variance go to \\infty. In that case, we’re spreading out this distribution across the entire real number line. We can say that the density is just a constant across the whole real line.\n\nf(\\mu) \\propto 1\n\nThis is an improper prior because if you integrate the real line you get an infinite answer. However, if we go ahead and find the posterior\n\nf(\\mu \\mid y) \\propto f(y \\mid \\mu) f(\\mu) \\propto \\exp \\left (-\\frac{1}{2\\sigma^2}\\sum{(y_i - \\mu)^2} \\right ) (1)\n\n\nf(\\mu \\mid y) \\propto \\exp(-\\frac{1}{2\\sigma^2/n}(\\mu - \\bar{y})^2)\n\n\n\\mu \\mid y \\sim \\mathcal{N}(\\bar{y}, \\frac{\\sigma^2}{n})\n\nThis should look just like the maximum likelihood estimate.\n\n\n26.1.4 Normal with unknown Variance\nIn the case that \\sigma^2 is unknown, the standard non-informative prior is\n\nf(\\sigma^2) \\propto \\frac{1}{\\sigma^2}\n\n\n\\sigma^2 \\sim \\Gamma^{-1}(0,0)\n\n This is an improper prior and it’s uniform on the log scale of \\sigma^2.\nIn this case, we’ll end up with a posterior for \\sigma^2\n\n\\sigma^2 \\mid y \\sim \\Gamma^{-1}\\left (\\frac{n-1}{2}, \\frac{1}{2}\\sum{(y_i - \\bar{y})^2}\\right)\n\nThis should also look reminiscent of the quantities we get as a frequentist. For example, the samples standard deviation",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Non-Informative Priors - M4L11</span>"
    ]
  },
  {
    "objectID": "C1-L11.html#sec-jeffreys-prior",
    "href": "C1-L11.html#sec-jeffreys-prior",
    "title": "26  Non-Informative Priors - M4L11",
    "section": "26.2 Jeffrey’s Prior",
    "text": "26.2 Jeffrey’s Prior\n\n\n\n\n\n\n\nFigure 26.2: Jeffrey’s Prior\n\n\nChoosing a uniform prior depends upon the particular parameterization.\nSuppose I used a prior which is uniform on the log scale for \\sigma^2\n\nf(\\sigma^2) \\propto \\frac{1}{\\sigma^2}\n\nSuppose somebody else decides, that they just want to put a uniform prior on \\sigma^2 itself.\n\nf(\\sigma^2) \\propto 1\n\nThese are both uniform on certain scales or certain parameterizations, but they are different priors. So when we compute the posteriors, they will be different as well.\nThe key thing is that uniform priors are not invariant with respect to transformation. Depending on how you parameterize the problem, you can get different answers by using a uniform prior\nOne attempt to round this out is to use Jeffreys Prior\n Jeffrey’s Prior is defined as the following\n\nf(\\theta) \\propto \\sqrt{\\mathcal{I(\\theta)}}\n\nWhere \\mathcal{I}(\\theta) is the fisher information of \\theta.\nIn most cases, this will be an improper prior.\n\n\n\n\nHarold Jeffreys\n\n\n\n\n\n\n\nTipHistorical Note on Sir Harold Jeffreys\n\n\n\nJeffreys’ Prior is due to Sir Harold Jeffreys (1891-1989) a British geophysicist who who used sophisticated mathematical models to study the Earth and solar system. His hypotheses were uncertain, requiring revision in the face of incoming results, Jeffreys tried to construct a formal theory of scientific reasoning based on Bayesian probability. He made significant contributions to mathematics and statistics. His book, Theory of Probability (Jeffreys 1983), first published in 1939, played an important role in the revival of the objective Bayesian view of probability.\nInductive and Reductive Inference\n“The fundamental problem of scientific progress, and a fundamental one of everyday life, is that of learning from experience. Knowledge obtained in this way is partly merely description of what we have already observed, but part consists of making inferences from past experience to predict future experience. This part may be called generalization or induction.”\nJEFFREYS’ RULES FOR A THEORY OF INDUCTIVE INFERENCE\n\nAll hypotheses used must be explicitly stated and the conclusions must follow from the hypotheses.\nA theory of induction must be self-consistent; that is, it must not be possible to derive contradictory conclusions from the postulates and any given set of observational data.\nAny rule given must be applicable in practice. A definition is useless unless the thing defined can be recognized in terms of the definition when it occurs. The existence of a thing or the estimate of a quantity must not involve an impossible experiment.\nA theory of induction must provide explicitly for the possibility that inferences made by it may turn out to be wrong.\nA theory of induction must not deny any empirical proposition a priori; any precisely stated empirical proposition must be formally capable of being accepted in the sense of the last rule, given a moderate amount of relevant evidence.\nThe number of postulates should be reduced to a minimum. (Occam’s Razor)\nAlthough we do not regard the human mind as a perfect reasoner, we must accept it as a useful one and the only one available. The theory need not represent actual thought processes in detail but should agree with them in outline.\nIn view of the greater complexity of induction, we cannot hope to develop it more thoroughly than deduction. We therefore take it as a rule that an objection carries no weight if an analogous objection invalidates part of generally accepted pure mathematics.\n\n\n\n\n26.2.1 Normal Data\nFor the example of Normal Data\n\nY_i \\sim N(\\mu, \\sigma^2)\n\n\nf(\\mu) \\propto 1\n\n\nf(\\sigma^2) \\propto \\frac{1}{\\sigma^2}\n\nWhere \\mu is uniform and \\sigma^2 is uniform on the log scale.\nThis prior will then be transformation invariant. We will end up putting the same information into the prior even if we use a different parameterization for the Normal.\n\n\n26.2.2 Binomial\n\nY_i \\sim B(\\theta)\n\n\nf(\\theta) \\propto \\theta^{-\\frac{1}{2}}(1-\\theta)^{-\\frac{1}{2}} \\sim \\mathrm{Beta}(\\frac{1}{2},\\frac{1}{2})\n\nThis is a rare example of where the Jeffrey’s prior turns out to be a proper prior.\nYou’ll note that this prior actually does have some information in it. It’s equivalent to an effective sample size of one data point. However, this information will be the same, not depending on the parameterization we use.\nIn this case, we have \\theta as a probability, but another alternative which is sometimes used is when we model things on a logistics scale.\nBy using the Jeffreys prior, we’ll maintain the exact same information.\n\n\n26.2.3 Closing information about priors\nOther possible approaches to objective Bayesian inference include priors such as reference priors and maximum entropy priors.\nA related concept to this is called empirical Bayesian analysis. The idea in empirical Bayes is that you use the data to help inform your prior; such as by using the mean of the data to set the mean of the prior distribution. This approach often leads to reasonable point estimates in your posterior. However, it’s sort of cheating since you’re using your data twice and as a result may lead to improper uncertainty estimates.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Non-Informative Priors - M4L11</span>"
    ]
  },
  {
    "objectID": "C1-L11.html#sec-fisher-information",
    "href": "C1-L11.html#sec-fisher-information",
    "title": "26  Non-Informative Priors - M4L11",
    "section": "26.3 Fisher Information",
    "text": "26.3 Fisher Information\nThe Fisher information (for one parameter) is defined as\n\n\\mathcal{I}(\\theta) = E\\left[\\left(\\frac{d}{d\\theta}log{(f(X \\mid \\theta))}\\right)^2\\right]\n\nWhere the expectation is taken with respect to X which has PDF f(X \\mid \\theta). This quantity is useful in obtaining estimators for \\theta with good properties, such as low variance. It is also the basis for Jeffrey’s prior.\n\n\n\n\n\n\nTipJeffreys prior violates the likelihood principle.\n\n\n\nUse of the Jeffreys prior violates the strong version of the likelihood principle. Which proposes that, given a statistical model, all the evidence in a sample relevant to model parameters is contained in the likelihood function. When using the Jeffreys prior, inferences about \\theta depend not just on the probability of the observed data as a function of \\theta, but also on the universe of all possible experimental outcomes, as determined by the experimental design, because the Fisher information is computed from an expectation over the chosen universe. Accordingly, the Jeffreys prior, and hence the inferences made using it, may be different for two experiments involving the same \\theta parameter even when the likelihood functions for the two experiments are the same a violation of the strong likelihood principle.\n\n\n\nExample 26.1 (Jeffreys prior) Let\n\nX \\mid \\theta \\sim N(\\theta, 1)\n\nThen we have\n\nf(x \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}}\\exp[-\\frac{1}{2}(x-\\theta)^2]\n\n\n\\log{(f(x \\mid \\theta))} = -\\frac{1}{2}\\log{(2\\pi)}-\\frac{1}{2}(x-\\theta)^2\n\n\n\\left ( \\frac{d}{d\\theta}log{(f(x \\mid \\theta))} \\right )^2 = (x-\\theta)^2\n\nand so\n\n\\mathcal{I}(\\theta) = \\mathbb{E}[(X - \\theta)^2] = \\mathbb{V}ar[X] = 1",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Non-Informative Priors - M4L11</span>"
    ]
  },
  {
    "objectID": "C1-L11.html#sensitivity-analysis-of-priors",
    "href": "C1-L11.html#sensitivity-analysis-of-priors",
    "title": "26  Non-Informative Priors - M4L11",
    "section": "26.4 Sensitivity analysis of priors",
    "text": "26.4 Sensitivity analysis of priors\nThe general approach to using priors in models is to start with some justification for a prior, run the analysis, then come up with competing priors and reexamine the conclusions under the alternative priors. The results of the final model and the analysis of the sensitivity of the analysis to the choice of prior are written up as a package.\nFor a discussion of steps and methods to use in a sensitivity analysis, see: (Gelman et al. 2013, page: 38) which discusses two approaches:\nMany times we choose priors out of convenience. How to judge when assumptions of convenience can be made safely is a central task of Bayesian sensitivity analysis.\n\nAnalysis using different conjugate prior distributions.\n\n\nStarting with a uniform prior\nMore informative priors are tested and the 95% posterior CI is compared against the posterior mean and the prior mean.\nA table of prior mean, prior effective sample size , posterior mean and posterior 95 CI is created for the results\nWe are interested primarily to see how well the the posterior CI can excludes the prior mean even for priors with large effective sample size.\n\n\nAnalysis using a non-conjugate prior distribution follows the same approach but uses non conjugate prior. The comparisons described in 1. can be carried out using sampling.\n\n(Gelman et al. 2013, pages: 141)\n\n\n\n\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis.\n\n\nJeffreys, H. 1983. Theory of Probability. International Series of Monographs on Physics. Clarendon Press.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Non-Informative Priors - M4L11</span>"
    ]
  },
  {
    "objectID": "C1-L11-Ex1.html",
    "href": "C1-L11-Ex1.html",
    "title": "27  Homework Alternative Priors - M4L11HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Homework Alternative Priors - M4L11HW1</span>"
    ]
  },
  {
    "objectID": "C1-L12.html",
    "href": "C1-L12.html",
    "title": "28  Brief Review of Regression - M4L12",
    "section": "",
    "text": "28.1 Conjugate Modeling\nRecall that linear regression is a model for predicting a response or dependent variable (Y, also called an output) from one or more covariates or independent variables (X, also called explanatory variables, inputs, or features). For a given value of a single x, the expected value of y is\n\\mathbb{E}[y] = \\beta_0 + \\beta_1x\nor we could say that\nY \\sim \\mathcal{N}(\\beta_0 + \\beta_1x, \\sigma^2)\nFor data (x_1, y_1), \\dots , (x_n, y_n), the fitted values for the coefficients, \\hat{\\beta_0} and \\hat{\\beta_1} are those that minimize the sum of squared errors \\sum_{i = 1}^n{(y_i - \\hat{y_i})^2}, where the predicted values for the response are \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x. We can get these values from R. These fitted coefficients give the least-squares line for the data.\nThis model extends to multiple covariates, with one \\beta_j for each k covariates\n\\mathbb{E}[y_i] = \\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_kx_{ik}\nOptionally, we can represent the multivariate case using vector-matrix notation.\nIn the Bayesian framework, we treat the \\beta parameters as unknown, put a prior on them, and then find the posterior. We might treat \\sigma^2 as fixed and known, or we might treat it as an unknown and also put a prior on it. Because the underlying assumption of a regression model is that the errors are independent and identically normally distributed with mean 0 and variance \\sigma^2, this defines a normal likelihood.",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Brief Review of Regression - M4L12</span>"
    ]
  },
  {
    "objectID": "C1-L12.html#conjugate-modeling",
    "href": "C1-L12.html#conjugate-modeling",
    "title": "28  Brief Review of Regression - M4L12",
    "section": "",
    "text": "28.1.1 \\sigma^2 known\nSometimes we may know the value of the error variance \\sigma^2 . This simplifies calculations. The conjugate prior for the \\beta is a normal prior. In practice, people typically use a non-informative prior, i.e., the limit as the variance of the normal prior goes to infinity, which has the same mean as the standard least-squares estimates. If we are only estimating \\beta and treating \\sigma^2 as known, then the posterior for \\beta is a (multivariate) normal distribution. If we just have a single covariate, then the posterior for the slope is:\n\n\\beta_1 \\mid y \\sim N\\left(\\frac{\\sum_{i = 1}^n{(x_i-\\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n{(x_i-\\bar{x})^2}}, \\frac{\\sigma^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}}\\right)\n\nIf we have multiple covariates, then using a matrix-vector notation, the posterior for the vector of coefficients is \n\\beta \\mid y \\sim N((X^tX)^{-1}X^ty, (X^tX)^{-1}\\sigma^2)\n\nwhere X denotes the design matrix and X^t is the transpose of X. The intercept is typically included in X as a column of 1’s. Using an improper prior requires us to have at least as many data points as we have parameters to ensure that the posterior is proper.\n\n\n28.1.2 \\sigma^2 Unknown\nIf we treat both \\beta and \\sigma^2 as unknown, the standard prior is the non-informative Jeffreys prior, f(\\beta, \\sigma^2) \\propto \\frac{1}{\\sigma^2} . Again, the posterior mean for \\beta will be the same as the standard least-squares estimates. The posterior for \\beta conditional on \\sigma^2 is the same normal distributions as when \\sigma^2 is known, but the marginal posterior distribution for \\beta, with \\sigma^2 integrated out is a t distribution, analogous to the t tests for significance in standard linear regression. The posterior t distribution has mean (X^tX)^{-1}X^ty and scale matrix (related to the variance matrix) s^2(X^tX)^{-1} , where s^2 = \\sum_{i = 1}^n{(y_i - \\hat{y_i})^2/(n - k - 1)} . The posterior distribution for \\sigma^2 is an inverse gamma distribution\n\n\\sigma^2 \\mid y \\sim \\Gamma^{-1}(\\frac{n - k - 1}{2}, \\frac{n - k - 1}{2}s^2)\n\nIn the simple linear regression case (single variable), the marginal posterior for \\beta is a t distribution with mean \\frac{\\sum_{i = 1}^n{(x_i-\\bar{x})(y_i - \\bar{y})}}{\\sum_{i=1}^n{(x_i-\\bar{x})^2}} and scale \\frac{s^2}{\\sum_{i=1}^n{(x_i - \\bar{x})^2}} . If we are trying to predict a new observation at a specified input x^* , that predicted value has a marginal posterior predictive distribution that is a t distribution, with mean \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x^* and scale se_r\\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n - 1)s_x^2}} . se_r is the residual standard error of the regression, which can be found easily in R. s_x^2 is the sample variance of x. Recall that the predictive distribution for a new observation has more variability than the posterior distribution for \\hat{y}, because individual observations are more variable than the mean.",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Brief Review of Regression - M4L12</span>"
    ]
  },
  {
    "objectID": "C1-L12.html#linear-regression",
    "href": "C1-L12.html#linear-regression",
    "title": "28  Brief Review of Regression - M4L12",
    "section": "28.2 Linear Regression",
    "text": "28.2 Linear Regression\n\n\n28.2.1 Single Variable Regression\n We’ll be looking at the Challenger dataset. It contains 23 past launches where it has the temperature at the day of launch and the O-ring damage index\nChallenger dataset\nRead in the data https://pdixon.stat.iastate.edu/stat511/datasets/challenger2.txt\n\noring=read.table(\"data/challanger.txt\", header=T)\n# Note that attaching this masks T which is originally TRUE\nattach(oring)\n\n\nhead(oring)\n\n   t  i\n1 53 11\n2 57  4\n3 58  4\n4 63  2\n5 66  0\n6 67  0\n\n\nNow we’ll see the plot\n\nplot(t,i)\n\n\n\n\n\n\n\n\nFit a linear model\n\noring.lm = lm(i ~ t)\nsummary(oring.lm)\n\n\nCall:\nlm(formula = i ~ t)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3025 -1.4507 -0.4928  0.7397  5.5337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.36508    4.43859   4.138 0.000468 ***\nt           -0.24337    0.06349  -3.833 0.000968 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.102 on 21 degrees of freedom\nMultiple R-squared:  0.4116,    Adjusted R-squared:  0.3836 \nF-statistic: 14.69 on 1 and 21 DF,  p-value: 0.0009677\n\n\nAdd the fitted line into the scatter plot\n\nplot(t,i)\nlines(t,fitted(oring.lm))     \n\n\n\n\n\n\n\n\nCreate a 95% posterior interval for the slope\n\n-0.24337 - 0.06349*qt(.975,21)\n\n[1] -0.3754047\n\n\n\n-0.24337 + 0.06349*qt(.975,21)\n\n[1] -0.1113353\n\n\nNote: These are the same as the frequentist confidence intervals\nIf the challenger launch was at 31 degrees Fahrenheit, how much O-Ring damage would we predict?\n\ncoef(oring.lm)[1] + coef(oring.lm)[2]*31  \n\n(Intercept) \n   10.82052 \n\n# [1] 10.82052 \n\nLet’s make our posterior prediction interval\n\npredict(oring.lm,data.frame(t=31),interval=\"predict\")\n\n       fit      lwr      upr\n1 10.82052 4.048269 17.59276\n\n\nWe can calculate the lower bound through the following formula\n\n10.82052-2.102*qt(.975,21)*sqrt(1+1/23+((31-mean(T))^2/22/var(t)))\n\n[1] 4.850937\n\n\nWhat’s the posterior probability that the damage index is greater than zero?\n\n1-pt((0-10.82052)/(2.102*sqrt(1+1/23+((31-mean(T))^2/22/var(T)))),21)\n\n[1] NA\n\n\n\n\n28.2.2 Multivariate Regression\n \nWe’re looking at Galton’s seminal data predicting the height of children from the height of the parents.\n\n\n  Family Father Mother Gender Height Kids\n1      1   78.5   67.0      M   73.2    4\n2      1   78.5   67.0      F   69.2    4\n3      1   78.5   67.0      F   69.0    4\n4      1   78.5   67.0      F   69.0    4\n5      2   75.5   66.5      M   73.5    4\n6      2   75.5   66.5      M   72.5    4\n7      2   75.5   66.5      F   65.5    4\n8      2   75.5   66.5      F   65.5    4\n\n\nWhat are the columns in the dataset?\n\nnames(heights)\n\n[1] \"Family\" \"Father\" \"Mother\" \"Gender\" \"Height\" \"Kids\"  \n\n# [1] \"Family\" \"Father\" \"Mother\" \"Gender\" \"Height\" \"Kids\"  \n\nexplanation of the columns:\n\nFamily: the family the child is from\nFather: height of the father\nMother: height of the mother\nKids: count of children in the family\nGender: the gender of the child\nHeight: the height the child\n\nThe Height is out target variables.\nLet’s look at the relationship between the different variables\n\npairs(heights)\n\n\n\n\n\n\n\n\nPair plots are a great tool for doing EDA in R. You need to get used read them.\nWe care primarily about the Height so we can should first consider the row of the height. The other rows can inform us if there is a relation between other variables.\n\nthe Father and Mother are correlated with height.\nGender male children are generally taller.\nKids and Family don’t seem to have a clear pattern.\n\nFirst let’s start by creating a linear model taking all of the columns into account\n\nsummary(lm(Height~Father+Mother+Gender+Kids))\n\n\nCall:\nlm(formula = Height ~ Father + Mother + Gender + Kids)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.4748 -1.4500  0.0889  1.4716  9.1656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.18771    2.79387   5.794 9.52e-09 ***\nFather       0.39831    0.02957  13.472  &lt; 2e-16 ***\nMother       0.32096    0.03126  10.269  &lt; 2e-16 ***\nGenderM      5.20995    0.14422  36.125  &lt; 2e-16 ***\nKids        -0.04382    0.02718  -1.612    0.107    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.152 on 893 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6391 \nF-statistic: 398.1 on 4 and 893 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can see here, the Kids column is not statistically significant. Let’s look at a model with it removed.\n\nheights.lm=lm(Height~Father+Mother+Gender)\nsummary(heights.lm)\n\n\nCall:\nlm(formula = Height ~ Father + Mother + Gender)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.523 -1.440  0.117  1.473  9.114 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 15.34476    2.74696   5.586 3.08e-08 ***\nFather       0.40598    0.02921  13.900  &lt; 2e-16 ***\nMother       0.32150    0.03128  10.277  &lt; 2e-16 ***\nGenderM      5.22595    0.14401  36.289  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.154 on 894 degrees of freedom\nMultiple R-squared:  0.6397,    Adjusted R-squared:  0.6385 \nF-statistic:   529 on 3 and 894 DF,  p-value: &lt; 2.2e-16\n\n\nThis model looks good. We can tell from the summary that:\n\neach extra inch of the father’s height contributes an extra 0.4 inches height of the child.\neach extra inch of the mother’s height contributes an extra 0.3 inches height of the child.\nmale gender contributes 5.2 inches to the height of the child.\n\nLet’s create a 95% posterior interval for the difference in height by gender\n\n5.226 - 0.144 * qt(.975,894)\n\n[1] 4.943383\n\n\n\n5.226 + 0.144 * qt(.975,894)\n\n[1] 5.508617\n\n\nLet’s make a posterior prediction interval for a male and female with a father whose 68 inches and a mother whose 64 inches.\n\npredict(heights.lm,data.frame(Father=68,Mother=64,Gender=\"M\"), interval=\"predict\")\n\n       fit      lwr     upr\n1 68.75291 64.51971 72.9861\n\n\n\npredict(heights.lm,data.frame(Father=68,Mother=64,Gender=\"F\"), interval=\"predict\")\n\n       fit      lwr      upr\n1 63.52695 59.29329 67.76062",
    "crumbs": [
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Brief Review of Regression - M4L12</span>"
    ]
  },
  {
    "objectID": "C1-L12-Ex1.html",
    "href": "C1-L12-Ex1.html",
    "title": "29  Homework Regression - M4L12HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Homework Regression - M4L12HW1</span>"
    ]
  },
  {
    "objectID": "C1-L12-Ex2.html",
    "href": "C1-L12-Ex2.html",
    "title": "30  Homework Regression - M4L12HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Homework Regression - M4L12HW2</span>"
    ]
  },
  {
    "objectID": "C2-L01.html",
    "href": "C2-L01.html",
    "title": "31  Statistical Modeling - M1L1",
    "section": "",
    "text": "31.1 A Poll for a political candidate\nThis course is about statistical modelling which falls under the analyzing data objective.\nFor what kinds of problems might we use a statistical model?\nThe step is addressed in detail in most introductory statistics courses.\nGenerally, it is desirable to find a model where the parameters we estimate can be interpreted in the context of the original problem. You might also have to strike a balance between model complexity, and model generalizability. This is often referred to as the bias variance trade-off. Large complex models, might be able to fit your particular dataset very well. But may fail to generalize to future data.",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Statistical Modeling - M1L1</span>"
    ]
  },
  {
    "objectID": "C2-L01.html#a-poll-for-a-political-candidate",
    "href": "C2-L01.html#a-poll-for-a-political-candidate",
    "title": "31  Statistical Modeling - M1L1",
    "section": "",
    "text": "57% for a candidate\nthe 99% CI (51,63)\ndemographics:\n\n55% women\n63% men",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Statistical Modeling - M1L1</span>"
    ]
  },
  {
    "objectID": "C2-L01.html#sec-modeling-process",
    "href": "C2-L01.html#sec-modeling-process",
    "title": "31  Statistical Modeling - M1L1",
    "section": "31.2 Modeling Process 🎥",
    "text": "31.2 Modeling Process 🎥\n\n\n\n\n\n\n\nFigure 31.1: statistical modeling process\n\n\nBuilding statistical models is a process, and each step should be taken carefully. Here we outline the general process and offer some practical advice. We’ll call this the statistical modeling process.\n The first step in this process is to understand the problem. This may seem obvious, but understanding the problem and context is critical to success. A sophisticated model might be useless if it is applied inappropriately.understand the problem\n\nExample 31.1 (international stores) For example, suppose you have revenue data from several different locations of a store chain at unknown locations.\n\nIt seems reasonable to average these revenue numbers as a summary of how the store is doing.\nSuppose you discover that the stores are located in different countries and reported revenues in different currencies.\nNow that average doesn’t seem to have much meaning unless, of course, we get the revenue numbers converted to the same scale.\n\n\n The second step is to plan and properly collect relevant data. There may be multiple quantities that you could potentially measure to help answer your question. In this step, you decide what information will be most useful to solving your problem. How to collect the data and how many data points to collect. The quality of your data collection plan determines the value of your data.plan and collect data",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Statistical Modeling - M1L1</span>"
    ]
  },
  {
    "objectID": "C2-L01.html#peer-review-survey",
    "href": "C2-L01.html#peer-review-survey",
    "title": "31  Statistical Modeling - M1L1",
    "section": "31.3 peer review survey",
    "text": "31.3 peer review survey\nFor example, if you conduct a survey of peers in your workplace. Your results would likely not generalize to all workers in the company, especially if there are multiple work sites. If you want generalizable results, a better plan would be to select a random sample among all employees to participate in your survey.",
    "crumbs": [
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Statistical Modeling - M1L1</span>"
    ]
  },
  {
    "objectID": "C2-L02.html",
    "href": "C2-L02.html",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "",
    "text": "32.1 Components of a Bayesian Model 🎥\nIn lesson one, we defined a statistical model as a mathematical structure used to imitate or approximate the data generating process. It incorporates uncertainty and variability using the theory of probability. A model could be very simple, involving only one variable.\nA model can be as simple as the one right here or as complicated and sophisticated as we need to capture the behavior of the data. So far, this model is the same for Frequentists and Bayesians.\nAs you may recall from the previous course. The frequentist approach to fitting this model right here would be to consider \\mu and \\sigma to be fixed but unknown constants, and then we would estimate them. To calculate our uncertainty in those estimates a frequentist approach would consider how much the estimates of \\mu and \\sigma might change if we were to repeat the sampling process and obtain another sample of 15 men, over, and over.\nThe Bayesian approach, the one we’re going to take in this class. Tackles our uncertainty in \\mu and \\sigma^2 with probability directly. By treating them as random variables with their own probability distributions. These are often called priors, and they complete a Bayesian model.\nIn the rest of this segment, we’re going to review three key components of Bayesian models. That were used extensively in the previous course The three primary components of Bayesian models that we often work with are the likelihood, the prior and the posterior.\n\\begin{aligned}\n\\mathbb{P}r(\\theta \\mid y) &= \\frac{\\mathbb{P}r(\\theta,y)}{\\mathbb{P}r(y)}\n\\\\ &= \\frac{\\mathbb{P}r(\\theta,y)}{\\int \\mathbb{P}r(\\theta,y)}\n\\\\ &= \\frac{\\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)}{\\int \\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)\\ d\\theta}\n\\end{aligned}\nWe start with the definition of conditional probability (1). The conditional distribution, \\mathbb{P}r(\\theta \\mid y) is the ratio of the joint distribution of \\theta and y, i.e. \\mathbb{P}r(\\theta,y); with the marginal distribution of y, \\mathbb{P}r(y).\nTo make this look like the Bayes theorem that we’re familiar with the joint distribution can be rewritten as the product of the prior and the likelihood. We start with the likelihood, because that’s how we usually write Bayes’ theorem. We have the same thing in the denominator here. But we’re going to integrate over the values of theta. These integrals are replaced by summations if we know that \\theta is a discrete random variable. The marginal distribution is another important piece which we may use when we more advanced Bayesian modeling.\nThe posterior distribution is our primary tool for achieving the statistical modeling objectives from lesson one.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#sec-c2l02-components",
    "href": "C2-L02.html#sec-c2l02-components",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "",
    "text": "a Bayesian Model\n\n\n\nExample 32.1 (heights of men) Suppose our data consists of the heights of N=15 adult men. . Clearly it would be very expensive or even impossible to collect the genetic information that fully explains the variability in these men’s heights. We only have the height measurements available to us. To account for the variability, we might assume that the men’s heights follow a normal distribution.heights of N=15 men\nSo we could write the model like this:  where y_i will represent the height for person i, i will be our index. This will be equal to a constant, a number \\mu which will represents the mean for all men plus \\epsilon_i. the individual error term for individual i.y_i= \\mu + \\epsilon_i\n We’re going to assume that \\epsilon_i comes from a normal distribution with mean zero and variance \\sigma^2. We are also going to assume that these epsilons are independent and identically distributed from this normal distribution. This is also for i equal to 1 up to N which will be 15 in our case. Equivalently we could write this model directly for the y_i themselves.\\epsilon_i \\stackrel{iid}\\sim N(0,\\sigma^2) \\  i\\in 1 \\dots N\n So each y_i comes from a normal distribution independent and identically distributed with the normal distribution. With mean \\mu and variance \\sigma^2. This specifies a probability distribution and a model for the data.y_i \\stackrel{iid}\\sim N(\\mu,\\sigma^2) \\ i \\in 1 \\dots N\n\n\n\n\n\n\nNoteheights of men\n\n\n\n\n\\begin{aligned}\ny_i&= \\mu+\\epsilon_i,\n\\\\ \\epsilon_i &\\stackrel{iid}\\sim N(0,\\sigma^2)\n\\end{aligned}\n another way to write this:\n\n\\begin{aligned}\ny_i &\\stackrel{iid}\\sim N(\\mu,\\sigma^2)\n\\end{aligned}\n\n\n\nIf we know the values of \\mu and \\sigma. It also suggests how we might generate more fake data that behaves similarly to our original data set.\n\n\n\n\n\n\n\n\n\n\nFigure 32.1: Components of a Bayesian Model\n\n\n\n\n The likelihood is the probabilistic model for the data. It describes how, given the unknown parameters, the data might be generated. We’re going to call unknown parameter theta right here. Also, in this expression, you might recognize this from the previous class, as describing a probability distribution.\\mathbb{P}r(y\\mid \\theta)\\ \\text{(likelihood)}\n The prior, the next step, is the probability distribution that characterizes our uncertainty with the parameter theta. We’re going to write it as \\mathbb{P}r(\\theta). It’s not the same distribution as this one. We’re just using this notation p to represent the probability distribution of theta. By specifying a likelihood and a prior.\\mathbb{P}r(\\theta)\\ \\text{(prior)}\n We now have a joint probability model for both the knowns, the data, and the unknowns, the parameters. We can see this by using the chain rule of probability. If we wanted the joint distribution of both the data and the parameters theta. Using the chain rule of probability, we could start with the distribution of \\theta. And multiply that by the probability or the distribution of y given theta. That gives us an expression for the joint distribution. However if we’re going to make inferences about data and we already know the values of y, we don’t need the joint distribution, what we need is the posterior distribution.\\mathbb{P}r(y,\\theta) = \\mathbb{P}r(\\theta)\\mathbb{P}r(y\\mid\\theta) \\ \\text{(joint probability)}\n The posterior distribution is the distribution of \\mathbb{P}r(\\theta \\mid y), i.e. \\theta given y. We can obtain this expression using the laws of conditional probability and specifically using Bayes’ theorem.\\mathbb{P}r(\\theta \\mid y)\\ \\text{(posterior)}\n\n\n We start with the joint distribution like we have on top, and we integrate out or marginalize over the values of theta (2)How do we get the marginal distribution of y?\n\n\n\n\n\n\n\n\nNoteAnatomy of a posterior probability\n\n\n\n\n  \\begin{aligned}\n  &\\mathbb{P}r(y\\mid \\theta) && (likelihood) \\\\\n&  \\mathbb{P}r(\\theta) && (prior) \\\\\n   \\mathbb{P}r(y,\\theta) &= \\mathbb{P}r(\\theta)\\mathbb{P}r(y|\\theta) &&(joint\\ distribution) \\\\\n   \\mathbb{P}r(\\theta \\mid y) &= \\frac{\\mathbb{P}r(\\theta,y)}{\\mathbb{P}r(y)} && (conditional\\ probability) \\\\\n&= \\frac{\\mathbb{P}r(\\theta,y)}{\\int \\mathbb{P}r(\\theta,y)} \\\\\n&= \\frac{\\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)}{\\int \\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)\\ d\\theta} \\\\\n\\end{aligned}\n\\tag{32.1}\n\n\n\nWhereas non-Bayesian approaches consider a probability model for the data only, the hallmark characteristic of Bayesian models is that they specify a joint probability distribution for both data and parameters. How does the Bayesian paradigm leverage this additional assumption?\n\n\n\nThis allows us to make probabilistic assessments about how likely our particular data outcome is under any parameter setting.\nThis allows us to select the most accurate prior distribution.\nThis allows us to make probabilistic assessments about hypothetical data outcomes given particular parameter values.\nThis allows us to use the laws of conditional probability to describe our updated information about parameters given the data.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#sec-c2l02-model-specification",
    "href": "C2-L02.html#sec-c2l02-model-specification",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "32.2 Model Specification 🎥",
    "text": "32.2 Model Specification 🎥\n\n\n\n\nModel specification\n\nBefore fitting any model we first need to specify all of its components.\n\n\n\n\n\n\n\n\nFigure 32.2: The graphical model specification for the height model\n\n\n\n\n\n\n32.2.1 Hierarchical representation\nOne convenient way to do this is to write down the hierarchical form of the model. By hierarchy, we mean that the model is specified in steps or in layers. We usually start with the model for the data directly, or the likelihood. Let’s write, again, the model from the previous lesson.\nWe had the height for person i, given our parameters \\mu and \\sigma^2, so conditional on those parameters, y_i came from a normal distribution that was independent and identically distributed, where the normal distribution has mean \\mu and variance \\sigma^2, and we’re doing this for individuals 1 up to N, which was 15 in this example. y_i | \\mu,\\sigma^2 \\stackrel{iid}\\sim N(\\mu,\\sigma^2) \\ for\\ i \\in 1,\\dots,15\nThe next level that we need is the prior distribution from \\mu and \\sigma^2. For now we’re going to say that they’re independent priors. So that our prior from \\mu and \\sigma^2 is going to be able to factor Into the product of two independent priors.  We can assume independents in the prior and still get dependents in the posterior distribution.\\mathbb{P}r(\\mu,\\sigma^2)~=~\\mathbb{P}r(\\mu)\\mathbb{P}r(\\sigma^2)\\ (independence)\nIn the previous course we learned that the conjugate prior for \\mu, if we know the value of \\sigma^2, is a normal distribution, and that the conjugate prior for \\sigma^2 when \\mu is known is the Inverse Gamma distribution.\nLet’s suppose that our prior distribution for \\mu is a normal distribution where mean will be \\mu_0.  This is just some number that you’re going to fill in here when you decide what the prior should be. Mean \\mu_0, and less say \\sigma^2_0 would be the variance of that prior.\\mu \\sim N(\\mu_0,\\sigma^2_0)\nThe prior for \\sigma^2 will be Inverse Gamma  which has two parameters:\\sigma^2 \\sim \\mathcal{IG}(\\nu_0,\\beta_0)\n\nIt has a shape parameter, we’re going to call that \\nu_0, and\nIt has a scale parameter, we’ll call that \\beta_0.\n\nWe need to choose values for these hyper-parameters here. But we do now have a complete Bayesian model.\nWe now introduce some new ideas that were not presented in the previous course.\n\n\n\n\n\n\nNoteHierarchical representation\n\n\n\nBy hierarchy, we mean that the model is specified in steps or in layers.\n\nstart with the model for the data, or the likelihood.\nwrite the priors\nadd hyper-priors for the parameters of the priors.\n\nMore details can be seen on this wikipedia article and on this one\n\n\n\n\n32.2.2 Graphical representation\nAnother useful way to write out this model Is using what’s called a graphical representation. To write a graphical representation, we’re going to do the reverse order, we’ll start with the priors and finish with the likelihood.\nIn the graphical representation we draw what are called nodes so this would be a node for mu. The circle means that the this is a random variable that has its own distribution. So \\mu with its prior will be represented with that. And then we also have \\sigma^2. The next part of a graphical model is showing the dependence on other variables. Once we have the parameters, we can generate the data.\nFor example we have y_1, \\dots y_n. These are also random variables, so we’ll create these as nodes. And I’m going to double up the circle here to indicate that these nodes are observed, you see them in the data. So we’ll do this for all of the ys here. And to indicate the dependence of the distributions of the ys on \\mu and \\sigma^2, we’re going to draw arrows. So \\mu influences the distribution of y for each one of these ys. The same is true for sigma squared, the distribution of each y depends on the distribution of \\sigma^2. Again, these nodes right here, that are double-circled, mean that they’ve been observed. If they’re shaded, which is the usual case, that also means that they’re observed. The arrows indicate the dependence between the random variables and their distributions.\nNotice that in this hierarchical representation, I wrote the dependence of the distributions also. We can simplify the graphical model by writing exchangeable random variables and I’ll define exchangeable later.\nWe’re going to write this using a representative of the ys here on what’s called the plate. So I’m going to re draw this hierarchical structure, we have \\mu and \\sigma^2. And we don’t want to have to write all of these notes again. So I’m going to indicate that there are n of them, And I’m just going to draw one representative, y_i. And they depend on \\mu and \\sigma^2. To write a model like this, we must assume that the ys are exchangeable. That means that the distribution for the ys does not change if we were to switch the index label like the i on the y there. So, if for some reason, we knew that one of the ys was different from the other ys in its distribution, and if we also know which one it is, then we would need to write a separate node for it and not use a plate like we have here.\n\n\n\n\n\n\nNoteGraphical representation\n\n\n\n\n\n\n\n\n\n\n\nFigure 32.3: pgm-posterior\n\n\n\n\n\nIn the graphical representation we start at the top by drawing:\n\ncircle nodes for the hyperparameters.\narrows indicating that they determine the\nnodes for the priors.\nnodes for the RVs (doubled circles)\nplates (rectangles) indicating RVs that are exchangeable. We add an index to the corner of the plate to indicate the amount of replicated RVs\n\nMore details can be seen on this wikipedia article\n\n\nBoth the hierarchical and graphical representations show how you could hypothetically simulate data from this model. You start with the variables that don’t have any dependence on any other variables. You would simulate those, and then given those draws, you would simulate from the distributions for these other variables further down the chain.\nThis is also how you might simulate from a prior predictive distribution.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#sec-c2l02-posterior-derivation",
    "href": "C2-L02.html#sec-c2l02-posterior-derivation",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "32.3 Posterior derivation 🎥",
    "text": "32.3 Posterior derivation 🎥\n\n\n\n\nPosterior derivation\n\nSo far, we’ve only drawn the model with two levels. But in reality, there’s nothing that will stop us from adding more layers.\nFor example, instead of fixing the values for the hyper parameters in the previous segment, those hyper parameters were the \\mu_0, the \\sigma_0, the \\nu_0 and the \\beta_0.\nWe could specify just fixed numeric values for those, or we could learn them from the data and model them using additional prior distributions for those variables to make this a hierarchical model.\nOne reason we might do this is if the data are hierarchically organized so that the observations are naturally grouped together. We will examine these types of hierarchical models in depth later in the course.\nAnother simple example of a hierarchical model is one you saw already in the previous course.\nLet’s write it as y_i \\mid \\mu,\\sigma^2, so this is just like the model from the previous lesson, will be independent and identically distributed normal with a mean \\mu and a variance, \\sigma^2. The next step, instead of doing independent priors for \\mu and \\sigma^2, we’re going to have the prior for \\mu depend on the value of \\sigma^2. That is given \\sigma^2, \\mu follows a normal distribution with mean \\mu naught, just some hyper parameter that you’re going to chose. And the variance of this prior will be \\sigma^2, this parameter, divided by omega naught. Another hyper parameter that will scale it.\nWe now have a joint distribution of y and \\mu given \\sigma^2 So finally, we need to complete the model with the prior for \\sigma^2. We’ll use our standard inverse gamma with the same hyper parameters as last time. This model has three layers. And \\mu depends on sigma right here. The graphical representation for this model looks like this. We start with the variables that don’t depend on anything else. So that would be \\sigma^2 and move down the chain.\nSo here, the next variable is \\mu which depends on \\sigma^2. And then dependent on both, we have the yi’s. We use a double circle because the yi’s are observed, their data, and we’re going to assume that they’re exchangeable. So let’s put them on a plate here for i in 1 to n The distribution of yi depends on both \\mu and \\sigma^2, so we’ll draw curves connecting those pieces there. To simulate hypothetical data from this model, we would have to first draw from the distribution of the prior for \\sigma^2. Then the distribution for mu which depends on \\sigma^2. And once we’ve drawn both of these, then we can draw random draws from the y’s, which of course depends on both of those. With multiple levels, this is an example of a hierarchical model. Once we have a model specification, we can write out what the full posterior distribution for all the parameters given the data looks like. Remember that the numerator in Bayes’ theorem is the joint distribution of all random quantities, all the nodes in this graphical representation over here from all of the layers. So for this model that we have right here, we have a joint distribution that’ll look like this. We’re going to write the joint distribution of everything y1 up to yn, \\mu and \\sigma^2, Using the chain rule of probability, we’re going to multiply all of the distributions in the hierarchy together. So let’s start with the likelihood piece. And we’ll multiply that by the next layer, the distribution of mu, given \\sigma^2. And finally, with the prior for sigma squared. So what do these expressions right here look like? The likelihood right here in this level because they’re all independent will be a product of normal densities. So we’re going to multiply the normal density for each yi, Given those parameters. This, again, is shorthand right here for the density of a normal distribution. So that represents this piece right here. The conditional prior of \\mu given sigma squared is also a normal. So we’re going to multiply this by a normal distribution of mu, where its parameters are \\mu naught and sigma squared over omega naught. And finally, we have the prior for sigma squared. We’ll multiply by the density of an inverse gamma for \\sigma^2 given the hyper parameters \\mu naught, sorry, that is given, the hyper parameters \\mu naught and and beta naught. What we have right here is the joint distribution of everything. It is the numerator in Bayes theorem. Let’s remind ourselves really fast what Bayes theorem looks like again. We have that the posterior distribution of the parameter given the data is equal to the likelihood, Times the prior. Over the same thing again. So this gives us in the numerator the joint distribution of everything which is what we’ve written right here.\nIn Bayes theorem, the numerator and the denominator are the exact same expression accept that we integrate or marginalize over all of the parameters.\nBecause the denominator is a function of the y’s only, which are known values, the denominator is just a constant number. So we can actually write the posterior distribution as being proportional to, this symbol right here represents proportional to. The joint distribution of the data and parameters, or the likelihood times the prior. The poster distribution is proportional to the joint distribution, or everything we have right here. In other words, what we have already written for this particular model is proportional to the posterior distribution of \\mu and \\sigma^2, given all of the data. The only thing missing in this expression right here is just some constant number that causes the expression to integrate to 1. If we can recognize this expression as being proportional to a common distribution, then our work is done, and we know what our posterior distribution is. This was the case for all models in the previous course. However, if we do not use conjugate priors or if the models are more complicated, then the posterior distribution will not have a standard form that we can recognize. We’re going to explore a couple of examples of this issue in the next segment.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#sec-c2l02-non-conjugate-models",
    "href": "C2-L02.html#sec-c2l02-non-conjugate-models",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "32.4 Non-conjugate models 🎥",
    "text": "32.4 Non-conjugate models 🎥\n\n\n\n\nNon-conjugate models\n\nWe’ll first look at an example of a one parameter model that is not conjugate.\n\n\n32.4.0.1 Company Personnel\nSuppose we have values that represent the percentage change in total personnel from last year to this year for, we’ll say, ten companies. These companies come from a particular industry. We’re going to assume for now, that these are independent measurements from a normal distribution with a known variance equal to one, but an unknown mean.\nSo we’ll say the percentage change in the total personnel for company I, given the unknown mean \\mu will be distributed normally with mean \\mu, and we’re just going to use variance 1.\nIn this case, the unknown mean could represent growth for this particular industry.\nIt’s the average of the growth of all the different companies. The small variance between the companies and percentage growth might be appropriate if the industry is stable.\nWe know that the conjugate prior for \\mu in this location would be a normal distribution.\nBut suppose we decide that our prior believes about \\mu are better reflected using a standard t distribution with one degree of freedom. So we could write that as the prior for \\mu is a t distribution with a location parameter 0. That’s where the center of the distribution is. A scale parameter of 1 to make it the standard t-distribution similar to a standard normal, and 1 degree of freedom.\nThis particular prior distribution has heavier tails than the conjugate and normal distribution, which can more easily accommodate the possibility of extreme values for mu. It is centered on zero so, that apriori, there is a 50% chance that the growth is positive and a 50% chance that the growth is negative.\n\n\nRecall that the posterior distribution of \\mu is proportional to the likelihood times the prior. Let’s write the expression for that in this model. That is the posterior distribution for \\mu given the data y_1 \\dots y_n is going to be proportional to the likelihood.\nIt is a product from i equals 1 to n, in this case that’s 10.\nDensities from a normal distribution.\nLet’s write the density from this particular normal distribution.\nIs 1 over the square root of 2 pi.\nE to the negative one-half.\nYi minus the mean squared, this is the normal density for each individual Yi and we multiplied it for likelihood.\nThe density for this t prior looks like this.\nIt’s 1 over pi times 1 plus \\mu squared.\nThis is the likelihood times the prior.\nIf we do a little algebra here, first of all, we’re doing this up to proportionality.\nSo, constants being multiplied by this expression are not important.\nThe square root of 2 pi being multiplied n times, is just a constant number, and \\pi creates a constant number. So we will drop them in our next step.\nSo this is now proportional too, we’re removing this piece and now we’re going to use properties of exponents.\nThe product of exponents is the sum of the exponentiated pieces.\nSo we have the exponent of negative one-half times the sum from i equals 1 to n, of Yi minus \\mu squared.\nAnd then we’re dropping the pie over here, so times 1 plus \\mu squared.\nWe’re going to do a few more steps of algebra here to get a nicer expression for this piece.\nBut we’re going to skip ahead to that.\nWe’ve now added these last two expressions.\nTo arrive at this expression here for the posterior, or what’s proportional to the posterior distribution.\nThis expression right here is almost proportional to a normal distribution except we have this 1 plus \\mu squared term in the denominator.\nWe know the posterior distribution up to a constant but we don’t recognize its form as a standard distribution.\nThat we can integrate or simulate from, so we’ll have to do something else.\nLet’s move on to our second example. For a two parameter example, we’re going to return to the case where we have a normal likelihood.\nAnd we’re now going to estimate \\mu and \\sigma^2, because they’re both unknown.\nRecall that if \\sigma^2 were known, the conjugate prior from \\mu would be a normal distribution.\nAnd if \\mu were known, the conjugate prior we could choose for \\sigma^2 would be an inverse gamma.\nWe saw earlier that if you include \\sigma^2 in the prior for \\mu, and use the hierarchical model that we presented earlier, that model would be conjugate and have a closed form solution. However, in the more general case that we have right here, the posterior distribution does not appear as a distribution that we can simulate or integrate.\nChallenging posterior distributions like these ones and most others that we’ll encounter in this course kept Bayesian in methods from entering the main stream of statistics for many years. Since only the simplest problems were tractable. However, computational methods invented in the 1950’s, and implemented by statisticians decades later, revolutionized the field. We do have the ability to simulate from the posterior distributions in this lesson as well as for many other more complicated models.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L03.html",
    "href": "C2-L03.html",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "",
    "text": "33.1 Monte Carlo Integration 🎥\nBefore we learn how to simulate from complicated posterior distributions, let’s review some of the basics of Monte Carlo estimation.\nMonte Carlo estimation refers to simulating hypothetical draws from a probability distribution in order to calculate important quantities. By “important quantities,” we mean things like the mean, the variance, or the probability of some event or distributional property.\nAll of these calculations involve integration, which except for the simplest distributions, may range from very difficult to impossible :-) .\nSuppose we have a random variable \\theta that follows a \\Gamma distribution\n\\theta \\sim \\mathrm{Gamma}(a,b) \\qquad\n\\tag{33.1}\nLet’s say a=2 and b=\\frac{1}{3} , where a is the shape parameter and b is the rate parameter.\na=2 \\qquad b=1/3 \\qquad\n\\tag{33.2}\nTo calculate the mean of this distribution, we would need to compute the following integral. It is possible to compute this integral, and the answer is \\frac{a}{b} (6 in this case).\n\\mathbb{E}[\\theta] = \\int_0^\\infty \\theta f(\\theta) d\\theta = \\int_0^\\infty \\theta \\frac{b^a}{\\Gamma(a)}\\theta^{a-1}e^{-b\\theta} d\\theta = \\frac{a}{b} \\qquad\n\\tag{33.3}\nHowever, we could verify this answer through Monte Carlo estimation.\nTo do so, we would simulate a large number of draws (call them \\theta^∗_i \\quad (i=1,\\ldots ,m) ) from this gamma distribution and calculate their sample mean.\nWhy can we do this?\nRecall from the previous course that if we have a random sample from a distribution, the average of those samples converges in probability to the true mean of that distribution by the Law of Large Numbers.\nFurthermore, by the Central Limit Theorem (CLT), this sample mean \\bar{\\theta}^* = \\frac{1}{m}\\sum_{i=1}^m \\theta_i^* approximately follows a normal distribution with mean \\mathbb{E}[\\theta] and variance \\mathbb{V}ar[\\theta]/m .\nThe theoretical variance of \\theta is the following integral:\n\\text{Var}[\\theta] = \\int_0^\\infty (\\theta-\\mathbb{E}(\\theta))^2 f(\\theta) d\\theta \\qquad\n\\tag{33.4}\nJust like we did with the mean, we could approximate this variance with the sample variance\n\\text{Var}[\\theta^*] = \\frac{1}{m}\\sum_{i=1}^m (\\theta_i^* - \\bar{\\theta}^*)^2 \\qquad\n\\tag{33.5}",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#sec-monte-carlo-integration",
    "href": "C2-L03.html#sec-monte-carlo-integration",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "",
    "text": "Monte Carlo Integration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n33.1.1 Calculating probabilities\n\n\n\n\nMonte Carlo Integration\n\nThis method can be used to calculate many different integrals. Say h(\\theta) is any function and we want to calculate\n\n\\int h(\\theta) \\mathbb{P}r(\\theta) d\\theta = \\mathbb{E}(h(\\theta)) \\approx \\frac{1}{m}\\sum_{i=1}^m h(\\theta_i^*) \\qquad\n\\tag{33.6}\nwhere \\mathbb{P}r(\\theta) is the probability density function of \\theta and h(\\theta) is any function of \\theta.\nThis integral is precisely what is meant by \\mathbb{E}[h(\\theta)] , so we can conveniently approximate it by taking the sample mean of h(\\theta_i^*). That is, we apply the function h to each simulated sample from the distribution, and take the average of all the results.\nOne extremely useful example of an h function is is the indicator I_A(\\theta) where A is some logical condition about the value of \\theta. To demonstrate, suppose h(\\theta)=I_{[\\theta&lt;5]}(\\theta), which will give a 1 if \\theta &lt;5 and return a 0 otherwise.\nWhat is \\mathbb{E}(h(\\theta))?\nThis is the integral:\n\n\\begin{aligned}\n\\mathbb{E}[h(\\theta)] &= \\int_0^\\infty \\mathbb{I}_{[\\theta&lt;5]}(\\theta) \\mathbb{P}r(\\theta) d\\theta \\\\\n&= \\int_0^5 1 \\cdot \\mathbb{P}r(\\theta) d\\theta + \\int_5^\\infty 0 \\cdot \\mathbb{P}r(\\theta) d\\theta \\\\\n&= \\mathbb{P}r(\\theta &lt; 5) \\qquad\n\\end{aligned}\n\\tag{33.7}\nSo what does this mean?\nIt means we can approximate the probability that \\theta &lt; 5 by drawing many samples \\theta^∗_i , and approximating this integral with \\frac{1}{m} \\sum_{i=1}^m I_{\\theta^* &lt; 5} (\\theta_i^*). This expression is simply counting how many of those samples come out to be less than 5 , and dividing by the total number of simulated samples.\nThat’s convenient!\nLikewise, we can approximate quantiles of a distribution. If we are looking for the value z such that \\mathbb{P}r(\\theta &lt; z) = 0.9 , we simply arrange the samples \\theta^∗_i in ascending order and find the smallest drawn value that is greater than 90% of the others.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#monte-carlo-error-and-marginalization",
    "href": "C2-L03.html#monte-carlo-error-and-marginalization",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "33.2 Monte Carlo Error and Marginalization",
    "text": "33.2 Monte Carlo Error and Marginalization\n\n\n\n\nMonte Carlo Error and Marginalization\n\nHow good is an approximation by Monte Carlo sampling?\nAgain we can turn to the CLT, which tells us that the variance of our estimate is controlled in part by m. For a better estimate, we want larger m.\nFor example, if we seek \\mathbb{E}[\\theta] , then the sample mean \\bar\\theta^∗ approximately follows a normal distribution with mean \\mathbb{E}[\\theta] and variance Var[\\theta]/m .\nThe variance tells us how far our estimate might be from the true value.\nOne way to approximate Var[\\theta] is to replace it with the sample variance.\nThe standard deviation of our Monte Carlo estimate is the square root of that, or the sample standard deviation divided by \\sqrt{m} .\nIf m is large, it is reasonable to assume that the true value will likely be within about two standard deviations of your Monte Carlo estimate.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#marginalization",
    "href": "C2-L03.html#marginalization",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "33.3 Marginalization",
    "text": "33.3 Marginalization\nWe can also obtain Monte Carlo samples from hierarchical models.\nAs a simple example, let’s consider a binomial random variable where y \\mid \\phi \\sim \\mathrm{Bin}(10,\\phi) and further suppose \\phi is random (as if it had a prior) and is distributed beta \\phi \\sim \\mathrm{Beta}(2,2) .\nGiven any hierarchical model, we can always write the joint distribution of y and \\phi as \\mathbb{P}r(y,\\phi) = \\mathbb{P}r(y \\mid \\phi)\\mathbb{P}r(\\phi) using the chain rule of probability.\nTo simulate from this joint distribution, repeat these steps for a large number m :\n\nSimulate \\phi^∗_i from its Beta(2,2) distribution.\nGiven the drawn \\phi^∗_i , simulate y^∗_i from Bin(10,\\phi^*_i) .\n\nThis will produce m independent pairs of (y^∗,\\phi^∗)_i drawn from their joint distribution.\nOne major advantage of Monte Carlo simulation is that marginalizing is easy. Calculating the marginal distribution of y , \\mathbb{P}r(y)=\\int^1_0 \\mathbb{P}r(y,\\phi)d\\phi, might be challenging. But if we have draws from the joint distribution, we can just discard the \\phi^∗_i draws and use the y^∗_i as samples from their marginal distribution.\nThis is also called the prior predictive distribution introduced in the previous course.\nIn the next segment, we will demonstrate some of these principles.\nRemember, we do not yet know how to sample from the complicated posterior distributions introduced in the previous lesson.\nBut once we learn that, we will be able to use the principles from this lesson to make approximate inferences from those posterior distributions.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#computing-examples",
    "href": "C2-L03.html#computing-examples",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "33.4 Computing Examples",
    "text": "33.4 Computing Examples\nMonte Carlo simulation from the most common distributions is very straightforward in R.\nLet’s start with the example from the previous segment, where \\theta \\sim Gamma(a,b) with a=2, b=1/3 . This could represent the posterior distribution of \\theta if our data came from a Poisson distribution with mean \\theta and we had used a conjugate gamma prior. Let’s start with m=100 .\n\nset.seed(32) # Initializes the random number generator so we can replicate these results. To get different random numbers, change the seed. \nm = 100 \na = 2.0 \nb = 1.0 / 3.0 \n\nTo simulate m independent samples, use the rgamma function.\n\ntheta &lt;- rgamma(n=m, shape = a, rate=b) \n\nWe can plot a histogram of the generated data, and compare that to the true density.\n\nhist(theta, freq=FALSE) \ncurve(dgamma(x=x, shape=a, rate=b), col=\"blue\", add=TRUE)\n\n\n\n\n\n\n\nFigure 33.1: Histogram of simulated gamma samples with true density\n\n\n\n\n\nTo find our Monte Carlo approximation to \\mathbb{E}(\\theta) , let’s take the average of our sample (and compare it with the truth).\n\nsum(theta) / m # sample mean \n\n[1] 5.514068\n\n\n\nmean(theta) # sample mean \n\n[1] 5.514068\n\n\n\na / b # true expected value\n\n[1] 6\n\n\nNot bad, but we can do better if we increase m to say, 10,000.\n\nm = 1e4 \ntheta = rgamma(n=m, shape=a, rate=b) \nmean(theta)\n\n[1] 6.023273\n\n\nHow about the variance of \\theta ?\n\nvar(theta) # sample variance\n\n[1] 18.04318\n\n\n\na / b^2 # true variance of Gamma(a,b) \n\n[1] 18\n\n\nWe can also approximate the probability that \\theta &lt; 5 .\n\nind = theta &lt; 5.0 # set of indicators, TRUE if theta_i &lt; 5 \nmean(ind)         # automatically converts FALSE/TRUE to 0/1 \n\n[1] 0.497\n\n\n\npgamma(q=5.0, shape=a, rate=b) # true value of Pr( theta &lt; 5 )\n\n[1] 0.4963317\n\n\nWhat is the 0.9 quantile (90th percentile) of \\theta ? We can use the quantile function which will order the samples for us and find the appropriate sample quantile.\n\nquantile(x=theta, probs=0.9) \n\n     90% \n11.74338 \n\n\n\nqgamma(p=0.9, shape=a, rate=b) # true value of 0.9 quantile\n\n[1] 11.66916",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#monte-carlo-error",
    "href": "C2-L03.html#monte-carlo-error",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "33.5 Monte Carlo error",
    "text": "33.5 Monte Carlo error\nWe can use the CLT to approximate how accurate our Monte Carlo estimates are. For example, if we seek E(\\theta) , then the sample mean \\bar\\theta^∗ approximately follows a normal distribution with mean \\mathbb{E}(\\theta) and variance Var(\\theta)/m . We will use the sample standard deviation divided by the square root of m to approximate the Monte Carlo standard deviation.\n\nse = sd(theta) / sqrt(m) \n2.0 * se # we are reasonably confident that the Monte Carlo estimate is no more than this far from the truth\n\n[1] 0.08495454\n\n\nThese numbers give us a reasonable range for the quantity we are estimating with Monte Carlo. The same applies for other Monte Carlo estimates, like the probability that \\theta &lt; 5.\n\nind = theta &lt; 5.0 \nse = sd(ind) / sqrt(m)\n2.0 * se # we are reasonably confident that the Monte Carlo estimate is no more than this far from the truth \n\n[1] 0.01000032",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#marginalization-1",
    "href": "C2-L03.html#marginalization-1",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "33.6 Marginalization",
    "text": "33.6 Marginalization\nLet’s also do the second example of simulating a hierarchical model. In our example from the previous segment, we had a binomial random variable where y \\mid \\phi \\overset{\\text{iid}}{\\sim}\\text{Binomial}(10,\\phi), and \\phi \\sim Beta(2,2). To simulate from this joint distribution, repeat these steps for a large number m :\n\nSimulate \\phi_i from its Beta(2,2) distribution.\nGiven the drawn \\phi_i , simulate y_i from Bin(10,\\phi_i) .\n\n\nm = 10e4\n\ny = numeric(m) # create the vectors we will fill in with simulations \nphi = numeric(m)\n\nfor (i in 1:m) {\n  phi[i] = rbeta(n=1, shape1=2.0, shape2=2.0)\n  y[i] = rbinom(n=1, size=10, prob=phi[i]) \n} \n# which is equivalent to the following 'vectorized' code \nphi = rbeta(n=m, shape1=2.0, shape2=2.0) \ny = rbinom(n=m, size=10, prob=phi)\n\nIf we are interested only in the marginal distribution of y , we can just ignore the draws for \\phi and treat the draws of y as a sample from its marginal distribution.\n\nmean(y) \n\n[1] 5.00008\n\n\n\nplot(prop.table(table(y)), ylab=\"Pr(y)\", main=\"Marginal distribution of y\")\n\n\n\n\n\n\n\nFigure 33.2",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#definition",
    "href": "C2-L03.html#definition",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "34.1 Definition",
    "text": "34.1 Definition\nIf we have a sequence of random variables X_1,X_2,\\dots X_n where the indices 1,2,\\dots,n represent successive points in time, we can use the chain rule of probability to calculate the probability of the entire sequence:\n\n\\mathbb{P}r(X_1, X_2, \\ldots X_n) = \\mathbb{P}r(X_1) \\cdot \\mathbb{P}r(X_2 \\mid X_1) \\cdot \\mathbb{P}r(X_3 \\mid X_2, X_1) \\cdot \\ldots \\cdot \\mathbb{P}r(X_n \\mid X_{n-1}, X_{n-2}, \\ldots, X_2, X_1) \\qquad\n\\tag{34.1}\nMarkov chains simplify this expression by using the Markov assumption. The assumption is that given the entire past history, the probability distribution for the random variable at the next time step only depends on the current variable. Mathematically, the assumption is written like this:\n\n\\mathbb{P}r(X_{t+1} \\mid X_t, X_{t-1}, \\ldots, X_2, X_1 ) = \\mathbb{P}r(X_{t+1} \\mid X_t) \\qquad\n\\tag{34.2}\nfor all t=2,\\dots,n. Under this assumption, we can write the first expression as\n\n\\mathbb{P}r(X_1, X_2, \\ldots X_n) = \\mathbb{P}r(X_1) \\cdot \\mathbb{P}r(X_2 \\mid X_1) \\cdot \\mathbb{P}r(X_3 \\mid X_2) \\cdot \\mathbb{P}r(X_4 \\mid X_3) \\cdot \\ldots \\cdot \\mathbb{P}r(X_n \\mid X_{n-1}) \\qquad\n\\tag{34.3}\nwhich is much simpler than the original. It consists of an initial distribution for the first variable, \\mathbb{P}r(X_1), and n−1 transition probabilities. We usually make one more assumption: that the transition probabilities do not change with time. Hence, the transition from time t to time t+1 depends only on the value of Xt.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#examples-of-markov-chains",
    "href": "C2-L03.html#examples-of-markov-chains",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "34.2 Examples of Markov chains",
    "text": "34.2 Examples of Markov chains\n\n34.2.1 Discrete Markov chain\nSuppose you have a secret number (make it an integer) between 1 and 5. We will call it your initial number at step 1. Now for each time step, your secret number will change according to the following rules:\n\nFlip a coin.\n\nIf the coin turns up heads, then increase your secret number by one (5 increases to 1).\nIf the coin turns up tails, then decrease your secret number by one (1 decreases to 5).\n\nRepeat n times, and record the evolving history of your secret number.\n\nBefore the experiment, we can think of the sequence of secret numbers as a sequence of random variables, each taking on a value in \\{1,2,3,4,5\\}. Assume that the coin is fair, so that with each flip, the probability of heads and tails are both 0.5.\nDoes this game qualify as a true Markov chain? Suppose your secret number is currently 4 and that the history of your secret numbers is (2,1,2,3). What is the probability that on the next step, your secret number will be 5? What about the other four possibilities? Because of the rules of this game, the probability of the next transition will depend only on the fact that your current number is 4. The numbers further back in your history are irrelevant, so this is a Markov chain.\nThis is an example of a discrete Markov chain, where the possible values of the random variables come from a discrete set. Those possible values (secret numbers in this example) are called states of the chain. The states are usually numbers, as in this example, but they can represent anything. In one common example, the states describe the weather on a particular day, which could be labeled as 1-fair, 2-poor.\n\n\n34.2.2 Random walk (continuous)\nNow let’s look at a continuous example of a Markov chain. Say X_t=0 and we have the following transition model:\n\n\\mathbb{P}r(X_{t+1}\\mid X_t=x_t)=N(x_t,1) \\qquad\n\\tag{34.4}\nThat is, the probability distribution for the next state is Normal with variance 1 and mean equal to the current state. This is often referred to as a “random walk.” Clearly, it is a Markov chain because the transition to the next state Xt+1 only depends on the current state Xt.\nThis example is straightforward to code in R:\n\nset.seed(34)\n\nn = 100\nx = numeric(n)\n\nfor (i in 2:n) {\n  x[i] = rnorm(1, mean=x[i-1], sd=1.0)\n}\n\nplot.ts(x)",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#transition-matrix",
    "href": "C2-L03.html#transition-matrix",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "34.3 Transition matrix",
    "text": "34.3 Transition matrix\nLet’s return to our example of the discrete Markov chain. If we assume that transition probabilities do not change with time, then there are a total of 25 (52) potential transition probabilities. Potential transition probabilities would be from State 1 to State 2, State 1 to State 3, and so forth. These transition probabilities can be arranged into a matrix Q:\n\nQ =\n\\begin{pmatrix}\n0 & .5 & 0 & 0 & .5 \\\\\n.5 & 0 & .5 & 0 & 0 \\\\\n0 & .5 & 0 & .5 & 0 \\\\\n0 & 0 & .5 & 0 & .5 \\\\\n.5 & 0 & 0 & .5 & 0 \\\\\n\\end{pmatrix} \\qquad\n\\tag{34.5}\nwhere the transitions from State 1 are in the first row, the transitions from State 2 are in the second row, etc. For example, the probability \\mathbb{P}r(X_{t+1}=5\\mid X_t=4) can be found in the fourth row, fifth column.\nThe transition matrix is especially useful if we want to find the probabilities associated with multiple steps of the chain. For example, we might want to know \\mathbb{P}r(X_{t+2}=3 \\mid X_t=1), the probability of your secret number being 3 two steps from now, given that your number is currently 1. We can calculate this as \\sum_{k=15} \\mathbb{P}r(X_t+2=3 \\mid X_t+1=k) \\cdot \\mathbb{P}r(X_{t+1}=k \\mid X_t=1), which conveniently is found in the first row and third column of Q_2.\nWe can perform this matrix multiplication easily in R:\n\nQ = matrix(c(0.0, 0.5, 0.0, 0.0, 0.5,\n             0.5, 0.0, 0.5, 0.0, 0.0,\n             0.0, 0.5, 0.0, 0.5, 0.0,\n             0.0, 0.0, 0.5, 0.0, 0.5,\n             0.5, 0.0, 0.0, 0.5, 0.0), \n           nrow=5, byrow=TRUE)\n\nQ %*% Q # Matrix multiplication in R. This is Q^2.\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,] 0.50 0.00 0.25 0.25 0.00\n[2,] 0.00 0.50 0.00 0.25 0.25\n[3,] 0.25 0.00 0.50 0.00 0.25\n[4,] 0.25 0.25 0.00 0.50 0.00\n[5,] 0.00 0.25 0.25 0.00 0.50\n\n\n\n(Q %*% Q)[1,3]\n\n[1] 0.25\n\n\nTherefore, if your secret number is currently 1, the probability that the number will be 3 two steps from now is .25.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L03.html#stationary-distribution",
    "href": "C2-L03.html#stationary-distribution",
    "title": "33  Monte Carlo estimation - M1L3",
    "section": "34.4 Stationary distribution",
    "text": "34.4 Stationary distribution\nSuppose we want to know the probability distribution of the your secret number in the distant future, say \\mathbb{P}r(X_{t+h} \\mid X_t) where h is a large number. Let’s calculate this for a few different values of h.\n\nQ5 = Q %*% Q %*% Q %*% Q %*% Q # h=5 steps in the future\nround(Q5, 3)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 0.062 0.312 0.156 0.156 0.312\n[2,] 0.312 0.062 0.312 0.156 0.156\n[3,] 0.156 0.312 0.062 0.312 0.156\n[4,] 0.156 0.156 0.312 0.062 0.312\n[5,] 0.312 0.156 0.156 0.312 0.062\n\n\n\nQ10 = Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q %*% Q # h=10 steps in the future\nround(Q10, 3)\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 0.248 0.161 0.215 0.215 0.161\n[2,] 0.161 0.248 0.161 0.215 0.215\n[3,] 0.215 0.161 0.248 0.161 0.215\n[4,] 0.215 0.215 0.161 0.248 0.161\n[5,] 0.161 0.215 0.215 0.161 0.248\n\n\n\nQ30 = Q\nfor (i in 2:30) {\n  Q30 = Q30 %*% Q\n}\nround(Q30, 3) # h=30 steps in the future\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 0.201 0.199 0.200 0.200 0.199\n[2,] 0.199 0.201 0.199 0.200 0.200\n[3,] 0.200 0.199 0.201 0.199 0.200\n[4,] 0.200 0.200 0.199 0.201 0.199\n[5,] 0.199 0.200 0.200 0.199 0.201\n\n\nNotice that as the future horizon gets more distant, the transition distributions appear to converge. The state you are currently in becomes less important in determining the more distant future. If we let h get really large, and take it to the limit, all the rows of the long-range transition matrix will become equal to (.2,.2,.2,.2,.2). That is, if you run the Markov chain for a very long time, the probability that you will end up in any particular state is 1/5=.2 for each of the five states. These long-range probabilities are equal to what is called the stationary distribution of the Markov chain.\nThe stationary distribution of a chain is the initial state distribution for which performing a transition will not change the probability of ending up in any given state. That is,\n\nc(0.2, 0.2, 0.2, 0.2, 0.2) %*% Q\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  0.2  0.2  0.2  0.2  0.2\n\n\nOne consequence of this property is that once a chain reaches its stationary distribution, the stationary distribution will remain the distribution of the states thereafter.\nWe can also demonstrate the stationary distribution by simulating a long chain from this example.\n\nn = 5000\nx = numeric(n)\nx[1] = 1 # fix the state as 1 for time 1\nfor (i in 2:n) {\n  x[i] = sample.int(5, size=1, prob=Q[x[i-1],]) # draw the next state from the intergers 1 to 5 with probabilities from the transition matrix Q, based on the previous value of X.\n}\n\nNow that we have simulated the chain, let’s look at the distribution of visits to the five states.\n\n\n\nTable 34.1\n\n\ntable(x) / n\n\nx\n     1      2      3      4      5 \n0.1996 0.2020 0.1980 0.1994 0.2010 \n\n\n\n\nThe overall distribution of the visits to the states is approximately equal to the stationary distribution.\nAs we have just seen, if you simulate a Markov chain for many iterations, the samples can be used as a Monte Carlo sample from the stationary distribution. This is exactly how we are going to use Markov chains for Bayesian inference. In order to simulate from a complicated posterior distribution, we will set up and run a Markov chain whose stationary distribution is the posterior distribution.\nIt is important to note that the stationary distribution doesn’t always exist for any given Markov chain. The Markov chain must have certain properties, which we won’t discuss here. However, the Markov chain algorithms we’ll use in future lessons for Monte Carlo estimation are guaranteed to produce stationary distributions.\n\n34.4.1 Continuous example\nThe continuous random walk example we gave earlier does not have a stationary distribution. However, we can modify it so that it does have a stationary distribution.\nLet the transition distribution be \\mathbb{P}r(X_{t + 1}\\mid X_t = x_t)=N(\\phi x_t,1) where -1 &lt; \\phi &lt; 1. That is, the probability distribution for the next state is Normal with variance 1 and mean equal to ϕ times the current state. As long as \\phi is between −1 and 1, then the stationary distribution will exist for this model.\nLet’s simulate this chain for \\phi=−0.6.\n\nset.seed(38)\n\nn = 1500\nx = numeric(n)\nphi = -0.6\n\nfor (i in 2:n) {\n  x[i] = rnorm(1, mean=phi*x[i-1], sd=1.0)\n}\n\nplot.ts(x)\n\n\n\n\n\n\n\nFigure 34.1: Simulated AR(1) process with phi=-0.6\n\n\n\n\n\nThe theoretical stationary distribution for this chain is normal with mean 0 and variance 1/(1−\\phi^2), which in our example approximately equals 1.562. Let’s look at a histogram of our chain and compare that with the theoretical stationary distribution.\n\n\\text{Var}_{\\text{stationary}} = \\frac{1}{1-\\phi^2} \\qquad\n\\tag{34.6}\n\nhist(x, freq=FALSE)\ncurve(dnorm(x, mean=0.0, sd=sqrt(1.0/(1.0-phi^2))), col=\"red\", add=TRUE)\nlegend(\"topright\", legend=\"theoretical stationary\\ndistribution\", col=\"red\", lty=1, bty=\"n\")\n\n\n\n\n\n\n\nFigure 34.2: Histogram of simulated AR(1) process with theoretical stationary distribution\n\n\n\n\n\nIt appears that the chain has reached the stationary distribution. Therefore, we could treat this simulation from the chain like a Monte Carlo sample from the stationary distribution, a normal with mean 0 and variance 1.562.\nBecause most posterior distributions we will look at are continuous, our Monte Carlo simulations with Markov chains will be similar to this example.",
    "crumbs": [
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Monte Carlo estimation - M1L3</span>"
    ]
  },
  {
    "objectID": "C2-L04.html",
    "href": "C2-L04.html",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "",
    "text": "34.1 Markov chain Monte Carlo (MCMC)\nMetropolis-Hastings (M-H) is an algorithm that allows us to sample from a generic probability distribution (which we will call the target distribution), even if we do not know the normalizing constant. To do this, we construct and sample from a Markov chain whose stationary distribution is the target distribution. It consists of picking an arbitrary starting value and iteratively accepting or rejecting candidate samples drawn from another distribution, one that is easy to sample.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#sec-m2l4-metropolis-hastings",
    "href": "C2-L04.html#sec-m2l4-metropolis-hastings",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "",
    "text": "ImportantWhy use M-H or MCMC?\n\n\n\nWe will use M-H or other MCMC methods if there is no easy way to simulate independent draws from the target distribution. This can be due to non-conjugate priors, challenges in evaluating the normalizing constant or multiple explanatory variables.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#sec-",
    "href": "C2-L04.html#sec-",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "34.2 The Metropolis-Hastings Algorithm 🎥",
    "text": "34.2 The Metropolis-Hastings Algorithm 🎥\n\n\n\n\nThe Metropolis-Hastings Algorithm\n\n Let’s say we wish to produce samples from a target distribution \\mathbb{P}r(\\theta) \\propto g(\\theta), where we don’t know the normalizing constant (since \\int g(\\theta)d\\theta is hard or impossible to compute), so we only have g(\\theta), the unnormalized joint probability to work with. The Metropolis-Hastings algorithm proceeds as follows.\n\nSelect an initial value \\theta_0.\nFor i=1,\\dots,m repeat the following steps:\n\nDraw a candidate sample \\theta^∗ from a proposal distribution  q(\\theta^* \\mid \\theta_{i−1}) .\nCompute the ratio \\alpha = \\frac{g(\\theta^*) / q(\\theta^* \\mid \\theta_{i-1}) }{g(\\theta_{i-1}) / q(\\theta_{i-1} \\mid \\theta^*)} = \\frac{g(\\theta^*)q(\\theta_{i-1} \\mid \\theta^*)}{g(\\theta_{i-1})q(\\theta^* \\mid \\theta_{i-1})}\n\nIf \\alpha\\ge 1, then accept \\theta^∗ and set \\theta_i=\\theta^∗.\nIf 0&lt;\\alpha&lt;1:\n\naccept \\theta^∗ and set \\theta_i=\\theta^∗ with probability \\alpha,\nreject \\theta^∗ and set \\theta_i=\\theta_{i−1} with probability 1−\\alpha.\n\n\n\n\nproposal distribution q\n\n\n\n\n\nImportantCorrection to the proposal distribution\n\n\n\nSteps 2.b and 2.c act as a correction  since the proposal distribution is not the target distribution. At each step in the chain, we draw a random candidate value of the parameter and decide whether to “move” the chain there or remain where we are. If the proposed move to the candidate is “advantageous,” (\\alpha \\ge 1) we “move” there and if it is not “advantageous,” we still might move there, but only with probability \\alpha. Since our decision to “move” to the candidate only depends on where the chain currently is, this is a Markov chain.\n\n\ncorrection",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#proposal-distribution-q",
    "href": "C2-L04.html#proposal-distribution-q",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "34.3 Proposal distribution q",
    "text": "34.3 Proposal distribution q\nOne careful choice we must make is the candidate generating distribution q(\\theta^∗\\mid\\theta_{i−1}). It may or may not depend on the previous iteration’s value of \\theta.\n\n\n\n\n\n\nImportantIndependent Metropolis-Hastings\n\n\n\nThe simpler case is when the proposal distribution q does not depend on the previous value. We then write it as q(\\theta^∗). This arises if it is always the same distribution. We call this case independent Metropolis-Hastings. If we use independent M-H, q(\\theta) should be as similar as possible to \\mathbb{P}r(\\theta).\n\n\n\n\n\n\n\n\nImportantRandom-Walk Metropolis-Hastings\n\n\n\nIn the more general case, the proposal distribution takes the form q(\\theta^∗\\mid\\theta_{i−1}) with dependence on the previous iteration, is Random-Walk Metropolis-Hastings. Here, the proposal distribution is centered on \\theta_{i−1}.\nFor instance, it might be a Normal distribution with mean \\theta_{i−1}. Because the Normal distribution is symmetric, this example comes with another advantage: q(\\theta^* \\mid \\theta_{i−1})=q(\\theta_{i−1}∣\\theta^*) causing it to cancel out when we calculate \\alpha.\nThus, in Random-Walk M-H where the candidate is drawn from a Normal with mean \\theta_{i−1} and constant variance, the acceptance ratio is simply \\alpha=g(\\theta^∗)/g(\\theta_{i−1}).",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#acceptance-rate-α",
    "href": "C2-L04.html#acceptance-rate-α",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "34.4 Acceptance rate α",
    "text": "34.4 Acceptance rate α\nClearly, not all candidate draws are accepted, so our Markov chain sometimes “stays” where it is, possibly for many iterations. How often you want the chain to accept candidates depends on the type of algorithm you use. If you approximate \\mathbb{P}r(\\theta) with q(\\theta^∗) and always draw candidates from that, accepting candidates often is good; it means q(\\theta^∗) is approximating \\mathbb{P}r(\\theta) well. However, you still may want q to have a larger variance than p and see some rejection of candidates as an assurance that q is covering the space well.\nAs we will see in coming examples, a high acceptance rate for the Random-Walk Metropolis-Hastings sampler is not a good thing. If the random walk is taking too small of steps, it will accept often but will take a very long time to fully explore the posterior. If the random walk is taking too large of steps, many of its proposals will have a low probability and the acceptance rate will be low, wasting many draws. Ideally, a random walk sampler should accept somewhere between 23% and 50% of the candidates proposed.\nIn the next segment, we will see a demonstration of this algorithm used in a discrete case, where we can show mathematically that the Markov chain converges to the target distribution. In the following segment, we will demonstrate coding a Random-Walk Metropolis-Hastings algorithm in R to solve one of the problems from the end of Lesson 2.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#demonstration-of-a-discrete-case",
    "href": "C2-L04.html#demonstration-of-a-discrete-case",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "34.5 Demonstration of a Discrete case",
    "text": "34.5 Demonstration of a Discrete case\n\n\n\n\nMCMC Coin Flip Example\n\nThe following segment is by Herbert Lee, a professor of statistics and applied mathematics at the University of California, Santa Cruz.\nThe following is a demonstration of using Markov chain Monte Carlo, used to estimate posterior probabilities in a simplified case, where we can actually work out the correct answer in closed form. We demonstrate that the Metropolis-Hastings algorithm is indeed working, and giving us the right answer.\nIf you recall from the previous course, the example where your brother or maybe your sister, has a loaded coin that you know will come up heads 70% of the time. But they come to you with some coin, you’re not sure if it’s the loaded coin or a fair coin, and they want to make a bet with you. And you have to figure out which coin this is.\nSuppose you have a prior probability that it’s a 60% probability, that they’ll bring a loaded coin to you. They let you flip it five times, and you get two heads and three tails.\nAnd then you need to figure out, what’s your posterior probability that this is a loaded coin.\nOur unknown parameter \\theta, can either take the values fair or loaded.\n\n\\theta = \\{\\text{fair, loaded} \\}\n\\tag{34.1}\nOur prior for \\theta is the probability of theta equals loaded, is 0.6.\n\n\\mathbb{P}r(\\theta=\\text{loaded})=0.6 \\qquad  \\text{(prior)}\n\\tag{34.2}\nOur likelihood will follow a Binomial distribution, depending upon the value of \\theta.\n\nf(x\\mid \\theta) = {5 \\choose x} \\frac{1}{2}^5\\mathbb{I}_{\\theta=\\text{fair}}+  {5 \\choose x} (.7)^x(.3)^{5-x}\\mathbb{I}_{\\theta=\\text{loaded}}  \\qquad  \\text{(likelihood)}\n\\tag{34.3}\nOur posterior then, we can look at posterior for theta, given that we saw x=2 equals two heads, posterior is the likelihood times the prior, divided by a normalizing constant.\n\n  \\begin{aligned}\n    f(\\theta \\mid X=2) &=\n      \\frac{ \\frac{1}{2}^5(0.4)\\mathbb{I}_{(\\theta=\\text{fair})} + (.7)^2(.3)^{3}(.6)\\mathbb{I}_{(\\theta=\\text{loaded})}}\n           { \\frac{1}{2}^5(0.4) + (.7)^2(.3)^{3}(.6)}  \n  \\\\&=\\frac{ 0.0125 \\mathbb{I}_{(\\theta=\\text{fair})} + 0.00794 \\mathbb{I}_{(\\theta=\\text{loaded})}}\n           { 0.0125 + 0.00794}\n  \\\\&= 0.612 \\mathbb{I}_{(\\theta=\\text{fair})} + 0.388 \\mathbb{I}_{(\\theta=\\text{loaded})}\n  \\qquad  \\text{(posterior) }\n  \\end{aligned}\n\\tag{34.4}\nIn this case, we can work out the binomial and our prior. And we see that we get these expressions at the end. We get posterior probability of \\theta is loaded given that we saw two heads, to be 0.388.\n\n\\therefore \\mathbb{P}r(\\theta=\\text{loaded}\\mid X=2) = 0.388 \\qquad  \\text{(posterior conditional probability ) }\n\\tag{34.5}\nThis is all review from the previous course so far.\nBut suppose we had a more complicated problem, where we couldn’t work this all out in closed form? We’ll know the likelihood and the prior, but we may not be able to get this normalizing constant. Can we instead do this by simulation? And indeed, yes we can.\nWe can do this with Markov chain Monte Carlo. In particular, using the Metropolis-Hastings algorithm. What we’ll do is, we’ll set up a Markov chain whose equilibrium distribution has this posterior distribution. So we’ll consider a Markov chain with two states, theta equals fair and theta equals loaded. And we’ll allow the chain to move between those two states, with certain transition probabilities. We set this up using this using the Metropolis-Hastings algorithm.\n\n\n\n\nThe Metropolis-Hastings Algorithm\n\nSo under the Metropolis-Hastings algorithm, step one is we start at an arbitrary location. And in this case, we can\n\nstart at either \\theta \\ne \\text{fair}, or \\theta \\ne \\text{loaded}.\n\nIt does not really matter where we start, we’ll be moving back and forth and we’re going to look at the long-term running average, the long-term simulations.\nSo the key is we’ll be simulating.\n\nRun m simulations and in each iteration, we’ll propose a candidate and either accept it or reject it.\n\n\nSo the first part is we’re proposing a new candidate. We’ll call this candidate \\theta^*, and we’re going to propose it be the other state compared to where we are now. Where we are now is \\theta_{i-1}, and so we’ll propose to move to \\theta^*.\n\nIf our current state is fair, we’ll propose \\theta^*=\\text{loaded}.\nIf our current state is loaded, we’ll propose \\theta^*=\\text{fair}.\n\n\nwhat’s our acceptance probability alpha?\nThe general form for \\alpha is:\n\n\\begin {aligned}\n\\alpha &= {\n            { g(\\theta^*)     / q(\\theta^*     \\mid  \\theta_{i-1}) }\n      \\over {g(\\theta_{i-1}) / q(\\theta_{i-1} \\mid  \\theta^*)     }\n      }\n\\\\      &= {\n            { f(x=2 \\mid \\theta^*) f(\\theta^*)     / 1 }\n      \\over { f(x=2 \\mid \\theta_{i-1})f(\\theta_{i-1}) / 1    }\n} \\qquad \\text {(sub. g,q)}\n\\end{aligned}\n\\tag{34.6}\nIn this case,\n\ng() is our un-normalized likelihood times prior\nq(), the proposal distribution, is, in this case, since we always accept the opposite state deterministically i.e. \\theta^*=\\neg \\theta{i_1} with P=1\nIf \\theta^* = \\text{loaded} \\implies \\alpha = {0.00794 \\over 0.0125}=0.635\nIf \\theta^* = \\text{fair} \\implies \\alpha = { 0.0125 \\over 0.00794}=1.574\n\n\n\n\n\nThe Metropolis-Hastings Algorithm\n\nGiven these probabilities, we then can do the acceptance or rejection step.\n\n\\begin{cases}\n\\text{ accept } \\theta^* \\text { and set } \\theta_i=\\text{fair} & \\text{If } \\theta^*=\\text{fair,  } \\alpha&gt;1\n\\\\ \\begin {cases}\n   \\text{ accept } \\theta^* \\text{  and set } \\theta_i=\\text{loaded} &  \\text{ With probability } 0.635\n\\\\ \\text{ reject } \\theta^* \\text{ and set } \\theta_i=\\text{fair}     &  \\text{ Otherwise }\n\\end{cases} & \\text{If } \\theta^*=\\text{loaded, } \\alpha=.635\n\\end{cases}\n\nIf the \\theta^*=\\text{loaded} \\implies \\alpha=0.635. So we accept theta star with probability 0.635. And if we accept it. Set \\theta_i=\\text{loaded} Otherwise, set \\theta_i = \\theta_{i- 1}, if we do not accept, it stays in that same old fair state.\nWe can draw this out as a Markov chain with two states, Fair and ‘loaded’. If it’s in the ‘loaded’ state, it will move with probability one to the fair state. If it’s in the fair state, it will move with a probability of 0.635 to the ‘loaded’ state. And with a probability of 0.365 it will stay in the fair state.\n\n\n\n\nstate diagram\n\nAnd so here’s a little diagram for this Markov chain with two states. In which case it will move back and forth with certain probabilities.\nThus, if we wanted to find our posterior probability , f(\\theta=\\text{loaded} \\mid x=2). We can simulate from this Markov chain using these transition probabilities. And observe the fraction of time that it spends in the state theta equals ‘loaded’. And this gives us a good estimate of the posterior probability that it’s the ‘loaded’ coin. In this particular case, we can also show that this gives us the theoretical right answer.\nIf you’ve seen a little bit of the theory of Markov chains. We can say that a Markov chain with transition probability capital P, has stationary distribution \\Pi.\n\n\\pi P = \\pi \\qquad \\text{(def. stationary distribution)}\n\\tag{34.7}\nHere we have a transition probability matrix P, where we can think about ‘fair’ and ‘loaded’. Moving from the ‘fair’ state, remaining in the ‘fair’ state happens with a probability of 0.365 and it moves from ‘fair’ to ‘loaded’, with a probability of 0.635. If it’s in the ‘loaded’ state, we’ll move to the ‘fair’ state with probability one, and it will stay in the ‘loaded’ state with probability 0.\n\nP=\n\\begin{bmatrix}\n   0.365 & 0.635 \\\\\n   1     & 0\n\\end{bmatrix}\n\nIn this case, we want our stationary distribution to be the posterior probabilities.\n\n\\Pi=\n\\begin{bmatrix}\n    0.612 & 0.388 \\\\\n\\end{bmatrix}\n\nWhich you can recall are 0.612 of being ‘fair’ and 0.388 of being ‘loaded’. And so indeed, if you do just the minimal amount of matrix algebra, you can see that 0.612, 0.388 Multiplied by this matrix, 0.365, 0.635, 1, 0, does indeed give you 0.612 and 0.388, at least to within rounding error.\n\n\\begin{aligned}\n  \\Pi P &=\n  \\begin{bmatrix}\n  0.612 & 0.388\n  \\end{bmatrix}\n  \\begin{bmatrix}\n  0.365 & 0.635 \\\\\n      1 & 0\n  \\end{bmatrix}   \\\\\n  &= \\begin{bmatrix}\n  0.612 & 0.388\n  \\end{bmatrix}\n  \\\\&= \\Pi\n\\end{aligned}\n\\tag{34.8}\nThus in this case we can see, that we do get the correct stationary distribution for the Markov chain using the Metropolis–Hastings algorithm. And that when we simulate it, we do get correct estimates then of the posterior probabilities.\nThis is a nice simple example where we can work out the posterior probabilities in closed form. We don’t need to run Markov chain Monte Carlo. But this method is very powerful because all we need is to be able to evaluate the likelihood and the prior, we don’t need to evaluate the full posterior and get that normalizing constant. And so this applies to a much broader range of more complicated problems. Where we can use Markov chain Monte Carlo to simulate, to be able to get these probabilities. We’ll make good use of this in the rest of this course.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#random-walk-with-normal-likelihood-t-prior",
    "href": "C2-L04.html#random-walk-with-normal-likelihood-t-prior",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "34.6 Random walk with Normal likelihood, t prior",
    "text": "34.6 Random walk with Normal likelihood, t prior\nRecall the model from the last segment of Lesson 2 where the data are the percent change in total personnel from last year to this year for n=10 companies. We used a normal likelihood with known variance and t distribution for the prior on the unknown mean. Suppose the values are y=(1.2,1.4,−0.5,0.3,0.9,2.3,1.0,0.1,1.3,1.9). Because this model is not conjugate, the posterior distribution is not in a standard form that we can easily sample. To obtain posterior samples, we will set up a Markov chain whose stationary distribution is this posterior distribution.\nRecall that the posterior distribution is\n\n\\mathbb{P}r(\\mu \\mid y_1, \\ldots, y_n) \\propto \\frac{\\exp[ n ( \\bar{y} \\mu - \\mu^2/2)]}{1 + \\mu^2}\n\nThe posterior distribution on the left is our target distribution and the expression on the right is our g(\\mu).\nThe first thing we can do in R is write a function to evaluate g(\\mu). Because posterior distributions include likelihoods (the product of many numbers that are potentially small), g(\\mu) might evaluate to such a small number that to the computer, it is effectively zero. This will cause a problem when we evaluate the acceptance ratio \\alpha. To avoid this problem, we can work on the log scale, which will be more numerically stable. Thus, we will write a function to evaluate\n\n\\log(g(\\mu)) = n ( \\bar{y} \\mu - \\mu^2/2) - \\log(1 + \\mu^2)\n\nThis function will require three arguments, \\mu, \\bar{y}, and n.\n\nlg = function(mu, n, ybar) {\n  mu2 = mu^2\n  n * (ybar * mu - mu2 / 2.0) - log(1 + mu2)\n}\n\nNext, let’s write a function to execute the Random-Walk Metropolis-Hastings sampler with Normal proposals.\n\nmh = function(n, ybar, n_iter, mu_init, cand_sd) {\n  ## Random-Walk Metropolis-Hastings algorithm\n  \n  ## Step 1, initialize\n  mu_out = numeric(n_iter)\n  accpt = 0\n  mu_now = mu_init\n  lg_now = lg(mu=mu_now, n=n, ybar=ybar)\n  \n  ## Step 2, iterate\n  for (i in 1:n_iter) {\n    ## step 2a\n    mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate\n    \n    ## Step 2b\n    lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate\n    lalpha = lg_cand - lg_now # log of acceptance ratio\n    alpha = exp(lalpha)\n    \n    ## step 2c\n    u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)\n    if (u &lt; alpha) { # then accept the candidate\n      mu_now = mu_cand\n      accpt = accpt + 1 # to keep track of acceptance\n      lg_now = lg_cand\n    }\n    \n    ## collect results\n    mu_out[i] = mu_now # save this iteration's value of mu\n  }\n  \n  ## return a list of output\n  list(mu=mu_out, accpt=accpt/n_iter)\n}\n\nNow, let’s set up the problem.\n\ny = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)\nybar = mean(y)\nn = length(y)\nhist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data\ncurve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu\npoints(y, rep(0,n), pch=1) # individual data points\npoints(ybar, 0, pch=19) # sample mean\n\n\n\n\n\n\n\n\nFinally, we’re ready to run the sampler! Let’s use m=1000 iterations and proposal standard deviation (which controls the proposal step size) 3.0, and initial value at the prior median 0.\n\nset.seed(43) # set the random seed for reproducibility\npost = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=3.0)\nstr(post)\n\nList of 2\n $ mu   : num [1:1000] -0.113 1.507 1.507 1.507 1.507 ...\n $ accpt: num 0.122\n\n\n\nlibrary(\"coda\")\ntraceplot(as.mcmc(post$mu))\n\n\n\n\n\n\n\n\nThis last plot is called a trace plot. It shows the history of the chain and provides basic feedback about whether the chain has reached its stationary distribution.\nIt appears our proposal step size was too large (acceptance rate below 23%). Let’s try another.\n\npost = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.05)\npost$accpt\n\n[1] 0.946\n\n\n\ntraceplot(as.mcmc(post$mu))\n\n\n\n\n\n\n\n\nOops, the acceptance rate is too high (above 50%). Let’s try something in between.\n\npost = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.9)\npost$accpt\n\n[1] 0.38\n\n\n\ntraceplot(as.mcmc(post$mu))\n\n\n\n\n\n\n\n\nWhich looks good. Just for fun, let’s see what happens if we initialize the chain at some far-off value.\n\npost = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=30.0, cand_sd=0.9)\npost$accpt\n\n[1] 0.387\n\n\n\ntraceplot(as.mcmc(post$mu))\n\n\n\n\n\n\n\n\nIt took awhile to find the stationary distribution, but it looks like we succeeded! If we discard the first 100 or so values, it appears like the rest of the samples come from the stationary distribution, our posterior distribution! Let’s plot the posterior density against the prior to see how the data updated our belief about \\mu.\n\npost$mu_keep = post$mu[-c(1:100)] # discard the first 200 samples\nplot(density(post$mu_keep, adjust=2.0), main=\"\", xlim=c(-1.0, 3.0), xlab=expression(mu)) # plot density estimate of the posterior\ncurve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu\npoints(ybar, 0, pch=19) # sample mean\n\ncurve(0.017*exp(lg(mu=x, n=n, ybar=ybar)), from=-1.0, to=3.0, add=TRUE, col=\"blue\") # approximation to the true posterior in blue\n\n\n\n\n\n\n\n\nThese results are encouraging, but they are preliminary. We still need to investigate more formally whether our Markov chain has converged to the stationary distribution. We will explore this in a future lesson.\nObtaining posterior samples using the Metropolis-Hastings algorithm can be time-consuming and require some fine-tuning, as we’ve just seen. The good news is that we can rely on software to do most of the work for us. In the next couple of videos, we’ll introduce a program that will make posterior sampling easy.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#setup",
    "href": "C2-L04.html#setup",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "35.1 Setup",
    "text": "35.1 Setup\n\n35.1.1 Introduction to JAGS\nThere are several software packages available that will handle the details of MCMC for us. See the supplementary material for a brief overview of options.\nThe package we will use in this course is JAGS (Just Another Gibbs Sampler) by Martyn Plummer. The program is free, and runs on Mac OS, Windows, and Linux. Better yet, the program can be run using R with the rjags and R2jags packages.\nIn JAGS, we can specify models and run MCMC samplers in just a few lines of code; JAGS does the rest for us, so we can focus more on the statistical modeling aspect and less on the implementation. It makes powerful Bayesian machinery available to us as we can fit a wide variety of statistical models with relative ease.\n\n\n35.1.2 Installation and setup\nThe starting place for JAGS users is mcmc-jags.sourceforge.net. At this site, you can find news about the features of the latest release of JAGS, links to program documentation, as well as instructions for installation.\nThe documentation is particularly important. It is available under the files page link in the Manuals folder.\nAlso under the files page, you will find the JAGS folder where you can download and install the latest version of JAGS. Select the version and operating system, and follow the instructions for download and installation.\nOnce JAGS is installed, we can immediately run it from R using the rjags package. The next segment will show how this is done.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04.html#modeling-in-jags",
    "href": "C2-L04.html#modeling-in-jags",
    "title": "34  Metropolis-Hastings - M2L4",
    "section": "35.2 Modeling in JAGS",
    "text": "35.2 Modeling in JAGS\nThere are four steps to implementing a model in JAGS through R:\n\nSpecify the model.\nSet up the model.\nRun the MCMC sampler.\nPost-processing.\n\nWe will demonstrate these steps with our running example with the data are the percent change in total personnel from last year to this year for n=10 companies. We used a normal likelihood with known variance and t distribution for the prior on the unknown mean.\n\n35.2.1 1. Specify the model\nIn this step, we give JAGS the hierarchical structure of the model, assigning distributions to the data (the likelihood) and parameters (priors). The syntax for this step is very similar to R, but there are some key differences.\n\nlibrary(\"rjags\")\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\nmod_string = \" model {\n  for (i in 1:n) {\n    y[i] ~ dnorm(mu, 1.0/sig2)\n  }\n  mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom\n  sig2 = 1.0\n} \"\n\nOne of the primary differences between the syntax of JAGS and R is how the distributions are parameterized. Note that the normal distribution uses the mean and precision (instead of variance). When specifying distributions in JAGS, it is always a good idea to check the JAGS user manual here in the chapter on Distributions.\n\n\n35.2.2 2. Set up the model\n\nset.seed(50)\ny = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)\nn = length(y)\n\ndata_jags = list(y=y, n=n)\nparams = c(\"mu\")\n\ninits = function() {\n  inits = list(\"mu\"=0.0)\n} # optional (and fixed)\n\nmod = jags.model(textConnection(mod_string), data=data_jags, inits=inits)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 10\n   Unobserved stochastic nodes: 1\n   Total graph size: 15\n\nInitializing model\n\n\nThere are multiple ways to specify initial values here. They can be explicitly set, as we did here, or they can be random, i.e., list(\"mu\"=rnorm(1)). Also, we can omit the initial values, and JAGS will provide them.\n\n\n35.2.3 3. Run the MCMC sampler\n\nupdate(mod, 500) # burn-in\n\nmod_sim = coda.samples(model=mod, variable.names=params, n.iter=1000)\n\nWe will discuss more options to the coda.samples function in coming examples.\n\n\n35.2.4 4. Post-processing\n\nsummary(mod_sim)\n\n\nIterations = 1501:2500\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 1000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n      0.892865       0.310328       0.009813       0.012867 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n0.3105 0.6764 0.8873 1.1039 1.4850 \n\n\n\nlibrary(\"coda\")\nplot(mod_sim)\n\n\n\n\n\n\n\n\nWe will discuss post processing further, including convergence diagnostics, in a coming lesson.",
    "crumbs": [
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Metropolis-Hastings - M2L4</span>"
    ]
  },
  {
    "objectID": "C2-L04-Ex1.html",
    "href": "C2-L04-Ex1.html",
    "title": "35  Homework on the Metropolis-Hastings algorithm - M2L4HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Homework on the Metropolis-Hastings algorithm - M2L4HW1</span>"
    ]
  },
  {
    "objectID": "C2-L05.html",
    "href": "C2-L05.html",
    "title": "36  Gibbs sampling - M2L5",
    "section": "",
    "text": "36.0.1 Multiple parameter sampling and full conditional distributions 🎥\nGibbs sampling is a Gibbs sampling is named after the physicist Josiah Willard Gibbs, in reference to an analogy between the sampling algorithm and statistical physics. The algorithm was described in (Geman and Geman 1984) by brothers Stuart and Donald Geman, and became popularized in the statistics community for calculating marginal probability distribution, especially the posterior distribution. Gibbs sampling is better suited for sampling from models with many variables by sampling them one at a time from a full conditional distribution.\nSo far, we have demonstrated MCMC for a single parameter.\nWhat if we seek the posterior distribution of multiple parameters, and that posterior distribution does not have a standard form?\nOne option is to perform Metropolis-Hastings (M-H) by sampling candidates for all parameters at once, and accepting or rejecting all of those candidates together. While this is possible, it can get complicated.\nAnother (simpler) option is to sample the parameters one at a time.\nAs a simple example, suppose we have a joint posterior distribution for two parameters \\theta and \\phi, written \\mathbb{P}r(\\theta, \\phi \\mid y) \\propto g(\\theta, \\phi). If we knew the value of \\phi, then we would just draw a candidate for \\theta and use g(\\theta, \\phi) to compute our Metropolis-Hastings ratio, and possibly accept the candidate. Before moving on to the next iteration, if we don’t know \\phi, then we can perform a similar update for it. Draw a candidate for \\phi using some proposal distribution and again use g(\\theta, \\phi) to compute our Metropolis-Hastings ratio. Here we pretend we know the value of \\theta by substituting its current iteration from the Markov chain. Once we’ve drawn for both \\theta and \\phi, that completes one iteration and we begin the next iteration by drawing a new \\theta. In other words, we’re just going back and forth, updating the parameters one at a time, plugging the current value of the other parameter into g(\\theta, \\phi).\nThis idea of one-at-a-time updates is used in what we call Gibbs sampling, which also produces a stationary Markov chain (whose stationary distribution is the posterior). If you recall, this is the namesake of JAGS, “just another Gibbs sampler.”",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Gibbs sampling - M2L5</span>"
    ]
  },
  {
    "objectID": "C2-L05.html#sec-conditionally-conjugate-prior-with-normal-likelihood",
    "href": "C2-L05.html#sec-conditionally-conjugate-prior-with-normal-likelihood",
    "title": "36  Gibbs sampling - M2L5",
    "section": "36.1 Conditionally conjugate prior example with Normal likelihood 🎥",
    "text": "36.1 Conditionally conjugate prior example with Normal likelihood 🎥\n\n36.1.1 Normal likelihood, unknown mean and variance\n\n\n\n\nNormal likelihood, unknown mean and variance\n\n\n\n\nNormal likelihood conjugate prior\n\n\nLet’s return to the example at the end of Lesson 2 where we have normal likelihood with unknown mean and unknown variance. The model is:\n\n\\begin{aligned}\ny_i \\mid \\mu, \\sigma^2 &\\overset{\\text{iid}}{\\sim} \\mathcal{N} ( \\mu, \\sigma^2 ), \\quad i=1,\\ldots,n \\\\\n\\mu &\\sim \\mathcal{N}(\\mu_0, \\sigma_0^2) \\\\\n\\sigma^2 &\\sim \\mathcal{IG}(\\nu_0, \\beta_0)\n\\end{aligned}\n\nWe chose a normal prior for \\mu because, in the case where \\sigma^2 is known, the normal is the conjugate prior for \\mu. Likewise, in the case where \\mu is known, the inverse-gamma is the conjugate prior for \\sigma^2. This will give us convenient full conditional distributions in a Gibbs sampler.\nLet’s first work out the form of the full posterior distribution. When we begin analyzing data, the JAGS software will complete this step for us. However, it is extremely valuable to see and understand how this works.\n\n\\begin{aligned}\n\\mathbb{P}r( \\mu, \\sigma^2 \\mid y_1, y_2, \\ldots, y_n ) &\\propto\n\\mathbb{P}r(y_1, y_2, \\ldots, y_n \\mid \\mu, \\sigma^2) \\mathbb{P}r(\\mu) \\mathbb{P}r(\\sigma^2)\n\\\\ &= \\prod_{i=1}^n \\mathcal{N} ( y_i \\mid \\mu, \\sigma^2 ) \\times \\mathcal{N}( \\mu \\mid \\mu_0, \\sigma_0^2) \\times \\mathcal{IG}(\\sigma^2 \\mid \\nu_0, \\beta_0)\n\\\\ &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left[ -\\frac{(y_i - \\mu)^2}{2\\sigma^2} \\right]\n\\\\ & \\qquad \\times\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp \\left[ -\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} \\right]\n\\\\ & \\qquad \\times \\frac{\\beta_0^{\\nu_0}}{\\Gamma(\\nu_0)}(\\sigma^2)^{-(\\nu_0 + 1)} \\exp \\left[ -\\frac{\\beta_0}{\\sigma^2} \\right] \\mathbb{I}_{\\sigma^2 &gt; 0}(\\sigma^2)\n\\\\ &\\propto (\\sigma^2)^{-n/2} \\exp \\left[ -\\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2\\sigma^2} \\right]\n\\\\ & \\qquad \\times \\exp \\left[ -\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} \\right] (\\sigma^2)^{-(\\nu_0 + 1)}\n\\\\ & \\qquad \\times \\exp \\left[ -\\frac{\\beta_0}{\\sigma^2} \\right] \\mathbb{I}_{\\sigma^2 &gt; 0}(\\sigma^2)\n\\end{aligned}\n\nFrom here, it is easy to continue on to find the two full conditional distributions we need.\nFirst let’s look at \\mu, assuming \\sigma^2 is known (in which case it becomes a constant and is absorbed into the normalizing constant):\n\n\\begin{aligned}\n\\mathbb{P}r(\\mu \\mid \\sigma^2, y_1, \\ldots, y_n) &\\propto \\mathbb{P}r( \\mu, \\sigma^2 \\mid y_1, \\ldots, y_n )\n\\\\ &\\propto \\exp \\left[ -\\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2\\sigma^2} \\right] \\exp \\left[ -\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} \\right]\n\\\\ &\\propto \\exp \\left[ -\\frac{1}{2} \\left( \\frac{ \\sum_{i=1}^n (y_i - \\mu)^2}{2\\sigma^2} + \\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2} \\right) \\right]\n\\\\ &\\propto \\text{N} \\left( \\mu \\mid \\frac{n\\bar{y}/\\sigma^2 + \\mu_0/\\sigma_0^2}{n/\\sigma^2 + 1/\\sigma_0^2} \\frac{1}{n/\\sigma^2 + 1/\\sigma_0^2} \\right)\n\\end {aligned}\n\\tag{36.1}\nwhich we derived in the supplementary material of the last course. So, given the data and \\sigma^2, \\mu follows this normal distribution.\nNow let’s look at \\sigma^2, assuming \\mu is known:\n\n\\begin{aligned}\n\\mathbb{P}r(\\sigma^2 \\mid \\mu, y_1, \\ldots, y_n) & \\propto \\mathbb{P}r( \\mu, \\sigma^2 \\mid y_1, \\ldots, y_n )\n\\\\ &\\propto (\\sigma^2)^{-n/2} \\exp \\left[ -\\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2\\sigma^2} \\right] (\\sigma^2)^{-(\\nu_0 + 1)} \\exp \\left[ -\\frac{\\beta_0}{\\sigma^2} \\right] I_{\\sigma^2 &gt; 0}(\\sigma^2)\n\\\\ &\\propto (\\sigma^2)^{-(\\nu_0 + n/2 + 1)} \\exp \\left[ -\\frac{1}{\\sigma^2} \\left( \\beta_0 + \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2} \\right) \\right] I_{\\sigma^2 &gt; 0}(\\sigma^2)\n\\\\ &\\propto \\text{IG}\\left( \\sigma^2 \\mid \\nu_0 + \\frac{n}{2}, \\, \\beta_0 + \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2} \\right)\n\\end{aligned}\n\\tag{36.2}\nThese two distributions provide the basis of a Gibbs sampler to simulate from a Markov chain whose stationary distribution is the full posterior of both \\mu and \\sigma^2. We simply alternate draws between these two parameters, using the most recent draw of one parameter to update the other.\nWe will do this in R in the next segment.",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Gibbs sampling - M2L5</span>"
    ]
  },
  {
    "objectID": "C2-L05.html#computing-example-with-normal-likelihood",
    "href": "C2-L05.html#computing-example-with-normal-likelihood",
    "title": "36  Gibbs sampling - M2L5",
    "section": "36.2 Computing example with Normal likelihood",
    "text": "36.2 Computing example with Normal likelihood\nTo implement the Gibbs sampler we just described, let’s return to our running example where the data are the percent change in total personnel from last year to this year for n=10 companies.  We’ll still use a normal likelihood, but now we’ll relax the assumption that we know the variance of growth between companies, \\sigma^2, and estimate that variance. Instead of the t prior from earlier, we will use the conditionally conjugate priors, normal for \\mu and inverse-gamma for \\sigma^2.Company personnel\nThe first step will be to write functions to simulate from the full conditional distributions we derived in the previous segment. The full conditional for \\mu, given \\sigma^2 and data is\n\n36.2.1 Conditionally conjugate priors for the mean 🎥\n\n\\text{N} \\left( \\mu \\mid \\frac{n\\bar{y}/\\sigma^2 + \\mu_0/\\sigma_0^2}{n/\\sigma^2 + 1/\\sigma_0^2}, \\, \\frac{1}{n/\\sigma^2 + 1/\\sigma_0^2} \\right)\n\\tag{36.3}\n\n#' update_mu\n#'\n#' @param n - sample size\n#' @param ybar - sample mean\n#' @param sig2 - current sigma squared\n#' @param mu_0 - mean hyper-parameter\n#' @param sig2_0 - variance  hyper-parameter\n#' \n#' @output - updated  value for mu the mean\n1update_mu = function(n, ybar, sig2, mu_0, sig2_0) {\n2  sig2_1 = 1.0 / (n / sig2 + 1.0 / sig2_0)\n3  mu_1 = sig2_1 * (n * ybar / sig2 + mu_0 / sig2_0)\n4  rnorm(n=1, mean=mu_1, sd=sqrt(sig2_1))\n}\n\n\n1\n\nwe don’t need the data y\n\n2\n\nwhere:  sig2_1 is \\sigma^2_1 the right term in Equation 36.3  sig2 is the current \\sigma_2 which we update in update_sigma below using Equation 36.4  sig2_0 is the hyper parameter for \\sigma^2_0\n\n3\n\nmu_1 is \\sigma^2_1 the left term in Equation 36.3 which uses sig2_1 we just computed\n\n4\n\nwe now draw from the a N(\\mu_1,\\sigma_1^2) for update_sig2 and the trace.\n\n\n\n\n\n\n36.2.2 conditionally conjugate priors for the variance\nThe full conditional for \\sigma^2 given \\mu and data is\n\n\\text{IG}\\left( \\sigma^2 \\mid \\nu_0 + \\frac{n}{2}, \\, \\beta_0 + \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2} \\right)\n\\tag{36.4}\n\n#' update_sig2\n#'\n#' @param n - sample size\n#' @param y - the data\n#' @param nu_0 - nu hyper-parameter\n#' @param beta_0 - beta hyper-parameter\n#' \n#' @output - updated  value for sigma2 the variance\n1update_sig2 = function(n, y, mu, nu_0, beta_0) {\n2  nu_1 = nu_0 + n / 2.0\n3  sumsq = sum( (y - mu)^2 )\n4  beta_1 = beta_0 + sumsq / 2.0\n5  out_gamma = rgamma(n=1, shape=nu_1, rate=beta_1)\n6  1.0 / out_gamma\n}\n\n\n1\n\nwe need the data to update beta\n\n2\n\nnu_1 the left term in Equation 36.4\n\n3\n\nvectorized\n\n4\n\nbeta_1 the right term in Equation 36.4\n\n5\n\ndraw a gamma sample with updated rate for \\text{Gamma}() is shape for \\text{IG}() inv-gamma\n\n6\n\nsince there is no rinvgamma in R we use the reciprocal of a gamma random variable which is distributed inv-gamma\n\n\n\n\nWith functions for drawing from the full conditionals, we are ready to write a function to perform Gibbs sampling.\n\n\n36.2.3 Gibbs sampler in R\n\ngibbs = function(y, n_iter, init, prior) {\n  ybar = mean(y)\n  n = length(y)\n  \n  ## initialize\n  mu_out = numeric(n_iter)\n  sig2_out = numeric(n_iter)\n  \n  mu_now = init$mu\n  \n  ## Gibbs sampler\n  for (i in 1:n_iter) {\n    sig2_now = update_sig2(n=n, y=y, mu=mu_now, nu_0=prior$nu_0, beta_0=prior$beta_0)\n    mu_now = update_mu(n=n, ybar=ybar, sig2=sig2_now, mu_0=prior$mu_0, sig2_0=prior$sig2_0)\n    \n    sig2_out[i] = sig2_now\n    mu_out[i] = mu_now\n  }\n  \n1  cbind(mu=mu_out, sig2=sig2_out)\n}\n\n\n1\n\ncbind for column bind will take a lists of list and convert them into a matrix of collumns.\n\n\n\n\nNow we are ready to set up the problem in R.\n\n\\begin{aligned}\n  y_i \\mid \\mu, \\sigma &\\stackrel {iid} \\sim \\mathcal{N}(\\mu,\\sigma^2), \\quad i=1,\\ldots,n\n  \\\\ \\mu &\\sim \\mathcal{N}(\\mu_0,\\sigma^2_0)\n  \\\\ \\sigma^2 & \\sim \\mathcal{IG}(\\nu,\\beta_0)\n\\end{aligned}\n\\tag{36.5}\nWe also need to create the prior hyperparameters for \\sigma^2, \\nu_0 and \\beta_0. If we chose these hyperperameters carefully, they are interpretable as a prior guess for sigma squared, as well as a prior effective sample size to go with that guess.\nThe prior effective sample size. Which we’ll call n_0, is two times this \\nu_0 parameter. So in other words, the nu parameter will be the prior sample size Divided by 2. We’re also going to create an initial guess for sigma squared, let’s call it s^2_0. The relationship between \\beta_0 and these two numbers is the following: It is the prior sample size times the prior guess divided by 2.\nThis particular parameterization of the Inverse gamma distribution is called the Scaled Inverse Chi Square Distribution, where the two parameters are n_0 and s^2_0.\n\ny = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)\nybar = mean(y)\nn = length(y)\n\n## prior\nprior = list()\nprior$mu_0 = 0.0\nprior$sig2_0 = 1.0\nprior$n_0 = 2.0 # prior effective sample size for sig2\nprior$s2_0 = 1.0 # prior point estimate for sig2\nprior$nu_0 = prior$n_0 / 2.0 # prior parameter for inverse-gamma\nprior$beta_0 = prior$n_0 * prior$s2_0 / 2.0 # prior parameter for inverse-gamma\n\nhist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data\ncurve(dnorm(x=x, mean = prior$mu_0, sd=sqrt(prior$sig2_0)), lty=2, add=TRUE) # prior for mu\npoints(y, rep(0,n), pch=1) # individual data points\npoints(ybar, 0, pch=19) # sample mean\n\n\n\n\n\n\n\n\nFinally, we can initialize and run the sampler!\n\nset.seed(53)\n\ninit = list()\ninit$mu = 0.0\n\npost = gibbs(y=y, n_iter=1e3, init=init, prior=prior)\n\n\nhead(post)\n\n            mu      sig2\n[1,] 0.3746992 1.5179144\n[2,] 0.4900277 0.8532821\n[3,] 0.2536817 1.4325174\n[4,] 1.1378504 1.2337821\n[5,] 1.0016641 0.8409815\n[6,] 1.1576873 0.7926196\n\n\n\nlibrary(\"coda\")\nplot(as.mcmc(post))\n\n\n\n\n\n\n\n\n\nsummary(as.mcmc(post))\n\n\nIterations = 1:1000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 1000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n       Mean     SD Naive SE Time-series SE\nmu   0.9051 0.2868  0.00907        0.00907\nsig2 0.9282 0.5177  0.01637        0.01810\n\n2. Quantiles for each variable:\n\n       2.5%    25%    50%   75% 97.5%\nmu   0.3024 0.7244 0.9089 1.090 1.481\nsig2 0.3577 0.6084 0.8188 1.094 2.141\n\n\nAs with the Metropolis-Hastings example, these chains appear to have converged. In the next lesson, we will discuss convergence in more detail.\n\n\n\n\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.” IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-6 (6): 721–41. https://doi.org/10.1109/tpami.1984.4767596.",
    "crumbs": [
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Gibbs sampling - M2L5</span>"
    ]
  },
  {
    "objectID": "C2-L05-Ex1.html",
    "href": "C2-L05-Ex1.html",
    "title": "37  Homework Gibbs-Sampling algorithm - M2L22HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Homework Gibbs-Sampling algorithm - M2L22HW1</span>"
    ]
  },
  {
    "objectID": "C2-L06.html",
    "href": "C2-L06.html",
    "title": "38  Assessing Convergence - M2L5",
    "section": "",
    "text": "38.1 Convergence diagnostics\nIn the previous two lessons, we have demonstrated ways you can simulate a Markov chain whose stationary distribution is the target distribution (usually the posterior). Before using the simulated chain to obtain Monte Carlo estimates, we should first ask ourselves: Has our simulated Markov chain converged to its stationary distribution yet? Unfortunately, this is a difficult question to answer, but we can do several things to investigate.",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Assessing Convergence - M2L5</span>"
    ]
  },
  {
    "objectID": "C2-L06.html#sec-convergence-diagnostics",
    "href": "C2-L06.html#sec-convergence-diagnostics",
    "title": "38  Assessing Convergence - M2L5",
    "section": "",
    "text": "38.1.1 Trace plots\nOur first visual tool for assessing chains is the trace plot. A trace plot shows the history of a parameter value across iterations of the chain. It shows you precisely where the chain has been exploring.\nFirst, let’s talk about what a chain should look like. Here is an example of a chain that has most likely converged.\n\nsource('mh.r')\n\nList of 2\n $ mu   : num [1:1000] 0 0 1.24 1.24 1.24 ...\n $ accpt: num 0.144\n\nlibrary(\"coda\")\nset.seed(61)\npost0 = mh(n=n, ybar=ybar, n_iter=10e3, mu_init=0.0, cand_sd=0.9)\ncoda::traceplot(as.mcmc(post0$mu[-c(1:500)]))\n\n\n\n\n\n\n\n\nIf the chain is stationary, it should not be showing any long-term trends. The average value for the chain should be roughly flat. It should not be wandering as in this example:\n\nset.seed(61)\npost1 = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=0.04)\ncoda::traceplot(as.mcmc(post1$mu[-c(1:500)]))\n\n\n\n\n\n\n\n\nIf this is the case, you need to run the chain many more iterations, as seen here:\n\nset.seed(61)\npost2 = mh(n=n, ybar=ybar, n_iter=100e3, mu_init=0.0, cand_sd=0.04)\ncoda::traceplot(as.mcmc(post2$mu))\n\n\n\n\n\n\n\n\nThe chain appears to have converged at this much larger time scale.\n\n\n38.1.2 Monte Carlo effective sample size\nOne major difference between the two chains we’ve looked at is the level of autocorrelation in each. Autocorrelation is a number between -1 and +1 which measures how linearly dependent the current value of the chain is on past values (called lags). We can see this with an autocorrelation plot:\n\ncoda::autocorr.plot(as.mcmc(post0$mu))\n\n\n\n\n\n\n\n\n\ncoda::autocorr.plot(as.mcmc(post1$mu))\n\n\n\n\n\n\n\n\n\ncoda::autocorr.diag(as.mcmc(post1$mu))\n\n            [,1]\nLag 0  1.0000000\nLag 1  0.9850078\nLag 5  0.9213126\nLag 10 0.8387333\nLag 50 0.3834563\n\n\nAutocorrelation is important because it tells us how much information is available in our Markov chain. Sampling 1000 iterations from a highly correlated Markov chain yields less information about the stationary distribution than we would obtain from 1000 samples independently drawn from the stationary distribution.\nAutocorrelation is a major component in calculating the Monte Carlo effective sample size of your chain. The Monte Carlo effective sample size is how many independent samples from the stationary distribution you would have to draw to have equivalent information in your Markov chain. Essentially it is the m (sample size) we chose in the lesson on Monte Carlo estimation.\n\nstr(post2) # contains 100,000 iterations\n\nList of 2\n $ mu   : num [1:100000] -0.0152 -0.1007 -0.0867 -0.1092 -0.0811 ...\n $ accpt: num 0.958\n\n\n\ncoda::effectiveSize(as.mcmc(post2$mu)) # effective sample size of ~350\n\n   var1 \n373.858 \n\n\n\n## thin out the samples until autocorrelation is essentially 0. This will leave you with approximately independent samples. The number of samples remaining is similar to the effective sample size.\ncoda::autocorr.plot(as.mcmc(post2$mu), lag.max=500)\n\n\n\n\n\n\n\n\n\nthin_interval = 400 # how far apart the iterations are for autocorrelation to be essentially 0.\nthin_indx = seq(from=thin_interval, to=length(post2$mu), by=thin_interval)\nhead(thin_indx)\n\n[1]  400  800 1200 1600 2000 2400\n\n\n\npost2mu_thin = post2$mu[thin_indx]\ntraceplot(as.mcmc(post2$mu))\n\n\n\n\n\n\n\n\n\ntraceplot(as.mcmc(post2mu_thin))\n\n\n\n\n\n\n\n\n\ncoda::autocorr.plot(as.mcmc(post2mu_thin), lag.max=10)\n\n\n\n\n\n\n\n\n\neffectiveSize(as.mcmc(post2mu_thin))\n\nvar1 \n 250 \n\n\n\nlength(post2mu_thin)\n\n[1] 250\n\n\n\nstr(post0) # contains 10,000 iterations\n\nList of 2\n $ mu   : num [1:10000] 0 0 0.315 0.315 0.949 ...\n $ accpt: num 0.382\n\n\n\ncoda::effectiveSize(as.mcmc(post0$mu)) # effective sample size of ~2,500\n\n    var1 \n2537.924 \n\n\n?effectiveSize\nThe chain from post0 has 10,000 iterations, but an effective sample size of about 2,500. That is, this chain essentially provides the equivalent of 2,500 independent Monte Carlo samples.\nNotice that the chain from post0 has 10 times fewer iterations than for post2, but its Monte Carlo effective sample size is about seven times greater than the longer (more correlated) chain. We would have to run the correlated chain for 700,000+ iterations to get the same amount of information from both chains.\nIt is usually a good idea to check the Monte Carlo effective sample size of your chain. If all you seek is a posterior mean estimate, then an effective sample size of a few hundred to a few thousand should be enough. However, if you want to create something like a 95% posterior interval, you may need many thousands of effective samples to produce a reliable estimate of the outer edges of the distribution. The number you need can be quickly calculated using the Raftery and Lewis diagnostic.\nraftery.diag(as.mcmc(post0$mu))\n\nraftery.diag(as.mcmc(post0$mu), q=0.005, r=0.001, s=0.95)\n\n\nQuantile (q) = 0.005\nAccuracy (r) = +/- 0.001\nProbability (s) = 0.95 \n\nYou need a sample size of at least 19112 with these values of q, r and s\n\n\n\n## \n## Quantile (q) = 0.005\n## Accuracy (r) = +/- 0.001\n## Probability (s) = 0.95 \n## \n## You need a sample size of at least 19112 with these values of q, r and s\n\n\n?raftery.diag\n\nIn the case of the first chain from post0, it looks like we would need about 3,700 effective samples to calculate reliable 95% intervals. With the autocorrelation in the chain, that requires about 13,200 total samples. If we wanted to create reliable 99% intervals, we would need at least 19,100 total samples.",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Assessing Convergence - M2L5</span>"
    ]
  },
  {
    "objectID": "C2-L06.html#burn-in",
    "href": "C2-L06.html#burn-in",
    "title": "38  Assessing Convergence - M2L5",
    "section": "38.2 Burn-in",
    "text": "38.2 Burn-in\nWe have also seen how the initial value of the chain can affect how quickly the chain converges. If our initial value is far from the bulk of the posterior distribution, then it may take a while for the chain to travel there. We saw this in an earlier example.\n\nset.seed(62)\npost3 = mh(n=n, ybar=ybar, n_iter=500, mu_init=10.0, cand_sd=0.3)\ncoda::traceplot(as.mcmc(post3$mu))\n\n\n\n\n\n\n\n\nClearly, the first 100 or so iterations do not reflect draws from the stationary distribution, so they should be discarded before we use this chain for Monte Carlo estimates. This is called the “burn-in” period. You should always discard early iterations that do not appear to be coming from the stationary distribution. Even if the chain appears to have converged early on, it is safer practice to discard an initial burn-in.\n\n38.2.1 Multiple chains, Gelman-Rubin\nIf we want to be more confident that we have converged to the true stationary distribution, we can simulate multiple chains, each with a different starting value.\n\nset.seed(61)\n\nnsim = 500\npost1 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=15.0, cand_sd=0.4)\npost1$accpt\n\n[1] 0.616\n\n\n\npost2 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=-5.0, cand_sd=0.4)\npost2$accpt\n\n[1] 0.612\n\n\n\npost3 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=7.0, cand_sd=0.1)\npost3$accpt\n\n[1] 0.844\n\n\n\npost4 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=23.0, cand_sd=0.5)\npost4$accpt\n\n[1] 0.53\n\n\n\npost5 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=-17.0, cand_sd=0.4)\npost5$accpt\n\n[1] 0.618\n\n\n\npmc = mcmc.list(as.mcmc(post1$mu), as.mcmc(post2$mu), \n                as.mcmc(post3$mu), as.mcmc(post4$mu), as.mcmc(post5$mu))\nstr(pmc)\n\nList of 5\n $ : 'mcmc' num [1:500] 14.8 14 14 13.8 13.8 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n $ : 'mcmc' num [1:500] -5 -5 -5 -5 -4.89 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n $ : 'mcmc' num [1:500] 7 7 7 6.94 6.94 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n $ : 'mcmc' num [1:500] 23 21.9 21.9 21.8 21.8 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n $ : 'mcmc' num [1:500] -17 -17 -16.9 -16.2 -15.7 ...\n  ..- attr(*, \"mcpar\")= num [1:3] 1 500 1\n - attr(*, \"class\")= chr \"mcmc.list\"\n\n\n\ncoda::traceplot(pmc)\n\n\n\n\n\n\n\n\nIt appears that after about iteration 200, all chains are exploring the stationary (posterior) distribution. We can back up our visual results with the Gelman Rubin diagnostic. This diagnostic statistic calculates the variability within chains, comparing that to the variability between chains. If all chains have converged to the stationary distribution, the variability between chains should be relatively small, and the potential scale reduction factor, reported by the the diagnostic, should be close to one. If the values are much higher than one, then we would conclude that the chains have not yet converged.\n\ncoda::gelman.diag(pmc)\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\n[1,]       1.01       1.02\n\n\n\ncoda::gelman.plot(pmc)\n\n\n\n\n\n\n\n\n\n?gelman.diag\n\n From the plot, we can see that if we only used the first 50 iterations, the potential scale reduction factor or “shrink factor” would be close to 10, indicating that the chains have not converged. But after about iteration 300, the “shrink factor” is essentially one, indicating that by then, we have probably reached convergence. Of course, we shouldn’t stop sampling as soon as we reach convergence. Instead, this is where we should begin saving our samples for Monte Carlo estimation.\n\n\n38.2.2 Monte Carlo estimation\nIf we are reasonably confident that our Markov chain has converged, then we can go ahead and treat it as a Monte Carlo sample from the posterior distribution. Thus, we can use the techniques from Lesson 3 to calculate posterior quantities like the posterior mean and posterior intervals from the samples directly.\n\nnburn = 1000 # remember to discard early iterations\npost0$mu_keep = post0$mu[-c(1:1000)]\nsummary(as.mcmc(post0$mu_keep))\n\n\nIterations = 1:9000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 9000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n      0.889449       0.304514       0.003210       0.006295 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n0.2915 0.6825 0.8924 1.0868 1.4890 \n\n\n\nmean(post0$mu_keep &gt; 1.0) # posterior probability that mu  &gt; 1.0\n\n[1] 0.3554444",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Assessing Convergence - M2L5</span>"
    ]
  },
  {
    "objectID": "C2-L06-Ex1.html",
    "href": "C2-L06-Ex1.html",
    "title": "39  Homework on the Gibbs-Sampling algorithm - M2L5HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Homework on the Gibbs-Sampling algorithm - M2L5HW1</span>"
    ]
  },
  {
    "objectID": "C2-L06-Ex2.html",
    "href": "C2-L06-Ex2.html",
    "title": "40  Homework on M-H algorithm M2L5HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Homework on M-H algorithm M2L5HW2</span>"
    ]
  },
  {
    "objectID": "C2-L07.html",
    "href": "C2-L07.html",
    "title": "41  Linear regression - M3L7",
    "section": "",
    "text": "41.1 Introduction to linear regression\nWe discussed linear regression briefly in the previous course. And we fit a few models with non-informative priors. Here, we’ll provide a brief review, demonstrate fitting linear regression models in JAGS And discuss a few practical skills that are helpful when fitting linear models in general.\nThis is not meant to be a comprehensive treatment of linear models, which you can find in numerous courses and textbooks.\nLinear regression is perhaps the simplest way to relate a continuous response variable to multiple explanatory variables.\nThis may arise from observing several variables together and investigating which variables correlate with the response variable. Or it could arise from conducting an experiment, where we carefully assign values of explanatory variables to randomly selected subjects. And try to establish a cause-and-effect relationship.\nA linear regression model has the following form:\ny_i=\\beta_0+\\beta_1 x_i +\\ldots+ \\beta_k x_k + \\epsilon_i\n\\\\ \\epsilon_i \\stackrel {iid} \\sim N(0,\\sigma^2)\n\\tag{41.1}\nThis describes the mean, and then we would also add an error, individual term for each observation. We would assume that the errors are IID from a normal distribution means 0 variance \\sigma^2 for observations 1 \\ldots k.\nEquivalently we can write this model for y_i directly as y_i given all of the x_i values, betas and a constant variance \\sigma^2. Again, k is the number of predictor variables.\ny_i\\mid x_i,\\beta_i,\\sigma^2 \\sim N(\\beta_0+\\beta_1 x_i +\\ldots+ \\beta_k x_k, \\sigma^2)\\\\\n\\beta_i \\sim \\mathbb{P}r(\\beta_i) \\\\\n\\sigma^2 \\sim \\mathbb{P}r(\\sigma^2)\n\\tag{41.2}\nThis yields the following graphical model structure.\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXGeneral'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXNonUnicode'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeOneSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeTwoSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeThreeSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeFourSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['STIXSizeFiveSym'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['cmtt10'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['cmb10'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['cmss10'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['DejaVu Sans Display'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\n\nFigure 41.1: The graphical model for linear regression\nThe terms of a linear model are always linearly related because of the structure of the model.\nBut the model does not have to be linear necessarily in the xy relationship. For example, it may be that y is related linearly to x^2. Hence we could transform the x and y variables to get new x’s and new y’s but we would still have a linear model. However, in that case, if we transform the variables, we must be careful about how this changes the final interpretation of the results.",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Linear regression - M3L7</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-introduction-to-linear-regression",
    "href": "C2-L07.html#sec-introduction-to-linear-regression",
    "title": "41  Linear regression - M3L7",
    "section": "",
    "text": "Introduction to linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantUnderstanding the Graphical Models\n\n\n\n\nThis graphical model uses plate notation\nWe’ll start with a plate for all of our different y variables,\n\nIt is repeated i = 1 \\ldots N times\n\ny_i, are random variable - (indicated by a circle)\n\nthey are observed - indicated by a filled shape.\n\nX_i variables.\n\nthey are drawn as squares around to indicate that they are constants and not random variables.\nWe’re always conditioning on the Xs. So they’ll just be constants.\nthey are observed, so they are filled in.\n\nThe y_i depend on the values of the x and the values of these parameters. So, we have \\beta_0, \\ldots, \\beta_k.\nSigma squared.\nSince the y_i depend on all of these, so this would be the graphical model representation.\n\n\n\n\n\n\n\n\n\n\n\nImportantInterpreting Coefficients\n\n\n\nThe basic interpretation of the \\beta_i coefficients is:\nWhile holding all other X variables constant, if we increases X_i by one then the mean of \\bar{y} is expected to increase by \\beta_i .\nThat is \\beta_i describes how the \\bar{y} changes with changes in X_i, while accounting for all the other X variables.\n\n\\beta \\approx  \\frac{\\partial \\bar{y} }{\\partial x_i}\n\\tag{41.3}\nThat’s true for all of the x variables.\n\n\n\n\n\n\n\n\nWarningRegression assumptions\n\n\n\nWe’re going to assume that\n\nThe y s are independent of each other, given the x s.\nThe y_i s have the same variance.\nThe residuals are normally distributed with mean 0 and variance \\sigma^2.\n\nThese are actually strong assumptions that are not often not realistic in many situations.\nThere are many statistical models to address that.\nWe’ll look at some hierarchical methods in the coming lessons.\n\n\n\n41.1.1 Priors\n The model is not complete until we add the prior distributions.\nSo we might say \\beta_0 comes from its prior.\n\\beta_1 would come from its prior, and so forth for all the \\betas. And sigma squared would come from its prior.\nThe most common choice for prior on the \\beta s, is a Normal distribution. Or we can do a Multivariate normal for all of the betas at once.\nThis is conditionally conjugate and allows us to do Gibbs sampling.\nIf we want to be non-informative, we can choose Normal(0,\\sigma^2=1e6) priors with very large variance. Which are practically flat for realistic values of beta. The non-informative priors used in the last class are equivalent to using normal priors with infinite variance.\nWe can also use the conditionally conjugate InverseGamma() prior for \\sigma^2 that we’re familiar with.\nAnother common prior for the betas is Double exponential, or the Laplace prior, or Laplace distribution.\n \nThe Laplace prior has this density:\n\nf(x\\mid \\mu,\\beta)=\\frac{1}{2\\beta} e^{|\\frac{x-\\mu}{\\beta}|}\n\\tag{41.4}\nwhere:\n\n\\mu is the location parameter and\n\\beta is the scale parameter.\n\nThe case where \\mu = 0 and \\beta = 1 is called the standard double exponential distribution\n\nf(x)=\\frac{1}{2} e^{|x|}\n\\tag{41.5}\nAnd the density looks like this.\n\n\n\n\n\n\n\nFigure 41.2: The Double Exponential Distribution\n\n\n\n\n\n\n\n\n\n\nFigure 41.3: The Double Exponential Distribution\n\n\n\n\nRPython\n\n\n\n# Grid of X-axis values\nx &lt;- seq(-10, 10, 0.1)\nplot(x,  ddexp(x, 0, 2), type = \"l\", ylab = \"\", lwd = 2, col = \"red\")\nlines(x, ddexp(x, 0, 1.5), type = \"l\", ylab = \"\", lwd = 2, col = \"green\")\nlines(x, ddexp(x, 0, 1), type = \"l\", ylab = \"\", lwd = 2, col = \"blue\")\nlegend(\"topright\",\n       c(expression(paste(, beta)), \"1.5\",\"1\", \"2\"),\n       lty = c(0, 1, 1, 1),\n       col = c(\"red\",\"green\", \"blue\"), box.lty = 0, lwd = 2\n      )\n\n#x &lt;- rdexp(500, location = 2, scale = 1)\n#de_sample=ddexp(x, 2, 1)\n#CDF &lt;- ecdf(de_sample )\n#plot(CDF)\n\n\n\n\n\nloc, scale = 0., 1.\ns = np.random.laplace(loc, scale, 1000)\n\ncount, bins, ignored = plt.hist(s, 30, density=True)\nx = np.arange(-8., 8., .01)\npdf = np.exp(-abs(x-loc)/scale)/(2.*scale)\nplt.plot(x, pdf);\n\ng = (1/(scale * np.sqrt(2 * np.pi)) *\n     np.exp(-(x - loc)**2 / (2 * scale**2)))\nplt.plot(x,g);\n\n\n\n\n\nIt’s called double exponential because it looks like the exponential distribution except it’s been reflected over the y axis. It has a sharp peak at x equals 0, or beta equals 0 in this case, which can be useful if we want to do variable selection among our x’s. Because it’ll favor values in your 0 for these betas.\nThis is related to the popular regression technique known as the LASSO. \nMore information is available from:\n\nNIST\nWikipedia",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Linear regression - M3L7</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#eda",
    "href": "C2-L07.html#eda",
    "title": "41  Linear regression - M3L7",
    "section": "42.1 EDA",
    "text": "42.1 EDA\n\n1pairs(Leinhardt)\n\n\n1\n\nUsing pairs to investigate the marginal relationships between each of the four variables.\n\n\n\n\n\n\n\n\n\n\n\n\n42.1.0.1 Simple linear Model\nWe’ll start with a simple linear regression model that relates infant mortality to per capita income.\n\nplot(infant ~ income, data=Leinhardt)\n\n\n\n\n\n\n\n\n\nhist(Leinhardt$infant)\n\n\n\n\n\n\n\n\nthis is right-skewed (many small values and a number of much larger one)\n\nhist(Leinhardt$income)\n\n\n\n\n\n\n\n\nalso right-skewed.\nthis indicates that we may do better if we do a log transform on these two variables.\n\n\n42.1.0.2 Log-Log Linear Model\n\n1Leinhardt$loginfant = log(Leinhardt$infant)\n2Leinhardt$logincome = log(Leinhardt$income)\n3plot(loginfant ~ logincome,data=Leinhardt)\n\n\n1\n\nlog transform infant column.\n\n2\n\nlog transform income column.\n\n3\n\nscatter plot of the log log transformed data.\n\n\n\n\n\n\n\n\n\n\nFigure 42.1: log log transformed infant mortality vs income\n\n\n\n\n\nSince infant mortality and per capita income are positive and right-skewed quantities, we consider modeling them on the logarithmic scale. A linear model appears much more appropriate on this scale.\n\n1scatterplot(loginfant ~ logincome,data=Leinhardt)\n\n\n1\n\nscatterplot with a regression fit and uncertainty for the data\n\n\n\n\n\n\n\n\n\n\nFigure 42.2: log log transformed infant mortality vs income scatterplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n42.1.1 Modeling\nThe reference Bayesian analysis (with a non-informative prior) is available directly in R.\n\nlmod0 = lm(loginfant ~ logincome, data=Leinhardt)\nsummary(lmod0)\n\n\nlm(formula = loginfant ~ logincome, data = Leinhardt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.66694 -0.42779 -0.02649  0.30441  3.08415 \n\nCoefficients: \n            Estimate Std. Error t value Pr(&gt;|t|)    \n1(Intercept)  7.14582    0.31654  22.575   &lt;2e-16 ***\n2logincome   -0.51179    0.05122  -9.992   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n3Residual standard error: 0.6867 on 99 degrees of freedom\n4  (4 observations deleted due to missingness)\n5Multiple R-squared:  0.5021,    Adjusted R-squared:  0.4971\nF-statistic: 99.84 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n1\n\nintercept is \\gg its error so it appears statistically significant (***)\n\n2\n\nposterior mean logincome too\n\n3\n\nResidual standard error gives us an estimate of the left over variance after fitting the model.\n\n4\n\n4 rows were dropped due to missing values\n\n5\n\nAdjusted R-squared is the explained variance adjusted for degrees of freedom",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Linear regression - M3L7</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-residual-checks",
    "href": "C2-L07.html#sec-residual-checks",
    "title": "41  Linear regression - M3L7",
    "section": "44.1 Residual checks",
    "text": "44.1 Residual checks\n\n\n\n\n\n\nImportant\n\n\n\nAnalysis gets complicated quickly when we have multiple models. What we shall soon see is how to get residuals from the Bayesian model in Stan so we can compare it visually with the reference model we got using LM.\n\n\nChecking residuals (the difference between the response and the model’s prediction for that value) is important with linear models since residuals can reveal violations of the assumptions we made to specify the model. In particular, we are looking for any sign that the model is not linear, normally distributed, or that the observations are not independent (conditional on covariates).\nFirst, let’s look at what would have happened if we fit the reference linear model to the un-transformed variables.\n\nlmod0 = lm(infant ~ income, data=Leinhardt)\nplot(resid(lmod0)) # to check independence (looks okay)\n\n\n\n\n\n\n\n\nthere should not be a pattern - but we can see an increase. This is not an issue and due to the data being presorted.\n\nplot(predict(lmod0), resid(lmod0)) # to check for linearity, constant variance (looks bad)\n\n\n\n\n\n\n\n\nafter 80 the variance starts increasing\n\nqqnorm(resid(lmod0)) # to check Normality assumption (we want this to be a straight line)\n\n\n\n\n\n\n\n#?qqnorm\n\nThis looks good except for the last few points.\nNow let’s return to our model fit to the log-transformed variables. In a Bayesian model, we have distributions for residuals, but we’ll simplify and look only at the residuals evaluated at the posterior mean of the parameters.\n\nX = cbind(rep(1.0, data1_jags$n), data1_jags$log_income)\nhead(X)\n\n     [,1]     [,2]\n[1,]    1 8.139149\n[2,]    1 8.116716\n[3,]    1 8.115521\n[4,]    1 8.466110\n[5,]    1 8.522976\n[6,]    1 8.105308\n\n\n\n1(pm_params1 = colMeans(mod1_csim))\n\n\n1\n\nposterior mean - using (var = expr) forces R to return the value of var\n\n\n\n\n      b[1]       b[2]        sig \n 7.1387786 -0.5107720  0.9711689 \n\n\n\n1yhat1 = drop(X %*% pm_params1[1:2])\n2resid1 = data1_jags$y - yhat1\n3plot(resid1)\n\n\n1\n\nwe are evaluating \\hat{y} = b_0 \\times 1 + b_1 \\times x_1 via matrix multiplication of [1, data1_jags$log_income] *[b_0,b_1]\n\n2\n\nres_i = y_i- \\hat y = y_i - (b_0 \\times 1 + b_1 \\times x_{1,i})\n\n3\n\nplots the residual against the data index\n\n\n\n\n\n\n\n\n\n\n\nSo to get the residuals from Stan we extract the b parameter.\nAlthough we did not discuss it we could estimate \\hat y by drawing K predictions for each x_i and look at res_i=\\frac{1}{K}\\sum_k|y_i -\\hat y_{i,k}| and plot upper and fit a line as well as lower and upper bounds as well. Also I’m not sure but I guess we can also do with using the predictive posterior dist. Anyhow here is a link to something like this: Extracting and visualizing tidy residuals from Bayesian models -jk\n\nplot(yhat1, resid1) # against predicted values\n\n\n\n\n\n\n\n\n\nqqnorm(resid1) # checking normality of residuals\n\n\n\n\n\n\n\n\n\nplot(predict(lmod0), resid(mod1)) # to compare with reference linear model\n\n\n\n\n\n\n\n\n\nrownames(dat)[order(resid1, decreasing=TRUE)[1:5]] # which countries have the largest positive residuals?\n\n[1] \"Saudi.Arabia\" \"Libya\"        \"Zambia\"       \"Brazil\"       \"Afganistan\"  \n\n\nThe residuals look pretty good here (no patterns, shapes) except for two strong outliers, Saudi Arabia and Libya. When outliers appear, it is a good idea to double check that they are not just errors in data entry. If the values are correct, you may reconsider whether these data points really are representative of the data you are trying to model. If you conclude that they are not (for example, they were recorded on different years), you may be able to justify dropping these data points from the data set.\nIf you conclude that the outliers are part of data and should not be removed, we have several modeling options to accommodate them. We will address these in the next segment.",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Linear regression - M3L7</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-additional-covariates",
    "href": "C2-L07.html#sec-additional-covariates",
    "title": "41  Linear regression - M3L7",
    "section": "45.1 Additional covariates",
    "text": "45.1 Additional covariates\nThe first approach is to look for additional covariates that may be able to explain the outliers. For example, there could be a number of variables that provide information about infant mortality above and beyond what income provides.\nLooking back at our data, there are two variables we haven’t used yet: region and oil. The oil variable indicates oil-exporting countries. Both Saudi Arabia and Libya are oil-exporting countries, so perhaps this might explain part of the anomaly.\n\nlibrary(\"rjags\")\n\nmod2_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dnorm(mu[i], prec)\n1        mu[i] = b[1] + b[2]*log_income[i] + b[3]*is_oil[i]\n    }\n    \n2    for (i in 1:3) {\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n    prec ~ dgamma(5/2.0, 5*10.0/2.0)\n    sig = sqrt( 1.0 / prec )\n} \"\n\n\nset.seed(73)\ndata2_jags = list(y=dat$loginfant, log_income=dat$logincome,\n3                  is_oil=as.numeric(dat$oil==\"yes\"))\ndata2_jags$is_oil\n\nparams2 = c(\"b\", \"sig\")\n\ninits2 = function() {\n4    inits = list(\"b\"=rnorm(3,0.0,100.0), \"prec\"=rgamma(1,1.0,1.0))\n}\n\nmod2 = jags.model(textConnection(mod2_string), data=data2_jags, inits=inits2, n.chains=3)\nupdate(mod2, 1e3) # burn-in\n\nmod2_sim = coda.samples(model=mod2,\n                        variable.names=params2,\n                        n.iter=5e3)\n\nmod2_csim = as.mcmc(do.call(rbind, mod2_sim)) # combine multiple chains\n\n\n1\n\nwe add the is_oil indicator parameter\n\n2\n\nwe increment the number of parameters\n\n3\n\nencode the is_oil from text to be binary\n\n4\n\ndraw another var for b.\n\n\n\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 101\n   Unobserved stochastic nodes: 4\n   Total graph size: 507\n\nInitializing model\n\n\nAs usual, check the convergence diagnostics.\n\npar(mar = c(2., 1, 2., 1))\nplot(mod2_sim)\n\n\n\n\n\n\n\n\n\ngelman.diag(mod2_sim)\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\nb[1]       1.01       1.03\nb[2]       1.01       1.03\nb[3]       1.00       1.00\nsig        1.00       1.00\n\nMultivariate psrf\n\n1.01\n\nautocorr.diag(mod2_sim)\n\n            b[1]       b[2]         b[3]           sig\nLag 0  1.0000000 1.00000000  1.000000000  1.000000e+00\nLag 1  0.9539876 0.95382897  0.089796849  2.778182e-02\nLag 5  0.7917379 0.79131058  0.007941997 -3.652922e-05\nLag 10 0.6280632 0.62634735 -0.004474289  2.054995e-03\nLag 50 0.0665811 0.06719241 -0.005011184 -3.660173e-03\n\n\n\n#autocorr.plot(mod2_sim,auto.layout=FALSE )\nautocorr.plot(mod2_csim,auto.layout=FALSE )\n\n\n\n\n\n\n\nFigure 45.1: auto-correlation plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 45.2: auto-correlation plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 45.3: auto-correlation plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 45.4: auto-correlation plot\n\n\n\n\n\n\neffectiveSize(mod2_sim)\n\n      b[1]       b[2]       b[3]        sig \n  346.9792   354.4084 12749.3842 13673.8406 \n\n\nWe can get a posterior summary of the parameters in our model.\n\nsummary(mod2_sim)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean      SD  Naive SE Time-series SE\nb[1]  7.1472 0.44563 0.0036386      0.0239085\nb[2] -0.5225 0.07224 0.0005898      0.0038358\nb[3]  0.7947 0.35571 0.0029044      0.0031502\nsig   0.9525 0.06697 0.0005468      0.0005734\n\n2. Quantiles for each variable:\n\n         2.5%     25%     50%     75%   97.5%\nb[1]  6.26339  6.8627  7.1522  7.4458  8.0033\nb[2] -0.66065 -0.5715 -0.5233 -0.4756 -0.3775\nb[3]  0.09031  0.5546  0.7960  1.0313  1.4853\nsig   0.83075  0.9060  0.9488  0.9955  1.0938\n\n\nIt looks like there is a positive relationship between oil-production and log-infant mortality. Because these data are merely observational, we cannot say that oil-production causes an increase in infant mortality (indeed that most certainly isn’t the case), but we can say that they are positively correlated.\nNow let’s check the residuals.\n\nX2 = cbind(rep(1.0, data1_jags$n), data2_jags$log_income, data2_jags$is_oil)\nhead(X2)\n\n     [,1]     [,2] [,3]\n[1,]    1 8.139149    0\n[2,]    1 8.116716    0\n[3,]    1 8.115521    0\n[4,]    1 8.466110    0\n[5,]    1 8.522976    0\n[6,]    1 8.105308    0\n\n\n\n(pm_params2 = colMeans(mod2_csim)) # posterior mean\n\n      b[1]       b[2]       b[3]        sig \n 7.1472076 -0.5224867  0.7947225  0.9524947 \n\n\n\nyhat2 = drop(X2 %*% pm_params2[1:3])\nresid2 = data2_jags$y - yhat2\nplot(resid2) # against data index\n\n\n\n\n\n\n\n\n\nplot(yhat2, resid2) # against predicted values\n\n\n\n\n\n\n\n\n\nplot(yhat1, resid1) # residuals from the first model\n\n\n\n\n\n\n\n\n\nsd(resid2) # standard deviation of residuals\n\n[1] 0.6488731\n\n\nThese look much better, although the residuals for Saudi Arabia and Libya are still more than three standard deviations away from the mean of the residuals. We might consider adding the other covariate region, but instead let’s look at another option when we are faced with strong outliers.\n\n45.1.1 Student-t likelihood\n\nLet’s consider changing the likelihood.\nThe normal likelihood has thin tails (almost all of the probability is concentrated within the first few standard deviations from the mean).\nThis does not accommodate outliers well.\nConsequently, models with the normal likelihood might be overly-influenced by outliers.\nRecall that the t distribution is similar to the normal distribution, but it has thicker tails which can accommodate outliers.\n\nThe t linear model might look something like this. Notice that the t distribution has three parameters, including a positive “degrees of freedom” parameter. The smaller the degrees of freedom, the heavier the tails of the distribution. We might fix the degrees of freedom to some number, or we can assign it a prior distribution.\n\ncurve(dnorm(x), from = -5, to = 5)\ncurve(dt(x,1), from = -5, to = 5,col=\"red\", add = TRUE)\n\n\n\n\n\nnormal and t distributions\n\n\n\n\nmod3_string = \" model {\n1    for (i in 1:length(y)) {\n        y[i] ~ dt( mu[i], tau, df )\n        mu[i] = b[1] + b[2]*log_income[i] + b[3]*is_oil[i]\n    }\n    \n    for (i in 1:3) {\n        b[i] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n2    nu ~ dexp(1.0)\n3    df = nu + 2.0\n    \n4    tau ~ dgamma(5/2.0, 5*10.0/2.0)\n5    sig = sqrt( 1.0 / tau * df / (df - 2.0) )\n}\"\n\n\n1\n\nwe replaced normal likelihood with a student t likelihood which has thicker tails\n\n2\n\n\\nu nu is the degrees of freedom (dof) but the outcome can be 0 or 1\n\n3\n\nwe force the degrees of freedom dof&gt;2 to guarantee the existence of mean and variance in the t dist.\n\n4\n\n\\tau tau is the inverse scale is close to, but not equal to the precision from above so we use the same prior as we used for precision.\n\n5\n\n\\sigma sig standard deviation of errors is a deterministic function of tau, and df\n\n\n\n\nWe fit this model.\n\nset.seed(73)\ndata3_jags = list(y=dat$loginfant, log_income=dat$logincome,\n                  is_oil=as.numeric(dat$oil==\"yes\"))\n\nparams3 = c(\"b\", \"sig\")\n\ninits3 = function() {\n    inits = list(\"b\"=rnorm(3,0.0,100.0), \"prec\"=rgamma(1,1.0,1.0))\n}\n\nmod3 = jags.model(textConnection(mod3_string), data=data3_jags, inits=inits3, n.chains=3)\nupdate(mod3, 1e3) # burn-in\n\nmod3_sim = coda.samples(model=mod3,\n                        variable.names=params3,\n                        n.iter=5e3)\n\nmod3_csim = as.mcmc(do.call(rbind, mod3_sim)) # combine multiple chains\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 101\n   Unobserved stochastic nodes: 5\n   Total graph size: 512\n\nInitializing model\n\n\ncheck MCMC convergence visually\n\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod3_sim)\n\n\n\n\n\n\n\n\ncheck MCMC convergence quantitatively using Rubin Gelman\n\ngelman.diag(mod3_sim)\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\nb[1]       1.01       1.02\nb[2]       1.00       1.02\nb[3]       1.00       1.00\nsig        1.04       1.04\n\nMultivariate psrf\n\n1\n\n\n\neffectiveSize(mod3_sim)\n\n     b[1]      b[2]      b[3]       sig \n 261.8937  261.0943 8759.7467 9675.1521",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Linear regression - M3L7</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-compare-models-using-deviance-information-criterion-dic",
    "href": "C2-L07.html#sec-compare-models-using-deviance-information-criterion-dic",
    "title": "41  Linear regression - M3L7",
    "section": "45.2 Compare models using Deviance information criterion (DIC)",
    "text": "45.2 Compare models using Deviance information criterion (DIC)\n We have now proposed three different models. How do we compare their performance on our data? In the previous course, we discussed estimating parameters in models using the maximum likelihood method. Similarly, we can choose between competing models using the same idea.\nWe will use a quantity known as the deviance information criterion (DIC). It essentially calculates the posterior mean of the log-likelihood and adds a penalty for model complexity.\nLet’s calculate the DIC for our first two models:\nthe simple linear regression on log-income,\n\ndic.samples(mod1, n.iter=1e3)\n\nMean deviance:  231.2 \npenalty 2.799 \nPenalized deviance: 234 \n\n\nand the second model where we add oil production.\n\ndic.samples(mod2, n.iter=1e3)\n\nMean deviance:  225.9 \npenalty 4.195 \nPenalized deviance: 230.1 \n\n\nand the second model where we introduce the Student t likelihood.\n\ndic.samples(mod3, n.iter=1e3)\n\nMean deviance:  230.8 \npenalty 3.94 \nPenalized deviance: 234.8 \n\n\nThe first number is the Monte Carlo estimated posterior mean deviance, which equals -2 times the log-likelihood (plus a constant that will be irrelevant for comparing models). Because of that -2 factor, a smaller deviance means a higher likelihood.\nNext, we are given a penalty for the complexity of our model. This penalty is necessary because we can always increase the likelihood of the model by making it more complex to fit the data exactly. We don’t want to do this because over-fit models generalize poorly. This penalty is roughly equal to the effective number of parameters in your model. You can see this here. With the first model, we had a variance parameter and two betas, for a total of three parameters. In the second model, we added one more beta for the oil effect.\n We add these two quantities to get the DIC (the last number). The better-fitting model has a lower DIC value. In this case, the gains we receive in deviance by adding the is_oil covariate outweigh the penalty for adding an extra parameter. The final DIC for the second model is lower than for the first, so we would prefer using the second model.\nWe encourage you to explore different model specifications and compare their fit to the data using DIC. Wikipedia provides a good introduction to DIC and we can find more details about the JAGS implementation through the rjags package documentation by entering ?dic.samples in the R console.\n\n#?dic.samples",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Linear regression - M3L7</span>"
    ]
  },
  {
    "objectID": "C2-L07.html#sec-regression-diagnostics",
    "href": "C2-L07.html#sec-regression-diagnostics",
    "title": "41  Linear regression - M3L7",
    "section": "45.3 Regression Diagnostics",
    "text": "45.3 Regression Diagnostics\nIn production we want to flag regression issues in an automated fashion. However while we develop models we should try to examine these issues visually.\nRegression diagnostics help identify:\n\nshortcoming of our model and the preferred ways to improve them\n\ntransforms of variables\ndifferent likelihood\nadding missing covariate relations to remove patterns in the residuals\nincreasing interpretability by removing covariates that do not contribute to the fit.\n\nissues in the data\n\ntransformation\n\n\nwe should consider the following issues: 1. testing heteroscedasticity with the Breusch-Pagan test\nLet’s try to cover the diagnostic plots which help us validate a regression model.\n\n45.3.1 Residuals vs Fitted\n\nThe “residuals versus fits plot” is the most first diagnostic tool we\nshould look at to determine if the regression is valid. If the regression assumptions are violated we should be able to identify the issues and possibly correct them.\nIt is a scatter plot of residuals on the y axis and fitted values (estimated responses) on the x axis.\nThe plot can be used to detect:\n\nnon-linearity,\nunequal error variances, and\noutliers.\n\n\n\nplot(lmod0, 1)\n\n\n\n\nResiduals vs Fitted plot\n\n\n\n\nResiduals will enable us to assess visually whether an appropriate model has been fit to the data no matter how many predictor variables are used. We can checking the validity of a linear regression model by plotting residuals versus x and look for patterns. - Lack of a discernible pattern is indicative of a valid model. - A pattern is is indicative that a function or transformation of X is missing from the model.\n\n\n\n\n\n\nImportantWhat to look for\n\n\n\nLook for patterns that can indicate non-linearity,\n\nthat the residuals all are high in some areas and low in others. Change in variability as X changes - U shape missing quadratic term · we can get this plot as follows.\n\nThe blue line is there to aid the eye - it should ideally be relatively close to a straight line (in this case, it isn’t perfectly straight, which could indicate a mild non-linearity).\n\n\n\n\n45.3.2 QQ plot of the residuals\n This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely?\nThe regression is valid if the residuals are lined well on the straight dashed line.\nwe can get this plot as follows\n\nplot(lmod0, 2)\n\n\n\n\n\n\n\n\nnotice that the two outliers are labeled and should be reviewed for - removal - more robust likelihood\nfor more info see understanding QQ plots\n\n\n45.3.3 Scale Location plot\nThis plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It’s good if you see a horizontal line with equally (randomly) spread points.\n\nplot(lmod0, 3)\n\n\n\n\n\n\n\n\nin this case: - most of the points are to the right - the red line is almost flat which is good - there is increasing variance after 80\n\n\n45.3.4 Cook’s Distance\n - Originally introduced in (Cook 1977) Cook’s Distance is an estimate of the influence of a data point. - It takes into account both the leverage and residual of each observation. - Cook’s Distance is a summary of how much a regression model changes when the ith observation is removed. - When it comes to outliers we care about outliers that have a high Cook’s distance as they can have a large impact on the regression model. by shifting the sample fit from the population fit. - Another aspect of Cook’s distance is it can be used to identify regions of the design space where the model is poorly supported by the data - i.e. where the model is extrapolating and if we can get more data in that region we can improve the model.\n\nplot(lmod0, 4)\n\n\n\n\n\n\n\n\nUsed to detect highly influential outliers, i.e. points that can shift the sample fit from the population fit. For large sample sizes, a rough guideline is to consider values above 4/(n-p), where n is the sample size and p is the number of predictors including the intercept, to indicate highly influential points.\nsee Williams (1987)\n\n\n45.3.5 Residuals vs Leverage\n\nplot(lmod0, 5)\n\n\n\n\n\n\n\n#plot(mod3, 5)\n\nThis plot helps us to sort through the outliers, if there are any. Not all outliers are influential in linear regression analysis. Even though data have extreme values, they might not be influential to determine a regression line. i.e. the fit wouldn’t be much different if we choose to omit them from the analysis. If a point is able to exert a influence on the regression line we call it a high leverage point. Even in this case it might not alter the trend. So we want to identify high leverage points that are at a large distance from their predictor’s mean.\nUnlike the other plots, this time patterns are not relevant. We watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. Look for cases outside of the dashed lines. When cases are outside of the dashed lines (meaning they have high “Cook’s distance” scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases.\nIn this case we see that the pints are within the cook’s distance contours so our outliers are not high leverage points.\n\n\n45.3.6 Cook’s Distance vs Leverage\n\nplot(lmod0, 6)\n\n\n\n\n\n\n\nFigure 45.5: Cooks distance v.s. Leverage\n\n\n\n\n\nCook’s distance and leverage are used to detect highly leverage points, i.e. data points that can shift the sample fit from the population fit.\nFor large sample sizes, a rough guideline is to consider Cook’s distance values above 1 to indicate highly influential points and leverage values greater than 2 times the number of predictors divided by the sample size to indicate high leverage observations. High leverage observations are ones which have predictor values very far from their averages, which can greatly influence the fitted model.\nThe contours in the scatterplot are standardized residuals labelled with their magnitudes\nsee Williams (1987)\n\n\n45.3.7 Python\n\nhttps://emredjan.xyz/blog/2017/07/11/emulating-r-plots-in-python/\nhttps://towardsdatascience.com/going-from-r-to-python-linear-regression-diagnostic-plots-144d1c4aa5a\nhttps://modernstatisticswithr.com/regression.html\n\n\n\n\n\n\n\nCook, R. Dennis. 1977. “Detection of Influential Observation in Linear Regression.” Technometrics 19 (1): 15. https://doi.org/10.2307/1268249.\n\n\nWilliams, D. A. 1987. “Generalized Linear Model Diagnostics Using the Deviance and Single Case Deletions.” Applied Statistics 36 (2): 181. https://doi.org/10.2307/2347550.",
    "crumbs": [
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Linear regression - M3L7</span>"
    ]
  },
  {
    "objectID": "C2-L07-Ex1.html",
    "href": "C2-L07-Ex1.html",
    "title": "42  Homework on Linear Regression Model Part 1 - M2L5HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Homework on Linear Regression Model Part 1 - M2L5HW1</span>"
    ]
  },
  {
    "objectID": "C2-L07-Ex2.html",
    "href": "C2-L07-Ex2.html",
    "title": "43  Homework on Deviance information criterion - M2L5HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Homework on Deviance information criterion - M2L5HW2</span>"
    ]
  },
  {
    "objectID": "C2-L08.html",
    "href": "C2-L08.html",
    "title": "44  ANOVA - M3L8",
    "section": "",
    "text": "44.1 Introduction to ANOVA 🎥",
    "crumbs": [
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>ANOVA - M3L8</span>"
    ]
  },
  {
    "objectID": "C2-L08.html#sec-intro-anova",
    "href": "C2-L08.html#sec-intro-anova",
    "title": "44  ANOVA - M3L8",
    "section": "",
    "text": "Introduction to ANOVA",
    "crumbs": [
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>ANOVA - M3L8</span>"
    ]
  },
  {
    "objectID": "C2-L08.html#one-way-anova-model-using-jags",
    "href": "C2-L08.html#one-way-anova-model-using-jags",
    "title": "44  ANOVA - M3L8",
    "section": "44.2 One way ANOVA model using JAGS",
    "text": "44.2 One way ANOVA model using JAGS\n\n44.2.1 Data & EDA\nAs an example of a one-way ANOVA, we’ll look at the Plant Growth data in R. \n\n\n\n\nListing 44.1: Plant Growth Query\n\n\ndata(\"PlantGrowth\")\n#?PlantGrowth\nhead(PlantGrowth)\n\n\n\n\n  weight group\n1   4.17  ctrl\n2   5.58  ctrl\n3   5.18  ctrl\n4   6.11  ctrl\n5   4.50  ctrl\n6   4.61  ctrl\n\n\nWe first load the dataset (Listing 44.1)\nBecause the explanatory variable group is a factor and not continuous, we choose to visualize the data with box plots rather than scatter plots.\n\nboxplot(weight ~ group, data=PlantGrowth)\n\n\n\n\n\n\n\nFigure 44.1: PlantGrowth boxplot\n\n\n\n\n\nThe box plots summarize the distribution of the data for each of the three groups. It appears that treatment 2 has the highest mean yield. It might be questionable whether each group has the same variance, but we’ll assume that is the case.\n\n\n44.2.2 Modeling\nAgain, we can start with the reference analysis (with a noninformative prior) with a linear model in R.\n\nlmod = lm(weight ~ group, data=PlantGrowth)\nsummary(lmod)\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\n\nanova(lmod)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngroup      2  3.7663  1.8832  4.8461 0.01591 *\nResiduals 27 10.4921  0.3886                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplot(lmod) # for graphical residual analysis\n\n\n\n\n\n\n\nFigure 44.2: Graphical residual analysis\n\n\n\n\n\n\n\n\n\n\n\nFigure 44.3: Graphical residual analysis\n\n\n\n\n\n\n\n\n\n\n\nFigure 44.4: Graphical residual analysis\n\n\n\n\n\n\n\n\n\n\n\nFigure 44.5: Graphical residual analysis\n\n\n\n\n\nThe default model structure in R is the linear model with dummy indicator variables. Hence, the “intercept” in this model is the mean yield for the control group. The two other parameters are the estimated effects of treatments 1 and 2. To recover the mean yield in treatment group 1, you would add the intercept term and the treatment 1 effect. To see how R sets the model up, use the model.matrix(lmod) function to extract the X matrix.\nThe anova() function in R compares variability of observations between the treatment groups to variability within the treatment groups to test whether all means are equal or whether at least one is different. The small p-value here suggests that the means are not all equal.\nLet’s fit the cell means model in JAGS.\n\nlibrary(\"rjags\")\n\n\nmod_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dnorm(mu[grp[i]], prec)\n    }\n    \n    for (j in 1:3) {\n        mu[j] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n    prec ~ dgamma(5/2.0, 5*1.0/2.0)\n    sig = sqrt( 1.0 / prec )\n} \"\n\nset.seed(82)\nstr(PlantGrowth)\n\n'data.frame':   30 obs. of  2 variables:\n $ weight: num  4.17 5.58 5.18 6.11 4.5 4.61 5.17 4.53 5.33 5.14 ...\n $ group : Factor w/ 3 levels \"ctrl\",\"trt1\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\ndata_jags = list(y=PlantGrowth$weight, \n              grp=as.numeric(PlantGrowth$group))\n\nparams = c(\"mu\", \"sig\")\n\ninits = function() {\n    inits = list(\"mu\"=rnorm(3,0.0,100.0), \"prec\"=rgamma(1,1.0,1.0))\n}\n\nmod = jags.model(textConnection(mod_string), data=data_jags, inits=inits, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 30\n   Unobserved stochastic nodes: 4\n   Total graph size: 74\n\nInitializing model\n\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,\n                        variable.names=params,\n                        n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim)) # combined chains\n\n\n\n44.2.3 Model checking\nAs usual, we check for convergence of our MCMC.\n\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim)\n\ngelman.diag(mod_sim)\n\nPotential scale reduction factors:\n\n      Point est. Upper C.I.\nmu[1]          1          1\nmu[2]          1          1\nmu[3]          1          1\nsig            1          1\n\nMultivariate psrf\n\n1\n\nautocorr.diag(mod_sim)\n\n              mu[1]         mu[2]        mu[3]          sig\nLag 0   1.000000000  1.0000000000  1.000000000  1.000000000\nLag 1  -0.003295982  0.0032550285  0.014229944  0.093790272\nLag 5  -0.002099076  0.0004635964 -0.015080105 -0.004950599\nLag 10 -0.005994920 -0.0016405035  0.002439778  0.003699796\nLag 50  0.007647856  0.0027188925 -0.008915493 -0.008770126\n\neffectiveSize(mod_sim)\n\n   mu[1]    mu[2]    mu[3]      sig \n15000.00 15152.88 14977.57 11527.49 \n\n\n\n\n\n\n\n\nFigure 44.6: MCMC convergence diagnostics\n\n\n\n\n\nWe can also look at the residuals to see if there are any obvious problems with our model choice.\n\n(pm_params = colMeans(mod_csim))\n\n    mu[1]     mu[2]     mu[3]       sig \n5.0311341 4.6644016 5.5220571 0.7139621 \n\n\n\nyhat = pm_params[1:3][data_jags$grp]\nresid = data_jags$y - yhat\nplot(resid)\n\n\n\n\n\n\n\nFigure 44.7: Residuals vs Index\n\n\n\n\n\n\nplot(yhat, resid)\n\n\n\n\n\n\n\nFigure 44.8: Residuals vs Fitted values for PlantGrowth model\n\n\n\n\n\nAgain, it might be appropriate to have a separate variance for each group. We will have you do that as an exercise.\n\n\n44.2.4 Results\nLet’s look at the posterior summary of the parameters.\n\nsummary(mod_sim)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n       Mean      SD  Naive SE Time-series SE\nmu[1] 5.031 0.22776 0.0018596      0.0018596\nmu[2] 4.664 0.22758 0.0018581      0.0018490\nmu[3] 5.522 0.22782 0.0018602      0.0018615\nsig   0.714 0.09329 0.0007617      0.0008709\n\n2. Quantiles for each variable:\n\n        2.5%    25%    50%    75%  97.5%\nmu[1] 4.5813 4.8780 5.0304 5.1825 5.4792\nmu[2] 4.2164 4.5151 4.6630 4.8149 5.1154\nmu[3] 5.0677 5.3731 5.5225 5.6728 5.9738\nsig   0.5592 0.6484 0.7042 0.7684 0.9269\n\n\n\nHPDinterval(mod_csim)\n\n          lower     upper\nmu[1] 4.5751119 5.4713908\nmu[2] 4.1972351 5.0959196\nmu[3] 5.0870461 5.9868314\nsig   0.5415318 0.8962747\nattr(,\"Probability\")\n[1] 0.95\n\n\nThe HPDinterval() function in the coda package calculates intervals of highest posterior density for each parameter.\nWe are interested to know if one of the treatments increases mean yield. It is clear that treatment 1 does not. What about treatment 2?\n\nmean(mod_csim[,3] &gt; mod_csim[,1])\n\n[1] 0.9368667\n\n\nThere is a high posterior probability that the mean yield for treatment 2 is greater than the mean yield for the control group.\nIt may be the case that treatment 2 would be costly to put into production. Suppose that to be worthwhile, this treatment must increase mean yield by 10%. What is the posterior probability that the increase is at least that?\n\nmean(mod_csim[,3] &gt; 1.1*mod_csim[,1])\n\n[1] 0.4848667\n\n\nWe have about 50/50 odds that adopting treatment 2 would increase mean yield by at least 10%.",
    "crumbs": [
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>ANOVA - M3L8</span>"
    ]
  },
  {
    "objectID": "C2-L08.html#two-factor-anova",
    "href": "C2-L08.html#two-factor-anova",
    "title": "44  ANOVA - M3L8",
    "section": "44.3 Two Factor ANOVA",
    "text": "44.3 Two Factor ANOVA\n\n44.3.1 Data\n Let’s explore an example with two factors. We’ll use the Warpbreaks data set in R. Check the documentation for a description of the data by typing ?warpbreaks.\n\ndata(\"warpbreaks\")\n#?warpbreaks\nhead(warpbreaks)\n\n  breaks wool tension\n1     26    A       L\n2     30    A       L\n3     54    A       L\n4     25    A       L\n5     70    A       L\n6     52    A       L\n\n\n\n\n\nTable 44.1: Preview of first few rows of warpbreaks data\n\n\n# This chunk is for displaying the output that was previously static.\n# If the static output below is preferred, this chunk can be removed \n# and the static output remains unlabelled as it's not a code cell.\n# For a labeled table, this chunk should generate it.\n# The original file had static output here:\n##   breaks wool tension\n## 1     26    A       L\n## 2     30    A       L\n## 3     54    A       L\n## 4     25    A       L\n## 5     70    A       L\n## 6     52    A       L\n# To make this a labeled table from code:\nhead(warpbreaks)\n\n  breaks wool tension\n1     26    A       L\n2     30    A       L\n3     54    A       L\n4     25    A       L\n5     70    A       L\n6     52    A       L\n\n\n\n\n\n\n\nTable 44.2: Contingency table of wool type vs tension level\n\n\ntable(warpbreaks$wool, warpbreaks$tension)\n\n   \n    L M H\n  A 9 9 9\n  B 9 9 9\n\n\n\n\nAgain, we visualize the data with box plots.\n\nboxplot(breaks ~ wool + tension, data=warpbreaks)\n\n\n\n\n\n\n\nFigure 44.9: Warpbreaks boxplot\n\n\n\n\n\n\nboxplot(log(breaks) ~ wool + tension, data=warpbreaks)\n\n\n\n\n\n\n\nFigure 44.10: Warpbreaks boxplot with log-transformed breaks\n\n\n\n\n\nThe different groups have more similar variance if we use the logarithm of breaks. From this visualization, it looks like both factors may play a role in the number of breaks. It appears that there is a general decrease in breaks as we move from low to medium to high tension. Let’s start with a one-way model using tension only.\n\n\n44.3.2 One-way model\n\nmod1_string = \" model {\n    for( i in 1:length(y)) {\n        y[i] ~ dnorm(mu[tensGrp[i]], prec)\n    }\n    \n    for (j in 1:3) {\n        mu[j] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n    prec ~ dgamma(5/2.0, 5*2.0/2.0)\n    sig = sqrt(1.0 / prec)\n} \"\n\nset.seed(83)\nstr(warpbreaks)\n\n'data.frame':   54 obs. of  3 variables:\n $ breaks : num  26 30 54 25 70 52 51 26 67 18 ...\n $ wool   : Factor w/ 2 levels \"A\",\"B\": 1 1 1 1 1 1 1 1 1 1 ...\n $ tension: Factor w/ 3 levels \"L\",\"M\",\"H\": 1 1 1 1 1 1 1 1 1 2 ...\n\ndata1_jags = list(y=log(warpbreaks$breaks), tensGrp=as.numeric(warpbreaks$tension))\n\nparams1 = c(\"mu\", \"sig\")\n\nmod1 = jags.model(textConnection(mod1_string), data=data1_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 54\n   Unobserved stochastic nodes: 4\n   Total graph size: 123\n\nInitializing model\n\nupdate(mod1, 1e3)\n\nmod1_sim = coda.samples(model=mod1,\n                        variable.names=params1,\n                        n.iter=5e3)\n\n\n## convergence diagnostics\nplot(mod1_sim)\n\n\n\n\n\n\n\nFigure 44.11: MCMC convergence diagnostics for one-way tension model\n\n\n\n\n\n\ngelman.diag(mod1_sim)\n\nPotential scale reduction factors:\n\n      Point est. Upper C.I.\nmu[1]          1          1\nmu[2]          1          1\nmu[3]          1          1\nsig            1          1\n\nMultivariate psrf\n\n1\n\nautocorr.diag(mod1_sim)\n\n              mu[1]        mu[2]         mu[3]          sig\nLag 0   1.000000000  1.000000000  1.0000000000  1.000000000\nLag 1  -0.001543382 -0.011482925 -0.0048970151  0.051048754\nLag 5  -0.006377853 -0.017028742 -0.0054862444 -0.021399059\nLag 10  0.001367067  0.009851261  0.0065758333 -0.001345930\nLag 50  0.021506593  0.012051621 -0.0006951465  0.001122476\n\neffectiveSize(mod1_sim)\n\n   mu[1]    mu[2]    mu[3]      sig \n14721.39 14972.50 15899.17 13717.48 \n\n\nThe 95% posterior interval for the mean of group 2 (medium tension) overlaps with both the low and high groups, but the intervals for low and high group only slightly overlap. That is a pretty strong indication that the means for low and high tension are different. Let’s collect the DIC for this model and move on to the two-way model.\n\ndic1 = dic.samples(mod1, n.iter=1e3)\n\n\n\n44.3.3 Two-way additive model\nWith two factors, one with two levels and the other with three, we have six treatment groups, which is the same situation we discussed when introducing multiple factor ANOVA. We will first fit the additive model which treats the two factors separately with no interaction. To get the X matrix (or design matrix) for this model, we can create it in R.\n\n\n\nTable 44.3: Head of the design matrix for the additive model\n\n\nX = model.matrix( ~ wool + tension, data=warpbreaks)\nhead(X)\n\n  (Intercept) woolB tensionM tensionH\n1           1     0        0        0\n2           1     0        0        0\n3           1     0        0        0\n4           1     0        0        0\n5           1     0        0        0\n6           1     0        0        0\n\n\n\n\n\n\n\nTable 44.4: Tail of the design matrix for the additive model\n\n\ntail(X)\n\n   (Intercept) woolB tensionM tensionH\n49           1     1        0        1\n50           1     1        0        1\n51           1     1        0        1\n52           1     1        0        1\n53           1     1        0        1\n54           1     1        0        1\n\n\n\n\nBy default, R has chosen the mean for wool A and low tension to be the intercept. Then, there is an effect for wool B, and effects for medium tension and high tension, each associated with dummy indicator variables.\n\nmod2_string = \" model {\n    for( i in 1:length(y)) {\n        y[i] ~ dnorm(mu[i], prec)\n        mu[i] = int + alpha*isWoolB[i] + beta[1]*isTensionM[i] + beta[2]*isTensionH[i]\n    }\n    \n    int ~ dnorm(0.0, 1.0/1.0e6)\n    alpha ~ dnorm(0.0, 1.0/1.0e6)\n    for (j in 1:2) {\n        beta[j] ~ dnorm(0.0, 1.0/1.0e6)\n    }\n    \n    prec ~ dgamma(3/2.0, 3*1.0/2.0)\n    sig = sqrt(1.0 / prec)\n} \"\n\ndata2_jags = list(y=log(warpbreaks$breaks), isWoolB=X[,\"woolB\"], isTensionM=X[,\"tensionM\"], isTensionH=X[,\"tensionH\"])\n\nparams2 = c(\"int\", \"alpha\", \"beta\", \"sig\")\n\nmod2 = jags.model(textConnection(mod2_string), data=data2_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 54\n   Unobserved stochastic nodes: 5\n   Total graph size: 243\n\nInitializing model\n\nupdate(mod2, 1e3)\n\nmod2_sim = coda.samples(model=mod2,\n                        variable.names=params2,\n                        n.iter=5e3)\n\n\n## convergence diagnostics\nplot(mod2_sim)\n\ngelman.diag(mod2_sim)    # Corrected from mod1_sim\n\nPotential scale reduction factors:\n\n        Point est. Upper C.I.\nalpha            1          1\nbeta[1]          1          1\nbeta[2]          1          1\nint              1          1\nsig              1          1\n\nMultivariate psrf\n\n1\n\nautocorr.diag(mod2_sim)  # Corrected from mod1_sim\n\n               alpha      beta[1]      beta[2]         int         sig\nLag 0   1.000000e+00  1.000000000  1.000000000 1.000000000 1.000000000\nLag 1   4.949732e-01  0.493664011  0.488555715 0.740486160 0.069036596\nLag 5   3.798922e-02  0.105554167  0.095890680 0.166494047 0.014138106\nLag 10 -4.884794e-05 -0.008619817 -0.010211226 0.001239427 0.015050523\nLag 50 -2.346203e-04 -0.003652400 -0.001246754 0.006844612 0.002058968\n\neffectiveSize(mod2_sim) # Corrected from mod1_sim\n\n    alpha   beta[1]   beta[2]       int       sig \n 4996.075  4095.918  4037.636  2551.051 12259.928 \n\n\n\n\n\n\n\n\nFigure 44.12: Convergence and diagnostics for the additive two-way ANOVA model\n\n\n\n\n\n\n\n\n\n\n\nFigure 44.13: Convergence and diagnostics for the additive two-way ANOVA model\n\n\n\n\n\nLet’s summarize the results, collect the DIC for this model, and compare it to the first one-way model.\n\nsummary(mod2_sim)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean      SD  Naive SE Time-series SE\nalpha   -0.1521 0.12461 0.0010174      0.0017642\nbeta[1] -0.2886 0.15075 0.0012309      0.0023670\nbeta[2] -0.4901 0.15036 0.0012277      0.0023658\nint      3.5769 0.12139 0.0009912      0.0024042\nsig      0.4536 0.04517 0.0003688      0.0004085\n\n2. Quantiles for each variable:\n\n           2.5%     25%     50%      75%     97.5%\nalpha   -0.3959 -0.2344 -0.1512 -0.07018  0.094806\nbeta[1] -0.5850 -0.3891 -0.2905 -0.18832  0.007825\nbeta[2] -0.7830 -0.5905 -0.4903 -0.38986 -0.192312\nint      3.3352  3.4954  3.5765  3.65790  3.817546\nsig      0.3758  0.4216  0.4502  0.48164  0.551043\n\n\n\n(dic2 = dic.samples(mod2, n.iter=1e3))\n\nMean deviance:  55.62 \npenalty 5.221 \nPenalized deviance: 60.84 \n\ndic1\n\nMean deviance:  66.45 \npenalty 4.064 \nPenalized deviance: 70.52 \n\n\nThis suggests there is much to be gained adding the wool factor to the model. Before we settle on this model however, we should consider whether there is an interaction. Let’s look again at the box plot with all six treatment groups.\n\nboxplot(log(breaks) ~ wool + tension, data=warpbreaks)\n\n\n\n\n\n\n\nFigure 44.14: Re-examining boxplot of log(breaks) by wool and tension for interaction effects\n\n\n\n\n\nOur two-way model has a single effect for wool B and the estimate is negative. If this is true, then we would expect wool B to be associated with fewer breaks than its wool A counterpart on average. This is true for low and high tension, but it appears that breaks are higher for wool B when there is medium tension. That is, the effect for wool B is not consistent across tension levels, so it may appropriate to add an interaction term. In R, this would look like:\n\nlmod2 = lm(log(breaks) ~ .^2, data=warpbreaks)\nsummary(lmod2)\n\n\nCall:\nlm(formula = log(breaks) ~ .^2, data = warpbreaks)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.81504 -0.27885  0.04042  0.27319  0.64358 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.7179     0.1247  29.824  &lt; 2e-16 ***\nwoolB           -0.4356     0.1763  -2.471  0.01709 *  \ntensionM        -0.6012     0.1763  -3.410  0.00133 ** \ntensionH        -0.6003     0.1763  -3.405  0.00134 ** \nwoolB:tensionM   0.6281     0.2493   2.519  0.01514 *  \nwoolB:tensionH   0.2221     0.2493   0.891  0.37749    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.374 on 48 degrees of freedom\nMultiple R-squared:  0.3363,    Adjusted R-squared:  0.2672 \nF-statistic: 4.864 on 5 and 48 DF,  p-value: 0.001116\n\n\nAdding the interaction, we get an effect for being in wool B and medium tension, as well as for being in wool B and high tension. There are now six parameters for the mean, one for each treatment group, so this model is equivalent to the full cell means model. Let’s use that.\n\n\n44.3.4 Two-way cell means model\nIn this new model, \\mu will be a matrix with six entries, each corresponding to a treatment group.\n\nmod3_string = \" model {\n    for( i in 1:length(y)) {\n        y[i] ~ dnorm(mu[woolGrp[i], tensGrp[i]], prec)\n    }\n    \n    for (j in 1:max(woolGrp)) {\n        for (k in 1:max(tensGrp)) {\n            mu[j,k] ~ dnorm(0.0, 1.0/1.0e6)\n        }\n    }\n    \n    prec ~ dgamma(3/2.0, 3*1.0/2.0)\n    sig = sqrt(1.0 / prec)\n} \"\n\nstr(warpbreaks)\n\n'data.frame':   54 obs. of  3 variables:\n $ breaks : num  26 30 54 25 70 52 51 26 67 18 ...\n $ wool   : Factor w/ 2 levels \"A\",\"B\": 1 1 1 1 1 1 1 1 1 1 ...\n $ tension: Factor w/ 3 levels \"L\",\"M\",\"H\": 1 1 1 1 1 1 1 1 1 2 ...\n\ndata3_jags = list(y=log(warpbreaks$breaks), woolGrp=as.numeric(warpbreaks$wool), tensGrp=as.numeric(warpbreaks$tension))\n\nparams3 = c(\"mu\", \"sig\")\n\nmod3 = jags.model(textConnection(mod3_string), data=data3_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 54\n   Unobserved stochastic nodes: 7\n   Total graph size: 179\n\nInitializing model\n\nupdate(mod3, 1e3)\n\nmod3_sim = coda.samples(model=mod3,\n                        variable.names=params3,\n                        n.iter=5e3)\nmod3_csim = as.mcmc(do.call(rbind, mod3_sim))\n\n\nplot(mod3_sim)\n\n\n\n\n\n\n\nFigure 44.15: Traceplots for the cell means model\n\n\n\n\n\n\n\n\n\n\n\nFigure 44.16: Traceplots for the cell means model\n\n\n\n\n\n\n## convergence diagnostics\ngelman.diag(mod3_sim)\n\nPotential scale reduction factors:\n\n        Point est. Upper C.I.\nmu[1,1]          1          1\nmu[2,1]          1          1\nmu[1,2]          1          1\nmu[2,2]          1          1\nmu[1,3]          1          1\nmu[2,3]          1          1\nsig              1          1\n\nMultivariate psrf\n\n1\n\nautocorr.diag(mod3_sim)\n\n             mu[1,1]       mu[2,1]      mu[1,2]      mu[2,2]      mu[1,3]\nLag 0   1.0000000000  1.0000000000  1.000000000  1.000000000  1.000000000\nLag 1  -0.0031449739  0.0088287336  0.006110741 -0.006245016  0.007298649\nLag 5   0.0026387814 -0.0053453974  0.008388578 -0.007357227 -0.003745448\nLag 10  0.0009629569 -0.0032304891  0.015507145 -0.011358803 -0.007156927\nLag 50 -0.0037293287 -0.0007610572 -0.009355279 -0.008208487  0.010905305\n             mu[2,3]          sig\nLag 0   1.000000e+00  1.000000000\nLag 1  -1.290280e-02  0.114523745\nLag 5   2.342043e-05  0.006218406\nLag 10  6.727064e-03  0.010898163\nLag 50  2.500647e-03 -0.004887363\n\neffectiveSize(mod3_sim)\n\n mu[1,1]  mu[2,1]  mu[1,2]  mu[2,2]  mu[1,3]  mu[2,3]      sig \n15000.00 14774.09 14325.64 14967.22 14774.57 15350.80 11208.04 \n\nraftery.diag(mod3_sim)\n\n[[1]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                               \n         Burn-in  Total Lower bound  Dependence\n         (M)      (N)   (Nmin)       factor (I)\n mu[1,1] 2        3741  3746         0.999     \n mu[2,1] 2        3680  3746         0.982     \n mu[1,2] 2        3741  3746         0.999     \n mu[2,2] 2        3741  3746         0.999     \n mu[1,3] 2        3866  3746         1.030     \n mu[2,3] 2        3680  3746         0.982     \n sig     2        3680  3746         0.982     \n\n\n[[2]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                               \n         Burn-in  Total Lower bound  Dependence\n         (M)      (N)   (Nmin)       factor (I)\n mu[1,1] 2        3866  3746         1.030     \n mu[2,1] 2        3930  3746         1.050     \n mu[1,2] 2        3803  3746         1.020     \n mu[2,2] 2        3803  3746         1.020     \n mu[1,3] 3        4129  3746         1.100     \n mu[2,3] 2        3680  3746         0.982     \n sig     1        3712  3746         0.991     \n\n\n[[3]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                               \n         Burn-in  Total Lower bound  Dependence\n         (M)      (N)   (Nmin)       factor (I)\n mu[1,1] 2        3620  3746         0.966     \n mu[2,1] 2        3803  3746         1.020     \n mu[1,2] 2        3803  3746         1.020     \n mu[2,2] 2        3803  3746         1.020     \n mu[1,3] 2        3803  3746         1.020     \n mu[2,3] 2        3741  3746         0.999     \n sig     2        3680  3746         0.982     \n\n\nLet’s compute the DIC and compare with our previous models.\n\n(dic3 = dic.samples(mod3, n.iter=1e3))\n\nMean deviance:  52.05 \npenalty 7.185 \nPenalized deviance: 59.24 \n\ndic2\n\nMean deviance:  55.62 \npenalty 5.221 \nPenalized deviance: 60.84 \n\ndic1\n\nMean deviance:  66.45 \npenalty 4.064 \nPenalized deviance: 70.52 \n\n\nThis suggests that the full model with interaction between wool and tension (which is equivalent to the cell means model) is the best for explaining/predicting warp breaks.\n\n\n44.3.5 Results\n\nsummary(mod3_sim)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean      SD  Naive SE Time-series SE\nmu[1,1] 3.7179 0.14854 0.0012128      0.0012129\nmu[2,1] 3.2821 0.14809 0.0012091      0.0012186\nmu[1,2] 3.1165 0.14918 0.0012180      0.0012488\nmu[2,2] 3.3081 0.15043 0.0012283      0.0012297\nmu[1,3] 3.1174 0.14833 0.0012111      0.0012210\nmu[2,3] 2.9058 0.14723 0.0012021      0.0011887\nsig     0.4427 0.04511 0.0003683      0.0004282\n\n2. Quantiles for each variable:\n\n          2.5%   25%    50%    75%  97.5%\nmu[1,1] 3.4207 3.620 3.7179 3.8156 4.0118\nmu[2,1] 2.9935 3.183 3.2822 3.3803 3.5780\nmu[1,2] 2.8176 3.018 3.1158 3.2159 3.4132\nmu[2,2] 3.0088 3.209 3.3083 3.4075 3.6034\nmu[1,3] 2.8264 3.019 3.1170 3.2164 3.4073\nmu[2,3] 2.6189 2.806 2.9058 3.0038 3.1987\nsig     0.3636 0.411 0.4391 0.4708 0.5421\n\n\n\nHPDinterval(mod3_csim)\n\n            lower     upper\nmu[1,1] 3.4211281 4.0119744\nmu[2,1] 2.9850428 3.5675903\nmu[1,2] 2.8162798 3.4112760\nmu[2,2] 3.0137257 3.6068973\nmu[1,3] 2.8229589 3.4028172\nmu[2,3] 2.6252050 3.2036412\nsig     0.3589913 0.5347668\nattr(,\"Probability\")\n[1] 0.95\n\n\n\npar(mfrow=c(3,2)) # arrange frame for plots\ndensplot(mod3_csim[,1:6], xlim=c(2.0, 4.5))\n\n\n\n\n\n\n\nFigure 44.17: Posterior densities for cell means\n\n\n\n\n\nIt might be tempting to look at comparisons between each combination of treatments, but we warn that this could yield spurious results. When we discussed the statistical modeling cycle, we said it is best not to search your results for interesting hypotheses, because if there are many hypotheses, some will appear to show “effects” or “associations” simply due to chance. Results are most reliable when we determine a relatively small number of hypotheses we are interested in beforehand, collect the data, and statistically evaluate the evidence for them.\nOne question we might be interested in with these data is finding the treatment combination that produces the fewest breaks. To calculate this, we can go through our posterior samples and for each sample, find out which group has the smallest mean. These counts help us determine the posterior probability that each of the treatment groups has the smallest mean.\n\n\n\nTable 44.5: Posterior probabilities of each treatment group having the smallest mean break rate\n\n\nprop.table( table( apply(mod3_csim[,1:6], 1, which.min) ) )\n\n\n         2          3          4          5          6 \n0.01653333 0.11993333 0.01173333 0.11546667 0.73633333 \n\n\n\n\nThe evidence supports wool B with high tension as the treatment that produces the fewest breaks.",
    "crumbs": [
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>ANOVA - M3L8</span>"
    ]
  },
  {
    "objectID": "C2-L08-Ex1.html",
    "href": "C2-L08-Ex1.html",
    "title": "45  Homework on ANOVA - M3L8HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Homework on ANOVA - M3L8HW1</span>"
    ]
  },
  {
    "objectID": "C2-L08-Ex2.html",
    "href": "C2-L08-Ex2.html",
    "title": "46  Homework on Multiple Factor ANOVA - M3L8HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Homework on Multiple Factor ANOVA - M3L8HW2</span>"
    ]
  },
  {
    "objectID": "C2-L09.html",
    "href": "C2-L09.html",
    "title": "47  Logistic regression - M3L9",
    "section": "",
    "text": "47.1 Introduction to Logistic Regression 🎥\nLogistic regression is the preferred model when modelling a problem where the response variable is binary such as a classification or the outcome of a Bernoulli trial. In such the traditional least square fit suffers from a number of shortcomings. The main idea here is a log transform. However a naive approach this transform imposes issues with 0 valued inputs since log(0)=-\\infty",
    "crumbs": [
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Logistic regression - M3L9</span>"
    ]
  },
  {
    "objectID": "C2-L09.html#sec-intro-logistic-regression",
    "href": "C2-L09.html#sec-intro-logistic-regression",
    "title": "47  Logistic regression - M3L9",
    "section": "",
    "text": "Figure 47.1: Introduction to logistic regression\n\n\n\n47.1.1 Data\n For an example of logistic regression , we’ll use the urine data set from the boot package in R. The response variable is r, which takes on values of 0 or 1. We will remove some rows from the data set which contain missing values.logistic regression\n\nlibrary(\"boot\")\ndata(\"urine\")\n?urine\nhead(urine)\n\n  r gravity   ph osmo cond urea calc\n1 0   1.021 4.91  725   NA  443 2.45\n2 0   1.017 5.74  577 20.0  296 4.49\n3 0   1.008 7.20  321 14.9  101 2.36\n4 0   1.011 5.51  408 12.6  224 2.15\n5 0   1.005 6.52  187  7.5   91 1.16\n6 0   1.020 5.27  668 25.3  252 3.34\n\n\n\n1dat = na.omit(urine)\n\n\n1\n\ndrop missing values\n\n\n\n\nLet’s look at pairwise scatter plots of the seven variables.\n\npairs(dat)\n\n\n\n\n\n\n\n\nOne thing that stands out is that several of these variables are strongly correlated with one another. For example gravity and osmo appear to have a very close linear relationship. Collinearity between x variables in linear regression models can cause trouble for statistical inference. Two correlated variables will compete for the ability to predict the response variable, leading to unstable estimates. This is not a problem for prediction of the response, if prediction is the end goal of the model. But if our objective is to discover how the variables relate to the response, we should avoid collinearity.\n\n\n\n\n\n\nImportantCollinearity and Multicollinearity\n\n\n\n When two covariates are highly correlated we call this relation collinearity. When one covariate in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy we call this relation multicollinearity. It is possible that no two pairs of a such a group of covariates are correlated.\nIn both cases this will lead to the design matrix being almost singular. Near singular matrices are a strong cause of instability in numerical calculations. Statistical this tends to lead to a model with inflated standard errors compared to models where we only keep the a subset where variables are neither collinear nor multicollinear. A consequence of this is that we will see a drop in statistical significance for these variables, which will make interpreting the model harder.\nWe have seen a few strategies ways to deal with these issues:\n\ninclude pair plot in the exploratory data analysis phase.\npicking subsets and checking DIC or,\nvariable selection using double exponential priors.\nPCA creates independent covariates with a lower dimension with a trade of losing interpretability. See (Johnson and Wichern 2001, 386) (Belsley, Kuh, and Welsch 1980, 85–191) (Härdle and Simar 2019)\nFeature elimination based on combination of Variance inflation factors (VIF) (Sheather 2009, 203)\n\n\n\nWe can more formally estimate the correlation among these variables using the corrplot package.\n\nlibrary(\"corrplot\")\n\ncorrplot 0.95 loaded\n\nCor = cor(dat)\ncorrplot(Cor, type=\"upper\", method=\"ellipse\", tl.pos=\"d\")\ncorrplot(Cor, type=\"lower\", method=\"number\", col=\"black\", \n         add=TRUE, diag=FALSE, tl.pos=\"n\", cl.pos=\"n\")\n\n\n\n\n\n\n\n\n\n\n47.1.2 Variable selection\nOne primary goal of this analysis is to find out which variables are related to the presence of calcium oxalate crystals. This objective is often called “variable selection.” We have already seen one way to do this: fit several models that include different sets of variables and see which one has the best DIC. Another way to do this is to use a linear model where the priors for the \\beta coefficients favor values near 0 (indicating a weak relationship). This way, the burden of establishing association lies with the data. If there is not a strong signal, we assume it doesn’t exist.\nRather than tailoring a prior for each individual \\beta based on the scale its covariate takes values on, it is customary to subtract the mean and divide by the standard deviation for each variable.\n\nX = scale(dat[,-1], center=TRUE, scale=TRUE)\nhead(X[,\"gravity\"])\n\n         2          3          4          5          6          7 \n-0.1403037 -1.3710690 -0.9608139 -1.7813240  0.2699514 -0.8240622 \n\n\n\ncolMeans(X)\n\n      gravity            ph          osmo          cond          urea \n-9.861143e-15  8.511409e-17  1.515743e-16 -1.829852e-16  7.335402e-17 \n         calc \n-1.689666e-18 \n\n\n\napply(X, 2, sd)\n\ngravity      ph    osmo    cond    urea    calc \n      1       1       1       1       1       1 \n\n\n\n\n47.1.3 Model\nOur prior for the \\beta (which we’ll call b in the model) coefficients will be the double exponential (or Laplace) distribution, which as the name implies, is the exponential distribution with tails extending in the positive direction as well as the negative direction, with a sharp peak at 0. We can read more about it in the JAGS manual. The distribution looks like:\n\nddexp = function(x, mu, tau) {\n  0.5*tau*exp(-tau*abs(x-mu)) \n}\ncurve(ddexp(x, mu=0.0, tau=1.0), from=-5.0, to=5.0, \n      ylab=\"density\", \n      main=\"Double exponential\\ndistribution\") # double exponential distribution\ncurve(dnorm(x, mean=0.0, sd=1.0), from=-5.0, to=5.0, \n      lty=2, add=TRUE) # normal distribution\nlegend(\"topright\", \n      legend=c(\"double exponential\", \"normal\"), \n      lty=c(1,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\nlibrary(\"rjags\")\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\n\n\nmod1_string = \n  \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dbern(p[i])\n        logit(p[i]) = int + b[1]*gravity[i] + \n                            b[2]*ph[i] + \n                            b[3]*osmo[i] + \n                            b[4]*cond[i] + \n                            b[5]*urea[i] + \n                            b[6]*calc[i]\n    }\n    int ~ dnorm(0.0, 1.0/25.0)\n    for (j in 1:6) {\n        b[j] ~ ddexp(0.0, sqrt(2.0)) # has var 1.0\n    }\n} \"\n\nset.seed(92)\nhead(X)\n\n     gravity         ph       osmo       cond        urea        calc\n2 -0.1403037 -0.4163725 -0.1528785 -0.1130908  0.25747827  0.09997564\n3 -1.3710690  1.6055972 -1.2218894 -0.7502609 -1.23693077 -0.54608444\n4 -0.9608139 -0.7349020 -0.8585927 -1.0376121 -0.29430353 -0.60978050\n5 -1.7813240  0.6638579 -1.7814497 -1.6747822 -1.31356713 -0.91006194\n6  0.2699514 -1.0672806  0.2271214  0.5490664 -0.07972172 -0.24883614\n7 -0.8240622 -0.5825618 -0.6372741 -0.4379226 -0.51654898 -0.83726644\n\ndata_jags = list(y=dat$r, \n                 gravity=X[,\"gravity\"], \n                 ph=X[,\"ph\"], \n                 osmo=X[,\"osmo\"], \n                 cond=X[,\"cond\"], \n                 urea=X[,\"urea\"], \n                 calc=X[,\"calc\"])\nparams = c(\"int\", \"b\")\n\nmod1 = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 77\n   Unobserved stochastic nodes: 7\n   Total graph size: 1085\n\nInitializing model\n\nupdate(mod1, 1e3)\n\nmod1_sim = coda.samples(model=mod1,\n                        variable.names=params,\n                        n.iter=5e3)\nmod1_csim = as.mcmc(do.call(rbind, mod1_sim))\n\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod1_sim, ask=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngelman.diag(mod1_sim)\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\nb[1]          1       1.01\nb[2]          1       1.00\nb[3]          1       1.01\nb[4]          1       1.01\nb[5]          1       1.01\nb[6]          1       1.00\nint           1       1.00\n\nMultivariate psrf\n\n1\n\nautocorr.diag(mod1_sim)\n\n              b[1]         b[2]       b[3]       b[4]       b[5]       b[6]\nLag 0   1.00000000  1.000000000 1.00000000 1.00000000 1.00000000 1.00000000\nLag 1   0.83139527  0.282140123 0.89937587 0.75981398 0.79178141 0.48729555\nLag 5   0.40470384 -0.006327089 0.58956682 0.36469706 0.36883565 0.06484881\nLag 10  0.17463933  0.005297018 0.34396080 0.19057429 0.16280890 0.01297326\nLag 50 -0.02257132  0.005922485 0.03869131 0.02883672 0.01546907 0.01094675\n                int\nLag 0   1.000000000\nLag 1   0.292404424\nLag 5   0.027938430\nLag 10 -0.005587174\nLag 50 -0.024954679\n\nautocorr.plot(mod1_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffectiveSize(mod1_sim)\n\n     b[1]      b[2]      b[3]      b[4]      b[5]      b[6]       int \n1381.6646 8406.5088  781.1091 1460.5169 1467.1893 4620.9686 7352.8806 \n\n## calculate DIC\ndic1 = dic.samples(mod1, n.iter=1e3)\n\nLet’s look at the results.\n\nsummary(mod1_sim)\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean     SD Naive SE Time-series SE\nb[1]  1.7152 0.7790 0.006360       0.022276\nb[2] -0.1446 0.2875 0.002348       0.003120\nb[3] -0.2905 0.8612 0.007032       0.032268\nb[4] -0.7802 0.5209 0.004253       0.014612\nb[5] -0.6468 0.6256 0.005108       0.017049\nb[6]  1.6097 0.4990 0.004074       0.007142\nint  -0.1786 0.3096 0.002528       0.003542\n\n2. Quantiles for each variable:\n\n        2.5%     25%     50%      75%  97.5%\nb[1]  0.3724  1.1671  1.6583  2.19458 3.4378\nb[2] -0.7508 -0.3280 -0.1261  0.04550 0.3916\nb[3] -2.1719 -0.7578 -0.1949  0.20984 1.3341\nb[4] -1.8543 -1.1241 -0.7660 -0.41575 0.1720\nb[5] -2.0291 -1.0392 -0.5778 -0.18852 0.3953\nb[6]  0.7134  1.2605  1.5821  1.92323 2.6894\nint  -0.7691 -0.3883 -0.1815  0.02756 0.4411\n\n\n\n#par(mfrow=c(3,2))\npar(mar = c(2.5, 1, 2.5, 1))\n\ndensplot(mod1_csim[,1:6], xlim=c(-3.0, 3.0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolnames(X) # variable names\n\n[1] \"gravity\" \"ph\"      \"osmo\"    \"cond\"    \"urea\"    \"calc\"   \n\n\nIt is clear that the coefficients for variables gravity, cond (conductivity), and calc (calcium concentration) are not 0. The posterior distribution for the coefficient of osmo (osmolarity) looks like the prior, and is almost centered on 0 still, so we’ll conclude that osmo is not a strong predictor of calcium oxalate crystals. The same goes for ph.\nurea (urea concentration) appears to be a borderline case. However, if we refer back to our correlations among the variables, we see that urea is highly correlated with gravity, so we opt to remove it.\nOur second model looks like this:\n\nmod2_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dbern(p[i])\n        logit(p[i]) = int + b[1]*gravity[i] + b[2]*cond[i] + b[3]*calc[i]\n    }\n    int ~ dnorm(0.0, 1.0/25.0)\n    for (j in 1:3) {\n        b[j] ~ dnorm(0.0, 1.0/25.0) # noninformative for logistic regression\n    }\n} \"\n\nmod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)\n\nWarning in jags.model(textConnection(mod2_string), data = data_jags, n.chains =\n3): Unused variable \"ph\" in data\n\n\nWarning in jags.model(textConnection(mod2_string), data = data_jags, n.chains =\n3): Unused variable \"osmo\" in data\n\n\nWarning in jags.model(textConnection(mod2_string), data = data_jags, n.chains =\n3): Unused variable \"urea\" in data\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 77\n   Unobserved stochastic nodes: 4\n   Total graph size: 635\n\nInitializing model\n\n\n\nupdate(mod2, 1e3)\n\nmod2_sim = coda.samples(model=mod2,\n                        variable.names=params,\n                        n.iter=5e3)\nmod2_csim = as.mcmc(do.call(rbind, mod2_sim))\n\npar(mar = c(2.5, 1, 2.5, 1))\n#plot(mod2_sim, ask=TRUE)\nplot(mod2_sim)\n\n\n\n\n\n\n\ngelman.diag(mod2_sim)\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\nb[1]          1          1\nb[2]          1          1\nb[3]          1          1\nint           1          1\n\nMultivariate psrf\n\n1\n\nautocorr.diag(mod2_sim)\n\n              b[1]         b[2]          b[3]         int\nLag 0  1.000000000 1.0000000000  1.0000000000 1.000000000\nLag 1  0.583627533 0.6720443263  0.4990170960 0.284711480\nLag 5  0.114789188 0.1788950472  0.0546211037 0.002815859\nLag 10 0.010922205 0.0126509977 -0.0005439148 0.000864474\nLag 50 0.001713864 0.0006537317 -0.0011842400 0.004314294\n\nautocorr.plot(mod2_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffectiveSize(mod2_sim)\n\n    b[1]     b[2]     b[3]      int \n3518.752 2665.977 4500.056 8353.729 \n\ndic2 = dic.samples(mod2, n.iter=1e3)\n\n\n\n47.1.4 Results\n\ndic1\n\nMean deviance:  68.53 \npenalty 5.573 \nPenalized deviance: 74.1 \n\n\n\ndic2\n\nMean deviance:  71.23 \npenalty 4.095 \nPenalized deviance: 75.33 \n\n\n\nsummary(mod2_sim)\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean     SD Naive SE Time-series SE\nb[1]  1.3960 0.5052 0.004125       0.008532\nb[2] -1.3296 0.4619 0.003772       0.008955\nb[3]  1.8637 0.5514 0.004502       0.008238\nint  -0.1536 0.3217 0.002627       0.003523\n\n2. Quantiles for each variable:\n\n        2.5%     25%    50%      75%   97.5%\nb[1]  0.4794  1.0393  1.372  1.72259  2.4580\nb[2] -2.2829 -1.6280 -1.314 -1.00698 -0.4713\nb[3]  0.8744  1.4785  1.832  2.21421  3.0297\nint  -0.7783 -0.3713 -0.156  0.06063  0.4745\n\n\n\nHPDinterval(mod2_csim)\n\n          lower      upper\nb[1]  0.4008897  2.3673187\nb[2] -2.2333234 -0.4259612\nb[3]  0.7789707  2.9103415\nint  -0.7745084  0.4776574\nattr(,\"Probability\")\n[1] 0.95\n\n\n\n#par(mfrow=c(3,1))\npar(mar = c(2.5, 1, 2.5, 1))\ndensplot(mod2_csim[,1:3], xlim=c(-3.0, 3.0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncolnames(X)[c(1,4,6)] # variable names\n\n[1] \"gravity\" \"cond\"    \"calc\"   \n\n\n\nThe DIC is actually better for the first model. Note that we did change the prior between models, and generally we should not use the DIC to choose between priors. Hence comparing DIC between these two models may not be a fair comparison. Nevertheless, they both yield essentially the same conclusions. Higher values of gravity and calc (calcium concentration) are associated with higher probabilities of calcium oxalate crystals, while higher values of cond (conductivity) are associated with lower probabilities of calcium oxalate crystals.\nThere are more modeling options in this scenario, perhaps including transformations of variables, different priors, and interactions between the predictors, but we’ll leave it to you to see if you can improve the model.\n\n\n\n\n\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. 1980. Regression Diagnostics. John Wiley & Sons, Inc. https://doi.org/10.1002/0471725153.\n\n\nHärdle, Wolfgang Karl, and Léopold Simar. 2019. Applied Multivariate Statistical Analysis. Springer International Publishing. https://doi.org/10.1007/978-3-030-26006-4.\n\n\nJohnson, R. A., and D. W. Wichern. 2001. Applied Multivariate Statistical Analysis. Pearson Modern Classics for Advanced Statistics Series. Prentice Hall. https://books.google.co.il/books?id=QBqlswEACAAJ.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with r. Springer New York. https://doi.org/10.1007/978-0-387-09608-7.",
    "crumbs": [
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Logistic regression - M3L9</span>"
    ]
  },
  {
    "objectID": "C2-L09-Ex1.html",
    "href": "C2-L09-Ex1.html",
    "title": "48  Homework on Logistic Regression - M3L9HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Homework on Logistic Regression - M3L9HW1</span>"
    ]
  },
  {
    "objectID": "C2-L10.html",
    "href": "C2-L10.html",
    "title": "49  Poisson regression - M4L10",
    "section": "",
    "text": "49.1 Introduction to Poisson regression\nPoisson regression is the preferred method to handle count data where the response is positive values but includes zeroes. The gist of this method is that We use a log transform link function on the regressors but not on the response which allows the response to be zero valued which correspond to zero counts. One limit of this approach mentioned below is that the Poisson takes one parameter \\lambda for both its expected value and its variance. We look give a deeper solution to this problem in the next course on mixture models, however in this course we will consider more restricted cases where we extend the Poisson regression with the Negative Binomial Distribution which allows us to model over-dispersed data.\nWe now have experience fitting regression models when the response is continuous, and when it is binary. What about when we have count data? We could fit a linear normal regression, but here we have a couple of drawbacks. First of all, counts usually aren’t negative. And the variances might not be constant. The Poisson distribution provides a natural likelihood for count data.\ny_i\\mid \\lambda+i \\stackrel {iid} \\sim \\mathrm{Pois}(\\lambda_i) \\qquad i=1, \\ldots, n\nHere, \\lambda conveniently represents the expected value of y \\mathbb{E}[y]. It turns out that \\lambda is also the variance of y \\mathbb{V}ar[y]. So if we expect a count to be higher, we also expect the variability in counts to go up.\nWe saw this earlier with the warp breaks data.\nIf we model the mean directly, like we did with linear regression. That is, we had the expected value y_i was directly modeled with this linear form.\n\\mathbb{E}[y] = \\beta_0 + \\beta_1x_i \\qquad \\text{(linear regression)}\nWe would run into the same problem we did with logistic regression. The expected value has to be greater than zero in the Poisson distribution. To naturally deal with that restriction, we’re going to use the logarithmic link function.\nSo, the log link. That is, that the log of \\lambda_i is equal to this linear piece.\nlog link:\nlog(\\lambda_i) = \\beta_0+\\beta_1x_i \\qquad \\text{(log link)}\n\\tag{49.1}\n\\mathbb{E}[y]=\\beta_0+\\beta_1x_i \\qquad \\text{(linear regression)}\nFrom this, we can easily recover the expression for the mean itself. That is, we can invert this link function to get the expected value of y_i,\n\\implies \\mathbb{E}[y] = \\lambda_i = e^{\\left(\\beta_0+\\beta_1x_i \\right)}\n\\tag{49.2}\nIt might seem like this model is equivalent to fitting a normal linear regression to the log of y. But there are a few key differences. In the normal regression, we’re modeling the mean of the response directly. So we would be fitting a model to the \\log(y). Where we’re modeling the expected value of the \\log(y). This is different from what we’re modeling here, here we’re doing the log of the expected value of y.\n\\mathbb{E}[log(y)]\\ne log(\\mathbb{E}[y])\nThese are not equal, they’re usually similar, but they’re not the same. Another difference is that we have a separate independent parameter for the variants in a normal regression. In Poisson regression, the variance is automatically the same as \\lambda, which may not always be appropriate, as we’ll see in an upcoming example.\nAs usual, we can add more explanatory x variables to the Poisson regression framework. They can be continuous, categorical, or they could be counts themselves.\nIf we have three predictor variables x_i = ( x_{1,i}, x_{2,i}, x_{3,i} ), what would the likelihood part of the hierarchical representation of a Poisson regression with logarithmic link look like?\nHere we incorporated the (inverse) link function directly into the likelihood rather than writing it with two lines.",
    "crumbs": [
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Poisson regression - M4L10</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#sec-introduction-to-poisson-regression",
    "href": "C2-L10.html#sec-introduction-to-poisson-regression",
    "title": "49  Poisson regression - M4L10",
    "section": "",
    "text": "Figure 49.1: Introduction to Poisson regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny_i \\mid x_i, \\beta \\overset{\\text{ind}}{\\sim} \\mathrm{Pois} \\left( e^{-(\\beta_0+\\beta_1 x_{1,i}+\\beta_2 x_{2,i}+\\beta_3 x_{3,i})}\\right)\ny_i \\mid x_i, \\beta \\overset{\\text{ind}}{\\sim} \\mathrm{Pois} \\left(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} \\right)\ny_i \\mid x_i, \\beta \\overset{\\text{ind}}{\\sim} \\mathrm{Pois} \\left(e^{\\beta_0 + \\beta_1 x_{1,i}+\\beta_2 x_{2,i}+\\beta_3 x_{3,i}}\\right)\ny_i \\mid x_i, \\beta \\overset{\\text{ind}}{\\sim} \\mathrm{Pois} \\left( \\log[ \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} ] \\right)",
    "crumbs": [
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Poisson regression - M4L10</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#poisson-regression---jags-model",
    "href": "C2-L10.html#poisson-regression---jags-model",
    "title": "49  Poisson regression - M4L10",
    "section": "49.2 Poisson regression - JAGS model",
    "text": "49.2 Poisson regression - JAGS model\n  For an example of Poisson regression, we’ll use the badhealth data set from the COUNT package in R.doctor visits\n\nlibrary(\"COUNT\")\n\nLoading required package: msme\n\n\nLoading required package: MASS\n\n\nLoading required package: lattice\n\n\nLoading required package: sandwich\n\ndata(\"badhealth\")\n#?badhealth\nhead(badhealth)\n\n  numvisit badh age\n1       30    0  58\n2       20    0  54\n3       16    0  44\n4       20    0  57\n5       15    0  33\n6       15    0  28\n\n\naccording to the description:\n\n\n\n\n\n\nNoteData Card for badhealth\n\n\n\n1,127 observations from a 1998 German survey with 3 variables:\n\nnumvisit - number of visits to the doctor in 1998 (response)\nbadh - \\begin{cases} 1 \\qquad \\text{ patient claims to be in bad health} \\\\ 0 \\qquad \\text{ patient does not claim to be in bad health} \\end{cases}\nage - age of patient\n\n\n\n\nany(is.na(badhealth))\n\n[1] FALSE\n\n\n\nremove na\n\nAs usual, let’s visualize these data.\n\nhist(badhealth$numvisit, breaks=20)\n\n\n\n\n\n\n\nFigure 49.2: Histogram of number of doctor visits\n\n\n\n\n\n\nplot(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==0, xlab=\"age\", ylab=\"log(visits)\")\npoints(jitter(log(numvisit)) ~ jitter(age), data=badhealth, subset=badh==1, col=\"red\")\n\n\n\n\n\n\n\nFigure 49.3\n\n\n\n\n\n\n49.2.1 Doctor Visits Model\n It appears that both age and bad health are related to the number of doctor visits. We should include model terms for both variables. If we believe the age/visits relationship is different between healthy and non-healthy populations, we should also include an interaction term. We will fit the full model here and leave it to you to compare it with the simpler additive model.doctor visits\n\nlibrary(\"rjags\")\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.2\n\n\nLoaded modules: basemod,bugs\n\n\n\nmod_string = \" model {\n    for (i in 1:length(numvisit)) {\n        numvisit[i] ~ dpois(lam[i])\n        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]\n    }\n    \n    int ~ dnorm(0.0, 1.0/1e6)\n    b_badh ~ dnorm(0.0, 1.0/1e4)\n    b_age ~ dnorm(0.0, 1.0/1e4)\n    b_intx ~ dnorm(0.0, 1.0/1e4)\n} \"\n\nset.seed(102)\n\ndata_jags = as.list(badhealth)\n\nparams = c(\"int\", \"b_badh\", \"b_age\", \"b_intx\")\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1127\n   Unobserved stochastic nodes: 4\n   Total graph size: 3665\n\nInitializing model\n\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,  variable.names=params, n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim))\n\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim)\n\n\n\n\n\n\n\ngelman.diag(mod_sim)\n\nPotential scale reduction factors:\n\n       Point est. Upper C.I.\nb_age        1.03       1.08\nb_badh       1.02       1.04\nb_intx       1.02       1.05\nint          1.03       1.07\n\nMultivariate psrf\n\n1.02\n\nautocorr.diag(mod_sim)\n\n           b_age    b_badh    b_intx       int\nLag 0  1.0000000 1.0000000 1.0000000 1.0000000\nLag 1  0.9564612 0.9631774 0.9658324 0.9532463\nLag 5  0.8312494 0.8631160 0.8707547 0.8271818\nLag 10 0.7037841 0.7563816 0.7718513 0.6990018\nLag 50 0.1927075 0.3137387 0.3342178 0.1870366\n\nautocorr.plot(mod_csim)\n\n\n\n\n\n\n\neffectiveSize(mod_sim)\n\n   b_age   b_badh   b_intx      int \n248.6722 200.2180 190.8918 258.2672 \n\n## compute DIC\ndic = dic.samples(mod, n.iter=1e3)\n\n\n\n49.2.2 Model checking - Residuals\n\n“While inexact models may mislead, attempting to allow for every contingency a priori is impractical. Thus models must be built by an iterative feedback process in which an initial parsimonious model may be modified when diagnostic checks applied to residuals indicate the need.” —G. E. P. Box\n\nTo get a general idea of the model’s performance, we can look at predicted values and residuals as usual. Don’t forget that we must apply the inverse of the link function to get predictions for \\lambda .\n\n1X = as.matrix(badhealth[,-1])\n2X = cbind(X, with(badhealth, badh*age))\nhead(X)\n\n\n1\n\nwe drop the first column since it is the column for our y.\n\n2\n\nwe add a third column with \\mathbb{I}_{badh}\\times age\n\n\n\n\n     badh age  \n[1,]    0  58 0\n[2,]    0  54 0\n[3,]    0  44 0\n[4,]    0  57 0\n[5,]    0  33 0\n[6,]    0  28 0\n\n\n\n1(pmed_coef = apply(mod_csim, 2, median))\n\n\n1\n\nthis are the column medians of the coefficients.\n\n\n\n\n       b_age       b_badh       b_intx          int \n 0.008794878  1.596296699 -0.011603980  0.334580838 \n\n\n\n1llam_hat = pmed_coef[\"int\"] + X %*% pmed_coef[c(\"b_badh\", \"b_age\", \"b_intx\")]\n2lam_hat = exp(llam_hat)\n\nhist(lam_hat)\n\n\n1\n\nX \\cdot \\vec b_i gives the linear part.\n\n2\n\n\\hat\\lambda_i=e^{X \\cdot \\vec b_i} we need to apply the inverse link function\n\n\n\n\n\n\n\n\n\n\n\n\nresid = badhealth$numvisit - lam_hat\nplot(resid) # the data were ordered\n\n\n\n\n\n\n\n\nthis plot looks bad, it might not be iid but we can ignore the issue since the data is presorted/\n\nplot(lam_hat, badhealth$numvisit)\nabline(0.0, 1.0)\n\n\n\n\n\n\n\nFigure 49.4\n\n\n\n\n\n\nplot(lam_hat[which(badhealth$badh==0)], resid[which(badhealth$badh==0)], xlim=c(0, 8), ylab=\"residuals\", xlab=expression(hat(lambda)), ylim=range(resid))\npoints(lam_hat[which(badhealth$badh==1)], resid[which(badhealth$badh==1)], col=\"red\")\n\n\n\n\n\n\n\nFigure 49.5\n\n\n\n\n\nIt is not surprising that the variability increases for values predicted at higher values since the mean is also the variance in the Poisson distribution. However, observations predicted to have about two visits should have variance about two, and observations predicted to have about six visits should have variance about six.\n\nvar(resid[which(badhealth$badh==0)])\n\n[1] 7.022425\n\n\n\nvar(resid[which(badhealth$badh==1)])\n\n[1] 41.19663\n\n\nFor this data the variance is much bigger this is not the case with these data. This indicates that either the model fits poorly (meaning the covariates don’t explain enough of the variability in the data), or the data are “overdispersed” for the Poisson likelihood we have chosen. This is a common issue with count data. If the data are more variable than the Poisson likelihood would suggest, a good alternative is the negative binomial distribution, which we will not pursue here.",
    "crumbs": [
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Poisson regression - M4L10</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#sec-predictive-distributions",
    "href": "C2-L10.html#sec-predictive-distributions",
    "title": "49  Poisson regression - M4L10",
    "section": "49.3 Predictive distributions",
    "text": "49.3 Predictive distributions\nAssuming the model fit is adequate, we can interpret the results.\n\nsummary(mod_sim)\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean       SD  Naive SE Time-series SE\nb_age   0.008814 0.002062 1.684e-05      0.0001324\nb_badh  1.598876 0.176677 1.443e-03      0.0124820\nb_intx -0.011636 0.004110 3.356e-05      0.0002966\nint     0.333963 0.080252 6.553e-04      0.0050510\n\n2. Quantiles for each variable:\n\n            2.5%       25%       50%       75%    97.5%\nb_age   0.004826  0.007384  0.008795  0.010215  0.01281\nb_badh  1.248365  1.487161  1.596297  1.708719  1.95695\nb_intx -0.020084 -0.014161 -0.011604 -0.008997 -0.00353\nint     0.177768  0.280010  0.334581  0.389972  0.49017\n\n\nThe intercept is not necessarily interpretable here because it corresponds to the number of doctor visits for a healthy 0-year-old. While the number of visits for a newborn baby sounds like interesting information, the youngest person in the data set is 20 years old. In such cases we should avoid making such projections and say that the intercept is an artifact of the model.\n\nFor healthy individuals, it appears that age is associated with an increase in the Expected number of doctor visits.\nBad health is associated with an increase in expected number of visits.\nThe interaction coefficient is interpreted as an adjustment to the age coefficient for people in bad health. Hence, for people with bad health, age is essentially unassociated with number of visits.\n\n\n49.3.1 Predictive distributions\nLet’s say we have two people aged 35, one in good health and the other in poor health. Q. What is the posterior probability that the individual with poor health will have more doctor visits?\nThis goes beyond the posterior probabilities we have calculated comparing expected responses in previous lessons. Here we will create Monte Carlo samples for the responses themselves. This is done by taking the Monte Carlo samples of the model parameters, and for each of those, drawing a sample from the likelihood.\nLet’s walk through this.\nFirst, we need the x values for each individual. We’ll say the healthy one is Person 1 and the unhealthy one is Person 2. Their x values are:\n\n1x1 = c(0, 35, 0)\n2x2 = c(1, 35, 35)\n\n\n1\n\ngood health person’s data (bad_health_indicator=0,age=35,age*indicator=0)\n\n2\n\nbad health person’s (bad_health_indicator=1,age=35,age*indicator=35)\n\n\n\n\nThe posterior samples of the model parameters are stored in mod_csim:\n\nhead(mod_csim)\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 1 \nEnd = 7 \nThinning interval = 1 \n           b_age   b_badh      b_intx       int\n[1,] 0.009509357 1.560405 -0.01085825 0.2917920\n[2,] 0.009194738 1.557807 -0.01069928 0.3140711\n[3,] 0.009081102 1.516541 -0.01193797 0.3641633\n[4,] 0.009012898 1.670983 -0.01124829 0.3413271\n[5,] 0.008630395 1.659709 -0.01136070 0.3388258\n[6,] 0.009289037 1.638160 -0.01135475 0.3290226\n[7,] 0.009283968 1.619713 -0.01090726 0.3203386\n\n\nFirst, we’ll compute the linear part of the predictor:\n\nloglam1 = mod_csim[,\"int\"] + mod_csim[,c(2,1,3)] %*% x1\nloglam2 = mod_csim[,\"int\"] + mod_csim[,c(2,1,3)] %*% x2\n\nNext we’ll apply the inverse link:\n\nlam1 = exp(loglam1)\nlam2 = exp(loglam2)\n\nThe final step is to use these samples for the \\lambda parameter for each individual and simulate actual number of doctor visits using the likelihood:\n\n(n_sim = length(lam1))\n\n[1] 15000\n\n\nwe have distribution of 15000 samples of \\lambda for each person.\n\nplot(table(factor(y1, levels=0:18))/n_sim, pch=2, ylab=\"posterior prob.\", xlab=\"visits\")\npoints(table(y2+0.1)/n_sim, col=\"red\")\n\n\n\n\n\n\n\nFigure 49.6\n\n\n\n\n\n\ny1 = rpois(n=n_sim, lambda=lam1)\ny2 = rpois(n=n_sim, lambda=lam2)\n\nplot(table(factor(y1, levels=0:18))/n_sim, pch=2, ylab=\"posterior prob.\", xlab=\"visits\")\npoints(table(y2+0.1)/n_sim, col=\"red\")\n\n\n\n\n\n\n\n\nFinally, we can answer the original question: What is the probability that the person with poor health will have more doctor visits than the person with good health?\n\nmean(y2 &gt; y1)\n\n[1] 0.9212\n\n\nBecause we used our posterior samples for the model parameters in our simulation (the loglam1 and loglam2 step above), this posterior predictive distribution on the number of visits for these two new individuals naturally account for our uncertainty in the model estimates. This is a more honest/realistic distribution than we would get if we had fixed the model parameters at their MLE or posterior means and simulated data for the new individuals.",
    "crumbs": [
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Poisson regression - M4L10</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#sec-prior-sensitivity-analysis",
    "href": "C2-L10.html#sec-prior-sensitivity-analysis",
    "title": "49  Poisson regression - M4L10",
    "section": "49.4 Prior sensitivity analysis",
    "text": "49.4 Prior sensitivity analysis\n  When communicating results from any analysis, a responsible statistician will report and justify modeling decisions, especially assumptions. In a Bayesian analysis, there is an additional assumption that is open to scrutiny: the choices of prior distributions. In the models considered so far in this course, there are an infinite number of prior distributions we could have chosen from. When communicating results from any analysis, a responsible statistician will report and justify modeling decisions, especially assumptions. In a Bayesian analysis, there is another assumption that is open to scrutiny: the choices of prior distributions. In the models considered so far in this course, there are an infinite number of prior distributions we could have chosen from.Q. How do you justify the model you choose?\n If they truly represent your beliefs about the parameters before analysis and the model is appropriate, then the posterior distribution truly represents your updated beliefs. If you don’t have any strong beliefs beforehand, there are often default, reference, or non-informative prior options, and you will have to select one. However, a collaborator or a boss (indeed, somebody somewhere) may not agree with your choice of prior. One way to increase the credibility of your results is to repeat the analysis under a variety of priors, and report how the results differ as a result. This process is called prior sensitivity analysis. Q. How do you justify the priors you choose?\nAt a minimum you should always report your choice of model and prior. If you include a sensitivity analysis, select one or more alternative priors and describe how the results of the analysis change. If they are sensitive to the choice of prior, you will likely have to explain both sets of results, or at least explain why you favor one prior over another. If the results are not sensitive to the choice of prior, this is evidence that the data are strongly driving the results. It suggests that different investigators coming from different backgrounds should come to the same conclusions.\nIf the purpose of your analysis is to establish a hypothesis, it is often prudent to include a “skeptical” prior which does not favor the hypothesis. Then, if the posterior distribution still favors the hypothesis despite the unfavorable prior, you will be able to say that the data substantially favor the hypothesis. This is the approach we will take in the following example, continued from the previous lesson.\n\n49.4.1 Poisson regression example\n Let’s return to the example of number of doctor visits. We concluded from our previous analysis of these data that both bad health and increased age are associated with more visits. Suppose the burden of proof that bad health is actually associated with more visits rests with us, and we need to convince a skeptic.doctor visits\nFirst, let’s re-run the original analysis and remind ourselves of the posterior distribution for the badh (bad health) indicator.\n\nlibrary(\"COUNT\")\nlibrary(\"rjags\")\n\ndata(\"badhealth\")\n\n\nmod_string = \" model {\n    for (i in 1:length(numvisit)) {\n        numvisit[i] ~ dpois(lam[i])\n        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]\n    }\n    \n    int ~ dnorm(0.0, 1.0/1e6)\n    b_badh ~ dnorm(0.0, 1.0/1e4)\n    b_age ~ dnorm(0.0, 1.0/1e4)\n    b_intx ~ dnorm(0.0, 1.0/1e4)\n} \"\n\nset.seed(102)\n\ndata_jags = as.list(badhealth)\n\nparams = c(\"int\", \"b_badh\", \"b_age\", \"b_intx\")\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1127\n   Unobserved stochastic nodes: 4\n   Total graph size: 3665\n\nInitializing model\n\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,\n                        variable.names=params,\n                        n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim))\n\n\nplot(density(mod_csim[,\"b_badh\"]))\n\n\n\n\n\n\n\nFigure 49.7\n\n\n\n\n\nEssentially all of the posterior probability mass is above 0, suggesting that this coefficient is positive (and consequently that bad health is associated with more visits). We obtained this result using a relatively noninformative prior. What if we use a prior that strongly favors values near 0? Let’s repeat the analysis with a normal prior on the badh coefficient that has mean 0 and standard deviation 0.2, so that the prior probability that the coefficient is less than 0.6 is &gt;0.998 . We’ll also use a small variance on the prior for the interaction term involving badh (standard deviation 0.01 because this coefficient is on a much smaller scale).\n\nmod2_string = \" model {\n    for (i in 1:length(numvisit)) {\n        numvisit[i] ~ dpois(lam[i])\n        log(lam[i]) = int + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]\n    }\n    \n    int ~ dnorm(0.0, 1.0/1e6)\n    b_badh ~ dnorm(0.0, 1.0/0.2^2)\n    b_age ~ dnorm(0.0, 1.0/1e4)\n    b_intx ~ dnorm(0.0, 1.0/0.01^2)\n} \"\n\nmod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1127\n   Unobserved stochastic nodes: 4\n   Total graph size: 3672\n\nInitializing model\n\nupdate(mod2, 1e3)\n\nmod2_sim = coda.samples(model=mod2,\n                        variable.names=params,\n                        n.iter=5e3)\nmod2_csim = as.mcmc(do.call(rbind, mod2_sim))\n\nHow did the posterior distribution for the coefficient of badh change?\n\ncurve(dnorm(x, mean=0.0, sd=sqrt(1e4)), from=-3.0, to=3.0, ylim=c(0.0, 3.0), lty=2,\n      main=\"b_badh\", ylab=\"density\", xlab=\"b_badh\")\ncurve(dnorm(x, mean=0.0, sd=0.2), from=-3.0, to=3.0, col=\"red\", lty=2, add=TRUE)\nlines(density(mod_csim[,\"b_badh\"]))\nlines(density(mod2_csim[,\"b_badh\"]), col=\"red\")\nlegend(\"topleft\", legend=c(\"noninformative prior\", \"posterior\", \"skeptical prior\", \"posterior\"),\n       lty=c(2,1,2,1), col=rep(c(\"black\", \"red\"), each=2), bty=\"n\")\n\n\n\n\n\n\n\nFigure 49.8\n\n\n\n\n\nUnder the skeptical prior, our posterior distribution for b_badh has significantly dropped to between about 0.6 and 1.1. Although the strong prior influenced our inference on the magnitude of the bad health effect on visits, it did not change the fact that the coefficient is significantly above 0. In other words: even under the skeptical prior, bad health is associated with more visits, with posterior probability near 1.\nWe should also check the effect of our skeptical prior on the interaction term involving both age and health.\n\ncurve(dnorm(x, mean=0.0, sd=sqrt(1e4)), from=-0.05, to=0.05, ylim=c(0.0, 140.0), lty=2,\n      main=\"b_intx\", ylab=\"density\", xlab=\"b_intx\")\ncurve(dnorm(x, mean=0.0, sd=0.01), from=-0.05, to=0.05, col=\"red\", lty=2, add=TRUE)\nlines(density(mod_csim[,\"b_intx\"]))\nlines(density(mod2_csim[,\"b_intx\"]), col=\"red\")\nlegend(\"topleft\", legend=c(\"noninformative prior\", \"posterior\", \"skeptical prior\", \"posterior\"),\n       lty=c(2,1,2,1), col=rep(c(\"black\", \"red\"), each=2), bty=\"n\")\n\n\n\n\n\n\n\nFigure 49.9\n\n\n\n\n\n\nmean(mod2_csim[,\"b_intx\"] &gt; 0) # posterior probability that b_intx is positive\n\n[1] 0.9412667\n\n\nThe result here is interesting. Our estimate for the interaction coefficient has gone from negative under the non-informative prior to positive under the skeptical prior, so the result is sensitive. In this case, because the skeptical prior shrinks away much of the bad health main effect, it is likely that this interaction effect attempts to restore some of the positive effect of bad health on visits. Thus, despite some observed prior sensitivity, our conclusion that bad health positively associates with more visits remains unchanged.",
    "crumbs": [
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Poisson regression - M4L10</span>"
    ]
  },
  {
    "objectID": "C2-L10.html#overdispersed-model",
    "href": "C2-L10.html#overdispersed-model",
    "title": "49  Poisson regression - M4L10",
    "section": "49.5 Overdispersed model",
    "text": "49.5 Overdispersed model\nRecall that the Negative Binomial can be used to model overdispersed count data.\nstan has three parameterizations for the Negative Binomial.\nThe first looks like similar to a binomial parameterization:\n\n\\text{NegBinomial}(y~|~\\alpha,\\beta)  = \\binom{y +\n\\alpha - 1}{\\alpha - 1} \\, \\left( \\frac{\\beta}{\\beta+1}\n\\right)^{\\!\\alpha} \\, \\left( \\frac{1}{\\beta + 1} \\right)^{\\!y} \\!.\n\n\n\\mathbb{E}[y] = \\frac{\\alpha}{\\beta} \\ \\ \\text{ and } \\ \\ \\text{Var}[Y] = \\frac{\\alpha}{\\beta^2} (\\beta + 1).\n\nwe can sample from this using the following statement\nn ~ neg_binomial(alpha, beta)\nBut this parameterization if not a match to the Poisson model, so we move on\nThe second parametrization \\mu \\in \\mathbb{R}^+ and \\phi \\in \\mathbb{R}^+:\n\n\\mathrm{NegBinomial2}(n \\mid \\mu, \\phi)  = \\binom{n + \\phi - 1}{n} \\,\n\\left( \\frac{\\mu}{\\mu+\\phi} \\right)^{\\!n} \\, \\left(\n\\frac{\\phi}{\\mu+\\phi} \\right)^{\\!\\phi}\n\n\n\\mathbb{E}[n] = \\mu \\ \\ \\text{ and } \\ \\ \\ \\mathbb{V}\\text{ar}[n] = \\mu + \\frac{\\mu^2}{\\phi}\n\nwe can sample from this using the following statement\nn ~ neg_binomial_2(mu, phi)\nAnd there is a third parametrization\n\nNegBinomial2Log(y\\mid\\mu,\\phi) = NegBinomial2(y\\mid exp(\\eta),\\phi).\n\nwe can sample from this using the following statement:\ny \\~ **neg_binomial_2\\_log**(y\\|mu, phi)`\njags has just one parameterization:\n\nf(y \\mid r, p) = \\frac{\\Gamma(y+r)}{\\Gamma(r)\\Gamma(y+1)}p^r(1-p)^y\n\nWe think of the Negative Binomial Distribution as the probability of completing y successful trials allowing for r failures in a sequence of (y+r) Bernoulli trials where success is defined as drawing (with replacement) a white ball from an urn of white and black balls with a probability p of success.\n\n\\mathbb{E}[Y] = \\mu = { r(1-p) \\over p } \\qquad \\text{ and } \\qquad \\mathbb{V}\\text{ar}[Y] = \\mu + \\frac{\\mu^2}{r}\n\n\n49.5.1 Transformations:\nSince we want to have a model corresponding to a poisson regression we will transform the model as follows:\nIf we set p = {\\text{r} \\over {r} + \\lambda } then the mean becomes : \\lambda\nand if we also set r= {\\lambda^2 \\over \\omega} then the variance becomes a sum of \\lambda + \\omega where \\omega is our over dispersion term.\n\n\\omega = \\lambda^2 / r\n\n\n\\begin{aligned}\n\\mathbb{E}[Y] &= { r(1-p) \\over p }\n\\\\ &= rp^{-1} -r\n\\\\ & \\stackrel {sub\\ p} =  {\\cancel{r}(\\bcancel{r}+\\lambda) \\over \\cancel{r}} - \\bcancel{r}\n\\\\ &= \\lambda \\mathbb{V}\\text{ar}[Y]\n\\\\ &= { (1-p) r \\over p^2 }\n\\\\ & \\stackrel {sub \\ \\lambda } = {1 \\over p} \\lambda\n\\\\ & \\stackrel {sub p} = \\lambda { (r+ \\lambda) \\over r}\n\\\\ &=  {\\lambda r +  \\lambda^2 \\over r }\n\\\\ &= 1 \\lambda + {\\lambda^2 \\over r}\n\\\\ &\\stackrel { sub \\ \\omega}= \\lambda + \\omega\n\\end{aligned}\n\nWhere we interpret \\lambda as the mean and \\omega as the overdispersion \n\nlibrary(\"rjags\")\nlibrary(\"COUNT\")\ndata(\"badhealth\")\n\n\nmod3_string = \"\nmodel {\n    for (i in 1:length(numvisit)) { \n1        mu[i]       = b0 + b_badh*badh[i] + b_age*age[i] + b_intx*age[i]*badh[i]\n2        lambda[i]   = exp(mu[i])\n3        p[i]        = r / (r + lambda[i])\n4        numvisit[i] ~ dnegbin(p[i], r)\n5        resid[i]       = numvisit[i] - p[i]\n    }\n    ## Priors\n6    b0        ~ dnorm(0.0, 1.0/1e6)\n7    b_badh    ~ dnorm(0.0, 1.0/0.2^2)\n8    b_age     ~ dnorm(0.0, 1.0/1e4)\n9    b_intx    ~ dnorm(0.0, 1.0/0.01^2)\n10    r ~ dunif(0,50)\n\n    ## extra deterministic parameters\n    omega      &lt;-  pow(mean(lambda),2)/2\n11    #theta      &lt;- pow(1/mean(p),2)\n12    #scale      &lt;- mean((1-p)/p)\n}\"\ndata3_jags = as.list(badhealth)\nmod3 = jags.model(textConnection(mod3_string), data=data3_jags, n.chains=3)\n13update(mod3, 1e3)\nparams3 = c(\"b_intx\", \"b_badh\", \"b_age\", 'over_disp', 'b0','omega','r')\n14mod3_sim = coda.samples(model=mod3,  variable.names=params3, n.iter=5e3)\n15mod3_csim = as.mcmc(do.call(rbind, mod3_sim))\n16(dic3 = dic.samples(mod3, n.iter=1e3))\n\n\n1\n\nthe linear part\n\n2\n\nlambda corresponds to the parameter used in the Poisson regression\n\n3\n\np is the success parameter\n\n4\n\nwe draw from the negative binomial distribution\n\n5\n\nsampling using the parametrization of the Negative Binomial distribution.\n\n6\n\nnormal prior for intercept b0\n\n7\n\nnormal prior for b_badh\n\n8\n\nnormal prior for b_age\n\n9\n\nnormal prior for b_intx\n\n10\n\nuniform prior for over_disp - at the upper limit of 50 NegBin converges to Poisson see (Jackman 2009, 280)\n\n11\n\ntheta param\n\n12\n\nscale param\n\n13\n\nburn in\n\n14\n\nsample\n\n15\n\nstack samples from the chains\n\n16\n\nestimate the DIC\n\n\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1127\n   Unobserved stochastic nodes: 5\n   Total graph size: 4204\n\nInitializing model\n\nMean deviance:  4478 \npenalty 4.3 \nPenalized deviance: 4482 \n\n\n\ngelman.diag(mod3_sim )\n\nPotential scale reduction factors:\n\n       Point est. Upper C.I.\nb0              1       1.01\nb_age           1       1.01\nb_badh          1       1.01\nb_intx          1       1.01\nomega           1       1.00\nr               1       1.00\n\nMultivariate psrf\n\n1\n\n\n\nraftery.diag(mod3_sim)\n\n[[1]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                              \n        Burn-in  Total Lower bound  Dependence\n        (M)      (N)   (Nmin)       factor (I)\n b0     32       31336 3746          8.37     \n b_age  58       63860 3746         17.00     \n b_badh 14       15154 3746          4.05     \n b_intx 18       20326 3746          5.43     \n omega  2        3995  3746          1.07     \n r      5        5706  3746          1.52     \n\n\n[[2]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                              \n        Burn-in  Total Lower bound  Dependence\n        (M)      (N)   (Nmin)       factor (I)\n b0     28       31676 3746         8.46      \n b_age  22       22376 3746         5.97      \n b_badh 12       13434 3746         3.59      \n b_intx 14       15652 3746         4.18      \n omega  2        3995  3746         1.07      \n r      5        5673  3746         1.51      \n\n\n[[3]]\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                              \n        Burn-in  Total Lower bound  Dependence\n        (M)      (N)   (Nmin)       factor (I)\n b0     24       24634 3746          6.58     \n b_age  39       43275 3746         11.60     \n b_badh 14       16292 3746          4.35     \n b_intx 14       15032 3746          4.01     \n omega  2        3930  3746          1.05     \n r      8        9322  3746          2.49     \n\n\n\nautocorr.diag(mod3_sim)\n\n              b0      b_age      b_badh      b_intx        omega            r\nLag 0  1.0000000 1.00000000 1.000000000 1.000000000  1.000000000  1.000000000\nLag 1  0.9453239 0.94607144 0.783237944 0.802659620  0.009311632  0.251902545\nLag 5  0.7907163 0.79019866 0.380843053 0.407553049 -0.006139507 -0.003215143\nLag 10 0.6363282 0.63518322 0.144426475 0.174376652 -0.002158889  0.007168229\nLag 50 0.0734395 0.07466777 0.003115752 0.002181953 -0.005465450  0.005356281\n\n\n\neffectiveSize(mod3_sim)\n\n        b0      b_age     b_badh     b_intx      omega          r \n  343.5346   343.6880  1452.7633  1352.2856 15421.8348  8614.1842 \n\n\n\nsummary(mod3_sim)\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean       SD  Naive SE Time-series SE\nb0     0.457057 0.134308 1.097e-03      0.0072396\nb_age  0.005909 0.003499 2.857e-05      0.0001888\nb_badh 0.381223 0.167170 1.365e-03      0.0043832\nb_intx 0.014817 0.004363 3.562e-05      0.0001187\nomega  2.777842 0.217846 1.779e-03      0.0017554\nr      0.989415 0.068349 5.581e-04      0.0007373\n\n2. Quantiles for each variable:\n\n            2.5%      25%      50%      75%   97.5%\nb0      0.193004 0.366347 0.459626 0.545461 0.72977\nb_age  -0.001073 0.003598 0.005859 0.008229 0.01273\nb_badh  0.050879 0.269449 0.379715 0.494252 0.70780\nb_intx  0.006219 0.011816 0.014789 0.017767 0.02344\nomega   2.381000 2.628263 2.765854 2.917303 3.22985\nr       0.861961 0.942401 0.987017 1.033107 1.13342\n\n\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod3_sim, auto.layout = FALSE)\n\n\n\n\n\n\n\n\n\nFigure 49.10\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.11\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.12\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.13\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.14\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.15\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.16\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.17\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.18\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.19\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.20\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.21\n\n\n\n\n\n\nautocorr.plot(mod3_csim,auto.layout = FALSE)\n\n\n\n\n\n\n\n\n\nFigure 49.22\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.23\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.24\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.25\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.26\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 49.27\n\n\n\n\n\n\n\nX = as.matrix(badhealth[,-1])\nX = cbind(X, with(badhealth, badh*age))\n(pmed_coef = apply(mod3_csim, 2, median))\n\n         b0       b_age      b_badh      b_intx       omega           r \n0.459626050 0.005859471 0.379714721 0.014789223 2.765854477 0.987016826 \n\n(r = pmed_coef[\"r\"] )\n\n        r \n0.9870168 \n\nmu_hat = pmed_coef[\"b0\"] + X %*% pmed_coef[c(\"b_badh\", \"b_age\", \"b_intx\")]\nlambda_hat = exp(mu_hat)\np_hat = r / (r + lambda_hat)\nhist(lambda_hat)\n\n\n\n\n\n\n\nhist(p_hat)\n\n\n\n\n\n\n\n\nresiduals\n\nresid = badhealth$numvisit - p_hat\nhead(resid)\n\n         [,1]\n[1,] 29.69265\n[2,] 19.68764\n[3,] 15.67492\n[4,] 19.69140\n[5,] 14.66062\n[6,] 14.65402\n\n\n\nplot(resid) # the data were ordered\n\n\n\n\n\n\n\nFigure 49.28: Plot of residuals\n\n\n\n\n\n\n\n\nTable 49.1: First few rows of mod3_csim\n\n\nhead(mod3_csim)\n\nMarkov Chain Monte Carlo (MCMC) output:\nStart = 1 \nEnd = 7 \nThinning interval = 1 \n            b0       b_age     b_badh     b_intx    omega         r\n[1,] 0.3665807 0.008320894 0.32089335 0.01868060 2.962679 0.9841853\n[2,] 0.3715623 0.008201852 0.26417861 0.01884734 2.887049 0.9186491\n[3,] 0.3510155 0.007559843 0.20241069 0.01843655 2.526543 0.9262750\n[4,] 0.4163905 0.007997608 0.15680506 0.01862688 2.927903 0.9961478\n[5,] 0.3957466 0.007218708 0.13657620 0.02165356 2.806187 0.8291313\n[6,] 0.4376488 0.005706062 0.14507976 0.02173840 2.723978 0.8770457\n[7,] 0.4833344 0.004627869 0.06040445 0.02112806 2.590341 1.1056166\n\n\n\n\n\nplot(p_hat, badhealth$numvisit)\nabline(0.0, 1.0)\n\n\n\n\n\n\n\nFigure 49.29: Plot of p_hat vs numvisit\n\n\n\n\n\n\nplot(p_hat[which(badhealth$badh==0)], resid[which(badhealth$badh==0)], xlim=range(p_hat), ylab=\"residuals\", xlab=expression(hat(p)), ylim=range(resid))\npoints(p_hat[which(badhealth$badh==1)], resid[which(badhealth$badh==1)], col=\"red\")\n\n\n\n\n\n\n\nFigure 49.30: Plot of p_hat vs residuals, colored by health status\n\n\n\n\n\n\nvar(resid[which(badhealth$badh==0)])\n\n[1] 7.061386\n\nvar(resid[which(badhealth$badh==1)])\n\n[1] 41.21239\n\n\n\n\n\n\n\n\nJackman, Simon. 2009. “Bayesian Analysis for the Social Sciences.” Wiley Series in Probability and Statistics, October. https://doi.org/10.1002/9780470686621.",
    "crumbs": [
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Poisson regression - M4L10</span>"
    ]
  },
  {
    "objectID": "C2-L10-Ex1.html",
    "href": "C2-L10-Ex1.html",
    "title": "50  Homework on Poisson regression - M4L10HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Homework on Poisson regression - M4L10HW1</span>"
    ]
  },
  {
    "objectID": "C2-L11.html",
    "href": "C2-L11.html",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "",
    "text": "51.1 Introduction to Hierarchical Modeling\nThroughout the last few lessons, we have assumed that all the observations were independent. But more often than not, this is too strong an assumption to make about our data. There is often a natural grouping to our data points, which leads us to believe that some observation pairs should be more similar to each other than to others.\nLet’s look at an example. In the previous course, we talked about using a Poisson model to count chocolate chips in cookies.\nLet’s suppose that you own a company that produces chocolate chip cookies. In an experiment, you’re going to produce 150 test cookies. Let’s say you’ll do 30 from your location and then 30 from four other factory locations. So let’s write this as 30 from each of 5 locations. We’re going to assume that all the locations use the same recipe when they make their chocolate chip cookies.\nI’d say probably.\nThere’s a natural grouping to the cookies. By making it a hierarchical model, we can account for the likely differences between locations in our Poisson model.\nThe original fully independent model for model for Poisson, the number of chips in the cookies would have looked like this.\nN=150 \\quad \\text{number of cookies}\nB=30 \\quad \\text{from each locations}",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#introduction-to-hierarchical-modeling",
    "href": "C2-L11.html#introduction-to-hierarchical-modeling",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "",
    "text": "Figure 51.1: Introduction to Hierarchical modeling\n\n\n\n\n\n\n\n\nTipWhat about correlated data?\n\n\n\nWhile the section shows the presence of correlation within groups, I wonder if we really need the iid assumptions below, particularly as we are told in (McElreath 2015) that hierarchical models can handle correlated data.\n\n\n\n\n\nShould we expect a cookie from your location, batch one, to be more similar to another cookie from that batch than to a cookie from another location’s batch?\n\n\n\n\n\n\n\n51.1.1 Fully independent model\n Fitting a single model to all data: This approach treats all observations as independent, ignoring any potential differences or relationships between groups. In the cookie example, fitting a single Poisson model to all 150 cookies would “be ignoring potential differences between locations and the fact that cookies from the same location are likely to be more similar to each other.”\nThis kind of model is called fully independent model or an completely pooled model as we consider all the cookies belonging to a single group\n\ny_i \\mid \\lambda \\stackrel{iid}{\\sim} \\mathrm{Pois}(\\lambda) \\qquad i=1, \\ldots, N \\qquad\n\\tag{51.1}\nWhere \\lambda is the expected number of chips per cookie.\n\nAt the other extreme we can think of fitting a model for each group separately. This kind of model is called a unpooled model.\n\n\n51.1.2 Location-dependent model\n Hierarchical models offer a powerful solution, acting as a “good compromise” between the two extreme approaches described above. They allow for the acknowledgment of natural groupings in data while still leveraging information across those groups. This is also called a partially pooled model, as it allows for some sharing of information between groups while still allowing for group-specific parameters.\nIn (Gelman et al. 2013, secs. 4.5,5.5,7.3) the authors describe the trade-offs between these types of options and how hierarchical models provide a good compromise. Hierarchical models, can incorporate a sufficient number of parameters to fit data well, while simultaneously using a population distribution to introduce dependence among parameters, thereby avoiding overfitting. This approach allows the parameters of a prior, or population, distribution to be estimated from the data itself\nAlso In (McElreath 2015), the author describes how shrinkage estimators can be used in hierarchical models to improve parameter estimates. But to keep things simple, we can say that shrinkage is how parameters for groups with less data are “pulled” towards the overall mean, while groups with more data are less affected by this pull.\n\n\\begin{aligned}\ny_i \\mid l_i, \\lambda_{l_i} & \\stackrel{iid}{\\sim} \\mathrm{Pois}(\\lambda_{l_i}) & i=1, \\ldots, N \\\\\n\\lambda_{l_i} |\\alpha, \\beta & \\sim \\mathrm{Gamma}(\\alpha, \\beta) & l_i = 1, \\ldots, B \\\\\n\\alpha &\\sim \\mathrm{Prior}(\\alpha) & \\text {(hyperparameters)} \\\\\n\\beta &\\sim \\mathrm{Prior}(\\beta) & \\text {(hyperparameters)}\n\\end{aligned}\n\\tag{51.2}\n\n\n51.1.3 Graphical Model\nThe structure of a hierarchical model can be visualized as follows:\n\nTop Level (Hyperparameters): Independent parameters like \\alpha and \\beta.\nSecond Level (Group Parameters): Parameters for each group (e.g., \\lambda_1, \\lambda_2, \\dots, \\lambda_5) that are dependent on the hyperparameters.\nLowest Level (Observations): Individual observations (e.g., cookies) are grouped by their respective locations, and their distributions depend on the specific group parameter.\n\nThis multi-level structure allows for the estimation of the hyperparameters from the group-specific parameters.\n\nimport daft\nimport matplotlib.pyplot as plt\nimport warnings\nimport logging\n\n# Suppress font warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib.font_manager\")\n\n# Also lower logging level for matplotlib font manager\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\npgm = daft.PGM([6.5, 4.5], node_unit=1.2)\n\n# Hyperparameters\npgm.add_node(daft.Node(\"alpha\", r\"$\\alpha$\", 2, 4));\npgm.add_node(daft.Node(\"beta\", r\"$\\beta$\", 4, 4));\n\npgm.add_node(daft.Node(\"lambda_1\", r\"$\\lambda_1$\", 1, 3));\npgm.add_node(daft.Node(\"lambda_2\", r\"$\\lambda_2$\", 2, 3));\npgm.add_node(daft.Node(\"lambda_3\", r\"$\\lambda_3$\", 3, 3));\npgm.add_node(daft.Node(\"lambda_4\", r\"$\\lambda_4$\", 4, 3));\npgm.add_node(daft.Node(\"lambda_5\", r\"$\\lambda_5$\", 5, 3));\n\n# Observed data y_i, grouped under lambda_l\n\npgm.add_node(daft.Node(\"y_1\", r\"$y_i$\", 1, 2, observed=True));\npgm.add_node(daft.Node(\"y_2\", r\"$y_i$\", 2, 2, observed=True));\npgm.add_node(daft.Node(\"y_3\", r\"$y_i$\", 3, 2, observed=True));\npgm.add_node(daft.Node(\"y_4\", r\"$y_i$\", 4, 2, observed=True));\npgm.add_node(daft.Node(\"y_5\", r\"$y_i$\", 5, 2, observed=True));\n\n# Edges\npgm.add_edge(\"alpha\", \"lambda_1\");\npgm.add_edge(\"alpha\", \"lambda_2\");\npgm.add_edge(\"alpha\", \"lambda_3\");\npgm.add_edge(\"alpha\", \"lambda_4\");\npgm.add_edge(\"alpha\", \"lambda_5\");\n\npgm.add_edge(\"beta\", \"lambda_1\");\npgm.add_edge(\"beta\", \"lambda_2\");\npgm.add_edge(\"beta\", \"lambda_3\");\npgm.add_edge(\"beta\", \"lambda_4\");\npgm.add_edge(\"beta\", \"lambda_5\");\n\npgm.add_edge(\"lambda_1\", \"y_1\");\npgm.add_edge(\"lambda_2\", \"y_2\");\npgm.add_edge(\"lambda_3\", \"y_3\");\npgm.add_edge(\"lambda_4\", \"y_4\");\npgm.add_edge(\"lambda_5\", \"y_5\");\n\n# Plates\npgm.add_plate(daft.Plate([0.6, 1.5, 0.8, 1], label=r\"$i=l_i=1$\", shift=-0.1));\npgm.add_plate(daft.Plate([1.6, 1.5, 0.8, 1], label=r\"$i=l_i=2$\", shift=-0.1));\npgm.add_plate(daft.Plate([2.6, 1.5, 0.8, 1], label=r\"$i=l_i=3$\", shift=-0.1));\npgm.add_plate(daft.Plate([3.6, 1.5, 0.8, 1], label=r\"$i=l_i=4$\", shift=-0.1));\npgm.add_plate(daft.Plate([4.6, 1.5, 0.8, 1], label=r\"$i=l_i=5$\", shift=-0.1));\n\n\npgm.render()\nplt.show()\n\n\n\n\n\n\n\nFigure 51.2: Hierarchical model for chocolate chip cookies\n\n\n\n\n\nA primary advantage of hierarchical models is their ability to share information, or borrow strength, from all the data.\n\nIndirect Information Sharing: Even though groups might have their own specific parameters (e.g., your location’s \\lambda_1), the common distribution from which these parameters are drawn means that “information about another location’s cookies might provide information about your cookies, at least indirectly.”\nImproved Parameter Estimation: This shared information leads to more robust and accurate parameter estimates. In the cookie example, “your lambda is not only estimated directly from your 30 cookies, but also indirectly from the other 120 cookies leveraging this hierarchical structure.”\n\nThe overarching benefit of hierarchical models is their capacity for “Being able to account for relationships in the data while estimating everything with the single model is a primary advantage of using hierarchical models.” This provides a flexible and powerful framework for analyzing data with inherent group structures, leading to more realistic and informative inferences.",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#correlations-within-the-normal-hierarchical-model",
    "href": "C2-L11.html#correlations-within-the-normal-hierarchical-model",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "51.2 Correlations within the Normal hierarchical model 📖",
    "text": "51.2 Correlations within the Normal hierarchical model 📖\nThis section is based on the handout titled Correlation from hierarchical models which deep dives into aspects of correlated quantities within hierarchical models by considering the special case of a Normal model.\n\n51.2.1 Correlated data\nIn the supplementary material from Module 2, we introduced covariance and correlation.\nRecall that covariance between two random variables is defined as:\n\n\\sigma_{xy} = \\mathbb{C}ov(X,Y) = \\mathbb{E}[(X − \\mu_x)(Y − \\mu_y)]\n\nwhere:\n\n\\mu_x = \\mathbb{E}(X) and\n\\mu_y = \\mathbb{E}(Y).\n\nAnd that Correlation between X and Y is defined as: \n\\rho_{xy} = \\mathbb{C}or(X,Y) = \\frac{\\mathbb{C}ov(X,Y)}{\\sqrt{\\mathbb{V}ar(X) \\cdot \\mathbb{V}ar(Y)}} = \\frac{\\sigma_{xy}}{\\sqrt{\\sigma^2_x \\cdot \\sigma^2_y}}.\n\nCorrelation measures the strength of linear relationship between two variables. Covariance has a useful mathematical property which we will use below.\nIf a, b, c, d are constants, and X,Y are random variables then\n\n\\begin{aligned}\n\\mathbb{C}ov(a + bX, c + dY) &= \\mathbb{C}ov(a, c) + \\mathbb{C}ov(a, dY) + \\mathbb{C}ov(bX, c) + \\mathbb{C}ov(bX, dY) \\\\\n&= \\mathbb{C}ov(a,c) + \\mathbb{C}ov(a,dY ) + \\mathbb{C}ov(bX,c) + \\mathbb{C}ov(bX,dY ) \\\\\n&= 0 + 0 + 0 + b \\cdot \\mathbb{C}ov(X,Y ) \\cdot d \\\\\n&= b \\cdot d \\cdot \\sigma_{xy} ,\n\\end{aligned}\n\nwhere the 0 terms are due to the fact that constants do not co-vary with anything.\nIn the examples from this lesson, we use hierarchical models when the data were grouped in some way, so that two observations from the same group were assumed to be more similar than two observations from different groups. We would therefore expect two observations from the same group to be correlated. It turns out that hierarchical models do correlate such variables, as we will demonstrate with a normal hierarchical model.\n\n\n51.2.2 Normal hierarchical model\nSuppose our data come from a normal distribution, where each group has its own mean.\nIn the second stage, we assume that all the group means come from a common normal distribution.\nLet’s write this hierarchical model. We y_{i,j} denote the i th observation in group j, with mean \\theta_j. Thus we get: \n\\begin{aligned}\ny_{i,j} \\mid \\theta_j &\\stackrel{iid}{\\sim} \\mathcal{N}(\\theta_j, \\sigma^2) \\\\\n\\theta_j &\\stackrel{iid}{\\sim} \\mathcal{N}(\\mu, \\tau^2)\n\\end{aligned}\n\nwhere \\sigma^2, \\tau^2, and \\mu are known constants. To get the marginal distribution of y_{i,j} only, we must compute:\n\n\\begin{aligned}\n\\mathbb{P}r(y_{i,j}) &= \\int \\mathbb{P}r(y_{i,j}, \\theta_j) d\\theta_j \\\\\n&= \\int \\mathbb{P}r(y_{i,j} \\mid \\theta_j) \\mathbb{P}r(\\theta_j) d\\theta_j\n\\end{aligned}\n\nWith normally distributed variables, it is often easier to work with the following equivalent formulation of the model\n\n\\begin{aligned}\n\\theta_j &= \\mu + \\nu_j, & \\nu_j &\\stackrel{iid}{\\sim} \\mathcal{N}(0, \\tau^2) \\\\\ny_{i,j} &= \\theta_j + \\epsilon_{i,j}, & \\epsilon_{i,j} &\\stackrel{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)\n\\end{aligned}\n\nwith all \\nu_j and \\epsilon_{i,j} independent. This allows us to substitute \\theta_j into the expression for y_{i,j} to get \ny_{i,j} = \\mu + \\nu_j + \\epsilon_{i,j}.\n\nOne nice property of normal random variables is that if X and Y are both normally distributed (correlated or uncorrelated), then the new variable X + Y will also be normally distributed. Hence \\mathbb{P}r(y_{i,j}) is a normal distribution with mean\n\n\\begin{aligned}\n\\mathbb{E}(y_{i,j}) &= \\mathbb{E}(\\mu + \\nu_j + \\epsilon_{i,j}) \\\\\n&= E(\\mu) + E(\\nu_j) + E(\\epsilon_{i,j}) \\\\\n&= \\mu + 0 + 0 \\\\\n&= \\mu\n\\end{aligned}\n\nand variance\n\n\\begin{aligned}\n\\mathbb{V}ar(y_{i,j}) =& \\mathbb{V}ar(\\mu + \\nu_j + \\epsilon_{i,j} ) \\\\\n=& \\mathbb{C}ov(\\mu + \\nu_j + \\epsilon_{i,j} , \\mu + \\nu_j + \\epsilon_{i,j} ) \\\\\n=& \\mathbb{C}ov(\\mu,\\mu) + \\mathbb{C}ov(\\nu_j ,\\nu_j ) + \\mathbb{C}ov(\\epsilon_{i,j} ,\\epsilon_{i,j} ) \\\\\n&          + 2 \\cdot \\mathbb{C}ov(\\mu,\\nu_j ) + 2 \\cdot \\mathbb{C}ov(\\mu,\\epsilon_{i,j} ) \\\\\n&          + 2 \\cdot \\mathbb{C}ov(\\nu_j ,\\epsilon_{i,j} ) \\\\\n=& 0 + \\mathbb{V}ar(\\nu_j ) + \\mathbb{V}ar(\\epsilon_{i,j} ) + 0 + 0 + 0 &&& \\text{(since } \\nu_j \\text{ and } \\epsilon_{i,j} \\text{ are independent)} \\\\\n=& \\tau^2 + \\sigma^2 .\n\\end{aligned}\n\nNow, we want to show that observations in the same group are correlated under this hierarchical model.\nLet’s take, for example, observations 1 and 2 from group j, y_{1,j} and y_{2,j}. It does not matter which two you select, as long as they are from the same group. We know that \\mathbb{V}ar(y_{1,j}) = \\mathbb{V}ar(y_{2,j}) = \\tau^2 + \\sigma^2. What about their covariance?\n\n\\begin{aligned}\n\\mathbb{C}ov(y_{1,j},y_{2,j}) &= \\mathbb{C}ov(\\mu + \\nu_j + \\epsilon_{2,j}, \\mu + \\nu_j + \\epsilon_{2,j}) \\\\\n&= \\mathbb{C}ov(\\mu,\\mu) + \\mathbb{C}ov(\\nu_j,\\nu_j)\n                         + \\mathbb{C}ov(\\epsilon_{1,j},\\epsilon_{2,j})\n                         + 2 \\cdot \\mathbb{C}ov(\\mu,\\nu_j)\n                         + \\mathbb{C}ov(\\mu,\\epsilon_{1,j})\n                         + \\mathbb{C}ov(\\mu,\\epsilon_{2,j})\n                         + \\mathbb{C}ov(\\nu_j,\\epsilon_{1,j})\n                         + \\mathbb{C}ov(\\nu_j,\\epsilon_{2,j}) \\\\\n&= 0 + \\mathbb{V}ar(\\nu_j) + 0\n                           + 2 \\cdot 0\n                           + 0 + 0 + 0\n                           + 0 \\qquad \\text{(as } \\epsilon_{1,j} \\text{ and } \\epsilon_{2,j} \\text{ are independent)}\n&= \\tau^2\n\\end{aligned}\n\nwhich gives us correlation:\n\n\\begin{aligned}\n\\mathbb{C}or(y_{1,j},y_{2,j}) &= \\mathbb{C}ov(y_{1,j},y_{2,j})/\\sqrt{\\mathbb{V}ar(y_{1,j}) \\cdot \\mathbb{V}ar(y_{2,j})} \\\\\n&= \\tau^2/\\sqrt{(\\tau^2 + \\sigma^2) \\cdot (\\tau^2 + \\sigma^2)} \\\\\n&= \\tau^2/\\sqrt{(\\tau^2 + \\sigma^2) \\cdot (\\tau^2 + \\sigma^2)} \\\\\n&= \\tau^2/\\sqrt{(\\tau^2 + \\sigma^2)^2} \\\\\n&= \\frac{\\tau^2}{\\tau^2 + \\sigma^2}.\n\\end{aligned}\n\nFinally, let’s check the covariance between observations in different groups. Let’s take observation i from groups 1 and 2 (again, our choices do not matter), y_{i,1} and y_{i,2}. Their covariance is\n\n\\begin{aligned}\n\\mathbb{C}ov(y_{i,1},y_{i,2}) &= \\mathbb{C}ov(\\mu + \\nu_1 + \\epsilon_{i,1}, \\mu + \\nu_2 + \\epsilon_{i,2}) \\\\\n&= \\mathbb{C}ov(\\mu,\\mu) + \\mathbb{C}ov(\\nu_1,\\nu_2) + \\mathbb{C}ov(\\epsilon_{i,1},\\epsilon_{i,2}) \\\\\n&\\quad + \\mathbb{C}ov(\\mu,\\nu_1) + \\mathbb{C}ov(\\mu,\\nu_2) + \\mathbb{C}ov(\\mu,\\epsilon_{i,1}) + \\mathbb{C}ov(\\mu,\\epsilon_{i,2}) \\\\\n&\\quad + \\mathbb{C}ov(\\nu_1,\\epsilon_{i,1}) + \\mathbb{C}ov(\\nu_2,\\epsilon_{i,2}) \\\\\n&= 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 \\qquad \\text{(as all } \\epsilon \\text{ and } \\nu \\text{ are independent)} \\\\\n&= 0,\n\\end{aligned}\n\nwhich obviously yields correlation 0.\nThus, we have demonstrated that observations from the same group are correlated while observations from different groups are uncorrelated in the marginal distribution for observations.",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#applications-of-hierarchical-modeling",
    "href": "C2-L11.html#applications-of-hierarchical-modeling",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "51.3 Applications of hierarchical modeling",
    "text": "51.3 Applications of hierarchical modeling\nHandout: Common applications of Bayesian hierarchical models",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#prior-predictive-simulation",
    "href": "C2-L11.html#prior-predictive-simulation",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "51.4 Prior predictive simulation",
    "text": "51.4 Prior predictive simulation\n\n51.4.1 Data\nLet’s fit our hierarchical model for counts of chocolate chips. The data can be found in cookies.dat.\n\ndat = read.table(file=\"data/cookies.dat\", header=TRUE)\nhead(dat)\n\n  chips location\n1    12        1\n2    12        1\n3     6        1\n4    13        1\n5    12        1\n6    12        1\n\n\n\ntable(dat$location)\n\n\n 1  2  3  4  5 \n30 30 30 30 30 \n\n\nWe can also visualize the distribution of chips by location.\n\nhist(dat$chips)\n\n\n\n\n\n\n\nFigure 51.3\n\n\n\n\n\n\nboxplot(chips ~ location, data=dat)\n\n\n\n\n\n\n\nFigure 51.4\n\n\n\n\n\n\n\n51.4.2 Prior predictive checks\nBefore implementing the model, we need to select prior distributions for \\alpha and \\beta, the hyperparameters governing the gamma distribution for the \\lambda parameters. First, think about what the \\lambda’s represent. For location j, \\lambda_j is the expected number of chocolate chips per cookie. Hence, \\alpha and \\beta control the distribution of these means between locations. The mean of this gamma distribution will represent the overall mean of number of chips for all cookies. The variance of this gamma distribution controls the variability between locations. If this is high, the mean number of chips will vary widely from location to location. If it is small, the mean number of chips will be nearly the same from location to location.\nTo see the effects of different priors on the distribution of \\lambda’s, we can simulate. Suppose we try independent exponential priors for \\alpha and \\beta.\n\nset.seed(112)\nn_sim = 500\nalpha_pri = rexp(n_sim, rate=1.0/2.0)\nbeta_pri = rexp(n_sim, rate=5.0)\nmu_pri = alpha_pri/beta_pri\nsig_pri = sqrt(alpha_pri/beta_pri^2)\n\nsummary(mu_pri)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.0213    2.9829    9.8522   61.1271   29.9801 4858.7861 \n\n\n\nsummary(sig_pri)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.1834    3.3663    8.5488   41.8137   22.2219 2865.6461 \n\n\nAfter simulating from the priors for \\alpha and \\beta, we can use those samples to simulate further down the hierarchy:\n\nlam_pri = rgamma(n=n_sim, shape=alpha_pri, rate=beta_pri)\nsummary(lam_pri)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n    0.000     1.171     7.668    83.062    28.621 11005.331 \n\n\nOr for a prior predictive reconstruction of the original data set:\n\n(lam_pri = rgamma(n=5, shape=alpha_pri[1:5], rate=beta_pri[1:5]))\n\n[1] 66.444084  9.946688  6.028319 15.922568 47.978587\n\n\n\n(y_pri = rpois(n=150, lambda=rep(lam_pri, each=30)))\n\n  [1] 63 58 64 63 70 62 61 48 71 73 70 77 66 60 72 77 69 62 66 71 49 80 66 75 74\n [26] 55 62 90 65 57 12  9  7 10 12 10 11  7 14 13  9  6  6 13  7 10 12  9  9 10\n [51]  7  8  6  9  7 10 13 13  8 12  6 10  3  6  7  4  6  7  5  5  4  3  6  2  8\n [76]  4  8  4  5  7  1  4  5  3  8  8  3  1  7  3 16 14 13 17 17 12 13 13 16 16\n[101] 15 14 11 10 13 17 16 19 16 17 15 16  7 17 21 16 12 15 14 13 52 44 51 46 39\n[126] 40 40 44 46 59 45 49 58 42 31 52 43 47 53 41 48 57 35 60 51 58 36 34 41 59\n\n\nBecause these priors have high variance and are somewhat noninformative, they produce unrealistic predictive distributions. Still, enough data would overwhelm the prior, resulting in useful posterior distributions. Alternatively, we could tweak and simulate from these prior distributions until they adequately represent our prior beliefs. Yet another approach would be to re-parameterize the gamma prior, which we’ll demonstrate as we fit the model.",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#jags-model",
    "href": "C2-L11.html#jags-model",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "51.5 JAGS Model",
    "text": "51.5 JAGS Model\n\nlibrary(\"rjags\")\n\n\nmod_string = \" model {\nfor (i in 1:length(chips)) {\n  chips[i] ~ dpois(lam[location[i]])\n}\n\nfor (j in 1:max(location)) {\n  lam[j] ~ dgamma(alpha, beta)\n}\n\nalpha = mu^2 / sig^2\nbeta = mu / sig^2\n\nmu ~ dgamma(2.0, 1.0/5.0)\nsig ~ dexp(1.0)\n\n} \"\n\nset.seed(113)\n\ndata_jags = as.list(dat)\n\nparams = c(\"lam\", \"mu\", \"sig\")\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 150\n   Unobserved stochastic nodes: 7\n   Total graph size: 315\n\nInitializing model\n\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,\n                       variable.names=params,\n                       n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim))\n\n\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim)\n\n\n\n\n\n\n\nFigure 51.5\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.6\n\n\n\n\n\n\ngelman.diag(mod_sim)\n\nPotential scale reduction factors:\n\n       Point est. Upper C.I.\nlam[1]          1          1\nlam[2]          1          1\nlam[3]          1          1\nlam[4]          1          1\nlam[5]          1          1\nmu              1          1\nsig             1          1\n\nMultivariate psrf\n\n1\n\nautocorr.diag(mod_sim)\n\n             lam[1]       lam[2]      lam[3]       lam[4]       lam[5]\nLag 0   1.000000000  1.000000000 1.000000000  1.000000000 1.0000000000\nLag 1   0.024194365  0.124021047 0.019111607  0.000355262 0.0664428799\nLag 5  -0.015704807 -0.003810331 0.005958220 -0.005751762 0.0105149694\nLag 10 -0.002111030  0.008210715 0.024357785 -0.002885581 0.0001820758\nLag 50 -0.004695945 -0.001131281 0.001797284  0.003237898 0.0075204542\n                 mu           sig\nLag 0  1.0000000000  1.0000000000\nLag 1  0.3747466092  0.5475759223\nLag 5  0.0254688975  0.0748964816\nLag 10 0.0012197344  0.0197119156\nLag 50 0.0003065432 -0.0001996277\n\n\npar(mar = c(2.5, 1, 2.5, 1))\nautocorr.plot(mod_sim)\n\n\n\n\n\n\n\n\n\nFigure 51.7\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.8\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.9\n\n\n\n\n\n\n\neffectiveSize(mod_sim)\n\n   lam[1]    lam[2]    lam[3]    lam[4]    lam[5]        mu       sig \n14123.659 10855.385 14305.054 15257.466 13129.698  6154.835  4073.565 \n\n## compute DIC\ndic = dic.samples(mod, n.iter=1e3)\n\n\n51.5.1 Model checking\nAfter assessing convergence, we can check the fit via residuals. With a hierarchical model, there are now two levels of residuals: the observation level and the location mean level. To simplify, we’ll look at the residuals associated with the posterior means of the parameters.\nFirst, we have observation residuals, based on the estimates of location means.\n\n## observation level residuals\n(pm_params = colMeans(mod_csim))\n\n   lam[1]    lam[2]    lam[3]    lam[4]    lam[5]        mu       sig \n 9.284372  6.222039  9.526465  8.955319 11.755696  9.094434  2.074664 \n\n\n\nyhat = rep(pm_params[1:5], each=30)\nresid = dat$chips - yhat\nplot(resid)\n\n\n\n\n\n\n\nFigure 51.10\n\n\n\n\n\n\nplot(jitter(yhat), resid)\n\n\n\n\n\n\n\nFigure 51.11\n\n\n\n\n\n\nvar(resid[yhat&lt;7])\n\n[1] 6.447126\n\n\n\nvar(resid[yhat&gt;11])\n\n[1] 13.72414\n\n\nAlso, we can look at how the location means differ from the overall mean \\mu.\n\n## location level residuals\nlam_resid = pm_params[1:5] - pm_params[\"mu\"]\nplot(lam_resid)\nabline(h=0, lty=2)\n\n\n\n\n\n\n\n\nWe don’t see any obvious violations of our model assumptions.\n\n\n51.5.2 Results\n\nsummary(mod_sim)\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n         Mean     SD Naive SE Time-series SE\nlam[1]  9.284 0.5420 0.004425       0.004568\nlam[2]  6.222 0.4611 0.003765       0.004428\nlam[3]  9.526 0.5478 0.004473       0.004583\nlam[4]  8.955 0.5313 0.004338       0.004302\nlam[5] 11.756 0.6218 0.005077       0.005427\nmu      9.094 0.9746 0.007957       0.012423\nsig     2.075 0.6911 0.005643       0.010852\n\n2. Quantiles for each variable:\n\n         2.5%    25%    50%    75%  97.5%\nlam[1]  8.248  8.913  9.275  9.640 10.387\nlam[2]  5.347  5.903  6.215  6.530  7.156\nlam[3]  8.474  9.147  9.515  9.895 10.623\nlam[4]  7.953  8.594  8.935  9.306 10.029\nlam[5] 10.572 11.330 11.746 12.164 13.011\nmu      7.222  8.476  9.065  9.691 11.117\nsig     1.093  1.585  1.946  2.436  3.708",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#posterior-predictive-simulation",
    "href": "C2-L11.html#posterior-predictive-simulation",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "51.6 Posterior predictive simulation",
    "text": "51.6 Posterior predictive simulation\nJust as we did with the prior distribution, we can use these posterior samples to get Monte Carlo estimates that interest us from the posterior predictive distribution.\nFor example, we can use draws from the posterior distribution of \\mu and \\sigma to simulate the posterior predictive distribution of the mean for a new location.\n\n(n_sim = nrow(mod_csim))\n\n[1] 15000\n\n\n\nlam_pred = rgamma(n=n_sim, shape=mod_csim[,\"mu\"]^2/mod_csim[,\"sig\"]^2, \n                  rate=mod_csim[,\"mu\"]/mod_csim[,\"sig\"]^2)\nhist(lam_pred)\n\n\n\n\n\n\n\nFigure 51.12\n\n\n\n\n\n\nmean(lam_pred &gt; 15)\n\n[1] 0.0172\n\n\nUsing these \\lambda draws, we can go to the observation level and simulate the number of chips per cookie, which takes into account the uncertainty in \\lambda:\n\ny_pred = rpois(n=n_sim, lambda=lam_pred)\nhist(y_pred)\n\n\n\n\n\n\n\n\n\nmean(y_pred &gt; 15)\n\n[1] 0.0582\n\n\n\nhist(dat$chips)\n\n\n\n\n\n\n\nFigure 51.13\n\n\n\n\n\nFinally, we could answer questions like: what is the posterior probability that the next cookie produced in Location 1 will have fewer than seven chips?\n\ny_pred1 = rpois(n=n_sim, lambda=mod_csim[,\"lam[1]\"])\nhist(y_pred1)\n\n\n\n\n\n\n\nFigure 51.14\n\n\n\n\n\n\nmean(y_pred1 &lt; 7)\n\n[1] 0.187",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#random-intercept-linear-model",
    "href": "C2-L11.html#random-intercept-linear-model",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "51.7 Random intercept linear model",
    "text": "51.7 Random intercept linear model\n We can extend the linear model for the Leinhardt data on infant mortality by incorporating the region variable. We’ll do this with a hierarchical model, where each region has its own intercept.\n\nlibrary(\"car\")\ndata(\"Leinhardt\")\n?Leinhardt\nstr(Leinhardt)\n\n'data.frame':   105 obs. of  4 variables:\n $ income: int  3426 3350 3346 4751 5029 3312 3403 5040 2009 2298 ...\n $ infant: num  26.7 23.7 17 16.8 13.5 10.1 12.9 20.4 17.8 25.7 ...\n $ region: Factor w/ 4 levels \"Africa\",\"Americas\",..: 3 4 4 2 4 4 4 4 4 4 ...\n $ oil   : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\npairs(Leinhardt)\n\n\n\n\n\n\n\nFigure 51.15\n\n\n\n\n\n\nhead(Leinhardt)\n\n          income infant   region oil\nAustralia   3426   26.7     Asia  no\nAustria     3350   23.7   Europe  no\nBelgium     3346   17.0   Europe  no\nCanada      4751   16.8 Americas  no\nDenmark     5029   13.5   Europe  no\nFinland     3312   10.1   Europe  no\n\n\nPreviously, we worked with infant mortality and income on the logarithmic scale. Recall also that we had to remove some missing data.\n\ndat = na.omit(Leinhardt)\ndat$logincome = log(dat$income)\ndat$loginfant = log(dat$infant)\nstr(dat)\n\n'data.frame':   101 obs. of  6 variables:\n $ income   : int  3426 3350 3346 4751 5029 3312 3403 5040 2009 2298 ...\n $ infant   : num  26.7 23.7 17 16.8 13.5 10.1 12.9 20.4 17.8 25.7 ...\n $ region   : Factor w/ 4 levels \"Africa\",\"Americas\",..: 3 4 4 2 4 4 4 4 4 4 ...\n $ oil      : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ logincome: num  8.14 8.12 8.12 8.47 8.52 ...\n $ loginfant: num  3.28 3.17 2.83 2.82 2.6 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:4] 24 83 86 91\n  ..- attr(*, \"names\")= chr [1:4] \"Iran\" \"Haiti\" \"Laos\" \"Nepal\"\n\n\nNow we can fit the proposed model:\n\nlibrary(\"rjags\")\n\n\nmod_string = \" model {\n  for (i in 1:length(y)) {\n    y[i] ~ dnorm(mu[i], prec)\n    mu[i] = a[region[i]] + b[1]*log_income[i] + b[2]*is_oil[i]\n  }\n  \n  for (j in 1:max(region)) {\n    a[j] ~ dnorm(a0, prec_a)\n  }\n  \n  a0 ~ dnorm(0.0, 1.0/1.0e6)\n  prec_a ~ dgamma(1/2.0, 1*10.0/2.0)\n  tau = sqrt( 1.0 / prec_a )\n  \n  for (j in 1:2) {\n    b[j] ~ dnorm(0.0, 1.0/1.0e6)\n  }\n  \n  prec ~ dgamma(5/2.0, 5*10.0/2.0)\n  sig = sqrt( 1.0 / prec )\n} \"\n\n\nset.seed(116)\ndata_jags = list(y=dat$loginfant, log_income=dat$logincome,\n                  is_oil=as.numeric(dat$oil==\"yes\"), region=as.numeric(dat$region))\ndata_jags$is_oil\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\ntable(data_jags$is_oil, data_jags$region)\n\n   \n     1  2  3  4\n  0 31 20 24 18\n  1  3  2  3  0\n\nparams = c(\"a0\", \"a\", \"b\", \"sig\", \"tau\")\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 101\n   Unobserved stochastic nodes: 9\n   Total graph size: 622\n\nInitializing model\n\nupdate(mod, 1e3) # burn-in\n\nmod_sim = coda.samples(model=mod,\n                       variable.names=params,\n                       n.iter=5e3)\n\nmod_csim = as.mcmc(do.call(rbind, mod_sim)) # combine multiple chains\n\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngelman.diag(mod_sim)\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\na[1]       1.04       1.13\na[2]       1.04       1.13\na[3]       1.04       1.12\na[4]       1.04       1.13\na0         1.01       1.04\nb[1]       1.04       1.13\nb[2]       1.00       1.00\nsig        1.00       1.01\ntau        1.00       1.00\n\nMultivariate psrf\n\n1.03\n\nautocorr.diag(mod_sim)\n\n            a[1]      a[2]      a[3]      a[4]         a0      b[1]       b[2]\nLag 0  1.0000000 1.0000000 1.0000000 1.0000000 1.00000000 1.0000000 1.00000000\nLag 1  0.9236793 0.9230317 0.9210966 0.9382209 0.25672516 0.9801634 0.13886787\nLag 5  0.8506661 0.8543049 0.8526044 0.8665871 0.24657389 0.9066750 0.02593193\nLag 10 0.7702622 0.7754805 0.7747608 0.7809924 0.22260691 0.8207662 0.03150141\nLag 50 0.2856952 0.2867057 0.2886358 0.2828899 0.07009041 0.3009284 0.02435816\n                sig          tau\nLag 0   1.000000000  1.000000000\nLag 1   0.060094445  0.275457817\nLag 5  -0.003795647 -0.001307823\nLag 10  0.011436618  0.018025788\nLag 50 -0.011981321  0.016768961\n\nautocorr.plot(mod_sim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffectiveSize(mod_sim)\n\n      a[1]       a[2]       a[3]       a[4]         a0       b[1]       b[2] \n  156.9881   162.2592   158.4710   158.5000   708.7858   147.7942  7831.2017 \n       sig        tau \n12620.3554  8548.4739 \n\n\n\n51.7.1 Results\nConvergence looks okay, so let’s compare this with the old model from Lesson 7 using DIC:\n\ndic.samples(mod, n.iter=1e3)\n\nMean deviance:  213.5 \npenalty 6.664 \nPenalized deviance: 220.1 \n\n### nonhierarchical model: 230.1\n\nIt appears that this model is an improvement over the non-hierarchical one we fit earlier. Notice that the penalty term, which can be interpreted as the “effective” number of parameters, is less than the actual number of parameters (nine). There are fewer “effective” parameters because they are “sharing” information or “borrowing strength” from each other in the hierarchical structure. If we had skipped the hierarchy and fit one intercept, there would have been four parameters. If we had fit separate, independent intercepts for each region, there would have been seven parameters (which is close to what we ended up with).\nFinally, let’s look at the posterior summary.\n\nsummary(mod_sim)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean      SD  Naive SE Time-series SE\na[1]  6.6229 0.57886 0.0047264      0.0454324\na[2]  6.0977 0.72975 0.0059584      0.0566921\na[3]  5.9261 0.64592 0.0052739      0.0505992\na[4]  5.6460 0.89153 0.0072793      0.0701273\na0    6.0715 1.35832 0.0110907      0.0515712\nb[1] -0.3549 0.11024 0.0009001      0.0089712\nb[2]  0.6554 0.34901 0.0028497      0.0040367\nsig   0.9203 0.06576 0.0005369      0.0005856\ntau   2.0513 1.04738 0.0085518      0.0113345\n\n2. Quantiles for each variable:\n\n         2.5%     25%     50%     75%   97.5%\na[1]  5.51110  6.2251  6.6166  7.0036  7.7577\na[2]  4.69866  5.5904  6.0871  6.5878  7.5529\na[3]  4.70357  5.4805  5.9194  6.3583  7.1967\na[4]  3.95552  5.0333  5.6349  6.2329  7.4161\na0    3.40307  5.2655  6.0664  6.8752  8.7627\nb[1] -0.57277 -0.4277 -0.3534 -0.2785 -0.1463\nb[2] -0.01538  0.4183  0.6502  0.8896  1.3402\nsig   0.80332  0.8739  0.9166  0.9621  1.0596\ntau   0.98329  1.4067  1.7883  2.3771  4.6476\n\n\nIn this particular model, the intercepts do not have a real interpretation because they correspond to the mean response for a country that does not produce oil and has USD0 log-income per capita (which is USD1 income per capita). We can interpret a_0 as the overall mean intercept and \\tau as the standard deviation of intercepts across regions.\n\n\n51.7.2 Other models\n We have not investigated adding interaction terms, which might be appropriate. We only considered adding hierarchy on the intercepts, but in reality nothing prevents us from doing the same for other terms in the model, such as the coefficients for income and oil. We could try any or all of these alternatives and see how the DIC changes for those models. This, together with other model checking techniques we have discussed could be used to identify your best model that you can use to make inferences and predictions.",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#mixture-models",
    "href": "C2-L11.html#mixture-models",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "51.8 Mixture models",
    "text": "51.8 Mixture models\nHistograms of data often reveal that they do not follow any standard probability distribution. Sometimes we have explanatory variables (or covariates) to account for the different values, and normally distributed errors are adequate, as in normal regression. However, if we only have the data values themselves and no covariates, we might have to fit a non-standard distribution to the data. One way to do this is by mixing standard distributions.\nMixture distributions are just a weighted combination of probability distributions. For example, we could take an exponential distribution with mean 1 and normal distribution with mean 3 and variance 1 (although typically the two mixture components would have the same support; here the exponential component has to be non-negative and the normal component can be positive or negative). Suppose we give them weights: 0.4 for the exponential distribution and 0.6 for the normal distribution. We could write the PDF for this distribution as\n\n\\mathbb{P}r(y) = 0.4 \\cdot \\exp(-y) \\cdot \\mathbb{I}_{(y \\ge 0)} + 0.6 \\cdot \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(- \\frac{1}{2} (y - 3)^2\\right)\n\\tag{51.3}\nwhere \\mathbb{I}_{(y \\ge 0)} is the indicator function that is 1 if y \\ge 0 and 0 otherwise. This PDF is a mixture of the exponential and normal distributions, with the weights indicating the relative contribution of each distribution to the overall PDF.\nThe PDF of this mixture distribution would look like this:\n\ncurve( 0.4*dexp(x, 1.0) + 0.6*dnorm(x, 3.0, 1.0), from=-2.0, to=7.0, ylab=\"density\", xlab=\"y\", main=\"40/60 mixture of exponential and normal distributions\", lwd=2)\n\n\n\n\n\n\n\nFigure 51.16\n\n\n\n\n\nWe could think of these two distributions as governing two distinct populations, one following the exponential distribution and the other following the normal distribution.\nLet’s draw the weighted PDFs for each population.\n\ncurve( 0.4*dexp(x, 1.0) + 0.6*dnorm(x, 3.0, 1.0), from=-2.0, to=7.0, ylab=\"density\", xlab=\"y\", main=\"40/60 mixture of exponential and normal distributions\", lwd=2)\ncurve( 0.4*dexp(x, 1.0), from=-2.0, to=7.0, col=\"red\", lty=2, add=TRUE)\ncurve( 0.6*dnorm(x, 3.0, 1.0), from=-2.0, to=7.0, col=\"blue\", lty=2, add=TRUE)\n\n\n\n\n\n\n\nFigure 51.17\n\n\n\n\n\nThe general form for a discrete mixture of distributions is as follows:\n\n\\mathbb{P}r(y) = \\sum_{j=1}^J \\omega_j \\cdot f_j (y)\n\\tag{51.4}\nwhere the \\omega’s are positive weights that add up to 1 (they are probabilities) and each of the J f_j(y) functions is a PDF for some distribution. In the example above, the weights were 0.4 and 0.6, f_1 was an exponential PDF and f_2 was a normal PDF.\nOne way to simulate from a mixture distribution is with a hierarchical model. We first simulate an indicator for which “population” the next observation will come from using the weights \\omega. Let’s call this z_i. In the example above, z_i would take the value 1 (indicating the exponential distribution) with probability 0.4 and 2 (indicating the normal distribution) with probability 0.6. Next, simulate the observation y_i from the distribution corresponding to z_i.\nLet’s simulate from our example mixture distribution.\n\nset.seed(117)\nn = 1000\nz = numeric(n)\ny = numeric(n)\nfor (i in 1:n) {\n  z[i] = sample.int(2, 1, prob=c(0.4, 0.6)) # returns a 1 with probability 0.4, or a 2 with probability 0.6\n  if (z[i] == 1) {\n    y[i] = rexp(1, rate=1.0)\n  } else if (z[i] == 2) {\n    y[i] = rnorm(1, mean=3.0, sd=1.0)\n  }\n}\nhist(y, breaks=30)\n\n\n\n\n\n\n\n\nIf we keep only the y values and throw away the z values, we have a sample from the mixture model above. To see that they are equivalent, we can marginalize the joint distribution of y and z:\n\n\\begin{aligned}\n\\mathbb{P}r(y) &= \\sum_{j=1}^2 \\mathbb{P}r(y, z=j) \\\\\n&= \\sum_{j=1}^2 \\mathbb{P}r(z=j) \\cdot \\mathbb{P}r(y \\mid z=j) \\\\\n&= \\sum_{j=1}^2 \\omega_j \\cdot f_j(y)\n\\end{aligned}\n\\tag{51.5}\n\n51.8.1 Bayesian inference for mixture models\nWhen we fit a mixture model to data, we usually only have the y values and do not know which “population” they belong to. Because the z variables are unobserved, they are called latent variables. We can treat them as parameters in a hierarchical model and perform Bayesian inference for them. The hierarchial model might look like this:\n\n\\begin{aligned}\ny_i \\mid z_i, \\theta & \\overset{\\text{ind}}{\\sim} f_{z_i}(y \\mid \\theta) \\, , \\quad i = 1, \\ldots, n \\\\\n\\text{Pr}(z_i = j \\mid \\omega) &= \\omega_j \\, , \\quad j=1, \\ldots, J \\\\\n\\omega &\\sim \\mathbb{P}r(\\omega) \\\\\n\\theta &\\sim  \\mathbb{P}r(\\theta)\n\\end{aligned}\n\\tag{51.6}\nwhere we might use a Dirichlet prior (see the review of distributions in the supplementary material) for the weight vector \\omega and conjugate priors for the population-specific parameters in \\theta. With this model, we could obtain posterior distributions for z (population membership of the observations), \\omega (population weights), and \\theta (population-specific parameters in f_j). Next, we will look at how to fit a mixture of two normal distributions in JAGS.",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11.html#example-with-jags",
    "href": "C2-L11.html#example-with-jags",
    "title": "51  Hierarchical modeling - M4L11",
    "section": "51.9 Example with JAGS",
    "text": "51.9 Example with JAGS\n\n51.9.1 Data\nFor this example, we will use the data in the attached file mixture.csv. \n\ndat = read.csv(\"data/mixture.csv\", header=FALSE)\ny = dat$V1\n(n = length(y))\n\n[1] 200\n\n\nLet’s visualize these data.\n\nhist(y, breaks=20)\n\n\n\n\n\n\n\nFigure 51.18\n\n\n\n\n\n\nplot(density(y))\n\n\n\n\n\n\n\nFigure 51.19\n\n\n\n\n\nIt appears that we have two populations, but we do not know which population each observation belongs to. We can learn this, along with the mixture weights and population-specific parameters with a Bayesian hierarchical model.\nWe will use a mixture of two normal distributions with variance 1 and different (and unknown) means.\n\n\n51.9.2 Model\n\nlibrary(\"rjags\")\n\n\nmod_string = \" model {\n    for (i in 1:length(y)) {\n        y[i] ~ dnorm(mu[z[i]], prec)\n      z[i] ~ dcat(omega)\n    }\n  \n  mu[1] ~ dnorm(-1.0, 1.0/100.0)\n    mu[2] ~ dnorm(1.0, 1.0/100.0) T(mu[1],) # ensures mu[1] &lt; mu[2]\n\n    prec ~ dgamma(1.0/2.0, 1.0*1.0/2.0)\n  sig = sqrt(1.0/prec)\n    \n    omega ~ ddirich(c(1.0, 1.0))\n} \"\n\nset.seed(11)\n\ndata_jags = list(y=y)\n\nparams = c(\"mu\", \"sig\", \"omega\", \"z[1]\", \"z[31]\", \"z[49]\", \"z[6]\") # Select some z's to monitor\n\nmod = jags.model(textConnection(mod_string), data=data_jags, n.chains=3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 204\n   Total graph size: 614\n\nInitializing model\n\nupdate(mod, 1e3)\n\nmod_sim = coda.samples(model=mod,\n                        variable.names=params,\n                        n.iter=5e3)\nmod_csim = as.mcmc(do.call(rbind, mod_sim))\n\n\n## convergence diagnostics\npar(mar = c(2.5, 1, 2.5, 1))\nplot(mod_sim, ask=TRUE)\n\n\n\n\n\n\n\nFigure 51.20\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.21\n\n\n\n\n\n\n\n\n\n\n\nFigure 51.22\n\n\n\n\n\n\nautocorr.diag(mod_sim)\n\n              mu[1]       mu[2]    omega[1]    omega[2]          sig\nLag 0   1.000000000 1.000000000 1.000000000 1.000000000  1.000000000\nLag 1   0.538254886 0.326772821 0.292807794 0.292807794  0.416449528\nLag 5   0.083997317 0.057020925 0.062593661 0.062593661  0.017149923\nLag 10  0.019465377 0.010151346 0.007230271 0.007230271 -0.002850070\nLag 50 -0.005585322 0.006517438 0.009811168 0.009811168 -0.004975608\n               z[1]        z[31]       z[49] z[6]\nLag 0   1.000000000  1.000000000 1.000000000  NaN\nLag 1  -0.003091796  0.044509351 0.026699093  NaN\nLag 5   0.004059621  0.005297103 0.011071694  NaN\nLag 10 -0.010121736 -0.021714603 0.015178298  NaN\nLag 50  0.004090286  0.003683641 0.003789031  NaN\n\neffectiveSize(mod_sim)\n\n    mu[1]     mu[2]  omega[1]  omega[2]       sig      z[1]     z[31]     z[49] \n 4058.434  5597.945  5970.649  5970.649  5680.320 15000.000 13445.002 13233.898 \n     z[6] \n 5000.000 \n\n\n\n\n51.9.3 Results\n\nsummary(mod_sim)\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean       SD  Naive SE Time-series SE\nmu[1]    -2.1224 0.167391 1.367e-03      2.628e-03\nmu[2]     1.4899 0.126256 1.031e-03      1.691e-03\nomega[1]  0.3872 0.040461 3.304e-04      5.240e-04\nomega[2]  0.6128 0.040461 3.304e-04      5.240e-04\nsig       1.1363 0.075378 6.155e-04      1.001e-03\nz[1]      1.0100 0.099502 8.124e-04      8.125e-04\nz[31]     1.5779 0.493906 4.033e-03      4.264e-03\nz[49]     1.8077 0.394095 3.218e-03      3.437e-03\nz[6]      1.9999 0.008165 6.667e-05      6.667e-05\n\n2. Quantiles for each variable:\n\n            2.5%     25%     50%     75%   97.5%\nmu[1]    -2.4459 -2.2360 -2.1230 -2.0100 -1.7924\nmu[2]     1.2382  1.4064  1.4914  1.5755  1.7314\nomega[1]  0.3094  0.3598  0.3867  0.4140  0.4682\nomega[2]  0.5318  0.5860  0.6133  0.6402  0.6906\nsig       1.0038  1.0840  1.1308  1.1824  1.3013\nz[1]      1.0000  1.0000  1.0000  1.0000  1.0000\nz[31]     1.0000  1.0000  2.0000  2.0000  2.0000\nz[49]     1.0000  2.0000  2.0000  2.0000  2.0000\nz[6]      2.0000  2.0000  2.0000  2.0000  2.0000\n\n\n\n## for the population parameters and the mixing weights\n\npar(mar = c(2.5, 1, 2.5, 1))\npar(mfrow=c(3,2))\ndensplot(mod_csim[,c(\"mu[1]\", \"mu[2]\", \"omega[1]\", \"omega[2]\", \"sig\")])\n\n\n\n\n\n\n\nFigure 51.23\n\n\n\n\n\n\n## for the z's\npar(mfrow=c(2,2))\npar(mar = c(2.5, 1, 2.5, 1))\ndensplot(mod_csim[,c(\"z[1]\", \"z[31]\", \"z[49]\", \"z[6]\")])\n\n\n\n\n\n\n\nFigure 51.24\n\n\n\n\n\n\ntable(mod_csim[,\"z[1]\"]) / nrow(mod_csim) ## posterior probabilities for z[1], the membership of y[1]\n\n\n   1    2 \n0.99 0.01 \n\n\n\ntable(mod_csim[,\"z[31]\"]) / nrow(mod_csim) ## posterior probabilities for z[31], the membership of y[31]\n\n\n        1         2 \n0.4220667 0.5779333 \n\n\n\ntable(mod_csim[,\"z[49]\"]) / nrow(mod_csim) ## posterior probabilities for z[49], the membership of y[49]\n\n\n        1         2 \n0.1922667 0.8077333 \n\n\n\ntable(mod_csim[,\"z[6]\"]) / nrow(mod_csim) ## posterior probabilities for z[6], the membership of y[6]\n\n\n           1            2 \n6.666667e-05 9.999333e-01 \n\n\n\ny[c(1, 31, 49, 6)]\n\n[1] -2.2661749 -0.3702666  0.0365564  3.7548080\n\n\nIf we look back to the y values associated with these z variables we monitored, we see that y_1 is clearly in Population 1’s territory, y_{31} is ambiguous, y_{49} is ambiguous but is closer to Population 2’s territory, and y_6 is clearly in Population 2’s territory. The posterior distributions for the z variables closely reflect our assessment.\n\n\n\n\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis.\n\n\nMcElreath, Richard. 2015. Statistical Rethinking, a Course in r and Stan.",
    "crumbs": [
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Hierarchical modeling - M4L11</span>"
    ]
  },
  {
    "objectID": "C2-L11-Ex1.html",
    "href": "C2-L11-Ex1.html",
    "title": "52  Homework on Hierarchical Models - M4L11HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Homework on Hierarchical Models - M4L11HW1</span>"
    ]
  },
  {
    "objectID": "C2-L11-Ex2.html",
    "href": "C2-L11-Ex2.html",
    "title": "53  Homework on Non-Normal Hierarchical Models - M4L11HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Homework on Non-Normal Hierarchical Models - M4L11HW1</span>"
    ]
  },
  {
    "objectID": "C2-L12.html",
    "href": "C2-L12.html",
    "title": "54  Capstone Project - M4L12",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Capstone Project - M4L12</span>"
    ]
  },
  {
    "objectID": "C2-L12-Ex1.html",
    "href": "C2-L12-Ex1.html",
    "title": "55  Homework on Predictive distributions and mixture models - M4L12HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Homework on Predictive distributions and mixture models - M4L12HW1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html",
    "href": "C3-L01.html",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "",
    "text": "56.1 Basic Definitions 🎥\nHandout: Mixture model\nMixture models provide a flexible approach to modeling data and are useful in density estimation, clustering and classification problems:\nThe expectation of a mixture is straightforward to compute, as it is a weighted sum of the expectations of the components.\nthe moment generating function of a mixture is also straightforward to compute, as it is a weighted sum of the moment generating functions of the components.\nThe variance of a mixture is not as straightforward to compute, as it involves the second moment of the components and the square of the expectation. However there is a degenerate case where the variance of the mixture is equal to the weighted sum of the variances of the components.",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html#sec-basic-definitions",
    "href": "C3-L01.html#sec-basic-definitions",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "",
    "text": "NoteTODO list\n\n\n\n\n\n\nfor each mixture add.\na formula and.\nan example why such a mixture is useful.\nin the graphs consider also plotting the the components in different color as well as the actual mixture.\nadd code for generating the mixtures in basic python.\nadd code for generating the mixtures in PYMC/bambi\nadd all images light box to a gallery - via a regex !?\nas needed extract from the lesson transcript some key points and add them to the notes - via the save note feature on Coursera.\nexplain the two forms of likelihoods.\n\n\n\n\n\n\n\nStandard families of probability distributions such as the Gaussian, exponential or Poisson are often too restrictive for modeling features of real data such as multimodality or zero inflation. Mixture models, which can be related to kernel density estimation procedures, address this issue in a way that allows for natural generalizations of well-known procedures.\nIn addition to providing flexible probability distributions, finite mixture models have a strong relationship with classical clustering and classification procedures such as K-mean clustering, as well as linear and quadratic discriminant analysis. More generally they provide a tool to understand and generalize these approaches, as well as to quantify the uncertainty associated with the estimates and predictions generated by them.",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html#introduction-to-mixture-modeling",
    "href": "C3-L01.html#introduction-to-mixture-modeling",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "56.2 Introduction to Mixture modeling",
    "text": "56.2 Introduction to Mixture modeling\n\n56.2.1 Definition of a finite mixture model\n\n\n\n\n\n\n\nFigure 56.1: Mixtures definitions\n\n\n\nDefinition 56.1 Let \\omega_1 , \\ldots , \\omega_K be a collection of real numbers such that 0 \\le \\omega_k \\le 1 and \\sum^K_{k=1} \\omega_k = 1, and G_1, \\ldots, G_K be a collection of cumulative distribution functions. A random variable X with cumulative distribution function F(x) = Pr(X \\le x) of the form\n\nF(x) =\\sum^K_{k=1} \\underbrace{\\omega_k}_{weight}\\ \\cdot \\ \\underbrace{G_k(X)}_{component} \\qquad\n\\tag{56.1}\nis said to follow a finite mixture distribution with K components.\n\nf(x) =\\sum^K_{k=1} \\underbrace{\\omega_k}_{weight}\\ \\cdot \\ \\underbrace{g_k(X)}_{component} \\qquad\n\\tag{56.2}\nwhere g_k(x) is the density associated with G_k(x)\nThe values \\omega_1, \\ldots, \\omega_K are usually called the “weights” of the mixture, and the distributions G_1 , \\ldots, G_K are called the “components” of the mixture.\n\nEach component will typically belong to a parametric family that is indexed by its own parameter \\theta_k .\nWe will write G_k(x) = G_k (x \\mid \\theta_k ) whenever it is necessary to highlight the dependence on these parameters.\nIt is often the case that G_1, \\ldots, G_K all belong to the same family and differ only in the value parameters associated with each of the distributions, so that G_k (x \\mid \\theta_k ) = G(x \\mid \\theta_k ). In that case, the function G (and sometimes its density/probability mass function g) are called the “kernel” of the mixture.",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html#three-component-exponential-mixture",
    "href": "C3-L01.html#three-component-exponential-mixture",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "56.3 Three component Exponential mixture",
    "text": "56.3 Three component Exponential mixture\nFor example, we could define a mixture with K = 3 components, with G(x \\mid \\theta_1 ), G(x \\mid \\theta_2 ) and G(x \\mid \\theta_3 ) all corresponding to exponential distributions with means \\theta_1 , \\theta_2 and \\theta_3 respectively.\nIn that case, the cumulative distribution function of the mixture is given by\n\nF(x) = \\left(\\omega_1 \\left[ 1 − e^ {x \\over \\theta_1}\\right] + \\omega_2\\left[ 1 − e^ {x \\over \\theta_2}\\right] + \\omega_3 \\left[ 1 − e^ {x \\over \\theta_3}\\right] \\right)\\mathbb{I}_{x\\ge0} \\qquad\n\\tag{56.3}\n\nf(x) = \\left({\\omega_1\\over \\theta_1} \\left[ 1 − e^ {x \\over \\theta_1}\\right] + {\\omega_2\\over \\theta_2}\\left[ 1 − e^ {x \\over \\theta_2}\\right] + {\\omega_3\\over \\theta_3} \\left[ 1 − e^ {x \\over \\theta_3}\\right] \\right)\\mathbb{I}_{x\\ge0} \\qquad\n\\tag{56.4}",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html#sec-mixture-gaussians",
    "href": "C3-L01.html#sec-mixture-gaussians",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "56.4 Mixtures of Gaussians",
    "text": "56.4 Mixtures of Gaussians\nHere we will look at a few examples of mixtures of Gaussians which display different properties not available in a single Gaussian distribution.\n\n56.4.1 Example of a Bimodal mixture of Gaussians\n\nRpython\n\n\n\n# Mixture of univariate Gaussians, bimodal\nx = seq(-5, 12, length=100)\ny = 0.6*dnorm(x, 0, 1) + 0.4*dnorm(x, 5, 2)\n\n\npar(mar=c(4,4,1,1)+0.1)\nplot(x, y, type=\"l\", ylab=\"Density\", las=1, lwd=2)\n# set the title\ntitle(\"Bimodal Mixture of Gaussians\")\n\n\n\n\n\n\n\nFigure 56.8: Bimodal Mixture of Gaussians\n\n\n\n\n\n\n\n\n# title: Mixture of univariate Gaussians, bimodal\nfrom scipy.stats import norm\n\n# Values to sample\nx = np.linspace(-5, 12.0, num = 100)\n# Normal 1 distribution\nmu_1 = 0\nstd_1 = 1\nr_n1 = norm.pdf(x,loc = mu_1, scale = std_1)\n# Normal 2 Distribution\nmu_2 = 5\nstd_2 = 2\nr_n2 = norm.pdf(x, loc = mu_2, scale = std_2)\n\n### computing mixture model\nmixture_model = (0.6 * r_n1) + (0.4 * r_n2)\n\n# Plotting the mixture models\nfig, ax = plt.subplots(1, 1)\nsns.lineplot(x=x, y=mixture_model)\nplt.xlabel('Data')\nplt.ylabel('Density')\nplt.title('Mixture of two Gaussians')\nplt.show()\n\n\n\n\n\n\n\nFigure 56.9: Bimodal Mixture of Gaussians\n\n\n\n\nplt.close()\n\n\n\n\n\n\n56.4.2 Example of a Uni-modal and skewed mixture of Gaussians\n\nf(x) = 0.55 \\times \\mathcal{N}(0, 2) + 0.45 \\times \\mathcal{N}(3, 4) \\qquad\n\\tag{56.7}\n\nRpython\n\n\n\nx = seq(-5, 12, length=100)\ny = 0.55*dnorm(x, 0, sqrt(2)) + 0.45*dnorm(x, 3, 4)\npar(mar=c(4,4,1,1)+0.1)\nplot(x, y, type=\"l\", ylab=\"Density\", las=1, lwd=2)\n\n\n\n\n\n\n\nFigure 56.10: Uni-modal Skewed Mixture of Gaussians\n\n\n\n\n\n\n\n\n# Values to sample\nx = np.linspace(-5, 12.0, num = 100)\n# Normal 1 distribution\nmu_1 = 0\nvar_1 = 2\nr_n1 = norm.pdf(loc = mu_1, scale = np.sqrt(var_1), x = x)\n# Normal 2 Distribution\nmu_2 = 3\nvar_2 = 16\nr_n2 = norm.pdf(loc = mu_2, scale = np.sqrt(var_2), x = x)\n\n### computing mixture model\nmixture_model = (0.55 * r_n1) + (0.45 * r_n2)\n\n# Plotting the mixture models\nfig, ax = plt.subplots(1, 1)\nsns.lineplot(x=x, y=mixture_model)\nplt.xlabel('Data')\nplt.ylabel('Density')\nplt.title('Mixture of two Gaussians Skewed')\nplt.show()\n\n\n\n\n\n\n\nFigure 56.11: Uni-modal Skewed Mixture of Gaussians\n\n\n\n\nplt.close()\n\n\n\n\n\n\n56.4.3 Example of a Uni-modal, symmetric and heavy tailed mixture of Gaussians\n\nf(x) = 0.40 \\times \\mathcal{N}(0, 2) + 0.40 \\times \\mathcal{N}(0, 4) + 0.20 \\times \\mathcal{N}(0, 5) \\qquad\n\\tag{56.8}\n\nRpython\n\n\n\n# simulate Mixture of univariate Gaussians, unimodal heavy tail\n\nx = seq(-12, 12, length=100)\ny = 0.40 * dnorm(x, 0, sqrt(2)) + \n    0.40 * dnorm(x, 0, sqrt(16)) + \n    0.20 * dnorm(x, 0, sqrt(20))\nz = dnorm(x, 0, sqrt(0.4*2 + 0.4*16 + 0.2*20))\n\n\npar(mar=c(4,4,1,1)+0.1)\nplot(x, y, type=\"l\", ylab=\"Density\", las=1, lwd=2)\nlines(x, z, lty=2, lwd=2)\nlegend(2, 0.16, c(\"Mixture\",\"Gaussian\"), lty=c(1,2), bty=\"n\", cex=0.77, lwd=c(2,2))\n\n\n\n\n\n\n\nFigure 56.12: Unimodal Heavy Tailed Mixture of Gaussians\n\n\n\n\n\n\n\n\n# Values to sample\nx = np.linspace(-12.0, 12.0, num = 100)\n# Normal 1 distribution\nmu_1 = 0\nvar_1 = 2\nr_n1 = norm.pdf(loc = mu_1, scale = np.sqrt(var_1), x = x)\n# Normal 2 Distribution\nmu_2 = 0\nvar_2 = 16\nr_n2 = norm.pdf(loc = mu_2, scale = np.sqrt(var_2), x = x)\n# Normal 3 Distribution\nmu_3 = 0\nvar_3 = 20\nr_n3 = norm.pdf(loc = mu_3, scale = np.sqrt(var_3), x = x)\n\n### computing mixture model\ny = (0.4 * r_n1) + (0.4 * r_n2) + (0.2 * r_n3)\nz = norm.pdf(loc = 0, scale = np.sqrt(0.4 * 2 + 0.4 * 16 + 0.2 * 20), x = x)\n\n\n# Plotting the mixture models\nfig, ax = plt.subplots(1, 1)\nsns.lineplot(x=x, y=y)\nax.plot(x, z, '--')\nplt.xlabel('Data')\nplt.ylabel('Density')\nplt.title('Mixture of gaussians heavy tailed')\nplt.legend(['Mixture', 'Gaussian'])\nplt.show()\n\n\n\n\n\n\n\nFigure 56.13: Unimodal Heavy Tailed Mixture of Gaussians\n\n\n\n\nplt.close()",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html#zero-inflated-mixtures",
    "href": "C3-L01.html#zero-inflated-mixtures",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "56.5 Zero Inflated Mixtures",
    "text": "56.5 Zero Inflated Mixtures\n\nZero inflated distributions are useful for modeling data with excess zeros (often in term of count data).\nWe learned in course 1 & 2 that the negative binomial or equivalent beta are zero inflated in comparison to the Poisson distribution.\nToday we see how we use mixture models by adding a point mass at zero to the distribution.\nExample from biology is the number of eggs in a nest.\nExample from insurance is the number of claims in a year.\nExample from survival analysis is the time to event data with a lot of censoring.\n\nNote there are two approaches to zero inflation:\n\none step models like the negative binomial.\nhurdle models - two step models where we first model the zero inflation and then the count data - This corresponds to the hierarchical representation of the mixture model.\n\n\n56.5.1 Example of a Zero-inflated log Gaussian distribution\nThis is a mixture of a point mass at zero and a log Gaussian distribution. This corresponds to the example where we have a light bulb factory and we want to model the time to failure of the light bulbs. We know that for the defective light bulbs, the time to failure is zero. For the non-defective light bulbs, the time to failure is log normally distributed with mean 1.5 and standard deviation 0.5\n\nf(x) = 0.3 \\times \\mathbb{I}_{x\\ge0} + 0.7 \\times \\mathcal{LN}(1.5, 0.5) \\qquad\n\\tag{56.9}\n\nRpython\n\n\n\n## The ZILN model \nx = seq(-2, 15, length=1000)\ny = plnorm(x, 1.5, 0.5)\nz = 0.3*as.numeric(x&gt;=0) + (1-0.3)*y\n\n\n## The plot\npar(mar=c(4,4,1,1)+0.1)\nplot(x, y, type=\"l\", las=1, lty=2, xlab=\"x\", \n     ylab=\"Cumulative distribution Function\", lwd=2)\nlines(x, z, lty=1, lwd=2)\nlegend(4, 0.45, c(\"Zero infla. log Gaussian\",\"log Gaussian\"), \n     lty=c(1,2), bty=\"n\", lwd=c(2,2))\n\n\n\n\n\n\n\nFigure 56.14: Zero inflated log Gaussian\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import lognorm\n\n# Zero-inflated continuous distribution\n# Values to sample\nx = np.linspace(-2.0, 15.0, num = 200)\n# See for parameterization\ny = lognorm.pdf(loc = 0, scale = np.exp(1.5), s = 0.5, x = x)\n\n# Point mass vector\np_mass = np.zeros(len(x))\np_mass[x &gt;= 0] = 1\nz = 0.3 * p_mass + (1 - 0.3) * y\n\n\n## title: Zero inflated negative binomial distribution\n\n# Plotting the mixture models\nfig, ax = plt.subplots(1, 1)\nax.plot(x, y)\nax.plot(x, z, '--')\nplt.xlabel('X')\nplt.ylabel('Cumulative distribution function')\nplt.title('Zero-inflated continuous distribution')\nplt.legend(['Log gaussian', 'Zero infla. Log gaussian'])\nplt.show()\n\n\n\n\n\n\n\nFigure 56.15: Zero inflated log Gaussian\n\n\n\n\n\n\n\n\n\n\n56.5.2 Example of a zero-inflated negative binomial distribution\n\nf(x) = 0.2 \\times \\mathbb{I}_{x=0} + 0.8 \\times NB(8, 0.6) \\qquad\n\\tag{56.10}\n\nRpython\n\n\n\n## title: Zero inflated negative binomial distribution\nx = seq(0, 15)\ny = dnbinom(x, 8, 0.6)\nz = 0.2*c(1,rep(0,length(x)-1)) + (1-0.2)*y\npar(mfrow=c(2,1))\npar(mar=c(4,4,2,2)+0.1)\nbarplot(y, names.arg=x, las=1, xlab = \"x\", ylab=\"Probability\", \n        border=NA, main=\"Negative Binomial\")\npar(mar=c(4,4,1,1)+0.1)\nbarplot(z, names.arg=x, las=1, xlab = \"x\", ylab=\"Probability\", \n        border=NA, main=\"Zero-inflated Negative Binomial\")\n\n\n\n\n\n\n\nFigure 56.16: Zero inflated negative binomial\n\n\n\n\n\n\n\n\n## title: Zero inflated negative binomial distribution\nfrom scipy.stats import nbinom\nimport seaborn as sns\n\n# Values to sample\nx = np.arange(0, 16)\ny = nbinom.pmf(x, n = 8, p = 0.6)\n\n# Plotting the negative binomial model\nfig, ax = plt.subplots(1, 1)\n\nsns.barplot(x=x, y=y, color = 'blue')\n\nplt.title('Negative Binomial')\nplt.xlabel('Count')\nplt.ylabel('PMF')\nplt.show()\n\n\n\n\n\n\n\nFigure 56.17: Zero inflated negative binomial\n\n\n\n\n# Point mass vector\np_mass = np.zeros(len(x))\np_mass[0] = 1\nz = 0.2 * p_mass + (1 - 0.2) * y\n\n# Plotting the zero-inflated model\nfig, ax = plt.subplots(1, 1)\nsns.barplot(x=x, y=z, color = 'blue')\nplt.title('Zero-Inflated model')\nplt.xlabel('Count')\nplt.ylabel('PMF')\nplt.show()\n\n\n\n\n\n\n\nFigure 56.18: Zero inflated negative binomial",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html#hierarchical-representations",
    "href": "C3-L01.html#hierarchical-representations",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "56.6 Hierarchical representations",
    "text": "56.6 Hierarchical representations\n\n\n\n\n\n\n\nFigure 56.19: Hierarchical representation of a mixture\n\n\n\n\n\n\n\n\nFigure 56.20: simulation of a mixture\n\n\n\nRecall that the cumulative distribution function of a mixture takes the form Equation 56.1, where G_k(x) is the cumulative distribution function of the k-th component of the mixture.\nWe can use a RV for each component and introduce an indicator RV for the component selector C_i to select the component from which we will sample. This results in a hierarchical representation of the mixture model.\n\nX \\mid c \\sim g_c(x) \\qquad \\mathbb{P}r(c=k) = \\omega_k \\qquad\n\\tag{56.11}\nwhere C is a categorical random variable with K categories, and G_k(x \\mid C=k) is the cumulative distribution function of the k-th component of the mixture given that we have selected the k-th component.\nThis allows us to write the cumulative distribution function of the mixture as a weighted sum of the cumulative distribution functions of the components\n\n\\mathbb{P}r(x) = \\sum^K_{k=1} \\mathbb{P}r(x \\mid C=k) \\cdot \\mathbb{P}r(C=k) = \\sum^K_{k=1} g_k(x) \\cdot \\omega_k \\qquad\n\\tag{56.12}\nwhere g_k(x) is the cumulative distribution function of the k-th component of the mixture.\n\n56.6.1 Sample code for simulating from a Mixture Model\n\nRpython\n\n\n\n# Generate n observations from a mixture of two Gaussian distributions\nn     = 50           # required sample size\nw     = c(0.6, 0.4)  # mixture weights\nmu    = c(0, 5)      # list of means\nsigma = c(1, 2)      # list of sds\ncc    = sample(1:2, n, replace=T, prob=w) # sample for the component selector\nx     = rnorm(n, mu[cc], sigma[cc]) # sample the selected component\n\n\n# Plot f(x) along with the observations just sampled\nxx = seq(-5, 12, length=200)\nyy = w[1] * dnorm(xx, mu[1], sigma[1]) + w[2] * dnorm(xx, mu[2], sigma[2])\npar(mar=c(4,4,1,1)+0.1)\nplot(xx, yy, type=\"l\", ylab=\"Density\", xlab=\"x\", las=1, lwd=2)\npoints(x, y=rep(0,n), pch=1, col=cc)\n\n\n\n\n\n\n\nFigure 56.21: Mixture of two Gaussians\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import norm\n\nn=50           # required sample size\nw=[0.6, 0.4]  # mixture weights\nmu=[0, 5]      # list of means\nsigma=[1, 2]  # list of sds\ncc = np.random.choice([0, 1], size=n, p=w) # sample for the component selector\n# sample the selected component\nx = np.array([np.random.normal(mu[i], sigma[i]) for i in cc])\n\n\n# Plot f(x) along with the observations just sampled\nxx = np.linspace(-5, 12, num=200)\nyy = w[0]*norm.pdf(loc=mu[0], scale=sigma[0], x=xx) + \\\n     w[1]*norm.pdf(loc=mu[1], scale=sigma[1], x=xx)\nplt.plot(xx, yy, label='Mixture of Gaussians')\nplt.scatter(x, np.zeros(n), c=cc, label='Sampled data')\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title('Mixture of Gaussians')\nplt.legend()\nplt.show() \n\n\n\n\n\n\n\nFigure 56.22: Mixture of two Gaussians",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html#the-likelihood-function",
    "href": "C3-L01.html#the-likelihood-function",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "56.7 The Likelihood function",
    "text": "56.7 The Likelihood function\n\n\n\n\n\n\n\nFigure 56.23: observed data likelihood\n\n\n\n\n\n\n\n\nFigure 56.24: complete data likelihood\n\n\n\n\nwe are now moving on to inferring the parameters of the mixture model from the observed data.\nwe can estimate these using the maximum likelihood estimation or with Bayesian estimation.\nin both cases we will need to compute the likelihood of the observed data.\nthere are two types of likelihoods:\n\nthe observed data likelihood is the probability of observing the data given the parameters of the model.\nthe complete data likelihood is the probability of observing the data and the latent variables given the parameters of the model.",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01.html#parameter-identifiability",
    "href": "C3-L01.html#parameter-identifiability",
    "title": "56  Definitions of Mixture Models - M1L1",
    "section": "56.8 Parameter identifiability",
    "text": "56.8 Parameter identifiability\n\n\n\n\n\n\n\nFigure 56.25: Identifiability - Label switching\n\n\n\n\n\n\n\n\nFigure 56.26: identifiability - split weights\n\n\n\n\n\n\n\n\nFigure 56.27: identifiability - zero weights\n\n\n\n\nA probability model is identifiable if and only if different values of the parameters generate different probability distributions of the observable variables.\nOne challenge involved in working with mixture models is that they are not fully identifiable.\nThe problem is that different representations exists for the same mixture.\nQuestion: Is there a “Canonical representation” which fixes this, essentially a convention like:\n1. picking the representation with the least components (no zero weights)\n2. ordered with descending w_i\n\n56.8.1 Label switching\nThe labels used to distinguish the components in the mixture are not identifiable. The literature sometimes refers to this type of lack of identifiability as the label switching “problem”. Whether label switching is an actual problem or not depends on the computational algorithm being used to fit the model, and the task we are attempting to complete in any particular case. For example, label switching tends to not be an issue for the purpose of density estimation or classification problems, but it can lead to serious difficulties in clustering problems.",
    "crumbs": [
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>Definitions of Mixture Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C3-L01-Ex1-Basic-Definitions.html",
    "href": "C3-L01-Ex1-Basic-Definitions.html",
    "title": "57  Basic Concepts of Mixture Models",
    "section": "",
    "text": "Exercise 57.1  \n\nWhich one of the following is not the density of a well defined mixture distribution with support on x \\ge 1 x\n\n\nf(x) = \\frac{1}{2}\\ e^{-x} + \\frac{1}{2} \\frac{1}{\\sqrt{2 \\pi}} \\exp^{-0.5x^2}\nf(x) = \\frac{1}{2}\\ e^{-x} + \\frac{1}{4}\\ e^{-x}\nf(x) = \\frac{1}{2}\\ e^{-x} + \\frac{1}{2}\\ e^{- 0.5 x}\n\nHint: the key here is to write the mixtures with the weights and the normalization constant clearly separated. This reveals that the last one is not a well defined mixture distribution because the weights do not sum to 1 while the the second answer is!\n\n\nExercise 57.2  \n\nWhat is the expected value of a random variable X whose distribution is a mixture of Poisson distributions of the form\n\n\nf(x) = 0.3 \\frac{2^x e^{-2}}{x!}  + 0.45 \\frac{2^x e^{-3}}{x!} + 0.25 \\frac{.5^x e^{-0.5}}{x!}\n\\tag{57.1}\n\n\n\n\n\n\nTipSolution:\n\n\n\n\n\n\nE(X) = 0.3 \\cdot 2 + 0.45 \\cdot 3 + 0.25 \\cdot 0.5 = 2.075\n\n\n\n\n\n\nExercise 57.3  \n\nWhat is the variance of an RV X whose distribution is a mixture of Poisson distributions of the form Equation 57.1 ?\n\n\nE(X^2) = 0.3 \\cdot (2+2^2 ) + 0.45 \\cdot (3+3^2) + 0.25 \\cdot (0.5 + 0.5^2)= 7.3875\n\n\n\n\n\n\n\nTipSolution:\n\n\n\n\n\n\nVar(X) = E(X^2) - E(X)^2 = 7.3875 - (2.075)^2 = 3.081875",
    "crumbs": [
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Basic Concepts of Mixture Models</span>"
    ]
  },
  {
    "objectID": "C3-L01-Ex2-Gaussian-mixtures.html",
    "href": "C3-L01-Ex2-Gaussian-mixtures.html",
    "title": "58  Mixtures of Gaussians",
    "section": "",
    "text": "Exercise 58.1 True or False? A scale mixture of normals with density \nf(x) = \\sum_{k=0}^{K} \\omega_k \\frac{1}{\\sqrt{2 \\pi}\\sigma_k} e^{-\\frac{x^2}{\\sigma_k^2}} \\qquad\n\\tag{58.1}\nis always unimodal?\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTrue\nWe can see from the functional form that this is a sum of K Gaussian densities, with maximum at 0 and monotonically decreasing everywhere else.\n\n\n\n\n\nExercise 58.2 True or False? A scale mixture of normals with density as in Equation 58.1 is always symmetric?\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTrue\nIf we inspect the functional form we can see that since x is squared in this function, it is symmetric around 0.",
    "crumbs": [
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Mixtures of Gaussians</span>"
    ]
  },
  {
    "objectID": "C3-L01-Ex3-Zero-Inflated-distribution.html",
    "href": "C3-L01-Ex3-Zero-Inflated-distribution.html",
    "title": "59  Zero inflated distributions",
    "section": "",
    "text": "Exercise 59.1 Consider a zero-inflated mixture that involves a point mass at 0 with weight 0.3 and an exponential distribution with mean 1 and weight 0.7. What is the mean of this mixture?\n\n1\n0.7\n0.5\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n0.7\n\n\n\n\n\nExercise 59.2 Consider a zero-inflated mixture that involves a point mass at 0 with weight 0.2 and an exponential distribution with mean 10000 and weight 0.8. If this mixture is used to represent the number of hours a light bulb works between the time it is installed and the time it fails, what is the probability that the bulb was defective when coming out of the factory and does not work when you install it?\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n0.2",
    "crumbs": [
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>Zero inflated distributions</span>"
    ]
  },
  {
    "objectID": "C3-L01-Ex4-Def-mixture-models.html",
    "href": "C3-L01-Ex4-Def-mixture-models.html",
    "title": "60  Definition of Mixture Models",
    "section": "",
    "text": "Exercise 60.1 Which one of the following is not the density of a well defined mixture distribution with support on the positive integers:\n\nf(x) = \\frac{1}{2} \\frac{e^{-1}}{x!} + \\frac{1}{2} \\frac{e^{-1}}{x!}\nf(x) = 0.5 \\times  \\frac{2^x e^{-2}}{x!} + 0.5 \\times  \\frac{3^x e^{-3}}{x!}\nf(x) = 0.45 \\times  \\frac{2^x e^{-1}}{x!} + 0.55 \\times  \\frac{3^x e^{-3}}{x!}\n\n\n\n\n\n\n\nTipHint\n\n\n\n\n\n\nPOISSON(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\n\n\\sum_{x=0}^{\\infty} \\frac{2^x e^{-1}}{x!} = e^{-1} \\sum_{x=0}^{\\infty} \\frac{2^x}{x!} = e^{-1} e^{2} = e^{1}\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nthe last one is not a well defined mixture distribution because exponent is has a lambda of 1 but should be 2 to be a poisson distribution.\nActually the answer is not 100% kosher as we have not demonstrated that it is not a well defined mixture distribution - we need to show it doesn’t sum to 1 or is not a valid distribution.\nI have given this fact in the hint above.\n\n\n\n\n\nExercise 60.2 Consider a zero-inflated mixture that involves a point mass at 0 with weight 0.2, a Gamma distribution with mean 1, variance 2 and weight 0.5, and another Gamma distribution with mean 2, variance 4 and weight 0.3. What is the mean of this mixture?\n\n2.5\n1.1\n1.6\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nE(X) = 0.2 \\cdot 0 + 0.5 \\cdot 1 + 0.3 \\cdot 2 = 1.1\n\n\n\n\n\n\nExercise 60.3 Consider a zero-inflated mixture that involves a point mass at 0 with weight 0.2, a Gamma distribution with mean 1, variance 2 and weight 0.5, and another Gamma distribution with mean 2, variance 4 and weight 0.3. What is the variance of this mixture?\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\\begin{aligned}\nV[X] &= E[X^2] - E[X]^2\n\\\\ &= (\\sum_{i=1}^{n} w_i Var_{g_k}[X^2_i] + E_{g_k}[X]^2) - E[X]^2\n\\\\ E[X^2] &= 0.2 \\cdot 0^2 + 0.5 \\cdot (1 + 1^2) + 0.3 \\cdot (2 + 4) = 3.9\n\\\\ V[X] &= 3.9 - 1.1^2 = 2.69\n\\end{aligned}\n\n\n\n\n\n\nExercise 60.4 True or False: A mixture of Gaussians of the form\n\nf(x) = 0.3 \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{x^2}{2}} + 0.7 \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{(x-4)^2}{2}}\n\nhas a bimodal density.\n\n\n\n\n\n\nNoteHint\n\n\n\n\n\nAccording to the 68-95-99.7 rule, if two gaussian have means that are separated by distance greater than 1 sd apart, then the mixture should appear bimodal.\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTrue. The two gaussians are separated by 4 standard deviations, so the mixture will be bimodal. We can verify this by plotting the density of the mixture.\n\nx = seq(-3, 7, length=100)\ny = 0.3*dnorm(x, 0, 1) + 0.7*dnorm(x, 4, 1)\npar(mar=c(4,4,1,1)+0.1)\nplot(x, y, type=\"l\", ylab=\"Density\", las=1, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 60.5 True or False: Consider a location mixture of normals \nf(x) = \\sum_{k=1}^{K} \\omega_k \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x - \\mu_k)^2}{2 \\sigma^2}}\n\nThe following 3 constraints make all parameters fully identifiable:\n\nThe means \\mu_1,\\ldots,\\mu_k are all different.\nThe weights \\omega_1,\\ldots,\\omega_k are all &gt; 0\nThe components are ordered based on the values of their means, i.e., the component with the smallest \\mu_k is labeled component 1, the one with the second smallest \\mu_k is labeled component 2, etc.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTrue.\nThe first and second address the number of components, while the last deals with label switching.",
    "crumbs": [
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>Definition of Mixture Models</span>"
    ]
  },
  {
    "objectID": "C3-L02.html",
    "href": "C3-L02.html",
    "title": "61  Likelihood functions for Mixture Models - M1L2",
    "section": "",
    "text": "61.1 Hierarchical representations 🎥\nRecall that the cumulative distribution function of a mixture takes the form Equation 56.1, where G_k(x) is the cumulative distribution function of the k-th component of the mixture.\nWe can use a RV for each component and introduce an indicator RV for the component selector C_i to select the component from which we will sample. This results in a hierarchical representation of the mixture model.\nX \\mid c \\sim g_c(x) \\qquad \\mathbb{P}r(c=k) = \\omega_k \\qquad\n\\tag{61.1}\nwhere C is a categorical random variable with K categories, and G_k(x \\mid C=k) is the cumulative distribution function of the k-th component of the mixture given that we have selected the k-th component.\nThis allows us to write the cumulative distribution function of the mixture as a weighted sum of the cumulative distribution functions of the components\n\\mathbb{P}r(x) = \\sum^K_{k=1} \\mathbb{P}r(x \\mid C=k) \\cdot \\mathbb{P}r(C=k) = \\sum^K_{k=1} g_k(x) \\cdot \\omega_k \\qquad\n\\tag{61.2}\nwhere g_k(x) is the cumulative distribution function of the k-th component of the mixture.",
    "crumbs": [
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Likelihood functions for Mixture Models - M1L2</span>"
    ]
  },
  {
    "objectID": "C3-L02.html#sec-hierarchical-representation",
    "href": "C3-L02.html#sec-hierarchical-representation",
    "title": "61  Likelihood functions for Mixture Models - M1L2",
    "section": "",
    "text": "Figure 61.1: Hierarchical representation of a mixture\n\n\n\n\n\n\n\n\nFigure 61.2: simulation of a mixture\n\n\n\n\n\n\n\n\n\n\n\n61.1.1 Sample code for simulating from a Mixture Model\n\nRpython\n\n\n\n# Generate n observations from a mixture of two Gaussian distributions\nn     = 50           # required sample size\nw     = c(0.6, 0.4)  # mixture weights\nmu    = c(0, 5)      # list of means\nsigma = c(1, 2)      # list of sds\ncc    = sample(1:2, n, replace=T, prob=w) # sample for the component selector\nx     = rnorm(n, mu[cc], sigma[cc]) # sample the selected component\n\n\n# Plot f(x) along with the observations just sampled\nxx = seq(-5, 12, length=200)\nyy = w[1] * dnorm(xx, mu[1], sigma[1]) + w[2] * dnorm(xx, mu[2], sigma[2])\npar(mar=c(4,4,1,1)+0.1)\nplot(xx, yy, type=\"l\", ylab=\"Density\", xlab=\"x\", las=1, lwd=2)\npoints(x, y=rep(0,n), pch=1, col=cc)\n\n\n\n\n\n\n\nFigure 61.3: Mixture of two Gaussians\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom scipy.stats import norm\n\nn=50           # required sample size\nw=[0.6, 0.4]  # mixture weights\nmu=[0, 5]      # list of means\nsigma=[1, 2]  # list of sds\ncc = np.random.choice([0, 1], size=n, p=w) # sample for the component selector\n# sample the selected component\nx = np.array([np.random.normal(mu[i], sigma[i]) for i in cc])\n\n\n# Plot f(x) along with the observations just sampled\nxx = np.linspace(-5, 12, num=200)\nyy = w[0]*norm.pdf(loc=mu[0], scale=sigma[0], x=xx) + \\\n     w[1]*norm.pdf(loc=mu[1], scale=sigma[1], x=xx)\nplt.plot(xx, yy, label='Mixture of Gaussians')\nplt.scatter(x, np.zeros(n), c=cc, label='Sampled data')\nplt.xlabel('x')\nplt.ylabel('Density')\nplt.title('Mixture of Gaussians')\nplt.legend()\nplt.show() \n\n\n\n\n\n\n\nFigure 61.4: Mixture of two Gaussians",
    "crumbs": [
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Likelihood functions for Mixture Models - M1L2</span>"
    ]
  },
  {
    "objectID": "C3-L02.html#the-likelihood-function",
    "href": "C3-L02.html#the-likelihood-function",
    "title": "61  Likelihood functions for Mixture Models - M1L2",
    "section": "61.2 The Likelihood function",
    "text": "61.2 The Likelihood function\n\n\n\n\n\n\n\nFigure 61.5: observed data likelihood\n\n\n\n\n\n\n\n\nFigure 61.6: complete data likelihood\n\n\n\n\nwe are now moving on to inferring the parameters of the mixture model from the observed data.\nwe can estimate these using the maximum likelihood estimation or with Bayesian estimation.\nin both cases we will need to compute the likelihood of the observed data.\nthere are two types of likelihoods:\n\nthe observed data likelihood is the probability of observing the data given the parameters of the model.\nthe complete data likelihood is the probability of observing the data and the latent variables given the parameters of the model.",
    "crumbs": [
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Likelihood functions for Mixture Models - M1L2</span>"
    ]
  },
  {
    "objectID": "C3-L02.html#parameter-identifiability",
    "href": "C3-L02.html#parameter-identifiability",
    "title": "61  Likelihood functions for Mixture Models - M1L2",
    "section": "61.3 Parameter identifiability",
    "text": "61.3 Parameter identifiability\n\n\n\n\n\n\n\nFigure 61.7: Identifiability - Label switching\n\n\n\n\n\n\n\n\nFigure 61.8: identifiability - split weights\n\n\n\n\n\n\n\n\nFigure 61.9: identifiability - zero weights\n\n\n\n\nA probability model is identifiable if and only if different values of the parameters generate different probability distributions of the observable variables.\nOne challenge involved in working with mixture models is that they are not fully identifiable.\nThe problem is that different representations exists for the same mixture.\nQuestion: Is there a “Canonical representation” which fixes this, essentially a convention like:\n1. picking the representation with the least components (no zero weights)\n2. ordered with descending w_i\n\n61.3.1 Label switching\nThe labels used to distinguish the components in the mixture are not identifiable. The literature sometimes refers to this type of lack of identifiability as the label switching “problem”. Whether label switching is an actual problem or not depends on the computational algorithm being used to fit the model, and the task we are attempting to complete in any particular case. For example, label switching tends to not be an issue for the purpose of density estimation or classification problems, but it can lead to serious difficulties in clustering problems.",
    "crumbs": [
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>Likelihood functions for Mixture Models - M1L2</span>"
    ]
  },
  {
    "objectID": "C3-L02-Ex1.html",
    "href": "C3-L02-Ex1.html",
    "title": "62  Homework The Likelihood function - M1L2HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Homework The Likelihood function - M1L2HW1</span>"
    ]
  },
  {
    "objectID": "C3-L02-Ex2.html",
    "href": "C3-L02-Ex2.html",
    "title": "63  Homework Identifiability - M1L2HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Homework Identifiability - M1L2HW2</span>"
    ]
  },
  {
    "objectID": "C3-L02-Ex3.html",
    "href": "C3-L02-Ex3.html",
    "title": "64  Homework The likelihood function M1L2HW3",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Homework The likelihood function M1L2HW3</span>"
    ]
  },
  {
    "objectID": "C3-L02-Ex4.html",
    "href": "C3-L02-Ex4.html",
    "title": "65  Homework on simulating from a Poisson Mixture Model - M1L2HW4",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Homework on simulating from a Poisson Mixture Model - M1L2HW4</span>"
    ]
  },
  {
    "objectID": "C3-L02-Ex5.html",
    "href": "C3-L02-Ex5.html",
    "title": "66  HW - Simulation of Poisson mixture model - M1L2HW5",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>HW - Simulation of Poisson mixture model - M1L2HW5</span>"
    ]
  },
  {
    "objectID": "C3-L02-Ex6.html",
    "href": "C3-L02-Ex6.html",
    "title": "67  Homework Sim mixture of exponential distributions - M1L2HW6",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Homework Sim mixture of exponential distributions - M1L2HW6</span>"
    ]
  },
  {
    "objectID": "C3-L03.html",
    "href": "C3-L03.html",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "",
    "text": "68.1 EM algorithms for general mixtures 🎥\nMaximum likelihood estimation is the frequentist approach to estimate the parameters of statistical models. However, attempting to obtain maximum likelihood estimates (MLEs) \\hat{\\omega} and \\hat{\\theta} by directly maximizing the observed-data likelihood:\n\\mathcal{L}(\\omega,\\theta) = \\arg \\max_{\\omega,\\theta}\n\\prod_{i=1}^{n} \\sum_{k=1}^{K} \\omega_k g_k(x_i|\\theta_k)\n\\tag{68.1}\nisn’t feasible, as it is a non-convex optimization problem.\nUsing numerical optimization methods, such as the Newton-Raphson algorithm, can be challenging due when there are many components in the mixture.\nIt worthwhile mentioning that MLE is more of a frequentist approach, as it provides point estimates of the parameters rather than a distributional view. In contrast, Bayesian methods we will consider later provide a full posterior distribution of the parameters, which is more informative and allows for uncertainty quantification.\nThe EM algorithm comes up a lot in NLP and other fields so it is worthwhile to understand it the way we will do so in the course.\nIt also important that the EM algorithm we use for mixture models is from the 1970s and is not the same as the general EM algorithm. c.f. (Dempster, Laird, and Rubin 1977)\nThe goal of the EM algorithm is to find the parameters \\omega and \\theta for which the observed-data likelihood is maximized. We start with the complete-data log-likelihood Q function and then use it to construct maximum likelihood estimators for the parameters we are interested in, these are primarily the weights \\omega and the parameters \\theta of the distributional components.\nwe can express the complete-data log-likelihood as:\nL(\\boldsymbol \\theta,\\boldsymbol \\omega) = \\prod_{i=1}^{N} \\sum_{k=1}^{K} \\omega_k g_k(x_i \\mid \\theta_k)\n\\tag{68.2}\nMLE’s \\hat{\\theta} and \\hat{\\omega} are defined\n(\\boldsymbol \\theta,\\boldsymbol \\omega) \\stackrel{.}{=} \\arg \\max_{\\boldsymbol \\theta,\\boldsymbol \\omega} L(\\boldsymbol \\theta,\\boldsymbol \\omega)\nThe EM algorithm is iterative and consists of two steps: the E-step and the M-step. The E-step computes the expected value of the complete-data log-likelihood given the observed data and the current parameter estimates, while the M-step maximizes this expected log-likelihood with respect to the parameters. However before we start these steps we need to set initial values for the parameters.",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#em-algorithms-for-mixture-models",
    "href": "C3-L03.html#em-algorithms-for-mixture-models",
    "title": "68  M2L3 - The EM algorithm for Mixture models",
    "section": "69.1 EM algorithms for Mixture Models",
    "text": "69.1 EM algorithms for Mixture Models\n\n\n\n\n\n\n\nFigure 69.1: EM - Challenge\n\n\n\n\n\n\n\n\nFigure 69.2: EM - Steps\n\n\n\n\n\n\n\n\nFigure 69.3: EM - Deep Dive\n\n\n\n\nEM algorithm comes up a lot in NLP and other fields so it is worthwhile to understand it the way we will do so in the course.\nIt also important that the EM algorithm we use for mixture models is from the 1970s and is not the same as the general EM algorithm. c.f. (Dempster, Laird, and Rubin 1977)\nThe EM algorithm is iterative and consists of two steps: the E-step and the M-step. The E-step computes the expected value of the complete-data log-likelihood given the observed data and the current parameter estimates, while the M-step maximizes this expected log-likelihood with respect to the parameters. However before we start these steps we need to set initial values for the parameters.\nE step: Set\n\nQ(\\omega,\\theta \\mid \\omega^{(t)}, \\theta^{(t)},x) = E_{c \\mid \\omega^{(t)},\\theta^{(t)}, x} \\left[ \\log \\mathbb{P}r(x,c \\mid \\omega,\\theta) \\right]\n\\tag{69.2}\nWhere c is the latent variable indicating the component from which each observation was generated, \\omega are the weights, and \\theta are the parameters of the Gaussian components (means and standard deviations).\nM step: Set\n\n\\hat{\\omega}^{(t+1)},\\hat{\\theta}^{(t+1)} = \\arg \\max_{\\omega,\\theta} Q(\\omega,\\theta \\mid \\hat{\\omega}^{(t)}, \\hat{\\theta}^{(t)},y)\n\\tag{69.3}\nwhere \\hat{\\omega}^{(t)} and \\hat{\\theta}^{(t)} are the current estimates of the parameters, and y is the observed data.\nThese two steps are repeated until convergence, which is typically defined as the change in the full-data log-likelihood Q function being below a certain threshold.\nA key point is that if we condition each component independently on the \\omega, \\theta, x we can write\n\n\\mathbb{P}r(c_i=k \\mid \\omega, \\theta, x_i) = \\frac{\\omega_k g_k(x_i \\mid \\theta_k)}{\\sum_{j=1}^{K} \\omega_j g_j(x_i \\mid \\theta_j)}= v_{ik}(\\omega, \\theta)\n\nwhere the value of v_{ik} is interpreted as the probability that the i-th observation comes from the k-th component of the mixture assuming the population parameters \\omega and \\theta.",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>M2L3 - The EM algorithm for Mixture models</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#em-for-general-mixture",
    "href": "C3-L03.html#em-for-general-mixture",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.2 EM for general Mixture",
    "text": "68.2 EM for general Mixture",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#em-for-location-mixture-of-gaussians",
    "href": "C3-L03.html#em-for-location-mixture-of-gaussians",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.2 EM for location Mixture of Gaussians",
    "text": "68.2 EM for location Mixture of Gaussians\n\n\n\n\n\n\n\nFigure 68.4: the responsibility\n\n\n\n\n\n\n\n\nFigure 68.5: the derivative of Q wrt to w\n\n\n\n\n\n\n\n\nFigure 68.6: the derivative of Q wrt to mu\n\n\n\n\n\n\n\n\nFigure 68.7: the derivative of Q wrt to sigma",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-em-example-1",
    "href": "C3-L03.html#sec-em-example-1",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.3 EM example 1 🎥",
    "text": "68.3 EM example 1 🎥\nThis video covers the code sample given in Listing 68.4 below. It is a simple implementation of the EM algorithm for fitting a 2-component Gaussian location mixture model to simulated data.\n\nThis code sample is both cool and awkward.\n\nIt is cool because it provides a step-by-step implementation of the EM algorithm, which is a fundamental concept in statistics and machine learning.\nIt is not broken in to functions lacks useful variables naming which would reduce the amounts of comments and cognitive load.\n\nHowever it does provide nice visualizations of the EM algorithm in action - particularly if run inside of RStudio IDE (as shown in the video).\nwould be interesting to make the number of components be drawn from a distribution rather than fixed at 2, then run the EM algorithm for multiple draws and pick the one with the best fit.\nLater on we learn about using BIC to select the number of components in a mixture model, which is a more principled approach than simply fixing the number of components at 2. However it stills seems that the number of components might be a RV even if it’s prior would be centred at the BIC estimate.",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-em-code-example-1",
    "href": "C3-L03.html#sec-em-code-example-1",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.4 Sample code for EM example 1 📖 ℛ",
    "text": "68.4 Sample code for EM example 1 📖 ℛ\n\n\n\n\nListing 68.1: EM algorithm for fitting a 2-component Gaussian location mixture\n\n\n#### The algorithm is tested using simulated data\n\n## Clear the environment and load required libraries\nrm(list=ls())\nset.seed(81196)    # So that results are reproducible (same simulated data every time)\n\n\n## Step 0 - Generate data from a mixture with 2 components:\n\n## Ground Truth parameters initialization\nKK         = 2          # Number of components of the mixture\nw.true     = 0.6        # GT True weights associated with the components\nmu.true    = rep(0, KK) # initialize the true means list\nmu.true[1] = 0   # GT mean for the first component\nmu.true[2] = 5   # GT mean for the second component\nsigma.true = 1   # GT standard deviation of all components\n\nn  = 120         # Number of synthetic samples to generate\n\n# simulate the latent variables for the component indicator function\ncc = sample(1:KK, n, replace=T, prob=c(w.true,1-w.true))\nx  = rep(0, n)   # initialize the data vector x (or load data)\n\nfor(i in 1:n){ # for each observation\n  # sample from a distribution with mean selected by component indicator\n  # the SD is the same for all components as this is a location mixture\n  x[i] = rnorm(1, mu.true[cc[i]], sigma.true)\n}\n\n# Plot the true density\npar(mfrow=c(1,1))\nxx.true = seq(-8,11,length=200)\nyy.true = w.true*dnorm(xx.true, mu.true[1], sigma.true) + \n          (1-w.true)*dnorm(xx.true, mu.true[2], sigma.true) \nplot(xx.true, yy.true, type=\"l\", xlab=\"x\", ylab=\"True density\", lwd=2)\npoints(x, rep(0,n), col=cc)\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 68.2: EM algorithm for fitting a 2-component Gaussian location mixture\n\n\n## Run the actual EM algorithm\n## Initialize the parameters\nw     = 1/2                         #Assign equal weight to each component to start with\nmu    = rnorm(KK, mean(x), sd(x))   #Random cluster centers randomly spread over the support of the data\nsigma = sd(x)                       #Initial standard deviation\n\n# Plot the initial guess for the density\nxx = seq(-8,11,length=200)\nyy = w*dnorm(xx, mu[1], sigma) + (1-w)*dnorm(xx, mu[2], sigma)\nplot(xx, yy, type=\"l\", ylim=c(0, max(yy)), xlab=\"x\", ylab=\"Initial density\")\npoints(x, rep(0,n), col=cc)\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 68.3: EM algorithm for fitting a 2-component Gaussian location mixture\n\n\ns  = 0\nsw = FALSE\nQQ = -Inf\nQQ.out = NULL\nepsilon = 10^(-5)\n\n\n##Checking convergence of the algorithm\nwhile(!sw){\n  ## E step\n  v = array(0, dim=c(n,KK))\n  v[,1] = log(w) + dnorm(x, mu[1], sigma, log=TRUE)    #Compute the log of the weights\n  v[,2] = log(1-w) + dnorm(x, mu[2], sigma, log=TRUE)  #Compute the log of the weights\n  for(i in 1:n){\n    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n  }\n  \n  ## M step\n  # Weights\n  w = mean(v[,1])\n  mu = rep(0, KK)\n  for(k in 1:KK){\n    for(i in 1:n){\n      mu[k]    = mu[k] + v[i,k]*x[i]\n    }\n    mu[k] = mu[k]/sum(v[,k])\n  }\n  # Standard deviations\n  sigma = 0\n  for(i in 1:n){\n    for(k in 1:KK){\n      sigma = sigma + v[i,k]*(x[i] - mu[k])^2\n    }\n  }\n  sigma = sqrt(sigma/sum(v))\n  \n  ##Check convergence\n  QQn = 0\n  for(i in 1:n){\n    QQn = QQn + v[i,1]*(log(w) + dnorm(x[i], mu[1], sigma, log=TRUE)) +\n                v[i,2]*(log(1-w) + dnorm(x[i], mu[2], sigma, log=TRUE))\n  }\n  if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n    sw=TRUE\n  }\n  QQ = QQn\n  QQ.out = c(QQ.out, QQ)\n  s = s + 1\n  print(paste(s, QQn))\n  \n  #Plot current estimate over data\n  layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))\n  par(mar=c(3.1,4.1,0.5,0.5))\n  plot(QQ.out[1:s],type=\"l\", xlim=c(1,max(10,s)), las=1, ylab=\"Q\", lwd=2)\n  \n  par(mar=c(5,4,1.5,0.5))\n  xx = seq(-8,11,length=200)\n  yy = w*dnorm(xx, mu[1], sigma) + (1-w)*dnorm(xx, mu[2], sigma)\n  plot(xx, yy, type=\"l\", ylim=c(0, max(c(yy,yy.true))), main=paste(\"s =\",s,\"   Q =\", round(QQ.out[s],4)), lwd=2, col=\"red\", lty=2, xlab=\"x\", ylab=\"Density\")\n  lines(xx.true, yy.true, lwd=2)\n  points(x, rep(0,n), col=cc)\n  legend(6,0.22,c(\"Truth\",\"Estimate\"),col=c(\"black\",\"red\"), lty=c(1,2))\n}\n\n\n\n\n[1] \"1 -343.425690465737\"\n\n\n\n\n\n\n\n\n\n[1] \"2 -339.993932553505\"\n\n\n\n\n\n\n\n\n\n[1] \"3 -333.742916535535\"\n\n\n\n\n\n\n\n\n\n[1] \"4 -322.087405606262\"\n\n\n\n\n\n\n\n\n\n[1] \"5 -299.927704463736\"\n\n\n\n\n\n\n\n\n\n[1] \"6 -265.515667629269\"\n\n\n\n\n\n\n\n\n\n[1] \"7 -246.004047691222\"\n\n\n\n\n\n\n\n\n\n[1] \"8 -243.982291955643\"\n\n\n\n\n\n\n\n\n\n[1] \"9 -243.880207718536\"\n\n\n\n\n\n\n\n\n\n[1] \"10 -243.873888447856\"\n\n\n\n\n\n\n\n\n\n[1] \"11 -243.873372520873\"\n\n\n\n\n\n\n\n\n\n\n\nListing 68.4: EM algorithm for fitting a 2-component Gaussian location mixture\n\n\n#Plot final estimate over data\nlayout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))\npar(mar=c(3.1,4.1,0.5,0.5))\nplot(QQ.out[1:s],type=\"l\", xlim=c(1,max(10,s)), las=1, ylab=\"Q\", lwd=2)\n\npar(mar=c(5,4,1.5,0.5))\nxx = seq(-8,11,length=200)\nyy = w*dnorm(xx, mu[1], sigma) + (1-w)*dnorm(xx, mu[2], sigma)\nplot(xx, yy, type=\"l\", ylim=c(0, max(c(yy,yy.true))), main=paste(\"s =\",s,\"   Q =\", round(QQ.out[s],4)), lwd=2, col=\"red\", lty=2, xlab=\"x\", ylab=\"Density\")\nlines(xx.true, yy.true, lwd=2)\npoints(x, rep(0,n), col=cc)\nlegend(6,0.22,c(\"Truth\",\"Estimate\"),col=c(\"black\",\"red\"), lty=c(1,2), bty=\"n\")",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#em-example-2",
    "href": "C3-L03.html#em-example-2",
    "title": "68  M2L3 - The EM algorithm for Mixture models",
    "section": "69.6 EM example 2",
    "text": "69.6 EM example 2",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>M2L3 - The EM algorithm for Mixture models</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sample-code-for-em-example-2",
    "href": "C3-L03.html#sample-code-for-em-example-2",
    "title": "68  M2L3 - The EM algorithm for Mixture models",
    "section": "69.7 Sample code for EM example 2",
    "text": "69.7 Sample code for EM example 2\nThis variant differs from the code sample above in that it uses the mvtnorm package to generate multivariate normal distributions. It also uses the ellipse package to plot the ellipses around the means of the components.\n\n#### Example of an EM algorithm for fitting a mixtures of K p-variate Gaussian components\n#### The algorithm is tested using simulated data\n\n## Clear the environment and load required libraries\nrm(list=ls())\nlibrary(mvtnorm)    # Multivariate normals are not default in R\nlibrary(ellipse)    # Required for plotting\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\nset.seed(63252)     # For reproducibility\n\n## Generate data from a mixture with 3 components\nKK      = 3\np       = 2\nw.true = c(0.5,0.3,0.2)  # True weights associated with the components\nmu.true     = array(0, dim=c(KK,p))\nmu.true[1,] = c(0,0)   #True mean for the first component\nmu.true[2,] = c(5,5)   #True mean for the second component\nmu.true[3,] = c(-3,7)   #True mean for the third component\nSigma.true      = array(0, dim=c(KK,p,p))\nSigma.true[1,,] = matrix(c(1,0,0,1),p,p)   #True variance for the first component\nSigma.true[2,,] = matrix(c(2,0.9,0.9,1),p,p)   #True variance for the second component\nSigma.true[3,,] = matrix(c(1,-0.9,-0.9,4),p,p)   #True variance for the third component\nn  = 120\ncc = sample(1:3, n, replace=T, prob=w.true)\nx  = array(0, dim=c(n,p))\nfor(i in 1:n){\n  x[i,] = rmvnorm(1, mu.true[cc[i],], Sigma.true[cc[i],,])\n}\n\npar(mfrow=c(1,1))\nplot(x[,1], x[,2], col=cc, type=\"n\", xlab=expression(x[1]), ylab=expression(x[2]))\ntext(x[,1], x[,2], seq(1,n), col=cc, cex=0.6)\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n}\n\n\n\n\n\n\n\n#title(main=\"Data + True Components\")\n\n\n### Run the EM algorithm\n## Initialize the parameters\nw   = rep(1,KK)/KK  #Assign equal weight to each component to start with\nmu  = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data\nSigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\nSigma[1,,] = var(x)/KK  \nSigma[2,,] = var(x)/KK\nSigma[3,,] = var(x)/KK\n\npar(mfrow=c(1,1))\nplot(x[,1], x[,2], col=cc, xlab=expression(x[1]), ylab=expression(x[2]))\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n}\ntitle(main=\"Initial estimate + Observations\")\n\n\n\n\n\n\n\ns       = 0\nsw      = FALSE\nQQ      = -Inf\nQQ.out  = NULL\nepsilon = 10^(-5)\n\nwhile(!sw){\n  ## E step\n  v = array(0, dim=c(n,KK))\n  for(k in 1:KK){\n    v[,k] = log(w[k]) + dmvnorm(x, mu[k,], Sigma[k,,],log=TRUE)  #Compute the log of the weights\n  }\n  for(i in 1:n){\n    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n  }\n  \n  ## M step\n  w     = apply(v,2,mean)\n  mu    = array(0, dim=c(KK, p))\n  for(k in 1:KK){\n    for(i in 1:n){\n      mu[k,]    = mu[k,] + v[i,k]*x[i,]\n    }\n    mu[k,] = mu[k,]/sum(v[,k])\n  }\n  Sigma = array(0, dim=c(KK, p, p))\n  for(k in 1:KK){\n    for(i in 1:n){\n      Sigma[k,,] = Sigma[k,,] + v[i,k]*(x[i,] - mu[k,])%*%t(x[i,] - mu[k,])\n    }\n    Sigma[k,,] = Sigma[k,,]/sum(v[,k])\n  }\n  \n  ##Check convergence\n  QQn = 0\n  for(i in 1:n){\n    for(k in 1:KK){\n      QQn = QQn + v[i,k]*(log(w[k]) + dmvnorm(x[i,],mu[k,],Sigma[k,,],log=TRUE))\n    }\n  }\n  if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n    sw=TRUE\n  }\n  QQ = QQn\n  QQ.out = c(QQ.out, QQ)\n  s = s + 1\n  print(paste(s, QQn))\n  \n  #Plot current components over data\n  layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))\n  par(mar=c(3.1,4.1,0.5,0.5))\n  plot(QQ.out[1:s],type=\"l\", xlim=c(1,max(10,s)), las=1, ylab=\"Q\")\n  \n  par(mar=c(5,4,1,0.5))\n  plot(x[,1], x[,2], col=cc, main=paste(\"s =\",s,\"   Q =\", round(QQ.out[s],4)), \n       xlab=expression(x[1]), ylab=expression(x[2]), lwd=2)\n  for(k in 1:KK){\n    lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n    lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n    lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n  }\n}\n\n[1] \"1 -582.05125374123\"\n\n\n\n\n\n\n\n\n\n[1] \"2 -559.067366495985\"\n\n\n\n\n\n\n\n\n\n[1] \"3 -543.8803866857\"\n\n\n\n\n\n\n\n\n\n[1] \"4 -527.840823447868\"\n\n\n\n\n\n\n\n\n\n[1] \"5 -511.540892774085\"\n\n\n\n\n\n\n\n\n\n[1] \"6 -483.797796090743\"\n\n\n\n\n\n\n\n\n\n[1] \"7 -464.070439621255\"\n\n\n\n\n\n\n\n\n\n[1] \"8 -455.865736477295\"\n\n\n\n\n\n\n\n\n\n[1] \"9 -455.214732499627\"\n\n\n\n\n\n\n\n\n\n[1] \"10 -455.176042939796\"\n\n\n\n\n\n\n\n\n\n[1] \"11 -455.171446608628\"\n\n\n\n\n\n\n\n\n\n[1] \"12 -455.170550189128\"\n\n\n\n\n\n\n\n\n#Plot current components over data\nlayout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))\npar(mar=c(3.1,4.1,0.5,0.5))\nplot(QQ.out[1:s],type=\"l\", xlim=c(1,max(10,s)), las=1, ylab=\"Q\", lwd=2)\n\npar(mar=c(5,4,1,0.5))\nplot(x[,1], x[,2], col=cc, main=paste(\"s =\",s,\"   Q =\", round(QQ.out[s],4)), xlab=expression(x[1]), ylab=expression(x[2]))\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n}",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>M2L3 - The EM algorithm for Mixture models</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-mixture-of-log-gaussians",
    "href": "C3-L03.html#sec-mixture-of-log-gaussians",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.7 Mixture of Log Gaussians",
    "text": "68.7 Mixture of Log Gaussians\n\n\n\n\n\n\nNotePrompt\n\n\n\n\n\nIf your data had support on the positive real numbers rather than the whole real line, how could you use the EM algorithm you just learned to instead fit a mixture of log-Gaussian distributions? Would you need to recode your algorithm?\n\nResponse\nUpdating the algorithm is nontrivial - it requires derivatives for each parameter. Depending on the distribution, we may need to add custom code to update each. We also need to update the distribution if these are changed.\nSo while the algorithm does not change, the code may change quite a bit.",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-advanced-em-algorithms",
    "href": "C3-L03.html#sec-advanced-em-algorithms",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.8 Advanced EM algorithms",
    "text": "68.8 Advanced EM algorithms\n\n68.8.1 HW: The EM for ZIP mixtures\nData on the lifetime (in years) of fuses produced by the ACME Corporation is available in the file fuses.csv:\nProvide the EM algorithm to fit the mixture model\n\n\n68.8.2 HW+: The EM for Mixture Models\n\n\n\n\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” Journal of the Royal Statistical Society: Series B (Methodological) 39 (1): 1–22.",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03-Ex1.html",
    "href": "C3-L03-Ex1.html",
    "title": "69  The EM algorithm for Zero-Inflated Mixtures",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>The EM algorithm for Zero-Inflated Mixtures</span>"
    ]
  },
  {
    "objectID": "C3-L03-Ex2.html",
    "href": "C3-L03-Ex2.html",
    "title": "70  The EM algorithm for Mixture Models",
    "section": "",
    "text": "70.1 Infer parameter of mixture of exponential and long normal for lifetime of fuses\n## Clear the environment and load required libraries\nrm(list=ls())\nset.seed(81196)    # So that results are reproducible (same simulated data every time)\n\n# Load the data\nfuses &lt;- read.csv(\"data/fuses.csv\",header=FALSE)\nx &lt;- fuses$V1\nn &lt;- length(x) # Number of observations\n\n# how many rows in the data\nnrow(fuses)\n\n[1] 400\n\n# how many zeros in x\nsum(x==0)\n\n[1] 0\n\n# almost half of the data is zeros!\n\npar(mfrow=c(1,1))\nxx.true = seq(0, max(x), by=1)\nhist(x, freq=FALSE, xlab=\"Fuses\", ylab=\"Density\", main=\"Empirical distribution of fuses failure times\")\nKK = 2                             # Number of components\nw     = 0.05                        # Assign equal weight to each component to start with\n#mu = rnorm(1,mean(log(x)), sd(log(x)))\nmu = mean(log(x))\ntau = sd(log(x))\nlambda = 20 / mean(x)\n\ns  = 0              # s_tep counter\nsw = FALSE          # sw_itch to stop the algorithm\nQQ = -Inf           # the Q function (log-likelihood function)\nQQ.out = NULL       # the Q function values\nepsilon = 10^(-5)   # the stopping criterion for the algorithm\n\ntrace &lt;- data.frame(iter=0, w=w, lambda=lambda, mu=mu, tau=tau)\n\nwhile(!sw){ ##Checking convergence\n\n  ## E step\n  v = array(0, dim=c(n,KK))\n  for(i in 1:n){\n    v[i,1] = log(w)   + dexp(x[i], rate=lambda, log=TRUE)\n    v[i,2] = log(1-w) + dlnorm(x[i], mu, tau, log=TRUE)    \n    v[i,]  = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,]))) \n  }\n\n  ## M step  \n  w      = mean(v[,1])  # Weights\n  lambda = sum(v[,1]) / sum(v[,1] * x)  # Lambda (rate)\n  mu     = sum(v[,2] * log(x)) / sum(v[,2]) # Mean\n  tau    = sqrt(sum(v[,2] * (log(x) - mu)^2) / sum(v[,2])) # Tau (standard deviation)\n  \n  # collect trace of parameters \n  trace  =  rbind(trace, data.frame(iter=s, w=w, lambda=lambda, mu=mu, tau=tau))\n\n  ## Check convergence\n  QQn = 0\n  #vectorized version\n  log_lik_mat = v[,1]*(log(w)   + dexp(x, lambda, log=TRUE)) +\n                v[,2]*(log(1-w) + dlnorm(x, mu, tau, log=TRUE))\n  QQn = sum(log_lik_mat)\n  if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n    sw=TRUE\n  }\n  QQ = QQn\n  QQ.out = c(QQ.out, QQ)\n  s = s + 1\n  print(paste(s, QQn))\n}\n\n[1] \"1 -621.636631915928\"\n[1] \"2 -576.564676680329\"\n[1] \"3 -562.326030957339\"\n[1] \"4 -558.240693010161\"\n[1] \"5 -559.062812699431\"\n[1] \"6 -560.433982999852\"\n[1] \"7 -561.504096778213\"\n[1] \"8 -562.257984979008\"\n[1] \"9 -562.779349634224\"\n[1] \"10 -563.139561270939\"\n[1] \"11 -563.389080841182\"\n[1] \"12 -563.562415697134\"\n[1] \"13 -563.683109594057\"\n[1] \"14 -563.767298770739\"\n[1] \"15 -563.826100698272\"\n[1] \"16 -563.867209281663\"\n[1] \"17 -563.895967519019\"\n[1] \"18 -563.916095326952\"\n[1] \"19 -563.930187401928\"\n[1] \"20 -563.94005598844\"\n[1] \"21 -563.946968029888\"\n[1] \"22 -563.951809840896\"\nnext report the MLE parameters of the model.\n# Report the MLE parameters\ncat(\"w =\", round(w, 2), \"lambda =\", round(lambda, 2), \"mu =\", round(mu, 2),\"tau =\", round(tau, 2))\n\nw = 0.09 lambda = 3.05 mu = 0.78 tau = 0.38",
    "crumbs": [
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>The EM algorithm for Mixture Models</span>"
    ]
  },
  {
    "objectID": "C3-L03-Ex2.html#infer-parameter-of-mixture-of-exponential-and-long-normal-for-lifetime-of-fuses",
    "href": "C3-L03-Ex2.html#infer-parameter-of-mixture-of-exponential-and-long-normal-for-lifetime-of-fuses",
    "title": "70  The EM algorithm for Mixture Models",
    "section": "",
    "text": "NoteInstructions\n\n\n\nData on the lifetime (in years) of fuses produced by the ACME Corporation is available in the file fuses.csv:\nIn order to characterize the distribution of the lifetimes, it seems reasonable to fit to the data a two-component mixture of the form:\n\nf(x)=wλexp{−λx}+(1−w) \\frac{1}{\\sqrt{2\\pi}\\tau x} \\exp{− \\frac{(log(x)−μ)^{2}}{2τ^{2}}}, \\quad x &gt; 0.\n\\tag{70.1}\nwhere w is the weight associated with the exponential distribution, \\lambda is the rate of the exponential distribution, and \\text{LN}(\\mu, \\tau) is a log-normal distribution with mean \\mu and standard deviation \\tau.\n\nModify code to Generate n observations from a mixture of two Gaussian # distributions into code to sample 100 random numbers from a mixture of 4 exponential distributions with means 1, 4, 7 and 10 and weights 0.3, 0.25, 0.25 and 0.2, respectively.\nUse these sample to approximate the mean and variance of the mixture.",
    "crumbs": [
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>The EM algorithm for Mixture Models</span>"
    ]
  },
  {
    "objectID": "C3-L04.html",
    "href": "C3-L04.html",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "",
    "text": "72 MCMC algorithm for Mixture Models\nMarkov chain Monte Carlo (MCMC) algorithm are typically used to perform Bayesian inference in complex models. In MCMC algorithms we repeatedly sample from the full conditional distributions of each block of parameters given fixed values for the rest. After an appropriate burn-in period, they generate samples that are dependent but identically distributed according to the posterior distribution of interest.",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#markov-chain-monte-carlo-algorithms-part-1",
    "href": "C3-L04.html#markov-chain-monte-carlo-algorithms-part-1",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.1 Markov Chain Monte Carlo algorithms part 1",
    "text": "72.1 Markov Chain Monte Carlo algorithms part 1\n\n\n\n\n\n\n\nFigure 72.1: MCMC - Priors of convenience\n\n\n\n\n\n\n\n\nFigure 72.2: MCMC - Complete data Likelihood\n\n\n\n\n\n\n\n\n\nTipNotation\n\n\n\n\nunder tildes signify a vector or collection.\n\\{i:c_i=l\\} mean the sum over the indices for a specific component k. We are regrouping the rows by their components\nThe double product mean we are iterating over the data rows times their likelihoods. (as usual)\nThe indicator in the exponent mean we are only taking one term per row which is picked using the component for which the latent indicator is true.\n\n\n\nThe model takes the form : \nf(x\\mid \\theta) = \\sum_{k=1}^K w_k g(x \\mid \\theta_k)\n\\tag{72.1}\nThe model is defined by the parameters \\theta = (\\theta_1, \\ldots, \\theta_K) and the weights w = (w_1, \\ldots, w_K).\nIn the Bayesian setting we also need priors for the weights and the parameters of each components.\n\n(w_1, \\ldots, w_K) \\sim Dirichlet(\\alpha_1, \\ldots, \\alpha_K) \\qquad \\mathbb{E}[w_k] = \\frac{\\alpha_k}{\\sum_{k=1}^K \\alpha_k}\n\\tag{72.2}\nalso if we use a_1 = a_2 = ... a_k=1 we end up with a uniform prior on the simplex.\n\n\\tilde{\\theta}_k if they admit a conjugate prior, we can use the conjugate prior for the parameters of the component k. Even though it won’t be conjugate for the whole model, it will be conjugate for the component k, at least given the sampling scheme outlined in Equation 72.4.\n\n\n\\mathbb{P}r(w) = \\frac{\\Gamma(\\sum_{k=1}^K \\alpha_k)}{\\prod_{k=1}^K \\Gamma(\\alpha_k)} \\prod_{k=1}^K w_k^{\\alpha_k - 1} \\qquad \\sum_{k=1}^K w_k = 1\n\\tag{72.3}\nTo develop a MCMC algorithm for mixture models we will use the hierarchical representation of the likelihood,\nComplete data likelihood:\n\n\\begin{aligned}\n\\mathbb{P}r(\\mathbf{x}, \\mathbf{c}, \\mid \\mathbf{\\omega}, \\mathbf{\\theta})  &= \\prod_{i=1}^n \\prod_{k=1}^K (\\omega_k\\ g_k(x_i \\mid \\theta_k))^{\\mathbb{1}(c_i = k)} &\n\\\\& = \\left[\\prod_{k=1}^K \\prod_{\\{i:c=k\\}}^n g_k(x_i \\mid \\theta_k)\\right] && \\left [\\prod_{k=1}^n \\omega_{k}^{\\sum \\mathbb{1}(c_i = k)}\\right ]\n\\\\& = \\mathbb{P}r(\\mathbf{x} \\mid \\mathbf{c}, \\mathbf{\\omega}, \\mathbf{\\theta}) && \\mathbb{P}r(\\mathbf{c} \\mid \\mathbf{\\omega}, \\mathbf{\\theta})\n\\\\& = \\mathbb{P}r(\\mathbf{x} \\mid \\mathbf{c}, \\mathbf{\\theta}) && \\mathbb{P}r(\\mathbf{c} \\mid \\mathbf{\\omega})\n\\end{aligned}\n\\tag{72.4}\nThe logic in this derivation is that we can rewrite the complete data likelihood as a product of two terms where we separate the weight from the other parameters.\n\n\n\n\n\n\nWarningUnclear !?\n\n\n\n\n\nI’m not sure this is 100% correct, we seem to be trying to write out the fact that each component is conditionally independent given the weights and the component parameters. This step from the first line to the second line is based on regrouping the terms in the product based on component k.\nAnother issue now that I’ve made an effort to clarify the notation is that the selection of the term in the product is based on picking the kernel from just one component. But it seems that we don’t know how to infer which component the data point belongs to.\n\n\n\nIn the third line we reinterpreting :\n\nthe left product in line 2 as a product of the likelihoods of the data if we know given their component, weights and parameters.\nthe right product in line 2 as the distribution of the indicators given the weights and parameters.\n\nIn the last line we remove \\omega on from the left term based on independence. And we remove \\theta from the right term based on independence.\n\n\n\n\n\n\n\nNoteVideo transcript\n\n\n\n\n\nIn previous lectures, we discussed the expectation maximization algorithm for fitting mixture models. In this lecture, we are going to discuss Markov Chain Monte Carlo for Bayesian inference in mixture models.\nWe’re going to move from frequentist inference which we were interested only on finding the point estimate for the parameters in the model to a situation in which we are going to try to explore a full posterior distribution for those parameters.\nRecall that the mixture model we are working with is going to take the form or the density of that mixture model. It is going to take the form of f of x is the sum over k components of weight multiplied by the components in the mixture. Those components are indexed by this parameter theta k, and we may have components that are all belong to the same family or that they belong to different families. If we are going to do Bayesian inference for this model, we need to compliment this density that is going to give us the likelihood with priors on the unknown parameters. In particular, we’re going to need priors for the weights, and we are going to need priors for the data suitcase. What is typically done in these situations is to use a priors of convenience.\nWhere are those priors of convenience? Well, first for the weights remember that we have a constraint that the sum of the weights needs to be equal to one.\nObviously each one of them individually needs to be between zero and one. So a natural prior for that type of parameters is a Dirichlet prior and that is precisely what we are going to use. So we’re going to assume that the prior for the vector that includes all these weights just follows a Dirichlet distribution, with parameters a1 all the way to a_k. Just as a quick reminder they expected value of each one of these parameters individually is just given by the corresponding a divided by the sum of the a’s. So in other words, the values of the a’s just constrains a prior that is the relative size of the different weights. In particular if you make them all the same, then you are saying that a prior you believe that all the weights are the same. We also know that as a special case if you make a_1=a_2= \\ldots = a_k and in particular equal to one then we just have the Uniform distribution on the simplex.\nWhich is actually one of the typical choices used for the hyperparameters when fearing mixture models. Now, this is our priori of convenience for the omegas and we will see that in addition to having a very nice interpretation it will also allow us to do computation in a very straight forward manner. Now, the other set of priors that we need is the priors for the data case. What is typically done here is that if they admit a conjugate prior under gk then that prior is used.\nThe reason for that is that even though for the full mixture this conjugate prior on the g_k1 conjugate for the full model it will be conditionally conjugate under our sampling scheme that we will derive in a minute. So it will make computation for the parameters theta k much simpler if we can find that conjugate prior under theta k. After we have set up priors for the model the next thing that we need to do before deriving our Gibbs sampler is to write down the complete data likelihood in a slightly more convenient way. If you remember the complete data likelihood that we used extensively for deriving the EM Algorithm has the form of the distribution of the data in all those indicators CSU either just tells you which component you belong to conditional on the weight, and all the Thetas is just going to take the form of a double product. So the product over the observations followed by the product over the components of omega sub k g sub k of x sub y given Theta k raised to the indicator function of ci equals to k. In other words, rather than write the complete sum that we had before, we replaced that completes sum by a product where each one of the terms is now raised to this indicator function that just depends on whether the component was generated, the observation was generated by the k component in the mixture. This complete data likelihood that we use extensively can now be written in a couple of different ways, and one that is going to be particularly helpful for us involves breaking this expression into two pieces, one that has to do with their omega’s, and one that has to do with g’s. So let me start with a piece that starts with the g’s. The way in which I’m going to do it is first I’m going to reverse the order of this products. So I am going to consider first the product over the components. Next I’m going to consider the product over the observations. But before I write exploration explicitly, let me interpret this expression up here a little bit. So what we’re doing here with this double product or one way to think about what we’re doing with a double product is to think about computing a bunch of terms that are in here in particular in this piece, that can be positioned onto a matrix where one dimension corresponds to the index i, and the second dimension corresponds to the index k. The entries of this matrix are just g of x i given theta k. So different combinations of i and different combinations of k gives you the values that you are going to put into this matrix. Now, what is this important? Because if you think about what they indicator or function up here is doing is it’s telling you well you need to compute the whole matrix but you’re actually not going to use the full matrix, you are just going to pick a few elements of it, and in particular you are going to pick one element in each row according to what the value of ci case. So for example, if the first observation belongs to the second component you’d be picking this value, second observation the first component you will pick this value, third observation with third component here and so on. So the values of the ci can be interpreted as giving you a way to select elements in this matrix, and in particular one per row. So another way to write the product over all the observations is used to think about grouping rows together according to which column is being selected. In particular, for example, we could put all the observations that have the first column being selected together, then all the observations that have the second column being selected together and so on.\nOne way to write that mathematically is to say that we’re going to do a product over the i’s but grouped together according to the value of k. Then we can get rid of the indicator and the numerator and write this as g sub k, xi given theta sub k. So this is one piece of this expression up here or one way to rewrite this expression up here or one piece of it that involves the g subcase. Of course we have a second piece that involves the omegas, that second piece that involves the omegas we can write as the product. Again, I’m going to consider the product over the case first. Then for a given k, omega k is exactly the same argument for all of them. So I can just write omega k and the product of omega k to the indicators just becomes omega k raised to the sum of the indicators.\nWell, once I have written the expression in this way, I can essentially think about this piece as being the distribution of the observations if I knew the indicators, the omegas, and the thetas. It so happens that this expression in particular doesn’t depend on the omegas. So for this model this is the same as p of x given c and the theta. In this expression here you can interpret as the distribution of the c’s given the omegas and the theta’s.\nAgain, in the particular structure of this model this happens to just depend on the weights omega. So we know that the product of these two quantities is just by the total law of probability the expression that we wanted in the beginning that is the distribution of the Theta and indicators together. So this particular form for the distribution is going to be particularly useful in terms of deriving the posterior distribution that we need for the Gibbs sampler. One last observation that I want to make that will be useful in the future is that if you think about what is the form of this piece down here the distribution or the Indicators even the weights, what you have is a form that resembles the kernel of multinomial distribution. So this is similar to the kernel of a multinomial.\nIn particular, it’s not only similar but it’s proportional to it. So it will be particularly useful in terms of deriving the algorithm using the fact that this looks like a multinomial distribution.",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#markov-chain-monte-carlo-algorithms-part-2",
    "href": "C3-L04.html#markov-chain-monte-carlo-algorithms-part-2",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.2 Markov Chain Monte Carlo algorithms part 2",
    "text": "72.2 Markov Chain Monte Carlo algorithms part 2\n\n\n\n\n\n\n\nFigure 72.3: posterior distribution - weights\n\n\n\n\n\n\n\n\nFigure 72.4: posterior distribution - components\n\n\n\n\n\n\n\n\nFigure 72.5: posterior distribution - parameters\n\n\n\n\nNow that we have a structure for the likelihood function that we and the prior distributions for all of our parameters, we can can derive the posterior distribution for our model.\nSo we want to write down the joint posterior distribution. In that joint posterior distribution includes, the weights and the parameters of the components, but it also involves the vector of indicators C, that we have introduced to simplify the structure of the likelihood, and by definition, this quantity is proportional to the distribution of the data, given all the parameters, multiplied by the distribution of the missing data parameters, given Omega and Theta, multiplied by the prior distribution on the parameters, multiplied by the prior distribution on the base.\n\n\\mathbb{P}r(c, \\theta, \\omega \\mid x) \\propto \\left \\{ \\prod_{i=1}^n \\mathbb{P}r(x | c, \\theta, \\omega) \\right \\} \\left \\{ \\prod_{i=1}^n\\prod_{k=1}^K \\mathbb{P}r(c \\mid \\omega, \\theta) \\right \\}\\ \\mathbb{P}r(\\omega)\\ \\mathbb{P}r(\\theta)\n\\tag{72.5}\nEach of the full conditional distributions can be derived from this joint posterior by retaining the terms that involve the parameter of interest, and recognizing the product of the selected terms as the kernel of a known family of distributions.\n\n\n\n\n\n\n\nNoteVideo transcript\n\n\n\n\n\nNow that we have a clear structure for the likelihood function that we will be using and we have prior distributions for all of our parameters, we can proceed to derive the posterior distribution that we will be working with.\nWe want to write down the joint posterior distribution. In that joint posterior distribution includes, of course, the weights and the parameters of the components, but it also involves the vector of indicators C, that we have introduced to simplify the structure of the likelihood, and by definition, this quantity is proportional to the distribution of the data, given all the parameters, multiplied by the distribution of the missing data parameters, given \\omega and \\theta, multiplied by the prior distribution on the parameters, multiplied by the prior distribution on the base.\nSo this is the general expression, the general form, for that posterior distribution. Now, we have already seen what the form of the different terms is. In particular, this joint distribution of the data can be written as a double product. First-order components, and then over the groups of observations that belong to each one of those components of g_k x_i, given \\theta_k. So that is the first piece that we’re interested in. The second piece, we have already seen also how to write it down. This is going to be the product from k equals one, to capital k of \\omega_k. Some of these indicators of c_i is equal to k from one to n. This is our second piece. Then we discussed using a Dirichlet distribution for this. So ignoring some proportionality constants, this becomes the product from k equals one to capital k of \\omega_k raised to the a_k - 1. That’s this piece. Then finally, we’re going to have another product. So typically, we use independent priors for each one of the data case. As I said, we’ll typically try to pick them so that they are conjugate to this kernel, g_k(), but for now, I’m going to write it in general form by writing this as p_k(\\theta_k), and that’s what the last term in the expression is. So we have written down a full expression for this. Now, it should be more or less clear how we need to proceed. So we need full conditionals for all the parameters in the model. In particular, we are going to need a full conditional for \\omega, given all the other parameters, we’re going to need a full conditional for each one of the c_i’s given the rest of the parameters, and we’re going to need a full conditional for each one of the data cases, given the rest of the parameters.\nSo to derive these full conditionals, what we will do is we will pick particular pieces from this expressions to retain and to construct this particular four conditionals.\nLet’s proceed now to derive each one of the four conditionals that we need to derive a Gibbs sampler or a Markov Chain Monte Carlo algorithm for this problem. Let’s just start with the full posterior distribution for the weights, and please note that we’re going to work with all the weights as a block, so we’re going to try to sample them all together, and rather than looking at each one of them at a time. o this full conditional distribution is made up of the terms in this full posterior distribution that involves \\omega_k, and if you look at this expression carefully, you will see that this piece doesn’t depend on \\omega_k anyway, and that this piece doesn’t depend on any of the \\omega_k either, so it’s just this two pieces in the middle that we need to consider. Furthermore, the two pieces are very similar, so both in both products over K of the weight raised to some power, so we can actually combine the two expressions together and just write them as the product from one to capital k of \\omega_k raised to the sum of these indicator functions, plus a_k minus 1. This looks exactly like the prior that we used, except that now, we have updated parameters. So I could write this as the product of \\omega_k raised to the a_k, call them stars, minus one, where a_k a star, is just the original a_k plus the sum from one to n of the indicators of c_i equals to k.\nSo just doing this little change, makes it very clear that the form of the posterior is exactly the same form as the prior.\nIn other words, this a conditionally conjugate prior for our problem, and that just means that \\omega is going to be distributed as a Dirichlet, given all the other parameters, but with this updated parameters, a_1 star all the way to a_k star, and this is very interesting because essentially, a posteriori, we know that the expected value of \\omega given all the other parameters, so this is the expected value of the full conditional. This is not expected value of the marginal posterior, but this is the expected value of the full conditional that is going to be a_k star divided by the sum from L equals one to k of a sub L, a star, but this is just a_k plus the sum from one to n of these indicators, c_i equals to k, divided by n plus the sum from L equals one to capital K of the a_l. N just comes from the fact that if I sum over all the components, then the sum of those values is going to be n. \nSo this is just the number of observations that are currently assigned to the case component, and if the values of a, k are small, then this is just roughly speaking.\nSo approximately, the proportion of observations in component K. This has a very nice analogy with the computations that we did in the EM algorithm.\nIf you remember the way in which we computed the weights in that case, or the MLE for the weights, was by essentially computing a quantity that could also be interpreted as, roughly speaking, the proportion of observations in that step of the algorithm that were assigned to that component.\nSo this provides a mirror image to what we did with the EM algorithm, but that has a Bayesian flavor rather than a frequentist flavor. Let’s continue now with the full conditional posterior distribution for the indicators, for the c_is. I’m interested in the probability that c_i is equal to K given the data.\nAs before, this is going to be proportional to just the terms in this large product that depends on c_i, and if you look at it carefully, c_i only appears in this two terms of the product. These have nothing to do with c_i. In particular, it appears in a single term within this really large product and in a single term within this product. So the term that depends on c_i being equal to k in here is \\omega_k. The term that depends on c_i equal to k in here, it’s just g_k(x_i \\mid \\theta_k), and this is true for every k from one to capital K. Remember that c_i is a discrete random variable, taking values between one and k because it indicates which component generated the observation. So if I want to get rid of the proportionality sign and actually being able to write what the probability is, I just need to normalize this by dividing over the sum of these quantities over k. So that means that p(c_i = k \\mid \\text{all other parameters}) = \\frac{\\omega_k g_k(x_i \\mid \\theta_k)}{\\sum_{l=1}^{K} \\omega_l g_l(x_i \\mid \\theta_l)}. If you look at this expression carefully, you will realize that it is very similar to the expression that we used when computing in the EM algorithm, the weights associated which is one of the observations. In fact, it is the same expression, and this is just what we called in the EM algorithm, V_{ik}. Finally, let’s consider the full conditional posterior distribution for the data case. So we need p(\\theta_k \\mid \\text{all other parameters}). Again, we just pick from this whole product the terms that have to do with \\theta_k, in this case, it is the two in the middle that do not depend on it, and within this big expression, we just have a few terms that contain \\theta_k, and those correspond to the observations that are currently assigned to that particular component. So this expression is proportional to the product over the i’s that have been assigned to the k th component of g_k(x_i \\mid \\theta_k), and among this product, again, there is a single term that belongs to \\theta_k. So the form of the full conditional posterior distribution for the parameter \\theta_k is simply this. Now, without a specific choice of G and P, it is hard to further simplify this expression. But what I do want to note here is that if this prior p_k is conjugate to this kernel g_k, then we typically know what family this posterior distribution will belong to, and that will make computation much simpler because you will typically be able to sample from that full posterior conditional distribution in using a direct sampler. This will become a little bit more clear once we do an example with mixture models, which is what we’re going to do next.",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#mcmc-for-location-mixtures-of-normals-part-1",
    "href": "C3-L04.html#mcmc-for-location-mixtures-of-normals-part-1",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.3 MCMC for location mixtures of normals Part 1",
    "text": "72.3 MCMC for location mixtures of normals Part 1\n\n\n\n\n\n\n\nFigure 72.6: location mixture of Normals - priors\n\n\n\n\n\n\n\n\nFigure 72.7: location mixture of Normals - marginals\n\n\n\n\n\n\n\n\nFigure 72.8: location mixture of Normals weights\n\n\n\n\n\n\n\n\nFigure 72.9: location mixture of Normals - components\n\n\n\n\n\n\n\n\nFigure 72.10: location mixture of Normals - \\sigma^2\n\n\n\n\n\n\nAs in the previous module we derive the full conditional distributions for the mixture of two univariate normals.\n\nf(x | ω, μ1, μ2, σ) = \\omega \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp\\left\\{-\\frac{(x - \\mu_1)^2}{2\\sigma^2}\\right\\} + (1- \\omega) \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp\\left\\{-\\frac{(x - \\mu_2)^2}{2\\sigma^2}\\right\\}\n\nwe use a beta distribution with a_1=1 and a_2=1 for \\omega which corresponds to a uniform distribution and is a special case of a Dirichlet for K=2.\n\\mu_k \\sim \\mathcal{N}(\\eta,\\tau^2)\nInverse Gamma for \\sigma^2 with shape parameter a and scale parameter b.\nAn empirical approach to priors:\n In the absence of real prior information we typically employ the observed data to guide the selection of the hyperparameter η, τ^2, d and q, in an approach that is reminiscent of empirical Bayes. In particular, we attempt to make the means of the different component lie in the same support of the observed data, so we take η to be approximately equal the mean (or median) of the observations, and τ^2 to be roughly equal to their variance. Similarly, for the prior on the variance σ^2 we set d = 2 (which implies that \\mathbb{E}(σ^2) = q and an infinite prior variance) and q to be roughly equal to the variance of the observations. Posteriors are often not very sensitive to changes on the prior means that remain within an order of magnitude of the values suggested above.\n\n\n\n\n\n\nNoteOverthinking the priors\n\n\n\n\n\nIt seems that since this is a hierarchical model, we set the priors for different components from shared hyper-priors. This way the parameters can also be inferred and we can reduce the number of parameters we need to estimate !",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#mcmc-for-location-mixtures-of-normals-part-2",
    "href": "C3-L04.html#mcmc-for-location-mixtures-of-normals-part-2",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.4 MCMC for location mixtures of normals Part 2",
    "text": "72.4 MCMC for location mixtures of normals Part 2\n\n\n\n\n\n\n\nFigure 72.11: location mixture of Normals \\mu\n\n\n\n\n\n\n\n\nFigure 72.12: location mixture of Normals \\mu continued 1\n\n\n\n\n\n\n\n\nFigure 72.13: location mixture of Normals \\mu continued 2",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#mcmc-example-1",
    "href": "C3-L04.html#mcmc-example-1",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.5 MCMC Example 1",
    "text": "72.5 MCMC Example 1\n\n#### Example of an MCMC algorithm for fitting a location mixture of 2 Gaussian components\n#### The algorithm is tested using simulated data\n\n## Clear the environment and load required libraries\nrm(list=ls())\nlibrary(MCMCpack)\n\nLoading required package: coda\n\n\nLoading required package: MASS\n\n\n##\n## Markov Chain Monte Carlo Package (MCMCpack)\n\n\n## Copyright (C) 2003-2025 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park\n\n\n##\n## Support provided by the U.S. National Science Foundation\n\n\n## (Grants SES-0350646 and SES-0350613)\n##\n\nset.seed(81196)  # So that results are reproducible\n\n\n## Generate data from a mixture with 2 components\nKK         = 2\nw.true     = 0.6  # True weights associated with the components\nmu.true    = rep(0, KK)\nmu.true[1] = 0   # True mean for the first component\nmu.true[2] = 5   # True mean for the second component\nsigma.true = 1   # True standard deviation of all components\nn          = 120         # Number of observations to be generated\ncc.true = sample(1:KK, n, replace=T, prob=c(w.true,1-w.true))\nx  = rep(0, n)\nfor(i in 1:n){\n  x[i] = rnorm(1, mu.true[cc.true[i]], sigma.true)\n}\n\n\n# Plot the true density\npar(mfrow=c(1,1))\nxx.true = seq(-8,11,length=200)\nyy.true = w.true*dnorm(xx.true, mu.true[1], sigma.true) + \n  (1-w.true)*dnorm(xx.true, mu.true[2], sigma.true) \nplot(xx.true, yy.true, type=\"l\", xlab=\"x\", ylab=\"True density\", lwd=2)\npoints(x, rep(0,n), col=cc.true)\n\n\n\n\n\n\n\nFigure 72.14: True density and data points\n\n\n\n\n\n\n## Initialize the parameters\nw     = 1/2                         #Assign equal weight to each component to start with\nmu    = rnorm(KK, mean(x), sd(x))   #Random cluster centers randomly spread over the support of the data\nsigma = sd(x)                       #Initial standard deviation\n\n# Plot the initial guess for the density\nxx = seq(-8,11,length=200)\nyy = w*dnorm(xx, mu[1], sigma) + (1-w)*dnorm(xx, mu[2], sigma)\nplot(xx, yy, type=\"l\", ylim=c(0, max(yy)), xlab=\"x\", \n     ylab=\"Initial density\", lwd=2)\npoints(x, rep(0,n), col=cc.true)\n\n\n\n\n\n\n\n## The actual MCMC algorithm starts here\n# Priors\naa  = rep(1,KK)  # Uniform prior on w\neta = 0          # Mean 0 for the prior on mu_k\ntau = 5          # Standard deviation 5 on the prior for mu_l\ndd  = 2\nqq  = 1\n\n# Number of iterations of the sampler\nrrr   = 6000\nburn  = 1000\n\n\n# Storing the samples\ncc.out    = array(0, dim=c(rrr, n))\nw.out     = rep(0, rrr)\nmu.out    = array(0, dim=c(rrr, KK))\nsigma.out = rep(0, rrr)\nlogpost   = rep(0, rrr)\n\n# MCMC iterations\nfor(s in 1:rrr){\n  # Sample the indicators\n  cc = rep(0,n)\n  for(i in 1:n){\n    v = rep(0,KK)\n    v[1] = log(w) + dnorm(x[i], mu[1], sigma, log=TRUE)  #Compute the log of the weights\n    v[2] = log(1-w) + dnorm(x[i], mu[2], sigma, log=TRUE)  #Compute the log of the weights\n    v = exp(v - max(v))/sum(exp(v - max(v)))\n    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)\n  }\n  \n  # Sample the weights\n  w = rbeta(1, aa[1] + sum(cc==1), aa[2] + sum(cc==2))\n\n  # Sample the means\n  for(k in 1:KK){\n    nk    = sum(cc==k)\n    xsumk = sum(x[cc==k])\n    tau2.hat = 1/(nk/sigma^2 + 1/tau^2)\n    mu.hat  = tau2.hat*(xsumk/sigma^2 + eta/tau^2)\n    mu[k]   = rnorm(1, mu.hat, sqrt(tau2.hat))\n  }\n\n  # Sample the variances\n  dd.star = dd + n/2\n  qq.star = qq + sum((x - mu[cc])^2)/2\n  sigma = sqrt(rinvgamma(1, dd.star, qq.star))\n\n  # Store samples\n  cc.out[s,]   = cc\n  w.out[s]     = w\n  mu.out[s,]   = mu\n  sigma.out[s] = sigma\n\n\n  # Compute the log posterior\n  for(i in 1:n){\n    if(cc[i]==1){\n      logpost[s] = logpost[s] + log(w) + dnorm(x[i], mu[1], sigma, log=TRUE)\n    }else{\n      logpost[s] = logpost[s] + log(1-w) + dnorm(x[i], mu[2], sigma, log=TRUE)\n    }\n  }\n\n  logpost[s] = logpost[s] + dbeta(w, aa[1], aa[2],log = T)\n  for(k in 1:KK){\n    logpost[s] = logpost[s] + dnorm(mu[k], eta, tau, log = T)\n  }\n\n  logpost[s] = logpost[s] + log(dinvgamma(sigma^2, dd, 1/qq))\n  \n  # print s every 500 iterations\n  if(s/500==floor(s/500)){\n    print(paste(\"s =\",s))\n  }\n}\n\n[1] \"s = 500\"\n[1] \"s = 1000\"\n[1] \"s = 1500\"\n[1] \"s = 2000\"\n[1] \"s = 2500\"\n[1] \"s = 3000\"\n[1] \"s = 3500\"\n[1] \"s = 4000\"\n[1] \"s = 4500\"\n[1] \"s = 5000\"\n[1] \"s = 5500\"\n[1] \"s = 6000\"\n\n\n\n## Plot the logposterior distribution for various samples\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1)+0.1)\nplot(logpost, type=\"l\", xlab=\"Iterations\", ylab=\"Log posterior\")\n\nxx = seq(-8,11,length=200)\ndensity.posterior = array(0, dim=c(rrr-burn,length(xx)))\nfor(s in 1:(rrr-burn)){\n  density.posterior[s,] = density.posterior[s,] + w.out[s+burn]*dnorm(xx,mu.out[s+burn,1],sigma.out[s+burn]) +\n                                                  (1-w.out[s+burn])*dnorm(xx,mu.out[s+burn,2],sigma.out[s+burn])\n}\n\n\n\n\n\n\n\nFigure 72.15: Log posterior distribution for various samples\n\n\n\n\n\n\n## report the posterior mean and 95% credible interval\ndensity.posterior.m = apply(density.posterior , 2, mean)\ndensity.posterior.lq = apply(density.posterior, 2, quantile, 0.025)\ndensity.posterior.uq = apply(density.posterior, 2, quantile, 0.975)\n\n## Plot the posterior density estimate\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1)+0.1)\nplot(xx, density.posterior.m, type=\"n\",ylim=c(0,max(density.posterior.uq)), xlab=\"x\", ylab=\"Density\")\npolygon(c(xx,rev(xx)), c(density.posterior.lq, rev(density.posterior.uq)), col=\"grey\", border=\"grey\")\nlines(xx, density.posterior.m, lwd=2)\npoints(x, rep(0,n), col=cc.true)\n\n\n\n\n\n\n\nFigure 72.16: Posterior density estimate",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#sample-code-for-mcmc-example-1",
    "href": "C3-L04.html#sample-code-for-mcmc-example-1",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.6 Sample code for MCMC example 1",
    "text": "72.6 Sample code for MCMC example 1",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#mcmc-example-2",
    "href": "C3-L04.html#mcmc-example-2",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.7 MCMC Example 2",
    "text": "72.7 MCMC Example 2",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#sample-code-for-mcmc-example-2",
    "href": "C3-L04.html#sample-code-for-mcmc-example-2",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.8 Sample code for MCMC example 2",
    "text": "72.8 Sample code for MCMC example 2\n\n#### Example of an MCMC algorithm for fitting a mixtures of K p-variate Gaussian components\n#### The algorithm is tested using simulated data\n\n## Clear the environment and load required libraries\nrm(list=ls())\nlibrary(mvtnorm)\nlibrary(ellipse)\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\nlibrary(MCMCpack)\n\n## Generate data from a mixture with 3 components\nKK      = 3\np       = 2\nw.true = c(0.5,0.3,0.2)  # True weights associated with the components\nmu.true     = array(0, dim=c(KK,p))\nmu.true[1,] = c(0,0)   #True mean for the first component\nmu.true[2,] = c(5,5)   #True mean for the second component\nmu.true[3,] = c(-3,7)   #True mean for the third component\nSigma.true      = array(0, dim=c(KK,p,p))\nSigma.true[1,,] = matrix(c(1,0,0,1),p,p)   #True variance for the first component\nSigma.true[2,,] = matrix(c(2,0.9,0.9,1),p,p)   #True variance for the second component\nSigma.true[3,,] = matrix(c(1,-0.9,-0.9,4),p,p)   #True variance for the third component\nset.seed(63252)    #Keep seed the same so that we can reproduce results\nn  = 120\ncc.true = sample(1:3, n, replace=T, prob=w.true)\nx  = array(0, dim=c(n,p))\nfor(i in 1:n){\n  x[i,] = rmvnorm(1, mu.true[cc.true[i],], Sigma.true[cc.true[i],,])\n}\n\npar(mfrow=c(1,1))\nplot(x[,1], x[,2], col=cc.true, xlab=expression(x[1]), ylab=expression(x[2]), type=\"n\")\ntext(x[,1], x[,2], seq(1,n), col=cc.true, cex=0.6)\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.50), col=\"grey\", lty=2)\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.82), col=\"grey\", lty=2)\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.95), col=\"grey\", lty=2)\n}\ntitle(main=\"Data + True Components\")\n\n\n\n\n\n\n\n## Initialize the parameters\nw          = rep(1,KK)/KK  #Assign equal weight to each component to start with\nmu         = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data\nSigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\nSigma[1,,] = var(x)/KK  \nSigma[2,,] = var(x)/KK\nSigma[3,,] = var(x)/KK\ncc         = sample(1:KK, n, replace=TRUE, prob=w)\n\npar(mfrow=c(1,1))\nplot(x[,1], x[,2], col=cc.true, xlab=expression(x[1]), ylab=expression(x[2]))\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n}\ntitle(main=\"Initial estimate + Observations\")\n\n\n\n\n\n\n\n# Priors\naa = rep(1, KK)\ndd = apply(x,2,mean)\nDD = 10*var(x)\nnu = p\nSS = var(x)/3\n\n# Number of iteration of the sampler\nrrr = 1000\n\n# Storing the samples\ncc.out    = array(0, dim=c(rrr, n))\nw.out     = array(0, dim=c(rrr, KK))\nmu.out    = array(0, dim=c(rrr, KK, p))\nSigma.out = array(0, dim=c(rrr, KK, p, p))\nlogpost   = rep(0, rrr)\n\nfor(s in 1:rrr){\n  # Sample the indicators\n  for(i in 1:n){\n    v = rep(0,KK)\n    for(k in 1:KK){\n      v[k] = log(w[k]) + mvtnorm::dmvnorm(x[i,], mu[k,], Sigma[k,,], log=TRUE)  #Compute the log of the weights\n    }\n    v = exp(v - max(v))/sum(exp(v - max(v)))\n    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)\n  }\n  \n  # Sample the weights\n  w = as.vector(rdirichlet(1, aa + tabulate(cc)))\n  \n  # Sample the means\n  DD.st = matrix(0, nrow=p, ncol=p)\n  for(k in 1:KK){\n    mk    = sum(cc==k)\n    xsumk = apply(x[cc==k,], 2, sum)\n    DD.st = solve(mk*solve(Sigma[k,,]) + solve(DD))\n    dd.st = DD.st%*%(solve(Sigma[k,,])%*%xsumk + solve(DD)%*%dd)\n    mu[k,] = as.vector(rmvnorm(1,dd.st,DD.st))\n  }\n  \n  # Sample the variances\n  xcensumk = array(0, dim=c(KK,p,p))\n  for(i in 1:n){\n    xcensumk[cc[i],,] = xcensumk[cc[i],,] + (x[i,] - mu[cc[i],])%*%t(x[i,] - mu[cc[i],])\n  }\n  for(k in 1:KK){\n    Sigma[k,,] = riwish(nu + sum(cc==k), SS + xcensumk[k,,])\n  }\n  \n  # Store samples\n  cc.out[s,]      = cc\n  w.out[s,]       = w\n  mu.out[s,,]     = mu\n  Sigma.out[s,,,] = Sigma\n  for(i in 1:n){\n    logpost[s] = logpost[s] + log(w[cc[i]]) + mvtnorm::dmvnorm(x[i,], mu[cc[i],], Sigma[cc[i],,], log=TRUE)\n  }\n  logpost[s] = logpost[s] + ddirichlet(w, aa)\n  for(k in 1:KK){\n    logpost[s] = logpost[s] + mvtnorm::dmvnorm(mu[k,], dd, DD, log=TRUE)\n    logpost[s] = logpost[s] + log(diwish(Sigma[k,,], nu, SS))\n  }\n  \n  if(s/250==floor(s/250)){\n    print(paste(\"s = \", s))\n  }  \n}\n\n[1] \"s =  250\"\n[1] \"s =  500\"\n[1] \"s =  750\"\n[1] \"s =  1000\"\n\n## Plot the logposterior distribution for various samples\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1)+0.1)\nplot(logpost, type=\"l\", xlab=\"Iterations\", ylab=\"Log posterior\")\n\n\n\n\n\n\n\n## Plot the density estimate for the last iteration of the MCMC\npar(mfrow=c(1,1))\npar(mar=c(4,4,2,1)+0.1)\nplot(x[,1], x[,2], col=cc.true, main=paste(\"s =\",s,\"   logpost =\", round(logpost[s],4)), xlab=expression(x[1]), ylab=expression(x[2]))\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n}",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#practice-graded-assignment-the-mcmc-algorithm-for-zero-inflated-mixtures",
    "href": "C3-L04.html#practice-graded-assignment-the-mcmc-algorithm-for-zero-inflated-mixtures",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.9 Practice Graded Assignment: The MCMC algorithm for zero-inflated mixtures",
    "text": "72.9 Practice Graded Assignment: The MCMC algorithm for zero-inflated mixtures\n\nrm(list=ls())\nlibrary(MCMCpack)\nset.seed(81196)  # So that results are reproducible\n\n# Data loading\n\nx &lt;- read.csv(\"./data/nestsize.csv\")[[1]]\nn &lt;- length(x)\n\n# The actual MCMC algorithm starts here\n\n## MCMC iterations of the sampler\n\niterations &lt;- 6000\nburn &lt;- 1000\n\n## Initialize the parameters\n\ncc         = rep(2, n)\ncc[x == 0] = sample(1:2, sum(x == 0), replace = TRUE, prob = c(0.5, 0.5))\n\n## Priors\n\naa = c(1, 1)  # Uniform prior on w\nw     = 0.2 # fewer zeros\nlambda = mean(x[x &gt; 0])  # Initial lambda from nonzero data\n\n# Storing the samples\nw.out      = rep(0, iterations)\ncc.out     = array(0, dim=c(iterations, n))\nlambda.out = array(0, dim=c(iterations, n))\n\n# logpost    = rep(0, iterations)\n# MCMC iterations\n\nfor (s in 1:iterations) {\n\n  # Sample latent indicators c_i\n\n  cc = numeric(n)\n  for (i in 1:n) {\n    if (x[i] == 0) {\n      logp1 = log(w)\n      logp2 = log(1 - w) + dpois(0, lambda, log=TRUE)\n      probs = exp(c(logp1, logp2) - max(logp1, logp2))\n      probs = probs / sum(probs)\n      cc[i] = sample(1:2, 1, prob = probs)\n    } else {\n      cc[i] = 2\n    }\n  }\n\n  # Sample the weights\n\n  w = rbeta(1, aa[1] + sum(cc==1), aa[2] + sum(cc==2))\n  lambda = rgamma(1, shape = 1 + sum(x[cc == 2]), rate = 1 + sum(cc == 2))\n\n  # Store samples\n\n  w.out[s] =  w\n  lambda.out[s]  = lambda\n  cc.out[s,] = cc\n\n}\n\n# Posterior summaries\n\nw.post = w.out[-(1:burn)]\nlambda.post = lambda.out[-(1:burn)]\ncat(\"Posterior mean of w:\", mean(w.post), \"\\n\")\n\nPosterior mean of w: 0.399678 \n\ncat(\"Posterior mean of lambda:\", mean(lambda.post), \"\\n\")\n\nPosterior mean of lambda: 0.008477621",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04.html#honors-peer-graded-assignment-markov-chain-monte-carlo-algorithms-for-mixture-models",
    "href": "C3-L04.html#honors-peer-graded-assignment-markov-chain-monte-carlo-algorithms-for-mixture-models",
    "title": "71  MCMC for Mixture Models - M4L1",
    "section": "72.10 Honors Peer-graded Assignment: Markov chain Monte Carlo algorithms for Mixture Models",
    "text": "72.10 Honors Peer-graded Assignment: Markov chain Monte Carlo algorithms for Mixture Models",
    "crumbs": [
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>MCMC for Mixture Models - M4L1</span>"
    ]
  },
  {
    "objectID": "C3-L04-Ex1.html",
    "href": "C3-L04-Ex1.html",
    "title": "72  The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1</span>"
    ]
  },
  {
    "objectID": "C3-L04-Ex2.html",
    "href": "C3-L04-Ex2.html",
    "title": "73  Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2</span>"
    ]
  },
  {
    "objectID": "C3-L05.html",
    "href": "C3-L05.html",
    "title": "74  Density Estimation - M4L5",
    "section": "",
    "text": "74.1 Density Estimation using Mixture Models 🎥",
    "crumbs": [
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>Density Estimation - M4L5</span>"
    ]
  },
  {
    "objectID": "C3-L05.html#sec-density-estimation-using-mixture-models",
    "href": "C3-L05.html#sec-density-estimation-using-mixture-models",
    "title": "74  Density Estimation - M4L5",
    "section": "",
    "text": "Density Estimation using Mixture Models\n\n\n74.1.1 KDE\n\nthe typical method for estimating the density of a random variable is to use a kernel density estimator (KDE)\nthe KDE is a non-parametric method that estimates the density of a random variable by averaging the contributions of a set of kernel functions centered at each data point\n\n\nX_1, \\ldots, X_n \\sim f(x)\n\n\n\\tilde{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h}  g \\left (\\frac{\\|X - X_i\\|}{h}\\right )\n\\tag{74.1}\n where h is the bandwidth of the kernel and g is a kernel function. The kernel function is a non-negative function that integrates to 1 and is symmetric around 0.\n For example, the Gaussian kernel is given by: \ng(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n\\tag{74.2}\ngiving us:\n\n\\tilde{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h}  \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{|X - X_i|^2}{2h^2}}\n\\tag{74.3}\n\n\n74.1.2 Mixture of K components\n\na mixture of K components is a parametric method that estimates the density of a random variable by averaging the contributions of K kernel functions, each centered at a different location and with a different scale\nthe mixture model is given by:\n\n\nX_1, \\ldots, X_n \\sim f(x) = \\sum_{k=1}^K w_k g(x \\mid \\hat{\\theta}_k)\n\\tag{74.4}\nwhere w_k is the weight of the k-th component, \\hat{\\theta}_k is the location and scale of the k-th component, and g(x \\mid \\hat{\\theta}_k) is the kernel function centered at \\hat{\\theta}_k. The weights are non-negative and sum to 1.\nExample: a location mixture of K Gaussian distributions is given by:\n\nX_1, \\ldots, X_n \\sim \\hat{f}(x) = \\sum_{k=1}^K \\hat{w}_k \\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma}} \\exp^{-\\frac{(x - \\hat{\\mu}_k)^2}{2\\hat{\\sigma}^2}}\n\\tag{74.5}\nwhere \\hat{w}_k is the weight of the k-th component, \\hat{\\mu}_k is the mean of the k-th component, and \\hat{\\sigma} is the standard deviation of the k-th component.\nwe can see the the two methods are quite similar, but the mixture model is more flexible and can capture more complex shapes in the data.\n\nThe KDE is a special case of the mixture model where all the components have the same scale and location.\nKDE needs as many components as the number of data points, while the mixture model can have fewer components.\nKDE uses a fixed bandwidth,\nMDE can adaptively choose the bandwidth for each component. In fact we have a weight for each component and a scale parameter that controls the width of the kernel function.\nMDE tends to use less components and the weights tend to be 1/K\n\nThe above model can be improved by:\n\nusing a scale-location mixture model, where the scale and location of each component are estimated from the data.\n\n\n\n74.1.3 Density Estimation Example 🎥\n We use the galaxies dataset to illustrate the differences between the two methods.\nThe galaxies dataset contains the velocities of 82 galaxies in the Virgo cluster. The data is available in the MASS package.\n\n\n74.1.4 Sample code for Density Estimation Problem\n\n## Using mixture models for density estimation in the galaxies dataset\n## Compare kernel density estimation, and estimates from mixtures of KK=6\n## components obtained using both frequentist and Bayesian procedures\nrm(list=ls())\n\n### Loading data and setting up global variables\nlibrary(MASS)\nlibrary(MCMCpack)\n\nLoading required package: coda\n\n\n##\n## Markov Chain Monte Carlo Package (MCMCpack)\n\n\n## Copyright (C) 2003-2025 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park\n\n\n##\n## Support provided by the U.S. National Science Foundation\n\n\n## (Grants SES-0350646 and SES-0350613)\n##\n\ndata(galaxies)\nKK = 6          # Based on the description of the dataset\nx  = galaxies\nn  = length(x)\nset.seed(781209)\n\n### First, compute the \"Maximum Likelihood\" density estimate associated with a location mixture of 6 Gaussian distributions using the EM algorithm\n## Initialize the parameters\nw     = rep(1,KK)/KK\nmu    = rnorm(KK, mean(x), sd(x))\nsigma = sd(x)/KK\n\nepsilon = 0.000001\ns       = 0\nsw      = FALSE\nKL      = -Inf\nKL.out  = NULL\n\nwhile(!sw){\n  ## E step\n  v = array(0, dim=c(n,KK))\n  for(k in 1:KK){\n    v[,k] = log(w[k]) + dnorm(x, mu[k], sigma,log=TRUE)  \n  }\n  for(i in 1:n){\n    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))\n  }\n  \n  ## M step\n  # Weights\n  w = apply(v,2,mean)\n  mu = rep(0, KK)\n  for(k in 1:KK){\n    for(i in 1:n){\n      mu[k]    = mu[k] + v[i,k]*x[i]\n    }\n    mu[k] = mu[k]/sum(v[,k])\n  }\n  # Standard deviations\n  sigma = 0\n  for(i in 1:n){\n    for(k in 1:KK){\n      sigma = sigma + v[i,k]*(x[i] - mu[k])^2\n    }\n  }\n  sigma = sqrt(sigma/sum(v))\n  \n  ##Check convergence\n  KLn = 0\n  for(i in 1:n){\n    for(k in 1:KK){\n      KLn = KLn + v[i,k]*(log(w[k]) + dnorm(x[i], mu[k], sigma, log=TRUE))\n    }\n  }\n  if(abs(KLn-KL)/abs(KLn)&lt;epsilon){\n    sw=TRUE\n  }\n  KL = KLn\n  KL.out = c(KL.out, KL)\n  s = s + 1\n  print(paste(s, KLn))\n}\n\n[1] \"1 -835.733942489325\"\n[1] \"2 -828.010809264972\"\n[1] \"3 -824.746233906969\"\n[1] \"4 -822.658999626022\"\n[1] \"5 -821.213895212478\"\n[1] \"6 -820.205593334589\"\n[1] \"7 -819.45255265569\"\n[1] \"8 -818.824551232431\"\n[1] \"9 -818.236534003549\"\n[1] \"10 -817.634208984436\"\n[1] \"11 -816.982967592922\"\n[1] \"12 -816.261886189958\"\n[1] \"13 -815.461265773593\"\n[1] \"14 -814.58192426664\"\n[1] \"15 -813.634925825188\"\n[1] \"16 -812.640825431584\"\n[1] \"17 -811.627832678685\"\n[1] \"18 -810.628730004626\"\n[1] \"19 -809.676914791807\"\n[1] \"20 -808.802324442178\"\n[1] \"21 -808.028006389222\"\n[1] \"22 -807.36782257363\"\n[1] \"23 -806.825578229162\"\n[1] \"24 -806.395771901538\"\n[1] \"25 -806.065864649222\"\n[1] \"26 -805.819434169721\"\n[1] \"27 -805.63925361852\"\n[1] \"28 -805.509531004204\"\n[1] \"29 -805.417065498588\"\n[1] \"30 -805.351515365637\"\n[1] \"31 -805.305136223748\"\n[1] \"32 -805.272302459567\"\n[1] \"33 -805.249006220074\"\n[1] \"34 -805.232424348309\"\n[1] \"35 -805.220578980422\"\n[1] \"36 -805.212086217934\"\n[1] \"37 -805.205976247832\"\n[1] \"38 -805.201567151818\"\n[1] \"39 -805.19837733157\"\n[1] \"40 -805.196065008484\"\n[1] \"41 -805.194386438224\"\n[1] \"42 -805.193166975027\"\n[1] \"43 -805.192280944541\"\n[1] \"44 -805.191637566216\"\n\nxx  = seq(5000,37000,length=300)\nnxx = length(xx)\ndensity.EM = rep(0, nxx)\nfor(s in 1:nxx){\n  for(k in 1:KK){\n    density.EM[s] = density.EM[s] + w[k]*dnorm(xx[s], mu[k], sigma)\n  }\n}\n\n### Get a \"Bayesian\" kernel density estimator based on the same location mixture of 6 normals\n## Priors set up using an \"empirical Bayes\" approach\naa  = rep(1,KK)  \neta = mean(x)    \ntau = sqrt(var(x))\ndd  = 2\nqq  = var(x)/KK\n\n## Initialize the parameters\nw     = rep(1,KK)/KK\nmu    = rnorm(KK, mean(x), sd(x))\nsigma = sd(x)/KK\ncc    = sample(1:KK, n, replace=T, prob=w)\n\n## Number of iterations of the sampler\nrrr   = 12000\nburn  = 2000\n\n## Storing the samples\ncc.out    = array(0, dim=c(rrr, n))\nw.out     = array(0, dim=c(rrr, KK))\nmu.out    = array(0, dim=c(rrr, KK))\nsigma.out = rep(0, rrr)\nlogpost   = rep(0, rrr)\n\n\nfor(s in 1:rrr){\n  # Sample the indicators\n  for(i in 1:n){\n    v = rep(0,KK)\n    for(k in 1:KK){\n      v[k] = log(w[k]) + dnorm(x[i], mu[k], sigma, log=TRUE)  #Compute the log of the weights\n    }\n    v = exp(v - max(v))/sum(exp(v - max(v)))\n    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)\n  }\n  \n  # Sample the weights\n  w = as.vector(rdirichlet(1, aa + tabulate(cc, nbins=KK)))\n  \n  # Sample the means\n  for(k in 1:KK){\n    nk    = sum(cc==k)\n    xsumk = sum(x[cc==k])\n    tau2.hat = 1/(nk/sigma^2 + 1/tau^2)\n    mu.hat  = tau2.hat*(xsumk/sigma^2 + eta/tau^2)\n    mu[k]   = rnorm(1, mu.hat, sqrt(tau2.hat))\n  }\n  \n  # Sample the variances\n  dd.star = dd + n/2\n  qq.star = qq + sum((x - mu[cc])^2)/2\n  sigma = sqrt(1/rgamma(1, dd.star, qq.star))\n  \n  # Store samples\n  cc.out[s,]   = cc\n  w.out[s,]    = w\n  mu.out[s,]   = mu\n  sigma.out[s] = sigma\n  for(i in 1:n){\n    logpost[s] = logpost[s] + log(w[cc[i]]) + dnorm(x[i], mu[cc[i]], sigma, log=TRUE)\n  }\n  logpost[s] = logpost[s] + log(ddirichlet(w, aa))\n  for(k in 1:KK){\n    logpost[s] = logpost[s] + dnorm(mu[k], eta, tau, log=TRUE)\n  }\n  logpost[s] = logpost[s] + dgamma(1/sigma^2, dd, qq, log=TRUE) - 4*log(sigma)\n  if(s/500==floor(s/500)){\n    print(paste(\"s =\",s))\n  }\n}\n\n[1] \"s = 500\"\n[1] \"s = 1000\"\n[1] \"s = 1500\"\n[1] \"s = 2000\"\n[1] \"s = 2500\"\n[1] \"s = 3000\"\n[1] \"s = 3500\"\n[1] \"s = 4000\"\n[1] \"s = 4500\"\n[1] \"s = 5000\"\n[1] \"s = 5500\"\n[1] \"s = 6000\"\n[1] \"s = 6500\"\n[1] \"s = 7000\"\n[1] \"s = 7500\"\n[1] \"s = 8000\"\n[1] \"s = 8500\"\n[1] \"s = 9000\"\n[1] \"s = 9500\"\n[1] \"s = 10000\"\n[1] \"s = 10500\"\n[1] \"s = 11000\"\n[1] \"s = 11500\"\n[1] \"s = 12000\"\n\n## Compute the samples of the density over a dense grid\ndensity.mcmc = array(0, dim=c(rrr-burn,length(xx)))\nfor(s in 1:(rrr-burn)){\n    for(k in 1:KK){\n        density.mcmc[s,] = density.mcmc[s,] + w.out[s+burn,k]*dnorm(xx,mu.out[s+burn,k],sigma.out[s+burn])\n    }\n}\ndensity.mcmc.m = apply(density.mcmc , 2, mean)\n\n## Plot Bayesian estimate with pointwise credible bands along with kernel density estimate and frequentist point estimate\ncolscale = c(\"black\", \"blue\", \"red\")\nyy = density(x)\ndensity.mcmc.lq = apply(density.mcmc, 2, quantile, 0.025)\ndensity.mcmc.uq = apply(density.mcmc, 2, quantile, 0.975)\nplot(xx, density.mcmc.m, type=\"n\",ylim=c(0,max(density.mcmc.uq)),xlab=\"Velocity\", ylab=\"Density\")\npolygon(c(xx,rev(xx)), c(density.mcmc.lq, rev(density.mcmc.uq)), col=\"grey\", border=\"grey\")\nlines(xx, density.mcmc.m, col=colscale[1], lwd=2)\nlines(xx, density.EM, col=colscale[2], lty=2, lwd=2)\nlines(yy, col=colscale[3], lty=3, lwd=2)\npoints(x, rep(0,n))\nlegend(27000, 0.00017, c(\"KDE\",\"EM\",\"MCMC\"), col=colscale[c(3,2,1)], lty=c(3,2,1), lwd=2, bty=\"n\")",
    "crumbs": [
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>Density Estimation - M4L5</span>"
    ]
  },
  {
    "objectID": "C3-L06.html",
    "href": "C3-L06.html",
    "title": "75  Clustering - M4L6",
    "section": "",
    "text": "75.1 Mixture Models for Clustering 🎥\nClustering, or unsupervised classification, aims to partition heterogeneous data into homogeneous groups (clusters). Common in biology and other domains, clustering helps identify underlying structure, such as species based on physiological features.\nA widely-used method is K-means clustering, which:\nThis process iterates until assignments stabilize.\nK-means is closely related to a mixture of Gaussians with:\nIf components are well-separated, EM approximates hard assignments similar to K-means.",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  },
  {
    "objectID": "C3-L06.html#clustering",
    "href": "C3-L06.html#clustering",
    "title": "75  Clustering - M4L6",
    "section": "",
    "text": "75.1.1 Mixture Models for Clustering\n\n\n75.1.2 Clustering example\n\n\n## Using mixture models for clustering in the iris dataset\n## Compare k-means clustering and a location and scale mixture model with K normals\n\n### Loading data and setting up global variables\nrm(list=ls())\nlibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nlibrary(mvtnorm)\n\n\nAttaching package: 'mvtnorm'\n\n\nThe following object is masked from 'package:mclust':\n\n    dmvnorm\n\n### Defining a custom function to create pair plots\n### This is an alternative to the R function pairs() that allows for \n### more flexibility. In particular, it allows us to use text to label \n### the points\npairs2 = function(x, col=\"black\", pch=16, labels=NULL, names = colnames(x)){\n  n = dim(x)[1]\n  p = dim(x)[2]\n  par(mfrow=c(p,p))\n  for(k in 1:p){\n    for(l in 1:p){\n      if(k!=l){\n        par(mar=c(3,3,1,1)+0.1)\n        plot(x[,k], x[,l], type=\"n\", xlab=\"\", ylab=\"\")\n        if(is.null(labels)){\n          points(x[,k], x[,l], pch=pch, col=col)\n        }else{\n          text(x[,k], x[,l], labels=labels, col=col)\n        }\n      }else{\n        plot(seq(0,5), seq(0,5), type=\"n\", xlab=\"\", ylab=\"\", axes=FALSE)\n        text(2.5,2.5,names[k], cex=1.2)\n      }\n    }\n  }\n}\n\n## Setup data\ndata(iris)\nx       = as.matrix(iris[,-5])\nn       = dim(x)[1]\np       = dim(x)[2]       # Number of features\nKK      = 3\nepsilon = 0.0000001\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1))\ncolscale = c(\"black\",\"blue\",\"red\")\nshortnam  = c(\"s\",\"c\",\"g\")\npairs2(x, col=colscale[iris[,5]], labels=shortnam[as.numeric(iris[,5])])\n\n\n\n\n\n\n\n# Initialize the parameters of the algorithm\nset.seed(63252)\nnumruns = 15\nv.sum   = array(0, dim=c(numruns, n, KK))\nQQ.sum  = rep(0, numruns)\n\nfor(ss in 1:numruns){\n  w   = rep(1,KK)/KK  #Assign equal weight to each component to start with\n  mu  = rmvnorm(KK, apply(x,2,mean), 3*var(x))   #Cluster centers randomly spread over the support of the data\n  Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\n  Sigma[1,,] = var(x)\n  Sigma[2,,] = var(x)\n  Sigma[3,,] = var(x)\n  \n  sw     = FALSE\n  QQ     = -Inf\n  QQ.out = NULL\n  s      = 0\n  \n  while(!sw){\n    ## E step\n    v = array(0, dim=c(n,KK))\n    for(k in 1:KK){  #Compute the log of the weights\n      v[,k] = log(w[k]) + mvtnorm::dmvnorm(x, mu[k,], Sigma[k,,], log=TRUE) \n    }\n    for(i in 1:n){\n      v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n    }\n    \n    ## M step\n    w = apply(v,2,mean)\n    mu = matrix(0, nrow=KK, ncol=p)\n    for(k in 1:KK){\n      for(i in 1:n){\n        mu[k,]    = mu[k,] + v[i,k]*x[i,]\n      }\n      mu[k,] = mu[k,]/sum(v[,k])\n    }\n    Sigma = array(0,dim=c(KK, p, p))\n    for(k in 1:KK){\n      for(i in 1:n){\n        Sigma[k,,] = Sigma[k,,] + v[i,k]*(x[i,] - mu[k,])%*%t(x[i,] - mu[k,])\n      }\n      Sigma[k,,] = Sigma[k,,]/sum(v[,k])\n    }\n    \n    ##Check convergence\n    QQn = 0\n    for(i in 1:n){\n      for(k in 1:KK){\n        QQn = QQn + v[i,k]*(log(w[k]) + mvtnorm::dmvnorm(x[i,],mu[k,],Sigma[k,,],log=TRUE))\n      }\n    }\n    if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n      sw=TRUE\n    }\n    QQ = QQn\n    QQ.out = c(QQ.out, QQ)\n    s = s + 1\n  }\n  \n  v.sum[ss,,] = v\n  QQ.sum[ss]  = QQ.out[s]\n  print(paste(\"ss =\", ss))\n}\n\n[1] \"ss = 1\"\n[1] \"ss = 2\"\n[1] \"ss = 3\"\n[1] \"ss = 4\"\n[1] \"ss = 5\"\n[1] \"ss = 6\"\n[1] \"ss = 7\"\n[1] \"ss = 8\"\n[1] \"ss = 9\"\n[1] \"ss = 10\"\n[1] \"ss = 11\"\n[1] \"ss = 12\"\n[1] \"ss = 13\"\n[1] \"ss = 14\"\n[1] \"ss = 15\"\n\n## Cluster reconstruction under the mixture model\ncc = apply(v.sum[which.max(QQ.sum),,], 1 ,which.max)\ncolscale = c(\"black\",\"blue\",\"red\")\npairs2(x, col=colscale[cc], labels=cc)\n\n\n\n\n\n\n\nARImle = adjustedRandIndex(cc, as.numeric(iris[,5]))  # Higher values indicate larger agreement\n\n## Cluster reconstruction under the K-means algorithm\nirisCluster &lt;- kmeans(x, 3, nstart = numruns)\ncolscale = c(\"black\",\"blue\",\"red\")\npairs2(x, col=colscale[irisCluster$cluster], labels=irisCluster$cluster)\n\n\n\n\n\n\n\nARIkmeans = adjustedRandIndex(irisCluster$cluster, as.numeric(iris[,5]))",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  },
  {
    "objectID": "C3-L07.html",
    "href": "C3-L07.html",
    "title": "76  Classification - M4L7",
    "section": "",
    "text": "76.1 Mixture Models and naive Bayes classifiers 🎥\nClassification is a supervised learning problem where we want to predict the class of a new observation based on its features.\nAccording to the instructor the main difference from clustering is that in classification we have a training set. I would think the main difference is that we have labels for some of the data, while in clustering we do not have labels at all.\nThe fact that we have labels and a training set means we should know how many classes we have and we can use these labels to train a model and use it to predict the class of a new observation.\nThe instructor mentions Support Vector Machines (SVM), logistic regression and linear discriminant analysis (LDA) as familiar examples of classification methods. These and a number of others are covered in (James et al. 2013). We will focus on Naive Bayes classifiers as it is the most similar to mixture models and the EM algorithm which we have seen earlier",
    "crumbs": [
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Classification - M4L7</span>"
    ]
  },
  {
    "objectID": "C3-L07-Ex1.html",
    "href": "C3-L07-Ex1.html",
    "title": "77  Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span>"
    ]
  },
  {
    "objectID": "C3-L07-Ex2.html",
    "href": "C3-L07-Ex2.html",
    "title": "78  Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2</span>"
    ]
  },
  {
    "objectID": "C3-L07-Ex3.html",
    "href": "C3-L07-Ex3.html",
    "title": "79  Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3</span>"
    ]
  },
  {
    "objectID": "C3-L08.html",
    "href": "C3-L08.html",
    "title": "80  Computational Considerations - M5L8",
    "section": "",
    "text": "80.1 Computational Considerations 🎥",
    "crumbs": [
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Computational Considerations - M5L8</span>"
    ]
  },
  {
    "objectID": "C3-L08.html#computational-considerations",
    "href": "C3-L08.html#computational-considerations",
    "title": "80  Computational Considerations - M5L8",
    "section": "",
    "text": "Figure 80.1: Computational Considerations\n\n\n\n80.1.1 Numerical stability\n\nThe issue is in how the computer represents numbers. The computer uses a finite number of bits to represent numbers, which can lead to numerical instability when performing calculations that involve very large or very small numbers. This can result in loss of precision and incorrect results.\n The solution is to use logarithmic transformations to avoid numerical instability. By taking the logarithm of the numbers, we can work with smaller and more manageable values, which reduces the risk of numerical instability.\n\n\n80.1.2 Sample code to illustrate numerical stability issues 📖\n\n## Consider a mixture of two normal distributions with equal weights (w1 = w2 = 1/2)\n## Component 1 has mean 0 and standard deviation 1\n## Component 2 has mean 1 and standard deviation 1\n## The observation is x = 50\n## What is Pr(c = 1 | x)?\ndnorm(50, 0, 1)\n\n[1] 0\n\ndnorm(50, 1, 1)\n\n[1] 0\n\ndnorm(50, 0, 1)/(dnorm(50, 0, 1) + dnorm(50, 1, 1))\n\n[1] NaN\n\n## What if x=3?  Two ways to do the calculation\n## One way:  Direct calculation\nz1 = dnorm(3, 0, 1)\nz2 = dnorm(3, 1, 1)\nz1/(z1+z2)\n\n[1] 0.07585818\n\n## A second way:  Compute in the logarithm scale, add b \n## to all values, and then exponentiate before standardizing\nlz1 = dnorm(3, 0, 1, log=T)\nlz2 = dnorm(3, 1, 1, log=T)\nb = 3\nexp(lz1+b)/(exp(lz1+b) + exp(lz2+b))\n\n[1] 0.07585818\n\n## Going back to the case x - 50:\n## Wrong\nlz1 = log(dnorm(50, 0, 1))\nlz2 = log(dnorm(50, 1, 1))\nb = max(lz1, lz2)\nexp(lz1-b)/(exp(lz1-b) + exp(lz2-b))\n\n[1] NaN\n\n## Wrong\nlz1 = log(exp(-0.5*50^2)/sqrt(2*pi))\nlz2 = log(exp(-0.5*49^2)/sqrt(2*pi))\nb = max(lz1, lz2)\nexp(lz1-b)/(exp(lz1-b) + exp(lz2-b))\n\n[1] NaN\n\n## Right\nlz1 = dnorm(50, 0, 1, log=TRUE)\nlz2 = dnorm(50, 1, 1, log=TRUE)\nb = max(lz1, lz2)\nexp(lz1-b)/(exp(lz1-b) + exp(lz2-b))\n\n[1] 3.179971e-22\n\n## Also right (just more cumbersome)\nlz1 = -0.5*log(2*pi) - 0.5*50^2\nlz2 = -0.5*log(2*pi) - 0.5*49^2\nb = max(lz1, lz2)\nexp(lz1-b)/(exp(lz1-b) + exp(lz2-b))\n\n[1] 3.179971e-22\n\n\n\n\n80.1.3 Computational issues associated with multimodality 🎥\n\n\n\n\n\n\n\nFigure 80.2: multimodality issues\n\n\n\n\n80.1.4 Sample code to illustrate multimodality issues 1 📖\n \n\n## Illustrating the fact that the likelihood for a mixture model is multimodal\n\n### Loading data and setting up global variables\nrm(list=ls())\nlibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nlibrary(mvtnorm)\n\n\nAttaching package: 'mvtnorm'\n\n\nThe following object is masked from 'package:mclust':\n\n    dmvnorm\n\n### Defining a custom function to create pair plots\n### This is an alternative to the R function pairs() that allows for \n### more flexibility. In particular, it allows us to use text to label \n### the points\npairs2 = function(x, col=\"black\", pch=16, labels=NULL, names = colnames(x)){\n  n = dim(x)[1]\n  p = dim(x)[2]\n  par(mfrow=c(p,p))\n  for(k in 1:p){\n    for(l in 1:p){\n      if(k!=l){\n        par(mar=c(3,3,1,1)+0.1)\n        plot(x[,k], x[,l], type=\"n\", xlab=\"\", ylab=\"\")\n        if(is.null(labels)){\n          points(x[,k], x[,l], pch=pch, col=col)\n        }else{\n          text(x[,k], x[,l], labels=labels, col=col)\n        }\n      }else{\n        plot(seq(0,5), seq(0,5), type=\"n\", xlab=\"\", ylab=\"\", axes=FALSE)\n        text(2.5,2.5,names[k], cex=1.2)\n      }\n    }\n  }\n}\n\n\n## Setup data\ndata(iris)\nx       = as.matrix(iris[,-5])\nn       = dim(x)[1]\np       = dim(x)[2]       # Number of features\nKK      = 3\nepsilon = 0.0000001\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1))\ncolscale = c(\"black\",\"blue\",\"red\")\nshortnam  = c(\"s\",\"c\",\"g\")\n\n# Initialize the parameters of the algorithm\nset.seed(63252)\n\nnumruns = 15\nv.sum   = array(0, dim=c(numruns, n, KK))\nQQ.sum  = rep(0, numruns)\n\nfor(ss in 1:numruns){\n  w   = rep(1,KK)/KK  #Assign equal weight to each component to start with\n  #mu  = as.matrix(aggregate(x, list(iris[,5]), mean)[,2:5])  # Initialize in the true values\n  #Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\n  #Sigma[1,,] = var(x[iris[,5]==\"setosa\",])\n  #Sigma[2,,] = var(x[iris[,5]==\"versicolor\",])\n  #Sigma[3,,] = var(x[iris[,5]==\"virginica\",])\n  mu  = rmvnorm(KK, apply(x,2,mean), 3*var(x))   #Cluster centers randomly spread over the support of the data\n  Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\n  Sigma[1,,] = var(x)\n  Sigma[2,,] = var(x)\n  Sigma[3,,] = var(x)\n  \n  sw     = FALSE\n  QQ     = -Inf\n  QQ.out = NULL\n  s      = 0\n  \n  while(!sw){\n    ## E step\n    v = array(0, dim=c(n,KK))\n    for(k in 1:KK){  #Compute the log of the weights\n      v[,k] = log(w[k]) + dmvnorm(x, mu[k,], Sigma[k,,], log=TRUE) \n    }\n    for(i in 1:n){\n      v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n    }\n    \n    ## M step\n    w = apply(v,2,mean)\n    mu = matrix(0, nrow=KK, ncol=p)\n    for(k in 1:KK){\n      for(i in 1:n){\n        mu[k,]    = mu[k,] + v[i,k]*x[i,]\n      }\n      mu[k,] = mu[k,]/sum(v[,k])\n    }\n    Sigma = array(0,dim=c(KK, p, p))\n    for(k in 1:KK){\n      for(i in 1:n){\n        Sigma[k,,] = Sigma[k,,] + v[i,k]*(x[i,] - mu[k,])%*%t(x[i,] - mu[k,])\n      }\n      Sigma[k,,] = Sigma[k,,]/sum(v[,k])\n    }\n    \n    ##Check convergence\n    QQn = 0\n    for(i in 1:n){\n      for(k in 1:KK){\n        QQn = QQn + v[i,k]*(log(w[k]) + dmvnorm(x[i,],mu[k,],Sigma[k,,],log=TRUE))\n      }\n    }\n    if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n      sw=TRUE\n    }\n    QQ = QQn\n    QQ.out = c(QQ.out, QQ)\n    s = s + 1\n  }\n  \n  v.sum[ss,,] = v\n  QQ.sum[ss]  = QQ.out[s]\n  print(paste(\"ss =\", ss))\n}\n\n[1] \"ss = 1\"\n[1] \"ss = 2\"\n[1] \"ss = 3\"\n[1] \"ss = 4\"\n[1] \"ss = 5\"\n[1] \"ss = 6\"\n[1] \"ss = 7\"\n[1] \"ss = 8\"\n[1] \"ss = 9\"\n[1] \"ss = 10\"\n[1] \"ss = 11\"\n[1] \"ss = 12\"\n[1] \"ss = 13\"\n[1] \"ss = 14\"\n[1] \"ss = 15\"\n\n## Boxplot of final values of the Q function for all runs of the algorithm\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1))\nboxplot(QQ.out, ylab=\"Q\", xlab=\"Iterations\",las=2)\n\n\n\n\n\n\n\n## Graphical representation of the best solution \ncc = apply(v.sum[which.max(QQ.sum),,], 1 ,which.max)\ncolscale = c(\"black\",\"blue\",\"red\")\n#pairs(x, col=colscale[cc], pch=cc)\npairs2(x, col=colscale[cc], labels=cc)\n\n\n\n\n\n\n\n## Graphical representation of the worst solution\ncc = apply(v.sum[which.min(QQ.sum),,], 1 ,which.max)\ncolscale = c(\"black\",\"blue\",\"red\")\n#pairs(x, col=colscale[cc], pch=cc)\npairs2(x, col=colscale[cc], labels=cc)\n\n\n\n\n\n\n\n\n\n\n80.1.5 Sample code to illustrate multimodality issues 2 📖\nThis code fails to converge because the algorithm is stuck in a local maximum of the likelihood function. The problem is that one of the components is “numerically empty” (i.e., it has no data points assigned to it). This can happen when the initial values for the means are too far apart or when the data is not well-separated.\n\n## Illustrating that the EM might fail for numerical reasons if a component is “numerically empty”\n\n### Loading data and setting up global variables\nrm(list=ls())\nlibrary(mclust)\nlibrary(mvtnorm)\nlibrary(ellipse)\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\n## Setup data\ndata(iris)\nx       = as.matrix(iris[,-5])\nn       = dim(x)[1]\np       = dim(x)[2]       # Number of features\nKK      = 3\nepsilon = 0.00000001\n\n# Initialize the parameters of the algorithm\nw   = rep(1,KK)/KK  #Assign equal weight to each component to start with\nmu  = matrix(0, KK, p)  # Initialize in the true values\nmu[1,] = apply(x, 2, mean)\nmu[2,] = apply(x, 2, mean) + c(2.2, 2.2, 2.2, 2.2)\nmu[3,] = apply(x, 2, mean) + c(-2.2, -2.2, -2.2, -2.2)\nSigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\nSigma[1,,] = var(x)/3\nSigma[2,,] = var(x)/3\nSigma[3,,] = var(x)/3\n\n# Plot the data along with the estimates of the components\ncolscale = c(\"black\",\"blue\",\"red\")\npar(mfrow=c(p,p))\nfor(k in 1:p){\n  for(l in 1:p){\n    if(k!=l){\n      par(mar=c(3,3,1,1)+0.1)\n      plot(x[,k], x[,l], type=\"n\", xlab=\"\", ylab=\"\", xlim=c(min(c(x[,k], mu[,k])),max(c(x[,k], mu[,k]))), ylim=c(min(c(x[,l], mu[,l])),max(c(x[,l], mu[,l]))))\n      for(r in 1:KK){\n        lines(ellipse(x=Sigma[r,c(k,l),c(k,l)], centre=mu[r,c(k,l)], level=0.50), col=\"gold1\", lty=1, lwd=1)\n        lines(ellipse(x=Sigma[r,c(k,l),c(k,l)], centre=mu[r,c(k,l)], level=0.82), col=\"gold1\", lty=1, lwd=1)\n        lines(ellipse(x=Sigma[r,c(k,l),c(k,l)], centre=mu[r,c(k,l)], level=0.95), col=\"gold1\", lty=1, lwd=1)\n      }\n      text(x[,k], x[,l], labels=as.numeric(iris[,5]), col=colscale[iris[,5]])\n      points(mu[,k], mu[,l], pch=19, col=\"gold1\", cex=2)\n    }else{\n      plot(seq(0,5), seq(0,5), type=\"n\", xlab=\"\", ylab=\"\", axes=FALSE)\n      text(2.5,2.5,colnames(x)[k], cex=1.5)\n    }\n  }\n}\n\n\n\n\n\n\n\n## Run the EM algorithm.  It will fail in the first iteration\nsw     = FALSE\nQQ     = -Inf\nQQ.out = NULL\ns      = 0\n\nwhile(!sw){\n  ## E step\n  v = array(0, dim=c(n,KK))\n  for(k in 1:KK){  #Compute the log of the weights\n    v[,k] = log(w[k]) + dmvnorm(x, mu[k,], Sigma[k,,], log=TRUE) \n  }\n  for(i in 1:n){\n    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n  }\n  \n  ## M step\n  w = apply(v,2,mean)\n  mu = matrix(0, nrow=KK, ncol=p)\n  for(k in 1:KK){\n    for(i in 1:n){\n      mu[k,]    = mu[k,] + v[i,k]*x[i,]\n    }\n    mu[k,] = mu[k,]/sum(v[,k])\n  }\n  Sigma = array(0,dim=c(KK, p, p))\n  for(k in 1:KK){\n    for(i in 1:n){\n      Sigma[k,,] = Sigma[k,,] + v[i,k]*(x[i,] - mu[k,])%*%t(x[i,] - mu[k,])\n    }\n    Sigma[k,,] = Sigma[k,,]/sum(v[,k])\n  }\n  \n  ##Check convergence\n  QQn = 0\n  for(i in 1:n){\n    for(k in 1:KK){\n      QQn = QQn + v[i,k]*(log(w[k]) + dmvnorm(x[i,],mu[k,],Sigma[k,,],log=TRUE))\n    }\n  }\n  if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n    sw=TRUE\n  }\n  QQ = QQn\n  QQ.out = c(QQ.out, QQ)\n  s = s + 1\n}\n\nError in if (abs(QQn - QQ)/abs(QQn) &lt; epsilon) {: missing value where TRUE/FALSE needed\n\nQQn\n\n[1] NaN",
    "crumbs": [
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Computational Considerations - M5L8</span>"
    ]
  },
  {
    "objectID": "C3-L08-Ex1.html",
    "href": "C3-L08-Ex1.html",
    "title": "81  Computational considerations for Mixture Models",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>Computational considerations for Mixture Models</span>"
    ]
  },
  {
    "objectID": "C3-L09.html",
    "href": "C3-L09.html",
    "title": "82  Determining the number of components - M5L9",
    "section": "",
    "text": "82.1 Bayesian Information Criteria - BIC 🎥\nSo far we considered the number of components in the mixture model as a known and fixed parameter. However, in practice, the number of components is often unknown and needs to be estimated from the data. We will cover two approaches to estimate the number of components in a mixture model:\nWe can consider the choice of the number of components in a mixture model as a model selection problem. In this context, we have a collection of J models, each with a different number of components. The goal is to select the model that best fits with the evidence/data while avoiding overfitting.\nA common approach to model selection that is useful for mixture models is the Bayesian Information Criteria (BIC). Given a collection of J models to be compared, the BIC for model j is given by the formula: \nBIC_k = - 2\\log L_j(\\hat{\\eta}) - r_k \\log (n) \\qquad\n\\tag{82.1}\nwhere L_j is the likelihood of the model, \\hat{\\eta} is the maximum likelihood estimate of the parameters, r_j is the number of effective (independent) parameters in model j, and n is the number of observations. The model with the lowest BIC value is considered the best model.\nWe can interpret the first term in Equation 82.1 as a measure of the goodness of fit of the model, while the second term penalizes the model for its complexity. The BIC is a trade-off between the goodness of fit and the complexity of the model.\nIn the case of mixture models, j corresponds to the number of components K in the model. and \\eta_k corresponds to the parameters of the model, which include the weights and parameters of the of the component distributions i.e. \\eta_k = (w_1,\\ldots,w_k, \\theta_1, \\ldots \\theta_K) . The number of effective parameters in a mixture model with K components is given by: \nL_k(\\hat{w}_1,...\\hat{w}_K, \\hat{\\theta}_1,...,\\hat{\\theta_K}) = \\prod_{i=1}^n \\sum_{k=1}^K \\hat{w}_k g(x_i|\\hat{\\theta}_k)\nfurthermore, the number of effective parameters is given by:\nr_k = K - 1 + \\sum_{k=1}^K \\dim(\\theta_k)",
    "crumbs": [
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Determining the number of components - M5L9</span>"
    ]
  },
  {
    "objectID": "C3-L09.html#sec-BIC",
    "href": "C3-L09.html#sec-BIC",
    "title": "82  Determining the number of components - M5L9",
    "section": "",
    "text": "Figure 82.1: BIC\n\n\n\n\n\n\n\n\nFigure 82.2: BIC",
    "crumbs": [
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Determining the number of components - M5L9</span>"
    ]
  },
  {
    "objectID": "C3-L09.html#bayesian-information-criteria-example",
    "href": "C3-L09.html#bayesian-information-criteria-example",
    "title": "82  Determining the number of components - M5L9",
    "section": "82.2 Bayesian Information Criteria Example 🎥",
    "text": "82.2 Bayesian Information Criteria Example 🎥\nThis is a walkthrough of the code in the next section.",
    "crumbs": [
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Determining the number of components - M5L9</span>"
    ]
  },
  {
    "objectID": "C3-L09.html#sample-code-bayesian-information-criteria",
    "href": "C3-L09.html#sample-code-bayesian-information-criteria",
    "title": "82  Determining the number of components - M5L9",
    "section": "82.3 Sample code: Bayesian Information Criteria 📖",
    "text": "82.3 Sample code: Bayesian Information Criteria 📖\n \nThis code sample illustrates the use of BIC to estimate the number of components of a Mixture Model using the galaxies dataset\n\n\n\n\nListing 82.1: Illustrating the use of BIC to estimate the number of components of a Mixture Model using the galaxies dataset\n\n\n## using the galaxies dataset\n1rm(list=ls())\n2library(MASS)\ndata(galaxies)\n3x  = galaxies\nn  = length(x)\n4set.seed(781209)\n\n5KKmax = 20\n6BIC   = rep(0, KKmax-1)\n\n7w.sum  = vector(\"list\", KKmax-1)\n8mu.sum = vector(\"list\", KKmax-1)\n9sigma.sum = rep(0, KKmax-1)\n\n10for(KK in 2:KKmax){\n  ### First, compute the \"Maximum Likelihood\" density estimate \n  ### associated with a location mixture of 6 Gaussian distributions \n  ### using the EM algorithm &lt;11&gt;\n12  w     = rep(1,KK)/KK\n  mu    = rnorm(KK, mean(x), sd(x))\n  sigma = sd(x)/KK\n  \n13  epsilon = 0.000001\n  s       = 0\n  sw      = FALSE\n  KL      = -Inf\n  KL.out  = NULL\n  \n14  while(!sw){\n    # E-step &lt;15&gt;\n    v = array(0, dim=c(n,KK))\n    for(k in 1:KK){\n      v[,k] = log(w[k]) + dnorm(x, mu[k], sigma,log=TRUE)  \n    }\n    for(i in 1:n){\n      v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))\n    }\n    \n    ## M step &lt;16&gt;\n    # Weights &lt;17&gt;\n    w = apply(v,2,mean)\n    mu = rep(0, KK)\n    for(k in 1:KK){\n      for(i in 1:n){\n        mu[k]    = mu[k] + v[i,k]*x[i]\n      }\n      mu[k] = mu[k]/sum(v[,k])\n    }\n    # Standard deviations &lt;22&gt;\n    sigma = 0\n    for(i in 1:n){\n      for(k in 1:KK){\n        sigma = sigma + v[i,k]*(x[i] - mu[k])^2\n      }\n    }\n    sigma = sqrt(sigma/sum(v))\n    \n    ##Check convergence &lt;26&gt;\n        KLn = 0\n    for(i in 1:n){\n      for(k in 1:KK){\n        KLn = KLn + v[i,k]*(log(w[k]) + \n                        dnorm(x[i], mu[k], sigma, log=TRUE))\n      }\n    }\n    if(abs(KLn-KL)/abs(KLn)&lt;epsilon){\n      sw=TRUE\n    }\n    KL = KLn\n    KL.out = c(KL.out, KL)\n    s = s + 1\n    if(s/20==floor(s/20)){      \n      print(paste(s, KLn))\n    }\n  }\n  \n  w.sum[[KK-1]]  = w\n  mu.sum[[KK-1]] = mu\n  sigma.sum[KK-1] = sigma\n  \n  \n  ## Computing BIC\n  for(i in 1:n){\n    BIC[KK-1] = BIC[KK-1] - 2*log(sum(w*dnorm(x[i], mu, sigma)))\n  }\n  BIC[KK-1] = BIC[KK-1] + ((KK-1) + 1 + KK)*log(n)  ### KK-1 independent weights, one variance, and KK means\n}\n\n## Plot of BIC as a function of K\npar(mar=c(4,4,1,1) + 0.1)\nplot(seq(2,KKmax), BIC, type=\"l\", xlab=\"K\", ylab=\"BIC\", lwd=2)\nabline(v=6, lty=3)\n\n\n\n\n\n1\n\nClear the environment\n\n2\n\nLoading data\n\n3\n\nSetting up global variables\n\n4\n\nSetting a random seed for reproducibility\n\n5\n\nSetting the maximum number of components to consider\n\n6\n\nInitializing the BIC vector to store the BIC values for each number of components\n\n7\n\nInitializing list to store the weights for each number of components\n\n8\n\nInitializing list to store the means for each number of components\n\n9\n\nInitializing vector to store the standard deviations for each number of components\n\n10\n\nLooping over the number of components from 2 to KKmax\n\n12\n\nInitializing the parameters of the mixture model\n\n13\n\nInitializing the convergence criteria\n\n14\n\nRunning the EM algorithm until convergence\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 82.2: Illustrating the use of BIC to estimate the number of components of a Mixture Model using the galaxies dataset\n\n\n## Computing density estimates for various values of K\ndensity.est = function(xx, w, mu, sigma){\n  KK  = length(w)\n  nxx = length(xx)\n  density.EM = rep(0, nxx)\n  for(s in 1:nxx){\n    for(k in 1:KK){\n      density.EM[s] = density.EM[s] + w[k]*dnorm(xx[s], mu[k], sigma)\n    }\n  }\n  return(density.EM)\n}\n\nxx  = seq(5000,37000,length=300)\nKK = 8\nmdeKK8 = density.est(xx, w.sum[[KK-1]], mu.sum[[KK-1]], sigma.sum[KK-1])\nKK = 7\nmdeKK7 = density.est(xx, w.sum[[KK-1]], mu.sum[[KK-1]], sigma.sum[KK-1])\nKK = 6\nmdeKK6 = density.est(xx, w.sum[[KK-1]], mu.sum[[KK-1]], sigma.sum[KK-1])\nKK = 5\nmdeKK5 = density.est(xx, w.sum[[KK-1]], mu.sum[[KK-1]], sigma.sum[KK-1])\nKK = 4\nmdeKK4 = density.est(xx, w.sum[[KK-1]], mu.sum[[KK-1]], sigma.sum[KK-1])\n\n## Comparing density estimates for K=4, 5 and 6\npar(mar=c(4,4,1,1)+0.1)\nplot(xx, mdeKK6, type=\"n\",ylim=c(0,max(c(mdeKK4,mdeKK5,mdeKK6,mdeKK7))), \n     xlab=\"Velocity\", ylab=\"Density\")\nlines(xx, mdeKK6, col=\"black\", lty=1, lwd=2)\nlines(xx, mdeKK5, col=\"red\", lty=2, lwd=2)\nlines(xx, mdeKK4, col=\"blue\", lty=3, lwd=2)\npoints(x, rep(0,n))\nlegend(26000, 0.00022, c(\"K = 6\",\"K = 5\",\"K = 4\"), \n       lty=c(1,2,3), col=c(\"black\",\"red\",\"blue\"), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 82.3: Illustrating the use of BIC to estimate the number of components of a Mixture Model using the galaxies dataset\n\n\n## Comparing density estimates for K=6, 7 and 8\npar(mar=c(4,4,1,1)+0.1)\nplot(xx, mdeKK6, type=\"n\",ylim=c(0,max(c(mdeKK6,mdeKK7,mdeKK8))), \n     xlab=\"Velocity\", ylab=\"Density\")\nlines(xx, mdeKK6, col=\"black\", lty=1, lwd=2)\nlines(xx, mdeKK7, col=\"red\", lty=2, lwd=2)\nlines(xx, mdeKK8, col=\"blue\", lty=3, lwd=2)\npoints(x, rep(0,n))\nlegend(26000, 0.00022, c(\"K = 6\",\"K = 7\",\"K = 8\"), \n       lty=c(1,2,3), col=c(\"black\",\"red\",\"blue\"), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\n\n[1] \"20 -861.300160340851\"\n[1] \"40 -861.845102704334\"\n[1] \"60 -862.021825056483\"\n[1] \"80 -862.107312512661\"\n[1] \"100 -862.156909986849\"\n[1] \"120 -862.188905642997\"\n[1] \"140 -862.211044984583\"\n[1] \"20 -892.435584724028\"\n[1] \"40 -893.104888016642\"\n[1] \"60 -893.40498202712\"\n[1] \"80 -893.591204291568\"\n[1] \"100 -893.723710425507\"\n[1] \"120 -893.825831831938\"\n[1] \"140 -893.908860452777\"\n[1] \"160 -893.979019408269\"\n[1] \"180 -894.040054525798\"\n[1] \"200 -894.094370451017\"\n[1] \"220 -894.143587647454\"\n[1] \"240 -894.188838741371\"\n[1] \"260 -894.230936144577\"\n[1] \"280 -894.27047113134\"\n[1] \"300 -894.307873877557\"\n[1] \"320 -894.343449688044\"\n[1] \"340 -894.37739943714\"\n[1] \"360 -894.40982826946\"\n[1] \"380 -894.440744086025\"\n[1] \"400 -894.470045342745\"\n[1] \"420 -894.497495562189\"\n[1] \"440 -894.522679009391\"\n[1] \"460 -894.544927117938\"\n[1] \"20 -833.432143007169\"\n[1] \"40 -818.168044247337\"\n[1] \"20 -826.416964018383\"\n[1] \"40 -770.789119074513\"\n[1] \"20 -828.321379092958\"\n[1] \"40 -795.942449657444\"\n[1] \"20 -794.662374351299\"\n[1] \"20 -809.439563423673\"\n[1] \"40 -808.663042378389\"\n[1] \"20 -801.80909130307\"\n[1] \"40 -799.673095387316\"\n[1] \"60 -795.809414492802\"\n[1] \"80 -796.314077121678\"\n[1] \"100 -796.933079286827\"\n[1] \"120 -797.365852316129\"\n[1] \"140 -797.658242104202\"\n[1] \"160 -797.855651509745\"\n[1] \"180 -797.990205412934\"\n[1] \"200 -798.083160070137\"\n[1] \"220 -798.148337498443\"\n[1] \"240 -798.194729342538\"\n[1] \"260 -798.228231798405\"\n[1] \"280 -798.252756139525\"\n[1] \"300 -798.270931855528\"\n[1] \"20 -798.319463648812\"\n[1] \"40 -794.845009758997\"\n[1] \"20 -826.983331709186\"\n[1] \"40 -821.239623208658\"\n[1] \"60 -819.799267130562\"\n[1] \"20 -816.295987712457\"\n[1] \"40 -806.709549732948\"\n[1] \"60 -799.9518751137\"\n[1] \"80 -799.129266973443\"\n[1] \"100 -799.002745451039\"\n[1] \"120 -798.945377403758\"\n[1] \"140 -798.91051104072\"\n[1] \"160 -798.887897813767\"\n[1] \"20 -815.12255712431\"\n[1] \"40 -815.861463325247\"\n[1] \"60 -816.65277780204\"\n[1] \"80 -817.367420905963\"\n[1] \"100 -817.981695199728\"\n[1] \"120 -818.482121987687\"\n[1] \"140 -818.87830886769\"\n[1] \"160 -819.188917828566\"\n[1] \"180 -819.43207261762\"\n[1] \"200 -819.622680437914\"\n[1] \"220 -819.772400789357\"\n[1] \"240 -819.890235329328\"\n[1] \"260 -819.983130882009\"\n[1] \"280 -820.05646679529\"\n[1] \"300 -820.114425855522\"\n[1] \"320 -820.160272929299\"\n[1] \"340 -820.196564739275\"\n[1] \"360 -820.225308853616\"\n[1] \"380 -820.248085081872\"\n[1] \"20 -851.834637132759\"\n[1] \"40 -846.408634750858\"\n[1] \"60 -845.373438543406\"\n[1] \"80 -845.241334679279\"\n[1] \"100 -845.152403322103\"\n[1] \"120 -844.93698044634\"\n[1] \"140 -844.478918069205\"\n[1] \"160 -843.859272082294\"\n[1] \"180 -843.396830866521\"\n[1] \"200 -843.157791850473\"\n[1] \"220 -842.987198075861\"\n[1] \"240 -842.746387043908\"\n[1] \"260 -842.311020162512\"\n[1] \"280 -841.525460525532\"\n[1] \"300 -840.13307886189\"\n[1] \"320 -837.637646283637\"\n[1] \"340 -834.447395567086\"\n[1] \"360 -833.024355453823\"\n[1] \"380 -832.778629503937\"\n[1] \"20 -830.712619627954\"\n[1] \"40 -831.59431933264\"\n[1] \"60 -831.760529398233\"\n[1] \"20 -839.780784325688\"\n[1] \"40 -831.549890278729\"\n[1] \"60 -799.604532650141\"\n[1] \"80 -798.987352669928\"\n[1] \"100 -794.989269879538\"\n[1] \"120 -789.077330165171\"\n[1] \"140 -782.992659155106\"\n[1] \"160 -779.078509255756\"\n[1] \"180 -777.844806633688\"\n[1] \"200 -777.752544725934\"\n[1] \"20 -833.977026832454\"\n[1] \"40 -829.411289375974\"\n[1] \"60 -807.265963558671\"\n[1] \"80 -801.845362767957\"\n[1] \"100 -796.131513754935\"\n[1] \"120 -795.383807283214\"\n[1] \"140 -793.411136510273\"\n[1] \"160 -790.669333999831\"\n[1] \"180 -787.910419504715\"\n[1] \"200 -786.399810943812\"\n[1] \"220 -785.118217074068\"\n[1] \"240 -784.899208579399\"\n[1] \"20 -833.733328938719\"\n[1] \"40 -812.315430568933\"\n[1] \"60 -808.877039049294\"\n[1] \"80 -799.484504043974\"\n[1] \"100 -781.392623208429\"\n[1] \"120 -779.207456613408\"\n[1] \"140 -778.924812359465\"\n\n\n\npar(mar=c(4,4,1,1) + 0.1)\nplot(seq(2,KKmax), sigma.sum, type=\"l\", xlab=\"K\", \n     ylab=expression(hat(sigma)), lwd=2)\nabline(v=6, lty=3)\n\n\n\n\n\n\n\nFigure 82.3: What happens with the variance (bandwidth) as K increases\n\n\n\n\n\n\n82.3.1 Estimating the number of components in Bayesian settings 🎥\nIs BIC Bayesian?\n\nThe BIC has the term Bayesian in its name, but it is not considered a Bayesian method. It is considered a frequentist method that uses the likelihood of the model and the number of parameters to estimate the number of components. In contrast, Bayesian methods use the posterior distribution of the model parameters to estimate the number of components.\nSo what we want is to have a posterior estimate of the number of components. We can do this by using a Dirichlet process prior on the weights of the mixture model. The Dirichlet process is a nonparametric prior that allows for an infinite number of components, but only a finite number of them will be used in the posterior distribution.\n\n\n82.3.2 Bayesian Information Criteria (BIC) for Mixture Models\n\n\n\n\n\n\n\nFigure 82.4: Estimating the number of components\n\n\n\n\n\n\n\n\nFigure 82.5: Estimating the number of components\n\n\n\n\nK= maximum number of components\nK* = number of components that really generated the model\n\n\nK&lt;&lt;K*\n\nso far we used\n\n\\tilde{w} \\sim \\mathrm{Dir}(1, \\ldots ,1) = U(0,1)\n\\tag{82.2}\nbut this won’t work because the number of weights in the prior increases with K and has increasing influence on the posterior. We need to use a prior that reduces the influence on the posterior as K increase like: \n\\tilde{w} \\sim \\mathrm{Dir}(\\alpha/K, \\ldots, \\alpha/K)\n\\tag{82.3}\nwhere \\alpha is a hyperparameter that controls the strength of the prior. The larger the value of \\alpha, the more influence the prior has on the posterior distribution.\nif (w_1,\\ldots,w_K) \\sim \\mathrm{Dir}(\\alpha/K,\\ldots,\\alpha/K), then the expected number of occupied components is given by:\n\n\\begin{aligned}\n\\lim_{K \\to \\infty} \\mathbb{E}[K^*] &= \\sum_{i=0}^n \\frac{\\alpha}{\\alpha+i-1}\n\\\\ & \\approx \\int_0^1 \\frac{\\alpha}{\\alpha +x-1} dx \\qquad \\text{(Riemann sum approximation)}\n\\\\ & = \\alpha \\log\\left(\\frac{n+\\alpha-1}{\\alpha}\\right)\n\\end{aligned} \\qquad\n\\tag{82.4}\n\n\\mathbb{E}[K^*] \\approx \\alpha \\log\\left(\\frac{n+\\alpha-1}{\\alpha}\\right)\n\\tag{82.5}\nleaving us with just a single parameter \\alpha to tune. This is a very useful result because it allows us to estimate the number of components in a mixture model without having to specify the number of components in advance.\n\n\n82.3.3 Sample code for estimating the number of components and the partition structure in Bayesian models 📖\n\n## Full Bayesian estimation of a mixture model for density estimation in the galaxies dataset\nrm(list=ls())\n\n### Loading data and setting up global variables\nlibrary(MASS)\nlibrary(MCMCpack)\n\nLoading required package: coda\n\n\n##\n## Markov Chain Monte Carlo Package (MCMCpack)\n\n\n## Copyright (C) 2003-2025 Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park\n\n\n##\n## Support provided by the U.S. National Science Foundation\n\n\n## (Grants SES-0350646 and SES-0350613)\n##\n\ndata(galaxies)\nx  = galaxies\nn  = length(x)\nset.seed(781209)\n\n### Fitting a Bayesian mixture model with \nKK = 30   ## In this formulation, it should be interpreted as the \n          ## maximum number of components allowed\n\n## Finding the value of alpha consistent with 6 expected components a priori\nff = function(alpha)  alpha*log((82+alpha-1)/alpha) - 6\nalph = uniroot(ff, c(0.01, 20))\nalph$root  # 1.496393\n\n[1] 1.496393\n\n## Priors set up using an \"empirical Bayes\" approach\naa  = rep(1.5/KK,KK)  # We approximate 1.496393 by 1.5\neta = mean(x)    \ntau = sqrt(var(x))\ndd  = 2\nqq  = var(x)/KK\n\n## Initialize the parameters\nw     = rep(1,KK)/KK\nmu    = rnorm(KK, mean(x), sd(x))\nsigma = sd(x)/KK\ncc    = sample(1:KK, n, replace=T, prob=w)\n\n## Number of iterations of the sampler\nrrr   = 25000\nburn  = 5000\n\n## Storing the samples\ncc.out    = array(0, dim=c(rrr, n))\nw.out     = array(0, dim=c(rrr, KK))\nmu.out    = array(0, dim=c(rrr, KK))\nsigma.out = array(0, dim=c(rrr, KK))\nlogpost   = rep(0, rrr)\n\n\nfor(s in 1:rrr){\n  # Sample the indicators\n  for(i in 1:n){\n    v = rep(0,KK)\n    for(k in 1:KK){\n      v[k] = log(w[k]) + dnorm(x[i], mu[k], sigma, log=TRUE)  #Compute the log of the weights\n    }\n    v = exp(v - max(v))/sum(exp(v - max(v)))\n    cc[i] = sample(1:KK, 1, replace=TRUE, prob=v)\n  }\n  \n  # Sample the weights\n  w = as.vector(rdirichlet(1, aa + tabulate(cc, nbins=KK)))\n  \n  # Sample the means\n  for(k in 1:KK){\n    nk    = sum(cc==k)\n    xsumk = sum(x[cc==k])\n    tau2.hat = 1/(nk/sigma^2 + 1/tau^2)\n    mu.hat  = tau2.hat*(xsumk/sigma^2 + eta/tau^2)\n    mu[k]   = rnorm(1, mu.hat, sqrt(tau2.hat))\n  }\n  \n  # Sample the variances\n  dd.star = dd + n/2\n  qq.star = qq + sum((x - mu[cc])^2)/2\n  sigma = sqrt(1/rgamma(1, dd.star, qq.star))\n  \n  # Store samples\n  cc.out[s,]   = cc\n  w.out[s,]    = w\n  mu.out[s,]   = mu\n  sigma.out[s] = sigma\n  for(i in 1:n){\n    logpost[s] = logpost[s] + log(w[cc[i]]) + dnorm(x[i], mu[cc[i]], sigma, log=TRUE)\n  }\n  logpost[s] = logpost[s] + log(ddirichlet(w, aa))\n  for(k in 1:KK){\n      logpost[s] = logpost[s] + dnorm(mu[k], eta, tau, log=TRUE)\n  }\n  logpost[s] = logpost[s] + dgamma(1/sigma^2, dd, qq, log=TRUE) - 4*log(sigma)\n  if(s/500==floor(s/500)){\n    print(paste(\"s =\",s))\n  }\n}\n\n[1] \"s = 500\"\n[1] \"s = 1000\"\n[1] \"s = 1500\"\n[1] \"s = 2000\"\n[1] \"s = 2500\"\n[1] \"s = 3000\"\n[1] \"s = 3500\"\n[1] \"s = 4000\"\n[1] \"s = 4500\"\n[1] \"s = 5000\"\n[1] \"s = 5500\"\n[1] \"s = 6000\"\n[1] \"s = 6500\"\n[1] \"s = 7000\"\n[1] \"s = 7500\"\n[1] \"s = 8000\"\n[1] \"s = 8500\"\n[1] \"s = 9000\"\n[1] \"s = 9500\"\n[1] \"s = 10000\"\n[1] \"s = 10500\"\n[1] \"s = 11000\"\n[1] \"s = 11500\"\n[1] \"s = 12000\"\n[1] \"s = 12500\"\n[1] \"s = 13000\"\n[1] \"s = 13500\"\n[1] \"s = 14000\"\n[1] \"s = 14500\"\n[1] \"s = 15000\"\n[1] \"s = 15500\"\n[1] \"s = 16000\"\n[1] \"s = 16500\"\n[1] \"s = 17000\"\n[1] \"s = 17500\"\n[1] \"s = 18000\"\n[1] \"s = 18500\"\n[1] \"s = 19000\"\n[1] \"s = 19500\"\n[1] \"s = 20000\"\n[1] \"s = 20500\"\n[1] \"s = 21000\"\n[1] \"s = 21500\"\n[1] \"s = 22000\"\n[1] \"s = 22500\"\n[1] \"s = 23000\"\n[1] \"s = 23500\"\n[1] \"s = 24000\"\n[1] \"s = 24500\"\n[1] \"s = 25000\"\n\nnunique = function(x)   length(unique(x))\nKstar = apply(cc.out[-seq(1,burn),],1,nunique)\npar(mar=c(4,4,1,1) + 0.1)\nbarplot(table(Kstar)/sum(table(Kstar)), xlab=expression(K^\"*\"), ylab=\"Frequency\")\n\n\n\n\n\n\n\n#dev.print(file=\"postKstaralpha2.pdf\", dev=pdf)\n\n\n## Construct pairwise co-clustering matrix for this dataset\npairwise = matrix(0, nrow=n, ncol=n)\nfor(s in 1:(rrr-burn)){\n  for(i in 1:n){\n    for(j in i:n){\n      pairwise[i,j] = pairwise[i,j] + as.numeric(cc.out[s+burn,i]==cc.out[s+burn,j])\n      pairwise[j,i] = pairwise[i,j]\n    }\n  }  \n}\nDD = pairwise/max(pairwise)\n\nheatmapplot = function(DD, alab, subsetaxis, llc=FALSE){\n  n = dim(DD)[1]\n  #colorscale = rev(gray(0:100 / 100))\n  colorscale = c(\"white\", rev(heat.colors(100)))\n  nf = layout(matrix(c(1,2),nrow=1,ncol=2), c(7,1), TRUE)\n  par(mar=c(4,3,1,0.5))\n  \n  ###Display heat-map\n  image(seq(1,n), seq(1,n), DD, axes=F, xlab=\"\", ylab=\"\", \n        col=colorscale[seq(floor(min(100*DD)), floor(max(100*DD))) + 1])\n  axis(1,at=subsetaxis,labels=alab[subsetaxis],las=2,cex.axis=1)\n  axis(2,at=subsetaxis,labels=alab[subsetaxis],las=2,cex.axis=1)\n  box()\n  abline(v = llc+0.5)\n  abline(h = llc+0.5)\n  \n  ###Display color scale\n  par(mar=c(3,0,0,0))\n  plot(1:100,1:100,xlim=c(0,2),ylim=c(0,100),type=\"n\",axes=F,xlab =\"\",ylab =\"\")\n  yposr = 1:100\n  rect(0, yposr-.5, 0.5, yposr+.5,col = colorscale, border=F)\n  rect(0, .5, 0.5, 100.5,col = \"transparent\")\n  text(0.42,c(yposr[1],yposr[25],yposr[50],yposr[75],yposr[100]),c(\"0.00\",\"0.25\",\"0.50\",\"0.75\",\"1.00\"),pos=4,cex=1.1)\n}\nheatmapplot(DD, seq(1,n), seq(1,n,by=3))\n\n\n\n\n\n\n\n## Plot Bayesian estimate with pointwise credible bands along with kernel density estimate and frequentist point estimate\n## Compute the samples of the density over a dense grid\nxx  = seq(5000,37000,length=300)\ndensity.mcmc = array(0, dim=c(rrr-burn,length(xx)))\nfor(s in 1:(rrr-burn)){\n  for(k in 1:KK){\n    density.mcmc[s,] = density.mcmc[s,] + w.out[s+burn,k]*dnorm(xx,mu.out[s+burn,k],sigma.out[s+burn])\n  }\n}\ndensity.mcmc.m = apply(density.mcmc , 2, mean)\n\nyy = density(x)\ncolscale = c(\"black\", \"red\")\ndensity.mcmc.lq = apply(density.mcmc, 2, quantile, 0.025)\ndensity.mcmc.uq = apply(density.mcmc, 2, quantile, 0.975)\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1)+0.1)\nplot(xx, density.mcmc.m, type=\"n\",ylim=c(0,max(density.mcmc.uq)),xlab=\"Velocity\", ylab=\"Density\")\npolygon(c(xx,rev(xx)), c(density.mcmc.lq, rev(density.mcmc.uq)), col=\"grey\", border=\"grey\")\nlines(xx, density.mcmc.m, col=colscale[1], lwd=2)\nlines(yy, col=colscale[2], lty=2, lwd=2)\npoints(x, rep(0,n))\nlegend(27000, 0.00017, c(\"MCMC\",\"KDE\"), col=colscale[c(1,2)], lty=c(1,2), lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n##### Finding optimal partition according to Binder's loss function\n##\n## Function that computes the loss at a particular configuration\nLstst = function(cch, DD, Dbar){\n  z = 0\n  for(i in 1:(n-1)){\n    for(j in (i+1):n){\n      if(cch[i]==cch[j]){\n        z = z + (DD[i,j]-Dbar)\n      }\n    }\n  }\n  return(z)\n}\n\n## Initial value of the algorithm is the last iteration of the sampler\n## Using as.numeric(factor()) is a cheap way to force the cluster labels \n## to be sequential starting at 1\ncch = as.numeric(factor(cc))\n\n## Setup parameters for the recursive alorithm\nDbar = 0.50\noptLstst.old  = -Inf\noptLstst.new = Lstst(cch, DD, Dbar=Dbar)\nmaxiter = 50\nniter   = 1\nwhile((optLstst.old!=optLstst.new)&(niter&lt;=maxiter)){\n  for(i in 1:n){\n    nq   = max(cch) + 1\n    q    = rep(0, nq)\n    for(s in 1:nq){\n      ccht    = cch\n      ccht[i] = s\n      q[s] = Lstst(ccht, DD, Dbar=Dbar)\n    }\n    cch[i] = which.max(q)\n    cch = as.numeric(factor(cch))\n  }\n  optLstst.old = optLstst.new\n  optLstst.new = Lstst(cch, DD, Dbar=Dbar)\n  niter = niter+1\n}\n#print(nunique(cch))\n\n## Create another heatmap plot of the co-clustering matrix in which the \n## optimal clusters are represented.\ncchlo    = as.numeric(as.character(factor(cch, labels=order(unique(cch)))))\ncchlotab = table(cchlo)\nllc      = cumsum(cchlotab[-length(cchlotab)])\nheatmapplot(DD, seq(1,n), seq(1,n,by=3), llc=llc)\n\n\n\n\n\n\n\n#dev.print(file=\"galaxiesheatmap50.pdf\", dev=pdf)",
    "crumbs": [
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Determining the number of components - M5L9</span>"
    ]
  },
  {
    "objectID": "C3-L09-Ex1.html",
    "href": "C3-L09-Ex1.html",
    "title": "83  Homework on Bayesian Information Criteria (BIC) - M5L09HW1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>Homework on Bayesian Information Criteria (BIC) - M5L09HW1</span>"
    ]
  },
  {
    "objectID": "C3-L09-Ex2.html",
    "href": "C3-L09-Ex2.html",
    "title": "84  Homework on Estimating the number of components in Bayesian settings - M5L09HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>84</span>  <span class='chapter-title'>Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span>"
    ]
  },
  {
    "objectID": "C3-L09-Ex3.html",
    "href": "C3-L09-Ex3.html",
    "title": "85  Homework on Estimating the partition structure in Bayesian models - M5L09HW3",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>85</span>  <span class='chapter-title'>Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span>"
    ]
  },
  {
    "objectID": "C3-L09-Ex4.html",
    "href": "C3-L09-Ex4.html",
    "title": "86  Homework on BIC for zero-inflated mixtures - M5L09HW4",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>86</span>  <span class='chapter-title'>Homework on BIC for zero-inflated mixtures - M5L09HW4</span>"
    ]
  },
  {
    "objectID": "C4-L00.html",
    "href": "C4-L00.html",
    "title": "87  Week 0: Introductions to time series analysis and the AR(1) process",
    "section": "",
    "text": "87.1 Course Card\nI decided to migrate some material that is auxiliary to the course:",
    "crumbs": [
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>Week 0: Introductions to time series analysis and the AR(1) process</span>"
    ]
  },
  {
    "objectID": "C4-L00.html#course-card",
    "href": "C4-L00.html#course-card",
    "title": "87  Week 0: Introductions to time series analysis and the AR(1) process",
    "section": "",
    "text": "Course: Bayesian Statistics: Time Series\nOffered by: University of California, Santa Cruz\nInstructor: Raquel Prado\nCertificate: Yes\nLevel: Graduate\nCommitment: 4 weeks of study, 3-4 hours/week",
    "crumbs": [
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>Week 0: Introductions to time series analysis and the AR(1) process</span>"
    ]
  },
  {
    "objectID": "C4-L00.html#overview-of-the-course",
    "href": "C4-L00.html#overview-of-the-course",
    "title": "87  Week 0: Introductions to time series analysis and the AR(1) process",
    "section": "87.2 Overview of the course",
    "text": "87.2 Overview of the course\n This course seems very similar to classic basic time series course without the Bayesian part. (AR, MA, ARMA, ARIMA, SARIMA, DLM etc.)\nOne of the questions I had when I started this course was what is the difference between a Bayesian approach to time series analysis and a classical approach. The following is a summary of what I found:\n\n\n\n\n\n\nImportantAre we Being Bayesian ?\n\n\n\n The Bayesian approach presents primarily in:\n\nSections on Bayesian inference where we do inference on the parameters of the models.\nBayesian prediction unlike an MLE prediction is a distribution of predictions not just a point estimate, and therefore is useful for quantifying uncertainty.\nWe also cover some material on model selection - this again is where the Bayesian approach to optimization presents more powerful tools than the classical approach.\nWhen we want to quantify the uncertainty in our model we have four sources of uncertainty:\n\nUncertainty due to using the correct model (structure).\n\nI consider this is an epistemic uncertainty -\nOne could reduce it by collecting more data, then applying the Bayesian model selection to choose the best model.\n\nUncertainty due to the estimation of the model parameters. This is an epistemic uncertainty - we can reduce it by collecting more data reducing the plausible intervals for these parameters under the bayesian approach.\nUncertainty due to random shocks \\epsilon_t. for the period being predicted. This is an aleatory uncertainty.\nUncertainty in the forecasted values X_{t+h} Items 2-3 can be quantified using a plausible interval in the Bayesian approach and as we predict further into the future the interval will grow.\n\nModel selection is a big part of the Bayesian approach. We can use the DIC, WAIC, and LOO to compare models.\n\n\n\n\nThe book by Professor Prado is very comprehensive and covers plenty of additional models and references lots of recent research. These including VAR, VARMA models, Kalman filters, SMC/Particle filters, etc. These are useful for the continuous control flavours of RL. But you will need to learn it on your own.\nIn the capstone project that is the next course in the specialization the teacher adds another layer of sophistication by introducing mixtures of TS models.\nHowever unlike some courses I took we dive deep enough and get sufficient examples to understand how to put all the bits together into more sophisticated time series models.\n\n\n87.2.1 Mathematical Review\n\nThere is a issues with mathematics most of the results and techniques are so rarely useful that students will soon forget most but a few very useful results. Having a good memory is a great asset in mathematics but is rarely enough. I like to review some mathematical results from my undergraduate days every five years or so. This helps me keep many of the results fresh in my mind and also makes reading new mathematics easier. Fundamentals in mathematics can fo a very long way. This is material from topology, determinants and solving linear equations, numerical methods for decomposing matrices, and so on. Definitions of certain groups.\nOne reason this and other Bayesian courses and books can be challenging and even overwhelming is that they can use lots of mathematics. This can range from high school material like complex numbers and quadratics formulas to intermediate results like finding root of characteristic polynomials, eigenvalues, Topelitz matrices, jordan forms, and advanced topics like the Durbin-Levinson recursion and certain results from functional analysis theory.\n\nNote that I have not even touched on probability and statistics in that list.\nRather than complain I see this as an opportunity to review/learn some mathematics and statistics that can be useful to a data scientist. During my last sting in Data science I often was able to write formulas but more often then not felt that I lacked sufficient mathematical tools to manipulate them to get the kind of results I wanted. Rather then learning lots of mathematics I wanted to find the most practical and useful results for wrangling maths. When I was a physics undergraduate these might be trigonometric identities, completing the square, being familiar with many integrals and Taylor or Maclaurin series approximations and a few useful inequalities occasionally we use l’Hopital’s rule. Familiarity with some ODEs was also greatly beneficial as these come up in many physical models. Later on hermitian and unitary matrices, fourier expansions, spectral theory, and some results from functional analysis were useful.\nFor statistics we have the variants of the law of large numbers and the central limit theorem, convergence theorems, manipulations of the normal distribution, linear properties of expectation can get you along way. But you have to remember lots of definitions and there are lots of results and theorems that seem to be stepping stones to other results rather than any practical use.\nOn the other hand conjugacy of certain distributions as demonstrated by Herbert Lee and other instructors in this specialization are often very challenging. Charts of Convergence of distributions to other distributions under certain conditions are neat but. There is Hoeffding’s inequality and the Markov’s inequality which can be useful but like most results in mathematics I never had a where they might be used. Then there are certain results - convergence of Markov chains, doubly stochastic matrices. De Finetti’s theorem in statistics.\nI have found that the more I learn the more I can understand and appreciate the material.\n\nThe autoregressive process gives rise to Toeplitz matrices which can be solved using the Durbin-Levinson recursion mentioned many times in the course.\nDurbin-Levinson recursion - is an advanced topic not covered in Numerical Analysis courses or Algebra courses I took.\nTo use it with time series we also need to understand the Yule-Walker equations.\nar(p) require some linear algebra concepts like eigenvalues and Eigenvectors, and characteristic polynomials.\nThe AR(p) the Wold decomposition theorem to get to the infinite order moving average representation and this is not a result I recall learning in my functional analysis course. We also use some complex numbers and Fourier analysis and spectral density functions.\n\nSummarize some of the extra curricular material I found useful in the course.\n\nComplex numbers\nEigenvalues, Eigenvectors and characteristic polynomials\nDurbin-Levinson recursion\nYule-Walker equations\nWiener process (Random walk)\nBrownian motion (Continuous Random walk with drift)\nMarkov Chains ()\nMartingales ()\nStopping theorem\nKalman filter\nWold’s theorem\nDe Finetti’s theorem\nCholesky decomposition\n\n\n\n87.2.2 Complex Numbers (Review)\nWhen we wish to find the roots of real valued polynomials we will often encounter complex numbers. In this course such polynomials arise naturally in the characteristic polynomials of AR(p) processes.\nWe will need the polar form of complex numbers to represent some variants of AR(p) process.\nThe numbers in the Complex field z \\in \\mathbb{C} numbers are numbers that can be expressed in the form z = a + bi, where a,b\\in\\mathbb{R} and i is the imaginary unit. The imaginary unit i is defined as the square root of -1. Complex numbers can be added, subtracted, multiplied, and divided just like real numbers.\nThe complex conjugate  of a complex number z = a + bi is denoted by \\bar{z} = a - bi. The magnitude of a complex number z = a + bi is denoted by |z| = \\sqrt{a^2 + b^2}. This is sometimes called the modulus of the complex number in this course. The argument of a complex number z = a + bi is denoted by \\text{arg}(z) = \\tan^{-1}(b/a). The polar form of a complex number is given by z = r e^{i \\theta}, where r = |z| and \\theta = \\text{arg}(z).complex conjugate\nThe polar form of a complex number is given by:\n\n\\begin{aligned}\nz &= \\mid z\\mid e^{i \\theta} \\\\\n  &= r (\\cos(\\theta) + i \\sin(\\theta))\n\\end{aligned}\n\\tag{87.1}\nwhere:\n\n|z| is the magnitude of the complex number, i.e. the distance from the origin to the point in the complex plane.\n\\theta is the angle of the complex number.\n\nI think we will also need the unit roots.\n\n\n87.2.3 Eigenvalues, Eigenvectors the characteristic polynomials and Unit roots\nThe Eigenvalues of a matrix are the roots of the characteristic polynomial of the matrix. The characteristic polynomial of a matrix A is defined as:\n\n\\begin{aligned}\n\\text{det}(A - \\lambda I) = 0\n\\end{aligned}\n\nwhere \\lambda is the Eigenvalue and I is the identity matrix. The eigenvectors of a matrix are the vectors that satisfy the equation:\n\n\\begin{aligned}\nA v = \\lambda v\n\\end{aligned}\n\nwhere v is the eigenvector and \\lambda is the eigenvalue. The eigenvalues and eigenvectors of a matrix are used in many applications in mathematics and physics, including the diagonalization of matrices, the solution of differential equations, and the analysis of dynamical systems.\n\n87.2.3.1 Unit Roots\nA unit root is a root of the characteristic polynomial of an autoregressive model that is equal to 1. The presence of a unit root in an autoregressive model indicates that the model is not stationary. The unit root test is a statistical test that is used to determine whether a time series is stationary or non-stationary. The unit root test is based on the null hypothesis that the time series has a unit root, and the alternative hypothesis that the time series is stationary. The unit root test is used to determine whether a time series is stationary or non-stationary, and is an important tool in time series analysis.\n\n\n\n87.2.4 Spectral analysis (1898)\nThe power spectrum of a signal is the squared absolute value of its Fourier transform. If it is estimated from the discrete Fourier transform it is also called periodogram. Usually estimated using the a fast Fourier transform (FFT) algorithm.\n\n\n87.2.5 Wold’s theorem - (extra curricular) circa 1939\n In the 1920 George Udny Yule and Eugen Slutsky were researching time series and they came up with two different ways to represent a time series.\n\nYule’s researches led to the notion of the autoregressive scheme. \n\\begin{aligned}\nY_{t} & = \\sum _{j=1}^{p} \\phi _{j} Y_{t-j} + u_{t}\n\\end{aligned}\n\\tag{87.2}\nSlutsky’s researches led to the notion of a moving average scheme. \n\\begin{aligned}\nY_{t} & =\\sum _{j=0}^{q} \\theta _{j} u_{t-j}\n\\end{aligned}\n\\tag{87.3}\n\nwe can use the two schemes together and get the ARMA(p,q) model:\n\n\\begin{aligned}\nY_{t} & = \\sum _{j=1}^{p} \\phi _{j} Y_{t-j} + u_{t} + \\sum _{j=0}^{q} \\theta _{j} u_{t-j}\n\\end{aligned}\n\\tag{87.4}\nwhere:\nThe following is extracted from: the wikipedia at https://en.wikipedia.org/wiki/Wold%27s_theorem\nWold’s decomposition AKA called the Wold representation theorem states that:\n\nEvery covariance-stationary time series Y_{t} can be written as the sum of two time series, one deterministic and one stochastic.\n\nFormally:\n\n\\begin{aligned}\nY_{t} & =\\sum _{j=0}^{\\infty }  \\underbrace{b_{j}\\epsilon _{t-j}}_{\\text{stochastic}} + \\underbrace{\\eta _{t}}_{\\text{deterministic}} \\\\\n&= \\sum _{j=0}^{\\infty } b_{j}\\epsilon _{t-j} + \\phi_{j} y_{t-j}\n\\end{aligned}\n\nwhere:\n\n{Y_{t}} is the time series being considered,\n{\\epsilon _{t}} is an white noise sequence called innovation process that acts as an input to the linear filter {\\{b_{j}\\}}.\n{b} is the possibly infinite vector of moving average weights (coefficients or parameters)\n{\\eta _{t}} is a “deterministic” time series, in the sense that it is completely determined as a linear combination of its past values It may include “deterministic terms” like sine/cosine waves of {t}, but it is a stochastic process and it is also covariance-stationary, it cannot be an arbitrary deterministic process that violates stationarity.\n\nThe moving average coefficients have these properties:\n\nStable, that is, square summable \\sum _{j=1}^{\\infty } \\mid b_{j}|^{2} &lt; \\infty\nCausal (i.e. there are no terms with j &lt; 0)\nMinimum delay\nConstant (b_j independent of t)\nIt is conventional to define b_0=1\n\nAny stationary process has this seemingly special representation. Not only is the existence of such a simple linear and exact representation remarkable, but even more so is the special nature of the moving average model.\nThis result is used without stating its name in the course when we are show the AR(p) representation in terms of moving averages.",
    "crumbs": [
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>Week 0: Introductions to time series analysis and the AR(1) process</span>"
    ]
  },
  {
    "objectID": "C4-L00.html#kalman-filter-1960",
    "href": "C4-L00.html#kalman-filter-1960",
    "title": "87  Week 0: Introductions to time series analysis and the AR(1) process",
    "section": "87.3 Kalman Filter (1960)",
    "text": "87.3 Kalman Filter (1960)\n\n\\begin{aligned}\nx_{t} & = F_{t} x_{t-1} + G_{t} u_{t} + w_{t} && \\text{(transition equation)} \\\\\ny_{t} & = H_{t} x_{t} + v_{t} && \\text{(observation equation)}\n\\end{aligned}\n\\tag{87.5}\nwhere:\n\nx_{t} is the state vector at time t,\nF_{t} is the state transition matrix,\nG_{t} is the control input matrix,\nu_{t} is the control vector,\nw_{t} is the process noise vector,\ny_{t} is the observation vector at time t,\nH_{t} is the observation matrix,\nv_{t} is the observation noise vector.\n\nThe Kalman filter is a recursive algorithm that estimates the state of a linear dynamic system from a series of noisy observations. The Kalman filter is based on a linear dynamical system model that is defined by two equations: the state transition equation and the observation equation. The state transition equation describes how the state of the system evolves over time, while the observation equation describes how the observations are generated from the state of the system. The Kalman filter uses these two equations to estimate the state of the system at each time step, based on the observations received up to that time step. This could be implemented in real time in the 1960s and was used in the Apollo missions.\nThe Extended Kalman Filter (EKF) is an extension of the Kalman filter that can be used to estimate the state of a nonlinear dynamic system. The EKF linearizes the nonlinear system model at each time step and then applies the Kalman filter to the linearized system. The EKF is an approximation to the true nonlinear system, and its accuracy depends on how well the linearized system approximates the true system.\n\n87.3.1 Box Jenkins Method (1970)\nsee Box Jenkins Method\nA five step process for identifying, selecting and assessing ARMA (and similar) models.\n\nThere are three courses on Stochastic Processes on MIT OCW that I found useful:\n\nIntroduction to Stochastic Processes\nDiscrete Stochastic Processes\nhas lecture videos and notes\npoisson processes\nAdvanced Stochastic Processes\nmartingales\nito calculus",
    "crumbs": [
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>Week 0: Introductions to time series analysis and the AR(1) process</span>"
    ]
  },
  {
    "objectID": "C4-L01.html",
    "href": "C4-L01.html",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "",
    "text": "88.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-c4-introduction",
    "href": "C4-L01.html#sec-c4-introduction",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "",
    "text": "88.1.1 Welcome to Bayesian Statistics: Time Series\n\nObligatory introduction to the course and the instructors.\nRaquel Prado is a professor of statistics in the Baskin School of Engineering at the University of California, Santa Cruz. She was the recipient 2022 Zellner Medal, see Weckerle (2022).\n\n\n\n88.1.2 Introduction to R\n\nIntroduction to R",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-stationarity-acf-pacf",
    "href": "C4-L01.html#sec-stationarity-acf-pacf",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.2 Stationarity the ACF and the PACF 🎥",
    "text": "88.2 Stationarity the ACF and the PACF 🎥\nBefore diving into the material here is a brief overview of the notations for timer series.\n\n\n\n\n\n\nTip 88.1: Notation\n\n\n\n\n\\{y_t\\} - the time series process, where each y_t is a univariate random variable and t are the time points that are equally spaced.\ny_{1:T} or y_1, y_2, \\ldots, y_T - the observed data.\nYou will see the use of ’ to denote the transpose of a matrix,\nand the use of \\sim to denote a distribution.\nunder tildes \\utilde{y} are used to denote estimates of the true values y.\nE matrix of eigenvalues\n\\Lambda = diagonal(\\alpha_1, \\alpha_2, \\ldots , \\alpha_p) is a diagonal matrix with the eigenvalues of \\Sigma on the diagonal.\nJ_p(1) = a p by p Jordan form matrix with 1 on the super-diagonal\n\nalso see (Prado, Ferreira, and West 2023, 2–3)\n\n\n\n88.2.1 Stationarity 🎥\n\n\n\n\n\n\n\nFigure 88.1: strong and weak stationarity\n\n\n Stationarity c.f. (Prado, Ferreira, and West 2023, sec. 1.2) is a fundamental concept in time series analysis.\n\n\n\n\n\n\nImportantTL;DR – Stationarity\n\n\n\n\nStationarity\n\n\n\n\nA time series is said to be stationary if its statistical properties such as mean, variance, and auto-correlation do not change over time.\n\n\n\nWe make this definition more formal in the definitions of strong and weak stationarity below.\n\n\n\nStationarityStationarity is a key concept in time series analysis. A time series is said to be stationary if its statistical properties such as mean, variance, and auto-correlation do not change over time.\n\nDefinition 88.1 (Strong Stationarity)  Let y_t be a time series. We say that y_t is stationary if the following conditions hold:Strong Stationarity\nLet \\{y_t\\} \\quad \\forall n&gt;0 be a time series and h &gt; 0 be a lag. If for any subsequence the distribution of y_t, y_{t+1}, \\ldots, y_{t+n} is the same as the distribution of y_{t+h}, y_{t+h+1}, \\ldots, y_{t+h+n} we call the series strongly stationary.\n\nAs it’s difficult to verify strong stationarity in practice, we will often use the following weaker notion of stationarity.\n\nDefinition 88.2 (Weak Stationarity)   The mean, variance, and auto-covariance are constant over time.Weak StationaritySecond-order Stationarity\n\n\\begin{aligned}\n\\mathbb{E}[y_t] &= \\mu \\quad \\forall t \\\\\n\\mathbb{V}ar[y_t] &= \\nu =\\sigma^2 \\quad \\forall t \\\\\n\\mathbb{C}ov[y_t , y_s ] &= γ(t − s)\n\\end{aligned}\n\\tag{88.1}\n\n\nStrong stationarity \\implies Weak stationarity, but\nThe converse is not true.\nIn this course when we deal with a Gaussian process, our typical use case, they are equivalent!\n\n\n\n\n\n\n\nCautionCheck your understanding\n\n\n\nQ. Can you explain with an example when a time series is weakly stationary but not strongly stationary?\n\n\n\n\n88.2.2 The auto-correlation function ACF 🎥\n\n\n\n\n\n\n\nFigure 88.2: The auto correlation function ACF\n\n\n The auto correlation is simply how correlated a time series is with itself at different lags.\n\nCorrelation in general is defined in terms of covariance of two variables.\nThe covariance is a measure of the joint variability of two random variables.\n\n\n\n\n\n\n\nImportant\n\n\n\nRecall that the Covariance between two random variables y_t and y_s is defined as:\n\n\\begin{aligned}\n\\mathbb{C}ov[y_t, y_s] &= \\mathbb{E}[(y_t-\\mathbb{E}[y_t])(y_s-\\mathbb{E}[y_s])] \\\\\n              &= \\mathbb{E}[(y_t-\\mu_t)(y_s-\\mu_s)] \\\\\n              &= E[y_t y_s] - \\mu_t \\times \\mu_s\n\\end{aligned} \\qquad\n\\tag{88.2}\nWe get the second line by substituting \\mu_t = \\mathbb{E}(y_t) and \\mu_s = \\mathbb{E}(y_s) using the definition of the mean of a RV. the third line is by multiplying out and using the linearity of the expectation operator.\n\n\n\n\n\n\n\n\nTip 88.2: AFC notation\n\n\n\nWe will frequently use the notation \\gamma(h) to denote the autocovariance for a lag h i.e. between y_t and y_{t+h}\n\n\\gamma(h) = \\mathbb{C}ov[y_t, y_{t+h}] \\qquad\n\\tag{88.3}\n\n\nWhen the time series is stationary, then the covariance only depends on the lag h = \\|t-s\\| and we can write the covariance as \\gamma(h).\nLet \\{y_t\\} be a time series. Recall that the covariance between two random variables y_t and y_s is defined as:\n\n\\gamma(t,s)=\\mathbb{C}ov[y_t, y_s] = \\mathbb{E}[(y_t-\\mu_t)(y_s-\\mu_s)] \\qquad\n\\tag{88.4}\nwhere \\mu_t = \\mathbb{E}(y_t) and \\mu_s = \\mathbb{E}(y_s) are the means of y_t and y_s respectively.\n\n\\mu_t = \\mathbb{E}(y_t) \\qquad \\mu_s = \\mathbb{E}(y_s)\n\\tag{88.5}\n\n\\text{Stationarity} \\implies \\mathbb{E}[y_t] = \\mu \\quad \\forall t \\qquad \\therefore \\quad \\gamma(t,s)=\\gamma(|t-s|)\n\nIf h&gt;0 \\qquad \\gamma(h)=\\mathbb{C}ov[y_t,y_{t-h}]\n\n\n\n\n\n\nImportantAutocorrelation Function (AFC)\n\n\n\n\n\n\\rho(t,s) = \\frac{\\gamma(t,s)}{\\sqrt{\\gamma(t,t)\\gamma(s,s)}}\n\\tag{88.6}\n\n\nauto-correlation AFC\n\\text{Stationarity} \\implies \\rho(h)=\\frac{\\gamma(h)}{\\gamma(o)} \\qquad \\gamma(0)=Var(y_t)\n\n\n\n\n\n\n\n\nFigure 88.3: sample AFC\n\n\n\ny_{1:T}\n\\tag{88.7}\n\n\n\n\n\n\nImportantThe sample AFC\n\n\n\n\n\\hat\\gamma(h)= \\frac{1}{T} \\sum_{t=1}^{T-h}(y_{t+h}-\\bar y )(y_t-\\hat y)\n\\tag{88.8}\nwhere \\bar y is the sample mean of the time series y_{1:T}, and \\hat y is the sample mean of the time series y_{1:T-h}.\n\n\n\n\\bar y = \\frac{1}{T} \\sum_{t=1}^{T}y_t\n\\tag{88.9}\n\n\\hat \\rho = \\frac{\\hat\\gamma(h)}{\\hat\\gamma(o)}\n\\tag{88.10}\n\n\n88.2.3 The partial auto-correlation function PACF 📖\n\nDefinition 88.3 (Partial Auto-correlation Function (PACF)) Let {y_t} be a zero-mean stationary process, and let\n\n\\hat{y}_t^{h-1} = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\ldots + \\beta_{h-1} y_{t-(h-1)}\n\\tag{88.11}\nbe the best linear predictor of y_t based on the previous h − 1 values \\{y_{t−1}, \\ldots , y_{t−h+1}\\}. The best linear predictor of y_t based on the previous h − 1 values of the process is the linear predictor that minimizes\n\nE[(y_t − \\hat{y}_y^{h-1})^2]\n\\tag{88.12}\nThe partial autocorrelation of this process at lag h, denoted by \\phi(h, h) is defined as: partial auto-correlation PAFC\n\n\\phi(h, h) = Corr(y_{t+h} − \\hat{y}_{t+h}^{h-1}, y_t − \\hat{y}_t^{h-1})\n\\tag{88.13}\nfor h \\ge 2 and \\phi(1, 1) = Corr(y_{t+1}, y_{t}) = \\rho(1).\n\nThe partial autocorrelation function can also be computed via the Durbin-Levinson recursion for stationary processes as \\phi(0, 0) = 0,\n\n\\phi(n, n) = \\frac{\\rho(n) − \\sum_{h=1}^{n-1} \\phi(n − 1, h)\\rho(n − h)}{1- \\sum_{h=1}^{n-1}\\phi(n − 1, h)\\rho(h)}\n\\tag{88.14}\nfor n \\ge 1, and\n\n\\phi(n, h) = \\phi(n − 1, h) − \\phi(n, n)\\phi(n − 1, n − h),\n\\tag{88.15}\nfor n \\ge 2, and h = 1, \\ldots , (n − 1).\nNote that the sample PACF can be obtained by substituting the sample autocorrelations and the sample auto-covariances in the Durbin-Levinson recursion.",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-differencing-and-smoothing",
    "href": "C4-L01.html#sec-differencing-and-smoothing",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.3 Differencing and smoothing 📖",
    "text": "88.3 Differencing and smoothing 📖\n Differencing and smoothing are techniques used to remove trends and seasonality in time series data. They are covered in the (Prado, Ferreira, and West 2023, sec. 1.4).\nMany synthetic time series models are built under the assumption of stationarity. However, in the real world time series data often present non-stationary features such as trends or seasonality. These features render such a time series non-stationary, and therefore, not suitable for analysis using the tools and methods we have discussed so far. However practitioners can use techniques for detrending, deseasonalizing and smoothing that when applied to such observed data transforms it into a new time series that is consistent with the stationarity assumption.\nWe briefly discuss two methods that are commonly used in practice for detrending and smoothing.\n\n88.3.1 Differencing\nDifferencing, is a method which removes the trend from a time series data. The first difference of a time series is defined in terms of the difference operator, denoted as D, that produces the transformation differencing operator D\n\nDy_t \\doteqdot y_t - y_{t-1}\n\\tag{88.16}\nHigher order differences are obtained by successively applying the operator D. For example,\n\nD^2y_t = D(Dy_t) = D(y_t - y_{t-1}) = y_t - 2y_{t-1} + y_{t-2}\n\\tag{88.17}\nDifferencing can also be written in terms of the so called back-shift operator B, with back-shift operator B\n\nBy_t \\doteqdot y_{t-1},\n\\tag{88.18}\nso that\n\nDy_t \\doteqdot (1 - B) y_t\n\\tag{88.19}\nand\n\nD^dy_t \\doteqdot (1 - B)^d y_t.\n\\tag{88.20}\nthis notation lets us write the differences in by referencing items backwards in time, which is often more intuitive and also useful, for example, when we will want to write the differencing operator in terms of a polynomial.\n\n\n88.3.2 Smoothing\n Moving averages, which is commonly used to “smooth” a time series by removing certain features (e.g., seasonality) to highlight other features (e.g., trends).\nA moving average is a weighted average of the time series around a particular time t. In general, if we have data y_{1:T}, we could obtain a new time series such that moving average\n\nz_t = \\sum_{j=-q}^{p} w_j y_{t+j} \\qquad\n\\tag{88.21}\nfor t = (q + 1) : (T − p), with weights w_j \\ge 0 and \\sum^p_{j=−q} w_j = 1\nWe will frequently work with moving averages for which\n\np = q \\qquad \\text{(centered)}\n\nand\n\nw_j = w_{−j} \\forall j  \\text{(symmetric)}\n\nAssume we have periodic data with period d. Then, symmetric and centered moving averages can be used to remove such periodicity as follows:\n\nIf d = 2q :\n\n\nz_t =  \\frac{1}{d} \\left(\\frac{1}{2} y_{t−q} + y_{t−q+1} + \\ldots + y_{t+q−1} + \\frac{1}{2} y_{t+q}\\right )\n\\tag{88.22}\n\nif d = 2q + 1 :\n\n\nz_t = \\frac{1}{d} \\left( y_{t−q} + y_{t−q+1} + \\ldots + y_{t+q−1} + y_{t+q}\\right )\n\\tag{88.23}\n\nExample 88.1 (Seasonal Moving Average) To remove seasonality in monthly data (i.e., seasonality with a period of d = 12 months), we use a moving average with p = q = 6, a_6 = a_{−6} = 1/24, and a_j = a_{−j} = 1/12 for j = 0, \\ldots , 5 , resulting in:\n\nz_t = \\frac{1}{24} y_{t−6} + \\frac{1}{12}y_{t−5} + \\ldots + \\frac{1}{12}y_{t+5} + \\frac{1}{24}y_{t+6}\n\\tag{88.24}",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-differencing-and-smoothing-examples",
    "href": "C4-L01.html#sec-differencing-and-smoothing-examples",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.4 ACF PACF Differencing and Smoothing Examples 🎥",
    "text": "88.4 ACF PACF Differencing and Smoothing Examples 🎥\nThis video walks us through the code snippets in Figure 88.4 and Listing 88.1 below and provides examples of how to compute the ACF and PACF of a time series, how to use differencing to remove trends, and how to use moving averages to remove seasonality.\n\nOutline:\n\nWe begin by simulating data using the code in Section 88.6\nWe simulates white noise data using the rnorm(1:2000,mean=0,sd=1) function in R\nWe plot the white noise data which we can see lacks a temporal structure.\nWe plot the ACF using the acf function in R:\n\nwe specify the number of lags using the lag.max=20\nwe shows a confidence interval for the ACF values\n\nWe plot the PACF using the pacf function in R\nNext we define some time series objects in R using the ts function\n\nwe define and plot monthly data starting in January 1960\nwe define and plot yearly data with one observation per year starting in 1960\nwe define and plot yearly data with four observations per year starting in 1960\n\nWe move on to smoothing and differencing in Section 88.3\nWe load the CO2 dataset in R and plot it\nwe plot the ACF and PACF of the CO2 dataset\nwe use the filter function in R to remove the seasonal component of the CO2 dataset we plot the resulting time series highlighting the trend.\nTo remove the trend we use the diff function in R to take the first and second differences of the CO2 dataset\n\nthe diff function takes a parameter differences which specifies the number of differences to take\n\nwe plot the resulting time series after taking the first and second differences\nthe ACF and PACF of the resulting time series are plotted, they look different, in that they no longer have the slow decay characteristic of time series with a trend.\n\n\nThe r-code for the examples is provided below.",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-differencing-and-smoothing-reading",
    "href": "C4-L01.html#sec-differencing-and-smoothing-reading",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.5 R code for Differencing and filtering via moving averages 📖",
    "text": "88.5 R code for Differencing and filtering via moving averages 📖\n \n\n1data(co2)\n2co2_1stdiff = diff(co2,differences=1)\n3co2_ma = filter(co2,filter=c(1/24,rep(1/12,11),1/24),sides=2)\n\npar(mfrow = c(3,1),\n    mar   = c(3, 4, 2, 1), \n    cex.lab=1.2,\n    cex.main=1.2)\n4plot(co2)\n5plot(co2_1stdiff)\n6plot(co2_ma)\n\n\n1\n\nLoad the CO2 dataset in R\n\n2\n\nTake first differences to remove the trend\n\n3\n\nFilter via moving averages to remove the seasonality\n\n4\n\nplot the original data\n\n5\n\nplot the first differences (removes trend, highlights seasonality)\n\n6\n\nplot the filtered series via moving averages (removes the seasonality, highlights the trend)\n\n\n\n\n\n\n\n\n\n\nFigure 88.4: R code: for Differencing and filtering via moving averages",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-white-noise-simulation",
    "href": "C4-L01.html#sec-white-noise-simulation",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.6 R Code: Simulate data from a white noise process 📖",
    "text": "88.6 R Code: Simulate data from a white noise process 📖\n \n\n\n\n\nListing 88.1: Simulate data with no temporal structure (white noise)\n\n\nset.seed(2021)\nT=200\nt =1:T\ny_white_noise=rnorm(T, mean=0, sd=1)\n\n1yt=ts(y_white_noise, start=c(1960), frequency=1)\n\n\npar(mfrow = c(3, 1),\n    mar = c(3, 4, 2, 1), \n    cex.lab = 1.3, \n    cex.main = 1.3) \nyt=ts(y_white_noise, start=c(1960), frequency=1)\nplot(yt, type = 'l', col='red', \n     xlab = 'time (t)', \n2     ylab = \"Y(t)\")\nacf(yt, \n    lag.max = 20, \n    xlab = \"lag\",\n    ylab = \"Sample ACF\",\n    ylim=c(-1,1),main=\"\")\npacf(yt, \n     lag.max = 20,\n     xlab = \"lag\",\n     ylab = \"Sample PACF\",\n     ylim=c(-1,1),main=\"\")\n\n\n\n\n\n1\n\nDefine a time series object in R - Assume the data correspond to annual observations starting in January 1960\n\n2\n\nPlot the simulated time series, their sample ACF and their sample PACF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nWeckerle, Melissa. 2022. “Statistics professor wins prestigious professional statistics society award  Baskin School of Engineering.” https://engineering.ucsc.edu/news/statistics-professor-wins-zellner-medal.",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L02.html",
    "href": "C4-L02.html",
    "title": "89  The AR(1) process: definitions and properties - M1L2",
    "section": "",
    "text": "89.1 The AR(1) process 🎥\nWe will next introduce the autoregressive process of order one, or AR(1) process, which is a fundamental model in time series analysis. We will discuss the definition of the AR(1) process, its properties, and how to simulate data from an AR(1) process.",
    "crumbs": [
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>The AR(1) process: definitions and properties - M1L2</span>"
    ]
  },
  {
    "objectID": "C4-L02.html#the-ar1-process",
    "href": "C4-L02.html#the-ar1-process",
    "title": "89  The AR(1) process: definitions and properties - M1L2",
    "section": "",
    "text": "Figure 89.1: AR(1) definition\n\n\n\n\n\n\n\n\nFigure 89.2: AR(1) properties\n\n\n\n\n89.1.1 AR(1) Definition\n The AR(1) process is defined as:\n\ny_t = \\phi y_{t-1} + \\varepsilon_t \\qquad \\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0, v)\n\\tag{89.1}\nwhere:\nwhere \\phi is the AR(1) coefficient\n\n\n89.1.2 AR(1) Recursive Expansion\nRecursive substitution yields:\n\n\\begin{aligned}\ny_t &= \\phi(\\phi y_{t-1} )+ \\varepsilon_t \\\\\n    &= \\phi^2 y_{t-2} + \\phi \\varepsilon_{t-1} + \\varepsilon_t \\\\\n    &= \\phi^k y_{t-k} + \\sum_{j=0}^{k-1} \\phi^j \\varepsilon_{t-j}\n\\end{aligned}\n\\tag{89.2}\nFor \\|\\phi\\| &lt; 1, as k \\to \\infty, this becomes:\n\ny_t = \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\tag{89.3}\nInterpreted as an infinite-order Moving Average \\operatorname{MA}(\\infty) process.\n\n\n89.1.3 AR(1) Mean\n Since \\mathbb{E}[\\varepsilon_t] = 0,\n\n\\mathbb{E}[y_t] = 0 \\text{ mean of the AR(1) process}\n\\tag{89.4}\n\n\n89.1.4 AR(1) Variance\n Using independence and identical distribution:\n\n\\mathbb{V}ar[y_t] = \\sum_{j=0}^{\\infty} \\phi^{2j} v = \\frac{v}{1 - \\phi^2}\n\\tag{89.5}\nRequires \\|\\phi\\| &lt; 1 for convergence (i.e., stationarity).\n\n\n89.1.5 AR(1) Autocovariance Function \\gamma(h)\n For lag h, the autocovariance:\n\n\\begin{aligned}\n\\gamma(h) &= \\mathbb{E}[y_t y_{t-h}] \\\\\n&= \\mathbb{E} \\left[ \\left( \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\\right )\\left (\\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-h-k}\\right) \\right] \\\\\n&= \\mathbb{E}[(\\varepsilon_{t} + \\phi \\varepsilon_{t-1} + \\phi^2 \\varepsilon_{t-2} + \\ldots ) \\times (\\varepsilon_{t-h} + \\phi \\varepsilon_{t-h-1} + \\phi^2 \\varepsilon_{t-h-2} + \\ldots ) ] \\\\\n&= \\mathbb{E}[\\phi ^h \\varepsilon_{t-h} \\varepsilon_{t} + \\phi^{h+1} \\varepsilon_{t-h-1} \\varepsilon_{t} + \\ldots] \\\\\n&= v \\sum_{j=0}^{\\infty} \\phi^{h+j}  \\phi^{j} \\\\\n&= v \\phi^h \\sum_{j=0}^{\\infty} \\phi^{2j}  \\\\\n&= \\frac{v \\phi^{\\|h\\|}}{1 - \\phi^2} \\qquad \\text { when } |\\phi| &lt; 1\n\\end{aligned}\n\\tag{89.6}\nWe used the definition and properties of the expectation, independence of the innovations \\varepsilon_t, and the fact that \\mathbb{E}[\\varepsilon_t^2] = v. In the cross product, only terms where lags are the same (j = k) contribute, as the others are independent, leading to the above result. In the final step, we used the formula for the sum of a geometric series.\n\n\n89.1.6 AR(1) Autocorrelation Function \\rho(h)\n Defined by:\n\n\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\phi^{\\|h\\|}\n\\tag{89.7}\n\n\n89.1.7 AR(1) other properties:\n\nfor any lag h:\n\n\n\\rho(h) = \\phi^{\\|h\\|}\n\\gamma(h) = \\frac{v \\phi^{\\|h\\|}}{1 - \\phi^2}\n\n\nExponential decay if \\|\\phi\\| &lt; 1\nIf \\phi &gt; 0: decay is monotonic\nIf \\phi &lt; 0: decay is oscillatory (alternates signs)\n\n\n\n89.1.8 Stationarity\n\nThe process is stationary when \\|\\phi\\| &lt; 1:\n\nMean and variance are constant over time\nAutocovariance depends only on lag h, not on t",
    "crumbs": [
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>The AR(1) process: definitions and properties - M1L2</span>"
    ]
  },
  {
    "objectID": "C4-L02.html#the-pacf-of-the-ar1-process",
    "href": "C4-L02.html#the-pacf-of-the-ar1-process",
    "title": "89  The AR(1) process: definitions and properties - M1L2",
    "section": "89.2 The PACF of the AR(1) process 📖",
    "text": "89.2 The PACF of the AR(1) process 📖\n It is possible to show that the PACF of an AR(1) process is zero after the first lag. We can use the Durbin-Levinson recursion to show this.\nFor lag n = 0 we have \\phi(0, 0) = 0\nFor lag n = 1 we have:\n\n\\phi(1, 1) =  \\rho(1) = \\phi\n\nFor lag n = 2 we compute \\phi(2, 2) as:\n\n\\phi(2, 2) = \\frac{(\\rho(2) − \\phi(1, 1)\\rho(1))}{ (1 − \\phi(1, 1)\\rho(1))} = \\frac{\\phi^2-\\phi^2}{1- \\phi^2}=0\n\nand we also obtain\n\n\\phi(2, 1) = \\phi(1, 1) − \\phi(2, 2)\\phi(1, 1) = \\phi.\n\nFor lag n = 3 we compute \\phi(3, 3) as\n\n\\begin{aligned}\n\\phi(3, 3) &= \\frac{(\\rho(3) − \\sum_{h=1}^2 \\phi(2, h)\\rho(3 − h))}{1 − \\sum_{h=1}^2 \\phi(2, h)\\rho(h)} \\newline\n&= \\frac{\\phi^3 - \\phi(2,1) \\rho(2) - \\phi(2,2) \\rho(1)}{1 - \\phi(2,1)\\rho(1) - \\phi(2,2)\\rho(2)} \\newline\n&= \\frac{\\phi^3 - \\phi^3 - 0}{1 - \\phi^2 } \\newline\n&= 0\n\\end{aligned}\n\nand we also obtain\n\n\\phi(3, 1) = \\phi(2, 1) − \\phi(3, 3)\\phi(2, 2) = \\phi\n\n\n\\phi(3, 2) = \\phi(2, 2) − \\phi(3, 3)\\phi(2, 1) = 0\n\nWe can prove by induction that in the case of an AR(1), for any lag n,\n\\phi(n, h) = 0, \\phi(n, 1) = \\phi and \\phi(n, h) = 0 for h \\ge 2 and n \\ge 2.\nThen, the PACF of an AR(1) is zero for any lag above 1 and the PACF coefficient at lag 1 is equal to the AR coefficient \\phi\n\n89.2.1 Simulate data from an AR(1) process 🎥\nThis video walks through the code snippet below and provides examples of how to sample data from an AR(1) process and plot the ACF and PACF functions of the resulting time series.\nPrado demonstrates how to simulate AR(1) processes using arima.sim in R:\n\nSimulation Setup:\n\nset.seed() ensures reproducibility.\nSimulate 500 time points from an AR(1) with \\phi = 0.9 and variance = 1.\nThe process is stationary since |\\phi| &lt; 1.\n\narima.sim Function:\n\nCan simulate ARIMA(p,d,q) processes; here, only AR(1) is used.\nModel specified via a list: list(ar = phi), with sd as the standard deviation (√variance).\n\nComparative Simulation:\n\nSecond AR(1) simulated with \\phi = –0.9 to show the impact of negative \\phi.\nThe positive \\phi process shows persistent values (random walk-like).\nThe negative \\phi process shows oscillatory behavior.\n\nACF and PACF Analysis:\n\nTrue ACF: Exponential decay for both cases, oscillatory when \\phi &lt; 0.\nSample ACF: Matches theoretical ACF for each process.\nSample PACF: Only lag 1 is non-negligible, aligning with AR(1) properties:\n\nPositive at lag 1 for \\phi = 0.9.\nNegative at lag 1 for \\phi = –0.9.\nAll other lags ≈ 0.\n\n\n\nThe demonstration confirms our theoretical results regarding ACF/PACF behavior in AR(1) processes.\n\n\n89.2.2 R code: Sample data from AR(1) processes 📖\n \nSample data from 2 ar(1) processes: and plot their ACF and PACF functions\n\n1set.seed(2021)\n2T=500\n\n3v=1.0\n4sd=sqrt(v)\n5phi1=0.9\n6yt1=arima.sim(n = T, model = list(ar = phi1), sd = sd)\n\n7phi2=-0.9\n8yt2=arima.sim(n = T, model = list(ar = phi2), sd = sd)\n\n\n1\n\nset seed for reproducibility\n\n2\n\nnumber of time points\n\n3\n\ninnovation variance\n\n4\n\ninnovation standard deviation\n\n5\n\nAR coefficient for the first process\n\n6\n\nSample data from an ar(1) with ar coefficient phi = 0.9 and variance = 1\n\n7\n\nAR coefficient for the second process\n\n8\n\nSample data from an ar(1) with ar coefficient phi = -0.9 and variance = 1\n\n\n\n\n\n\n89.2.3 Plot the time series of both processes\n\npar(mfrow = c(2, 1),mar = c(3, 4, 2, 1), cex.lab = 1.3)\nplot(yt1,main=expression(phi==0.9))\nplot(yt2,main=expression(phi==-0.9))\n\npar(mfrow = c(3, 2),mar = c(3, 4, 2, 1), cex.lab = 1.3)\nlag.max=50 # max lag\n\n\n\n\n\n\n\nFigure 89.3\n\n\n\n\n\n\n\n89.2.4 Plot true ACFs for both processes\n\n1cov_0=sd^2/(1-phi1^2)\n2cov_h=phi1^(0:lag.max)*cov_0\nplot(0:lag.max, cov_h/cov_0, pch = 1, \n     type = 'h', col = 'red',\n     ylab = \"true ACF\", \n     xlab = \"Lag\",\n     ylim=c(-1,1), \n3     main=expression(phi==0.9))\n\n\n1\n\ncompute auto-covariance at h=0\n\n2\n\ncompute auto-covariance at lag h\n\n3\n\nPlot autocorrelation function (ACF) for the first process\n\n\n\n\n\n\n\n\n\n\nFigure 89.4: True ACF for the first AR(1) process\n\n\n\n\n\n\n4cov_0=sd^2/(1-phi2^2)\n5cov_h=phi2^(0:lag.max)*cov_0\n# Plot autocorrelation function (ACF)\nplot(0:lag.max, cov_h/cov_0, pch = 1, \n     type = 'h', col = 'red',\n     ylab = \"true ACF\", \n     xlab = \"Lag\",\n     ylim=c(-1,1),\n6     main=expression(phi==-0.9))\n\n\n4\n\ncompute auto-covariance at h=0 for the second process\n\n5\n\ncompute auto-covariance at lag h for the second process\n\n6\n\nPlot autocorrelation function (ACF) for the second process\n\n\n\n\n\n\n\n\n\n\nFigure 89.5: True ACF for the second AR(1) process\n\n\n\n\n\n\n\n89.2.5 plot sample ACFs for both processes\n\nacf(yt1, lag.max = lag.max, type = \"correlation\", ylab = \"sample ACF\",\n    lty = 1, ylim = c(-1, 1), main = \" \")\nacf(yt2, lag.max = lag.max, type = \"correlation\", ylab = \"sample ACF\",\n    lty = 1, ylim = c(-1, 1), main = \" \")\n## plot sample PACFs for both processes\n\npacf(yt1, lag.ma = lag.max, ylab = \"sample PACF\", ylim=c(-1,1),main=\"\")\npacf(yt2, lag.ma = lag.max, ylab = \"sample PACF\", ylim=c(-1,1),main=\"\")\n\n\n\n\n\n\n\nFigure 89.6: Sample ACF for the first AR(1) process\n\n\n\n\n\n\n\n\n\n\n\nFigure 89.7: Sample ACF for the first AR(1) process\n\n\n\n\n\n\n\n\n\n\n\nFigure 89.8: Sample ACF for the first AR(1) process\n\n\n\n\n\n\n\n\n\n\n\nFigure 89.9: Sample ACF for the first AR(1) process",
    "crumbs": [
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>The AR(1) process: definitions and properties - M1L2</span>"
    ]
  },
  {
    "objectID": "C4-L03.html",
    "href": "C4-L03.html",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "",
    "text": "90.1 Maximum likelihood and Bayesian inference in regression 📖\nThis section is based on the handout provided in the course materials. The gist of this handout is that if we can write the regression as the equation of a straight line, we should be able to make use of basic algebra to invert the equation and obtain an expression for the regression coefficients. We do this by writing the equation in matrix form and then using the Moore-Penrose pseudoinverse to obtain the maximum likelihood estimator for the regression coefficients.",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#maximum-likelihood-and-bayesian-inference-in-regression",
    "href": "C4-L03.html#maximum-likelihood-and-bayesian-inference-in-regression",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "",
    "text": "CautionMoore-Penrose pseudoinverse\n\n\n\nPrado snuck the Moore-Penrose pseudoinverse in this handout, then she used it in the class and notes. I thought this isn’t the first time this has come up in the specialization as we used MLE to some degree in all the courses, and in depth in the previous course on mixtures. However a cursory review did not uncover previous use of this mathematical tool. So I thought it might be useful to provide a brief overview of the Moore-Penrose pseudoinverse, its properties and how it is used in regression models.\nMoore-Penrose pseudoinverse is a generalization of the inverse matrix that can be applied to non-square matrices.\n\nIt is not covered in typical linear algebra courses. It might come up in numerical methods courses or a second linear algebra course. But unless they cover application in regression models, it is quite likely that they skip an important point, that the Moore-Penrose pseudoinverse is used to obtain the maximum likelihood estimator for the regression coefficients in linear regression models.\n\nI found a couple of reference in (Schott 2016 ch. 5) but they it didn’t cover they MLE aspect of the pseudoinverse.\nI cover some of this material in an Section 110.3 to the course notes, however I am still missing a good source for Equation 90.1.\n\nIt is denoted as X^+ and has the following properties:\nThis pseudoinverse is a neat bit of mathematics which allows us to not only invert the equation from the left but it somehow minimizes the sum of squared errors in the regression - I.e. we get the parameters while minimizing the sum of squared errors due to the variance term. One strong assumption we must make is that the design matrix X is full rank, which means that the columns of the matrix are linearly independent. The columns of the design matrix correspond to the explanatory variables in the regression model.\nIn reality as we are dealing with hierarchical models, so we will not always have a full rank design matrix, but we can still use the Moore-Penrose pseudoinverse to obtain the maximum likelihood estimator for the regression coefficients.\n\n\n\n90.1.1 Regression Models: Maximum Likelihood Estimation\n \nAssume a regression model with the following structure: \ny_i = \\beta_1x_{i,1} + \\ldots + \\beta_kx_{i,k} + \\epsilon_i,\n\nfor i = 1, \\ldots, n and \\epsilon_i independent random variables with \\epsilon_i \\sim \\mathcal{N}(0, v) \\quad \\forall i. This model can be written in matrix form as:\n\ny = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol\\epsilon \\qquad \\boldsymbol\\epsilon \\sim \\mathcal{N} (0, v\\mathbf{I})\n\nwhere:\n\ny = (y_1, \\ldots, y_n)′ is an n-dimensional vector of responses,\n\\mathbf{X} is an n × k matrix containing the explanatory variables,\n\\boldsymbol \\beta = (\\beta_1, \\ldots, \\beta_k)' is the k-dimensional vector of regression coefficients,\n\\boldsymbol \\epsilon = (\\epsilon_1, \\ldots, \\epsilon_n)' is the n-dimensional vector of errors,\n\\mathbf{I} is an n \\times n identity matrix.\n\nIf \\mathbf{X} is a full rank matrix with rank k , the maximum likelihood estimator for \\boldsymbol\\beta, denoted as \\hat{\\boldsymbol\\beta}_{MLE} is given by:\n\n\\hat{\\boldsymbol{\\beta}}_{MLE} = (\\mathbf{X}'\\mathbf{X})^{−1}\\mathbf{X}'\\mathbf{y},\n\\tag{90.1}\n where (X^+ = \\mathbf{X}'\\mathbf{X})^{−1}\\mathbf{X}' is the Moore-Penrose pseudoinverse of the matrix \\mathbf{X}. This Moore-Penrose pseudoinverse of the matrix \\mathbf{X} is the most widely known generalization of the inverse matrix and is used to obtain the least squares solution to the linear regression problem.\nand the MLE for v is given by:\n\n\\hat{v}_{MLE} = \\frac{1}{n} (y − \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{MLE})′(y − \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{MLE})\n\\tag{90.2}\n\\hat{v}_{MLE} is not an unbiased estimator of v, therefore, the following unbiased estimator of v is typically used:\n\ns^2 = \\frac{1}{n-k}(y − \\mathbf{X} \\hat{\\boldsymbol\\beta}_{MLE} )′(y − \\mathbf{X} \\hat{\\boldsymbol\\beta}_{MLE} )\n\\tag{90.3}\n\n\n90.1.2 Regression Models: Bayesian Inference\nAssume once again we have a model with the structure in (1), which results in a likelihood of the form\n\n\\mathbb{P}r(y \\mid \\boldsymbol{\\beta} , v) = \\frac{1}{(2\\pi v)^{n/2}}\\exp \\left\\{ -\\frac{1}{2} (y − \\mathbf{X} \\boldsymbol{\\beta})′(y − \\mathbf{X} \\boldsymbol{\\beta}) \\right\\}\n\nIf a prior of the form :\n\n\\mathbb{P}r(\\boldsymbol{\\beta}, v) \\propto \\frac{1}{v}\n\\tag{90.4}\nis used, we obtain that the posterior distribution is given by:\n\n\\mathbb{P}r(\\boldsymbol{\\beta},v \\mid \\mathbf{y}) \\propto \\frac{1}{v^{n/2+1}}\\exp \\left\\{ -\\frac{1}{2v} (\\mathbf{y} − \\mathbf{X} \\boldsymbol{\\beta})′(\\mathbf{y} − \\mathbf{X} \\boldsymbol{\\beta}) \\right\\}\n\\tag{90.5}\nIn addition it can be shown that\n\n(\\boldsymbol{\\beta}\\mid v, \\mathbf{y}) \\sim \\mathcal{N} (\\hat{\\boldsymbol{\\beta}}_{MLE} , v(\\mathbf{X}'\\mathbf{X})^{-1})\n(v \\mid \\mathbf{y}) \\sim \\mathcal{IG}\\left(\\frac{(n − k)}{2}, \\frac{1}{2}d\\right) with\n\n\nd = (\\mathbf{y} − \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{MLE} )′(\\mathbf{y} − \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{MLE} )\n\\tag{90.6}\nwhere \\mathcal{IG}(a, b) denotes the inverse-gamma distribution with shape parameter a and scale parameter b.\nwith k = dim(\\boldsymbol\\beta).\nGiven that \\mathbb{P}r(\\boldsymbol\\beta, v \\mid \\mathbf{y}) = \\mathbb{P}r(\\boldsymbol\\beta \\mid v, \\mathbf{y})p(v \\mid \\mathbf{y}) the equations above provide a way to directly sample from the posterior distribution of \\boldsymbol \\beta and v by first sampling v from the inverse-gamma distribution above and then conditioning on this sampled value of v, sampling \\boldsymbol \\beta from the normal distribution above.",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#sec-mle-ar1",
    "href": "C4-L03.html#sec-mle-ar1",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.2 Maximum likelihood estimation in the AR(1) 🎥",
    "text": "90.2 Maximum likelihood estimation in the AR(1) 🎥\n\n\n\n\n\n\n\nFigure 90.1: MLE 1\n\n\n\n\n\n\n\n\nFigure 90.2: Full Likelihood MLE\n\n\n\n\n\n\n\n\nFigure 90.3: Conditional Likelihood MLE\n\n\n\n\nThere are two main strategies for performing MLE for an AR(1) model:\n\nFull Likelihood: Considers the joint distribution of all observations y_1, \\dots, y_T.\nConditional Likelihood: Conditions on the first observation (y_1) and works with the likelihood of the remaining observations (y_2, \\dots, y_T).\n\n\n90.2.1 Model Setup\nThe focus is on the zero-mean AR(1) model:\n\nY_t = \\phi Y_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0, v), \\quad \\phi \\in (-1,1)\n\nThis condition ensures stationarity of the process.\n\n\n90.2.2 Distributional Assumptions\n\nY_1 \\sim \\mathcal{N}\\left(0, \\frac{v}{1 - \\phi^2}\\right)\nY_t \\mid Y_{t-1} \\sim \\mathcal{N}(\\phi Y_{t-1}, v) for t \\geq 2\n\n\n\n90.2.3 Likelihood Approaches\nTwo approaches are considered:\n\n90.2.3.1 1. Full Likelihood\n\n\\begin{aligned}\np(y_{1:T} \\mid \\phi, v) &= p(y_1 \\mid \\phi, v) \\cdot \\prod_{t=2}^T p(y_t \\mid y_{t-1}, \\phi, v) \\\\\n&= \\frac{1}{\\sqrt{2\\pi \\frac{v}{1 - \\phi^2}}} \\exp\\left( -\\frac{y_1^2 (1 - \\phi^2)}{2v} \\right) \\cdot  \\prod_{t=2}^T \\frac{1}{\\sqrt{2\\pi v}} \\exp\\left( -\\frac{(y_t - \\phi y_{t-1})^2}{2v} \\right) \\\\\n  &= \\frac{(1 - \\phi^2)^{1/2}}{(2\\pi v)^{T/2}} \\cdot\n\\exp\\left( -\\frac{1}{2v} \\left[ \\underbrace{ y_1^2(1 - \\phi^2) + \\sum_{t=2}^T (y_t - \\phi y_{t-1})^2 }_{\\text{Quadratic Loss } Q^*(\\phi)} \\right] \\right) \\\\\n&= \\frac{(1 - \\phi^2)^{1/2}}{(2\\pi v)^{T/2}} \\exp\\left( -\\frac{Q^*(\\phi)}{2v} \\right)\n\\end{aligned}\n\\tag{90.7}\nwhere Q^*(\\phi) is defined as:\n\nQ^*(\\phi) =\n\\underbrace{y_1^2(1 - \\phi^2)\\vphantom{\\sum_{t=2}^T (y_t - \\phi y_{t-1})^2}}_{\\text{Initial Loss}}\n+ \\underbrace{\\sum_{t=2}^T (y_t - \\phi y_{t-1})^2}_{\\text{Remaining Loss } Q(\\phi)}\n\\tag{90.8}\n\n\\begin{aligned}\np(y_{1:T} \\mid \\phi) &= \\prod_{t=2}^T \\frac{1}{\\sqrt{2\\pi v}} \\exp\\left( -\\frac{(y_t - \\phi y_{t-1})^2}{2v} \\right) \\\\\n&= \\frac{1}{(2\\pi v)^{T/2}} \\exp\\left( -\\sum_{t=2}^T \\frac{1}{2v} \\left( y_t - \\phi y_{t-1} \\right)^2 \\right) \\\\\n&= \\frac{1}{(2\\pi v)^{T/2}} \\exp\\left( -\\frac{Q(\\phi)}{2v} \\right)\n\\end{aligned}\n\\tag{90.9}\nwhere Q(\\phi) is the quadratic loss function defined as:\n\n\\underbrace{ \\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} }_{\\utilde{y}} =\n\\underbrace{ \\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} }_{\\mathbb{X}}\n\\underbrace{ \\phi \\vphantom{\\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} }}_{\\beta} +\n\\underbrace{ \\begin{pmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_T\\end{pmatrix}}_{\\utilde {\\epsilon}}\n\\tag{90.10}\nwhere \\utilde{y} is the vector of observations, \\mathbb{X} is the design matrix with y_1 as the first column and y_2, \\ldots, y_{T-1} as the second column, \\beta = \\phi is the AR coefficient, and \\utilde{\\epsilon} \\sim \\mathcal{N}(0, vI) is the error term.\n\n\\utilde{y} = \\mathbb{X} \\beta + \\utilde{\\epsilon} \\qquad \\utilde{\\epsilon} \\sim \\mathcal{N}(0, vI)\n\nwhere \\mathbb{X} is the design matrix\nwith y_1 as the first column and y_2, \\ldots, y_{T-1} as the second column.\nIf the matrix \\mathbb{X} is full rank, the MLE for \\phi can be obtained as:\n\n\\hat{\\beta} = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'y\n and the MLE for v is given by:\n\n\\hat{v} = S^2 = \\frac{(y - \\mathbb{X}\\hat{\\beta})'(y - \\mathbb{X}\\hat{\\beta})}{\\dim(y)-\\dim(\\beta)}\n\nand the MLE for \\phi. \n\\hat{\\phi}_{MLE} = \\frac{\\sum_{t=2}^T y_t y_{t-1}}{\\sum_{t=2}^T y_{t-1}^2}\n\\tag{90.11}\nand the unbiased estimator for v is given by:\n\nS^2 = \\sum_{t=2}^T (y_t - \\hat{\\phi}_{MLE} y_{t-1})^2 / (T - 2)\n\\tag{90.12}\nwhere T is the number of time points and S^2 is the unbiased estimator for the variance v. And we usually use this unbiased estimator for v in practice, as the MLE for v is biased.\n\n\n90.2.3.2 2. Conditional Likelihood\nMaximizing the full likelihood requires numerical optimization methods (e.g., Newton-Raphson), as there’s no closed-form solution.\nThis setup is equivalent to a linear regression:\n\n\\mathbf{y} = X\\beta + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, vI)\n\nWhere:\n\n\\mathbf{y} = [y_2, \\dots, y_T]^T\nX = [y_1, \\dots, y_{T-1}]^T\n\\beta = \\phi\n\nCondition on y_1:\n\n\\begin{aligned}\np(y_1 \\mid \\phi) &\\sim \\mathcal{N}(0, 1/(1 - \\phi^2)) \\\\\np(y_t \\mid y_{t-1}, \\phi) &\\sim \\mathcal{N}(\\phi y_{t-1}, 1) \\\\\n\\end{aligned}\n\n\n\\begin{aligned}\np(y_{1:T} \\mid y_1, \\phi, v) &= p(y_1 \\mid \\phi)  \\cdot \\prod_{t=2}^T p(y_t \\mid y_{t-1}, \\phi)\\\\\n&= \\frac{(1 - \\phi^2)^{1/2}}{(2\\pi)^{T/2}} \\exp\\left(-\\frac{y_1^2(1 - \\phi^2)}{2}\\right) \\cdot \\prod_{t=2}^T \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y_t - \\phi y_{t-1})^2}{2}\\right) \\\\\n&= \\frac{(1 - \\phi^2)^{1/2}}{(2\\pi)^{T/2}} \\exp\\left(-\\frac{1}{2} \\left[y_1^2(1 - \\phi^2) + \\sum_{t=2}^T (y_t - \\phi y_{t-1})^2\\right]\\right) \\\\\n\\end{aligned}\n\nSo the log-likelihood becomes:\n\n\\log p(y_{1:T} \\mid y_1, \\phi) = \\frac{1}{2} \\log(1 - \\phi^2) - \\frac{1}{2} Q(\\phi) + K\n\\tag{90.13}\nwhere K is a constant that does not depend on \\phi.\nSo if I were to look at maximizing this function, we can think about taking first derivatives with respect to phi. And then we will see that again the expression that you obtain doesn’t allow you to obtain a close form expression for \\hat{\\phi}_{MLE}. Instead, we will need to use a numerical optimization method such as Newton Raphson to obtain the maximum likelihood estimator for phi.\n\n\n\n90.2.4 Conclusion\n\nFull likelihood is more general but computationally intensive.\nConditional likelihood simplifies estimation by leveraging regression theory.\nWhen variance v is known (e.g., v = 1), the optimization reduces to maximizing a univariate function of \\phi.\nFor full likelihood, optimization of Q^*(\\phi) is required.\n\n\n\n\n\n\n\n\n\nFeature\nFull Likelihood\nConditional Likelihood (Regression)\n\n\n\n\nAccounts for Y\\_1\n✅ Yes\n❌ No\n\n\nMLE for \\phi\n❌ No closed form\n✅ Closed form\n\n\nMLE for v\n❌ Biased unless adjusted\n✅ Unbiased estimator available\n\n\nOptimization Needed\n✅ Yes (numerical methods)\n❌ No (closed-form MLE for \\phi)\n\n\nUseful when\nModeling full joint process\nEstimating \\phi efficiently in practice",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#mle-for-the-ar1-ℛ",
    "href": "C4-L03.html#mle-for-the-ar1-ℛ",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.3 MLE for the AR(1) 📖 ℛ",
    "text": "90.3 MLE for the AR(1) 📖 ℛ\n The following code allows you to compute the MLE of the AR coefficient \\psi, the unbiased estimator of v, s^2 , and the MLE of v based on a dataset simulated from an AR(1) process and using the conditional likelihood.\n\nset.seed(2021)\n1phi=0.9\nv=1\n2sd=sqrt(v)\n3T=500\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Case 1: Conditional likelihood\n4y=as.matrix(yt[2:T])\n5X=as.matrix(yt[1:(T-1)])\n6phi_MLE=as.numeric((t(X)%*%y)/sum(X^2))\n7s2=sum((y - phi_MLE*X)^2)/(length(y) - 1)\n8v_MLE=s2*(length(y)-1)/(length(y))\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"MLE for the variance v: \", v_MLE, \"\\n\", \n    \"Estimate s2 for the variance v: \", s2, \"\\n\")\n\n\n1\n\nar coefficient\n\n2\n\ninnovation standard deviation\n\n3\n\nnumber of time points\n\n4\n\nresponse variable\n\n5\n\ndesign matrix\n\n6\n\nMLE for phi\n\n7\n\nUnbiased estimate for v\n\n8\n\nMLE for v\n\n\n\n\n\n MLE of conditional likelihood for phi:  0.9261423 \n MLE for the variance v:  1.048 \n Estimate s2 for the variance v:  1.050104 \n\n\nThis code allows you to compute estimates of the AR(1) coefficient and the variance using the arima function in R. The first case uses the conditional sum of squares, the second and third cases use the full likelihood with different starting points for the numerical optimization required to compute the MLE with the full likelihood.\n\n# Obtaining parameter estimates using the arima function in R\nset.seed(2021)\nphi=0.9 # ar coefficient\nv=1\nsd=sqrt(v) # innovation standard deviation\nT=500 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n# Using conditional sum of squares, equivalent to conditional likelihood \narima_CSS=arima(yt,order=c(1,0,0),method=\"CSS\",n.cond=1,include.mean=FALSE)\ncat(\"AR estimates with conditional sum of squares (CSS) for phi and v:\", arima_CSS$coef,arima_CSS$sigma2,\n\"\\n\")\n\nAR estimates with conditional sum of squares (CSS) for phi and v: 0.9261423 1.048 \n\n#Uses ML with full likelihood \narima_ML=arima(yt,order=c(1,0,0),method=\"ML\",include.mean=FALSE)\ncat(\"AR estimates with full likelihood for phi and v:\", arima_ML$coef,arima_ML$sigma2,\n\"\\n\")\n\nAR estimates with full likelihood for phi and v: 0.9265251 1.048434 \n\n#Default: uses conditional sum of squares to find the starting point for ML and \n#         then uses ML \narima_CSS_ML=arima(yt,order=c(1,0,0),method=\"CSS-ML\",n.cond=1,include.mean=FALSE)\ncat(\"AR estimates with CSS to find starting point for ML for phi and v:\", \narima_CSS_ML$coef,arima_CSS_ML$sigma2,\"\\n\")\n\nAR estimates with CSS to find starting point for ML for phi and v: 0.9265252 1.048434 \n\n\nThis code shows you how to compute the MLE for \\psi using the full likelihood and the function optimize in R.\n\nset.seed(2021)\n1phi=0.9\nv=1\n2sd=sqrt(v)\n3T=500\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## MLE, full likelihood AR(1) with v=1 assumed known \n\n4log_p &lt;- function(phi, yt){\n  0.5*(log(1-phi^2) - sum((yt[2:T] - phi*yt[1:(T-1)])^2) - yt[1]^2*(1-phi^2))\n}\n\nresult = optimize(log_p, c(-1, 1), \n                  tol = 0.0001, \n                  maximum = TRUE, \n5                  yt = yt)\ncat(\"\\n MLE of full likelihood for phi: \", result$maximum)\n\n\n1\n\nar coefficient\n\n2\n\ninnovation standard deviation\n\n3\n\nnumber of time points\n\n4\n\nlog likelihood function\n\n5\n\nusing a built-in optimization method to obtain MLE\n\n\n\n\n\n MLE of full likelihood for phi:  0.9265928",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#bayesian-inference-in-the-ar1",
    "href": "C4-L03.html#bayesian-inference-in-the-ar1",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.4 Bayesian inference in the AR(1)",
    "text": "90.4 Bayesian inference in the AR(1)\n\n\n\n\n\n\n\nFigure 90.4: inference\n\n\n\ny_t= \\phi y_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0, v), \\quad \\phi \\in (-1,1)\n\\tag{90.14}\n\n\\utilde{y} = \\mathbb{X}\\utilde{\\beta} + \\utilde{\\epsilon}\n\\tag{90.15}\nwhere \\mathbb{X} is the design matrix with y_1 as the first column and y_2, \\ldots, y_{T-1} as the second column, \\utilde{\\beta} = \\phi is the AR coefficient, and \\utilde{\\epsilon} \\sim \\mathcal{N}(0, v\\mathbf{I}) is the error term.\n\n\\utilde{y} = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_{T-1}\n\\end{pmatrix}\n\\tag{90.16}\n\n\\mathbb{X} = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_{T-1}\n\\end{pmatrix}\n\\tag{90.17}\n\n\\utilde{\\beta} = \\phi\n\\tag{90.18}\n\n\\utilde{\\epsilon} \\sim \\mathcal{N}(0, v\\mathbf{I})\n\\tag{90.19}\n\np(y_{2:T} \\mid y_1, \\phi, v) =  \\frac{1}{(2\\pi v)^{\\frac{T-1}{2}}} \\exp\\left(-\\frac{(\\utilde{y} - \\mathbb{X}\\utilde{\\beta})'(\\utilde{y} - \\mathbb{X}\\utilde{\\beta})}{2v}\\right)\n\\tag{90.20}\n\np(\\phi, v \\mid y_{1:T}) \\propto p(\\phi, v) p(y_{2:T} \\mid y_1, \\phi, v)\n\\tag{90.21}\n\n\\begin{aligned}\np(\\phi, v) \\propto \\frac{1}{v} &\\cdot (\\utilde{\\beta} \\mid v, \\mathbb{X}, \\utilde{y}) &\\sim & \\mathcal{N}(\\hat{\\utilde{\\beta}}, v(\\mathbb{X}'\\mathbb{X})^{-1}) \\\\\n  & \\cdot (v \\mid \\mathbb{X}, \\utilde{y}) &\\sim & \\mathcal{IG}\\left(\\frac{T-2}{2}, \\frac{1}{2}Q(\\hat{\\utilde{\\beta}}_{MLE}) \\right)\n\\end{aligned}\n\\tag{90.22}\n\n\\begin{aligned}\n\\hat{\\utilde{\\beta}_{MLE}} &= (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\utilde{y} \\\\\n&= \\hat{\\phi}_{MLE} = \\frac{\\sum_{t=2}^T y_t y_{t-1}}{\\sum_{t=2}^T y_{t-1}^2}\n\\end{aligned}\n\\tag{90.23}\n\nQ(\\hat{\\phi}_{MLE}) = \\sum_{t=2}^T (y_t - \\hat{\\phi}_{MLE}\\ y_{t-1})^2\n\\tag{90.24}",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#bayesian-inference-in-the-ar1-conditional-likelihood-example",
    "href": "C4-L03.html#bayesian-inference-in-the-ar1-conditional-likelihood-example",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.5 Bayesian inference in the AR(1): Conditional likelihood example 🎥",
    "text": "90.5 Bayesian inference in the AR(1): Conditional likelihood example 🎥\nThis video walks through the code snippet below and provides examples of how to sample from the posterior distribution of the AR coefficient \\psi and the variance v using the conditional likelihood and a reference prior.",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#r-code-ar1-bayesian-inference-conditional-likelihood-example",
    "href": "C4-L03.html#r-code-ar1-bayesian-inference-conditional-likelihood-example",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.6 R Code: AR(1) Bayesian inference, conditional likelihood example 📖",
    "text": "90.6 R Code: AR(1) Bayesian inference, conditional likelihood example 📖\n\n####################################################\n#####             MLE for AR(1)               ######\n####################################################\nset.seed(2021)\nphi=0.9 # ar coefficient\nsd=1 # innovation standard deviation\nT=200 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) # sample stationary AR(1) process\n\ny=as.matrix(yt[2:T]) # response\nX=as.matrix(yt[1:(T-1)]) # design matrix\nphi_MLE=as.numeric((t(X)%*%y)/sum(X^2)) # MLE for phi\ns2=sum((y - phi_MLE*X)^2)/(length(y) - 1) # Unbiased estimate for v\nv_MLE=s2*(length(y)-1)/(length(y)) # MLE for v \n\nprint(c(phi_MLE,s2))\n\n[1] 0.9178472 1.0491054\n\n#######################################################\n######     Posterior inference, AR(1)               ###\n######     Conditional Likelihood + Reference Prior ###\n######     Direct sampling                          ###\n#######################################################\n\nn_sample=3000   # posterior sample size\n\n## step 1: sample posterior distribution of v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2)/2, sum((yt[2:T] - phi_MLE*yt[1:(T-1)])^2)/2)\n\n## step 2: sample posterior distribution of phi from normal distribution\nphi_sample=rep(0,n_sample)\nfor (i in 1:n_sample){\nphi_sample[i]=rnorm(1, mean = phi_MLE, sd=sqrt(v_sample[i]/sum(yt[1:(T-1)]^2)))}\n\n## plot histogram of posterior samples of phi and v\npar(mfrow = c(1, 2),mar = c(3, 4, 2, 1), cex.lab = 1.3)\nhist(phi_sample, xlab = bquote(phi), \n     main = bquote(\"Posterior for \"~phi),xlim=c(0.75,1.05), col='lightblue')\nabline(v = phi, col = 'red')\nhist(v_sample, xlab = bquote(v), col='lightblue', main = bquote(\"Posterior for \"~v))\nabline(v = sd, col = 'red')",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#quiz---mle-and-bayesian-inference-in-the-ar1",
    "href": "C4-L03.html#quiz---mle-and-bayesian-inference-in-the-ar1",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.7 Quiz - MLE and Bayesian inference in the AR(1)",
    "text": "90.7 Quiz - MLE and Bayesian inference in the AR(1)\nOmitted per Coursera honor code",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1",
    "href": "C4-L03.html#practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.8 Practice Graded Assignment: MLE and Bayesian inference in the AR(1)",
    "text": "90.8 Practice Graded Assignment: MLE and Bayesian inference in the AR(1)\nThis peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you’ve learned in R and prepare you for your data analysis project in week 5.\n\nConsider the R code below: MLE for the AR(1)\n\n\n\n\n\nListing 90.1: R Code: MLE for the AR(1) process, conditional likelihood example\n\n\n####################################################\n#####             MLE for AR(1)               ######\n####################################################\nphi=0.9 # ar coefficient\nv=1\nsd=sqrt(v) # innovation standard deviation\nT=500 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Case 1: Conditional likelihood\ny=as.matrix(yt[2:T]) # response\nX=as.matrix(yt[1:(T-1)]) # design matrix\nphi_MLE=as.numeric((t(X)%*%y)/sum(X^2)) # MLE for phi\ns2=sum((y - phi_MLE*X)^2)/(length(y) - 1) # Unbiased estimate for v \nv_MLE=s2*(length(y)-1)/(length(y)) # MLE for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"MLE for the variance v: \", v_MLE, \"\\n\", \n    \"Estimate s2 for the variance v: \", s2, \"\\n\")\n\n\n\n\n\n MLE of conditional likelihood for phi:  0.9048951 \n MLE for the variance v:  1.084559 \n Estimate s2 for the variance v:  1.086737 \n\n\nModify the code above to sample 800 observations from an AR(1) with AR coefficient \\psi = -0.8 and variance v = 2. Plot your simulated data. Obtain the MLE for \\psi based on the conditional likelihood and the unbiased estimate s^2 for the variance v.\n\nConsider the R code below: AR(1) Bayesian inference, conditional likelihood\n\n\n\n\n\nListing 90.2: R Code: AR(1) Bayesian inference, conditional likelihood example\n\n\n#######################################################\n######     Posterior inference, AR(1)               ###\n######     Conditional Likelihood + Reference Prior ###\n######     Direct sampling                          ###\n#######################################################\n\nn_sample=3000   # posterior sample size\n\n## step 1: sample posterior distribution of v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2)/2, sum((yt[2:T] - phi_MLE*yt[1:(T-1)])^2)/2)\n\n## step 2: sample posterior distribution of phi from normal distribution\nphi_sample=rep(0,n_sample)\nfor (i in 1:n_sample){\nphi_sample[i]=rnorm(1, mean = phi_MLE, sd=sqrt(v_sample[i]/sum(yt[1:(T-1)]^2)))}\n\n## plot histogram of posterior samples of phi and v\npar(mfrow = c(1, 2), mar = c(3, 4, 2, 1), cex.lab = 1.3)\nhist(phi_sample, xlab = bquote(phi), \n     main = bquote(\"Posterior for \"~phi),xlim=c(0.75,1.05), col='lightblue')\nabline(v = phi, col = 'red')\nhist(v_sample, xlab = bquote(v), col='lightblue', main = bquote(\"Posterior for \"~v))\nabline(v = sd, col = 'red')\n\n\n\n\n\n\n\n\n\n\n\nUsing your simulated data from part 1 modify the code above to summarize your posterior inference for \\psi and v based on 5000 samples from the joint posterior distribution of \\psi and v.\n\n\n\n\n\n\nTipGrading Criteria\n\n\n\nThe responses should follow the same template as the sample code provided above but you will submit your code lines in plain text. Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect that :\n\nyou generate 800 time points from the AR(1) rather than 500 and plot your simulated data.\nyour simulated data is from an AR(1) with AR coefficients \\psi = -0.8 and variance v = 2 rather than AR(1) with AR coefficient \\psi = 0.9 and variance v = 1 and\nyou obtain 5000 rather than 3000 samples from the posterior distribution from the new simulated process.\n\n\n\n\n90.8.1 Bayesian Inference in the AR(1), : full likelihood example 📖\nWe consider a prior distribution that assumes that \\phi and v are independent:\n\n\\mathbb{P}r(v) \\propto \\frac{1}{v},\n\n\n\\mathbb{P}r(\\phi) = \\frac{1}{2}, \\quad \\text{for } \\phi \\in (-1, 1),\n\ni.e., we assume a Uniform prior for \\phi \\in (-1, 1). Combining this prior with the full likelihood in the AR(1) case, we obtain the following posterior density:\n\n\\mathbb{P}r(\\phi, v \\mid y_{1:T}) \\propto \\frac{(1 - \\phi^2)^{1/2} }{v^{T/2 + 1}} \\exp\\left(-\\frac{Q^*(\\phi)}{2v}\\right), \\quad -1 &lt; \\phi &lt; 1,\n\nwith\n\nQ^*(\\phi) = y_1^2(1 - \\phi^2) + \\sum_{t=2}^{T} (y_t - \\phi y_{t-1})^2.\n\nIt is not possible to get a closed-form expression for this posterior or to perform direct simulation. Therefore, we use simulation-based Markov Chain Monte Carlo (MCMC) methods to obtain samples from the posterior distribution.\n\n\n90.8.2 Transformation of \\phi\nWe first consider the following transformation on \\phi:\n\n\\eta = \\log\\left(\\frac{1 - \\phi}{\\phi + 1}\\right),\n\nso that \\eta \\in (-\\infty, \\infty). The inverse transformation on \\eta is:\n\n\\phi = \\frac{1 - \\exp(\\eta)}{1 + \\exp(\\eta)}.\n\nWriting down the posterior density for \\eta and v, we obtain\n\n\\mathbb{P}r(\\eta, v \\mid y_{1:T}) \\propto\\frac{ (1 - \\phi^2)^{1/2} }{v^{T/2 + 1}} \\exp\\left(-\\frac{Q^*(\\phi)}{2v}\\right) \\cdot \\frac{2 \\exp(\\eta)}{(1 + \\exp(\\eta))^2},\n\nwith \\phi written as a function of \\eta. We proceed to obtain samples from this posterior distribution using the MCMC algorithm outlined below. Once we have obtained M samples from \\eta and v after convergence, we can use the inverse transformation above to obtain posterior samples for \\phi.\n\n\n90.8.3 MCMC Algorithm: Bayesian Inference for AR(1), Full Likelihood\nAlgorithm:\n\nInitialize \\eta^{(0)} and \\beta^{(0)}.\nFor m in 1:M do:\n\nSample v^{(m)} \\sim \\text{IG}\\left(\\frac{T}{2}, \\frac{Q^*(\\phi^{(m-1)})}{2}\\right).\nSample \\eta^{(m)} using Metropolis-Hastings:\n\nSample \\eta^* \\sim N(\\eta^{(m-1)}, c), where c is a tuning parameter.\nCompute the importance ratio:\n\n\n\n\n        r = \\frac{p(\\eta^*, v^{(m)} \\mid y_{1:T})}{p(\\eta^{(m-1)}, v^{(m)} \\mid y_{1:T})}.\n\n\nSet:\n\n\n        \\eta^{(m)} =\n        \\begin{cases}\n        \\eta^* & \\text{with probability } \\min(r, 1), \\\\\n        \\eta^{(m-1)} & \\text{otherwise}.\n        \\end{cases}\n\n\n\n\n\n\n\nSchott, James R. 2016. Matrix Analysis for Statistics. Wiley Series in Probability and Statistics. Wiley. https://books.google.co.il/books?id=Y2PpCgAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L04.html",
    "href": "C4-L04.html",
    "title": "91  The AR(p) process - M2L4",
    "section": "",
    "text": "92 The general AR(p) process 🎥",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#definition-and-state-space-representation",
    "href": "C4-L04.html#definition-and-state-space-representation",
    "title": "91  The AR(p) process - M2L4",
    "section": "92.1 Definition and state-space representation 🎥",
    "text": "92.1 Definition and state-space representation 🎥\n\n\n\n\nAR(p) process, characteristic polynomial, stability, stationarity and MA representation\n\nAR(P), shorthand for autoregressive process of order p which generalizes the AR(1) process by defining the current time step in terms of the previous p time steps. Thus the number of parameter p, required is the order  of the autoregressive process. It tells us how many lags we will be considering. On the other hand, the AR(1) process is a special case of the AR(p) process with p=1.order\nAR(P)\nWe will assume AR(P) has the following structure:\n\n\\textcolor{red}{y_t} = \\textcolor{blue}{\\phi_1} \\textcolor{red}{y_{t-1}} + \\textcolor{blue}{\\phi_2} \\textcolor{red}{y_{t-2}} + \\ldots + \\textcolor{blue}{\\phi_p} \\textcolor{red}{y_{t-p}} + \\textcolor{grey}{\\epsilon_t} \\qquad\n\\tag{92.1}\nwhere:\n\n\\textcolor{red}{y_t} is the value of the time series at time t\n\\textcolor{blue}{\\phi_{1:p}} are the AR coefficients\n\\textcolor{grey}{\\epsilon_t} \\overset{\\text{iid}}{\\sim} \\text{N}(0,v) \\quad \\forall t is a white noise process.\nThe number of parameters has increased from one coefficient in AR(1) to p coefficients for AR(P).\n\nA central outcome of the autoregressive nature of the AR(p) is due to the properties the AR characteristic polynomial \\Phi.  This is defined as :\\Phi AR characteristic polynomial\nrecall the backshift operator B is defined as B y_t = y_{t-1}, so that B^j y_t = y_{t-j}.\n\n\\begin{aligned}\n       y_t &= \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t  && \\text{(Ar(p) defn.)} \\newline\n       y_t &= \\phi_1 By_{t} + \\phi_2 B^2y_{t} + \\ldots + \\phi_p B^p y_{t} + \\epsilon_t && \\text{(B defn.)} \\newline\n\\epsilon_t &= y_t - \\phi_1 B y_t + \\phi_2 B^2 y_t + \\ldots + \\phi_p B^p y_t    && \\text{(rearranging)} \\newline\n\\epsilon_t  &= (1- \\phi_1 B + \\phi_2 B^2 + \\ldots + \\phi_p B^p) y_t            && \\text{(factoring out $y_t$)}\n\\end{aligned}\n \n\\Phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\ldots - \\phi_p z^p \\qquad \\text{(Characteristic polynomial)}\n\\tag{92.2}\nwhere:\n\nz \\in \\mathbb{C} i.e. complex-valued.\n\nwe can also rewrite the characteristic polynomial in terms of the reciprocal roots of the polynomial.\nThe zeros of the characteristic polynomial are the roots of the AR(p) process.\n\n\\Phi(z) = \\prod_{j=1}^{p} (1 - \\alpha_j z) = 0  \\implies z = \\frac{1}{ \\alpha_j} \\qquad \\text{(reciprocal roots)}\n\nwhere:\n\n\\alpha_j are the reciprocal roots of the characteristic polynomial.\n\n\nWhy are we interested in this autoregresive lag polynomial?\n\n\nThis polynomial and its roots tells us a lot about the process and its properties.\nOne of the main characteristics is it allows us to think about things like quasi-periodic behavior, whether it’s present or not in a particular AR(p) process.\nIt allows us to think about whether a process is stationary or not, depending on some properties related to this polynomial.\nIn particular, we are going to say that the process is stable if all the roots of the characteristic polynomial have a modulus greater than one.  \n\\Phi(z) = 0 \\iff |z| &gt; 1  \\qquad \\text{(stability condition)}\n\\tag{92.3}\nFor any of the roots, it has to be the case that the modulus of that root, they have to be all outside the unit circle.\nIf a process is stable, it will also be stationary.\n\nstability conditionWe can show this as follows:\n\nOnce the process is stationary, and if all the roots of the characteristic polynomial are outside the unit circle, then we will be able to write this process in terms of an infinite order moving average process. In this case, if the process is stable, then we are going to be able to write it like this.\n\n\ny_t = \\Psi(B) \\epsilon_t = \\sum_{j=0}^{\\infty} \\psi_j \\epsilon_{t-j} \\ \\text {with} \\ \\psi_0 = 1 \\text{ and } \\sum_{j=0}^{\\infty} |\\psi_j| &lt; \\infty\n\\tag{92.4}\nwhere:\n\n\\epsilon_t is a white noise process with zero mean and constant variance v.\nB is the lag operator AKA the backshift operator defined by B \\varepsilon_t = \\varepsilon_{t-1}. This need to be applied to a time series \\epsilon_t to get the lagged values.\n\\Psi(B) is the infinite order polynomial in B that representing a linear filter applied to the noise process.​\n\\psi_t = 1 is the weight for the white noise at time t.\nthe constraint \\psi_0 = 1 ensures that the current shock contributes directly to y_t\nthe constraint on the weights \\sum_{j=0}^{\\infty} |\\psi_j| &lt; \\infty ensures that the weights decay sufficiently fast, so that the process does not explode i.e. it is stable and thus stationary.\n\nthe notation with \\psi a functional of operator B and \\psi_i as constants is confusing in both the reuse if the symbol and the complexity.\nHere, U is any complex valued number.\n\nI am going to have an infinite order polynomial here on B, the backshift operator that I can write down just as the sum, j goes from zero to infinity.\n\n\nHere \\psi_0=1. Then there is another condition on the Psi’s for this to happen. We have to have finite sum of these on these coefficients. Once again, if the process is stable, then it would be stationary and we will be able to write down the AR as an infinite order moving average process here. If you recall, B is the backshift operator. Again, if I apply this to y_t, I’m just going to get y_t-j. I can write down Psi of B, as 1 + \\psi_1 B, B squared, and so on. It’s an infinite order process.\n\n\nThe AR characteristic polynomial can also be written in terms of the reciprocal roots of the polynomial. So instead of considering the roots, we can consider the reciprocal roots. In that case, let’s say the $phi$ of u for Alpha 1, Alpha 2, and so on. The reciprocal roots.\n\n\nWhy do we care about all these roots? Why do we care about this structure? Again, we will be able to understand some properties of the process based on these roots as we will see.\n\n\n\n\n\nA state space representation of Ar(p)\n\nWe will now discuss another important representation of the AR(P) process, one that is based on a state-space representation of the process. Again, we care about this type of representations because they allow us to study some important properties of the process. In this case, our state-space or dynamic linear model representation, we will make some connections with these representations later when we talk about dynamic linear models, is given as follows for an AR(P). I have my y_t. I can write it as F transpose and then another vector x_t here. Then we’re going to have x_t is going to be a function of x_t minus 1. That vector there is going to be an F and a G. I will describe what those are in a second. Then I’m going to have another vector here with some distribution. In our case, we are going to have a normal distribution also for that one. In the case of the AR(P), we’re going to have x_t to be y_t, y_t minus 1.\n\nIt’s a vector that has all these values of the y_t process. Then F is going to be a vector. It has to match the dimension of this vector. The first entry is going to be a one, and then I’m going to have zeros everywhere else. The w here is going to be a vector as well.\nThe first component is going to be the Epsilon t. That we defined for the ARP process. Then every other entry is going to be a zero here. Again, the dimensions are going to match so that I get the right equations here. Then finally, my G matrix in this representation is going to be a very important matrix, the first row is going to contain the AR parameters, the AR coefficients. We have p of those. That’s my first row. In this block, I’m going to have an identity matrix. It’s going to have ones in the diagonal and zeros everywhere else. I’m going to have a one here, and then I want to have zeros everywhere else. In this portion, I’m going to have column vector here of zeros. This is my G matrix. Why is this G matrix important? This G matrix is going to be related to the characteristic polynomial, in particular, is going to be related to the reciprocal roots of the characteristic polynomial that we discussed before. The eigenvalues of this matrix correspond precisely to the reciprocal roots of the characteristic polynomial. We will think about that and write down another representation related to this process. But before we go there, I just want you to look at this equation and see that if you do the matrix operations that are described these two equations, you get back the form of your autoregressive process. The other thing is, again, this is called a state-space representation because you have two equations here. One, you can call it the observational level equation where you are relating your observed y’s with some other model information here. Then there is another equation that has a Markovian structure here, where x_t is a function of x_t minus 1. This is why this is a state-space representation. One of the nice things about working with this representation is we can use some definitions that apply to dynamic linear models or state-space models, and one of those definitions is the so-called forecast function. The forecast function, we can define it in terms of, I’m going to use here the notation f_t h to denote that is a function f that depends on the time t that you’re considering, and then you’re looking at forecasting h steps ahead in your time series. If you have observations up to today and you want to look at what is the forecast function five days later, you will have h equals 5 there. It’s just the expected value. We are going to think of this as the expected value of y_t plus h. Conditional on all the observations or all the information you have received up to time t. I’m going to write it just like this. Using the state-space representation, you can see that if I use the first equation and I think about the expected value of y_t plus h is going to be F transpose, and then I have the expected value of the vector x_t plus h in that case. I can think of just applying this, then I would have expected value of x_t plus h given y_1 up to t. But now when I look at the structure of x_t plus h, if I go to my second equation here, I can see that x_t plus h is going to be dependent on x_t plus h minus 1, and there is a G matrix here. I can write this in terms of the expected value of x_t plus h, which is just G, expected value of x_t plus h minus 1, and then I also have plus expected value of the w_t’s. But because of the structure of the AR process that we defined, we said that all the Epsilon T’s are independent normally distributed random variables center at zero. In this case, those are going to be all zero. I can write down this as F transpose G, and then I have the expected value of x_t plus h minus 1 given y_1 up to t. If I continue with this process all the way until I get to time t, I’m going to get a product of all these G matrices here, and because we are starting with this lag h, I’m going to have the product of that G matrix h times. I can write this down as F transpose G to the power of h, and then I’m going to have the expected value of, finally, I get up to here.\nThis is simply is going to be just my x_t vector. I can write this down as F transpose G^h, and then I have just my x_t. Again, why do we care? Now we are going to make that connection with this matrix and the eigenstructure of this matrix. I said before, one of the features of this matrix is that the eigenstructure is related to the reciprocal roots of the characteristic polynomial. In particular, the eigenvalues of this matrix correspond to the reciprocal roots of the characteristic polynomial. If we are working with the case in which we have exactly p different roots. We have as many different roots as the order of the AR process. Let’s say, p distinct. We can write down then G in terms of its eigendecomposition. I can write this down as E, a matrix Lambda here, E inverse.\nHere, Lambda is going to be a diagonal matrix, you just put the reciprocal roots, I’m going to call those Alpha 1 up to Alpha p. They are all different. You just put them in the diagonal and you can use any order you want. But the eigendecomposition, the eigenvectors, have to follow the order that you choose for the eigenvalues. Then what happens is, regardless of that, you’re going to have a unique G. But here, the E is a matrix of eigenvectors.\n\n\nAgain, why do we care? Well, if you look at what we have here, we have the power G to the power of h. Using that eigendecomposition, we can get to write this in this form. Whatever elements you have in the matrix of eigenvectors, they are now going to be functions of the reciprocal roots. The power that appears here, which is the number of steps ahead that you want to forecast in your time series for prediction, I’m just going to have the Alphas to the power of h. When I do this calculation, I can end up writing the forecast function just by doing that calculation as a sum from j equals 1 up to p of some constants. Those constants are going to be related to those E matrices but the important point is that what appears here is my Alpha to the power of h. What this means is I’m breaking this expected value of what I’m going to see in the future in terms of a function of the reciprocal roots of the characteristic polynomial. You can see that if the process is stable, is going to be stationary, all the moduli of my reciprocal roots are going to be below one. This is going to decay exponentially as a function of h. You’re going to have something that decays exponentially. Depending on whether those reciprocal roots are real-valued or complex-valued, you’re going to have behavior here that may be quasiperiodic for complex-valued roots or just non-quasiperiodic for the real valued roots. The other thing that matters is, if you’re working with a stable process, are going to have moduli smaller than one. The contribution of each of the roots to these forecasts function is going to be dependent on how close that modulus of that reciprocal root is to one or minus one. For roots that have relatively large values of the modulus, then they are going to have more contribution in terms of what’s going to happen in the future. This provides a way to interpret the AR process.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#examples",
    "href": "C4-L04.html#examples",
    "title": "91  The AR(p) process - M2L4",
    "section": "92.2 Examples 🎥",
    "text": "92.2 Examples 🎥\n\n\n\n\nAR(1)\n\n\n\n\nAR(2) two positive roots\n\n\n\n\nAR(2) complex roots\n\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#acf-of-the-arp",
    "href": "C4-L04.html#acf-of-the-arp",
    "title": "91  The AR(p) process - M2L4",
    "section": "92.3 ACF of the AR(p) 🎥",
    "text": "92.3 ACF of the AR(p) 🎥\n\n\n\n\nACF of the AR(p)\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#simulating-data-from-an-arp",
    "href": "C4-L04.html#simulating-data-from-an-arp",
    "title": "91  The AR(p) process - M2L4",
    "section": "92.4 Simulating data from an AR(p) 🎥",
    "text": "92.4 Simulating data from an AR(p) 🎥\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#computing-the-roots-of-the-ar-polynomial",
    "href": "C4-L04.html#computing-the-roots-of-the-ar-polynomial",
    "title": "91  The AR(p) process - M2L4",
    "section": "92.5 Computing the roots of the AR polynomial 📖",
    "text": "92.5 Computing the roots of the AR polynomial 📖\nCompute AR reciprocal roots given the AR coefficients\n\n# Assume the folloing AR coefficients for an AR(8)\nphi=c(0.27, 0.07, -0.13, -0.15, -0.11, -0.15, -0.23, -0.14)\nroots=1/polyroot(c(1, -phi)) # compute reciprocal characteristic roots\nr=Mod(roots) # compute moduli of reciprocal roots\nlambda=2*pi/Arg(roots) # compute periods of reciprocal roots\n\n# print results modulus and frequency by decreasing order\nprint(cbind(r, abs(lambda))[order(r, decreasing=TRUE), ][c(2,4,6,8),]) \n\n             r          \n[1,] 0.9722428 12.731401\n[2,] 0.8094950  5.103178\n[3,] 0.7196221  2.987712\n[4,] 0.6606487  2.232193",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#simulating-data-from-an-arp-1",
    "href": "C4-L04.html#simulating-data-from-an-arp-1",
    "title": "91  The AR(p) process - M2L4",
    "section": "92.6 Simulating data from an AR(p) 📖",
    "text": "92.6 Simulating data from an AR(p) 📖\n \n\nR code to simulate data from an AR(2) with one pair of complex-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n## simulate data from an AR(2)\nset.seed(2021)\n## AR(2) with a pair of complex-valued roots with modulus 0.95 and period 12 \nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]&lt;- 2*r*cos(2*pi/lambda) \nphi[2] &lt;- -r^2\nphi\n\n[1]  1.645448 -0.902500\n\nT=300 # number of time points\nsd=1 # innovation standard deviation\nyt=arima.sim(n=T, model = list(ar = phi), sd=sd)\n\npar(mfrow = c(3, 1), mar = c(3, 4, 2, 1), cex.lab = 1.5)\n## plot simulated data\nts.plot(yt)\n## draw sample autocorrelation function\nacf(yt, lag.max = 50,\n    type = \"correlation\", ylab = \"sample ACF\", \n    lty = 1, ylim = c(-1, 1), main = \" \")\n\n## draw sample partial autocorrelation function\npacf(yt, lag.ma = 50, main = \"sample PACF\")\n\n\n\n\n\n\n\n\n\nR=code to simulate data from an AR(2) with two different real-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n### Simulate from AR(2) with two real reciprocal roots (e.g., 0.95 and 0.5)\nset.seed(2021)\nrecip_roots=c(0.95, 0.5) ## two different real reciprocal roots\nphi=c(sum(recip_roots), -prod(recip_roots)) ## compute ar coefficients\nphi\n\n[1]  1.450 -0.475\n\nT=300 ## set up number of time points\nsd=1 ## set up standard deviation\nyt=arima.sim(n=T,model = list(ar=phi),sd=sd) # generate ar(2)\n\npar(mfrow = c(3, 1), mar = c(3, 4, 2, 1),  cex.lab = 1.5, cex.main = 1.5)\n### plot simulated data \nts.plot(yt)\n### plot sample ACF\nacf(yt, lag.max = 50, type = \"correlation\",  main = \"sample ACF\")\n### plot sample PACF\npacf(yt, lag.max = 50, main = \"sample PACF\")\n\n\n\n\n\n\n\n\n\nR code to simulate data from an AR(3) with one real reciprocal root and a pair of complex-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n### Simulate from AR(3) with one real root \n### and a pair of complex roots (e.g., r=0.95 and lambda = 12 and real root with\n### 0.8 modulus)\nset.seed(2021)\nr= c(0.95, 0.95, 0.8) ## modulus\nlambda=c(-12, 12) ## lambda\nrecip_roots=c(r[1:2]*exp(2*pi/lambda*1i), r[3]) ## reciprocal roots\nphi &lt;- numeric(3) # placeholder for phi\nphi[1]=Re(sum(recip_roots)) # ar coefficients at lag 1\nphi[2]=-Re(recip_roots[1]*recip_roots[2] + recip_roots[1]*recip_roots[3] + recip_roots[2]*recip_roots[3]) # ar coefficients at lag 2\nphi[3]=Re(prod(recip_roots))\nphi\n\n[1]  2.445448 -2.218859  0.722000\n\nT=300 # number of time points\nsd=1 # standard deviation\nyt=arima.sim(n=T,model = list(ar=phi), sd = sd) # generate ar(3)\n\npar(mfrow = c(3,1),  mar = c(3, 4, 2, 1), cex.lab = 1.5, cex.main = 1.5)\n### plot simulated data \nts.plot(yt)\n### plot sample ACF\nacf(yt, lag.max = 50, type = \"correlation\",  main = \"sample ACF\")\n### plot sample PACF\npacf(yt, lag.max = 50, main = \"sample PACF\")",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#the-arp-review",
    "href": "C4-L04.html#the-arp-review",
    "title": "91  The AR(p) process - M2L4",
    "section": "92.7 The AR(p): Review 📖",
    "text": "92.7 The AR(p): Review 📖\n\n92.7.1 AR(p): Definition, stability, and stationarity\n\n\n92.7.2 AR(p)\nA time series follows a zero-mean autoregressive process of order p, of AR(p), if:\n\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t \\qquad\n\\tag{92.5}\nwhere \\phi_1, \\ldots, \\phi_p are the AR coefficients and \\epsilon_t is a white noise process\nwith \\epsilon_t \\sim \\text{i.i.d. } N(0, v), for all t.\n\nThe AR characteristic polynomial is given by\n\n\\Phi(u) = 1 - \\phi_1 u - \\phi_2 u^2 - \\ldots - \\phi_p u^p,\n\nwith u complex-valued.\nThe AR(p) process is stable if \\Phi(u) = 0 only when \\|u\\| &gt; 1. In this case, the process is also stationary and can be written as\n\ny_t = \\psi(B) \\epsilon_t = \\sum_{j=0}^{\\infty} \\psi_j \\epsilon_{t-j},\n\nwith \\psi_0 = 1 and \\sum_{j=0}^{\\infty} |\\psi_j| &lt; \\infty. Here B denotes the backshift operator, so B^j \\epsilon_t = \\epsilon_{t-j} and\n\n\\psi(B) = 1 + \\psi_1 B + \\psi_2 B^2 + \\ldots + \\psi_j B^j + \\ldots\n\nThe AR polynomial can also be written as\n\n\\Phi(u) = \\prod_{j=1}^{p} (1 - \\alpha_j u),\n\nwith \\alpha_j being the reciprocal roots of the characteristic polynomial. For the process to be stable (and consequently stationary), |\\alpha_j| &lt; 1 for all j = 1, \\ldots, p.\n\n92.7.2.1 AR(p): State-space representation\nAn AR(p) can also be represented using the following state-space or dynamic linear (DLM) model representation:\n\ny_t = F' x_t,\n\n\nx_t = G x_{t-1} + \\omega_t,\n\nwith x_t = (y_t, y_{t-1}, \\dots, y_{t-p+1})', F = (1, 0, \\dots, 0)', \\omega_t = (\\epsilon_t, 0, \\dots, 0)', and\n\nG = \\begin{pmatrix}\n\\phi_1 & \\phi_2 & \\phi_3 & \\dots & \\phi_{p-1} & \\phi_p \\\\\n1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & 0 & \\dots & 0 & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1 & 0\n\\end{pmatrix}.\n\nUsing this representation, the expected behavior of the process in the future can be exhibited via the forecast function:\n\nf_t(h) = E(y_{t+h} | y_{1:t}) = F' G^h x_t, \\quad h &gt; 0,\n\nfor any t \\ge p. The eigenvalues of the matrix G are the reciprocal roots of the characteristic polynomial.\n\n\n\n\n\n\nNoteEigenvalues\n\n\n\n\nThe eigenvalues can be real-valued or complex-valued.\nIf they are Complex-valued the eigenvalues/reciprocal roots appear in conjugate pairs.\n\n\n\nAssuming the matrix G has p distinct eigenvalues, we can decompose G into G = E \\Lambda E^{-1}, with\n\n\\Lambda = \\text{diag}(\\alpha_1, \\dots, \\alpha_p),\n\nfor a matrix of corresponding eigenvectors E. Then, G^h = E \\Lambda^h E^{-1} and we have:\n\nf_t(h) = \\sum_{j=1}^{p} c_{tj} \\alpha_j^h.\n\n\n\n92.7.2.2 ACF of AR(p)\nFor a general AR(p), the ACF is given in terms of the homogeneous difference equation:\n\n\\rho(h) - \\phi_1 \\rho(h-1) - \\ldots - \\phi_p \\rho(h-p) = 0, \\quad h &gt; 0.\n\nAssuming that \\alpha_1, \\dots, \\alpha_r denotes the characteristic reciprocal roots each with multiplicity m_1, \\ldots, m_r, respectively, with \\sum_{i=1}^{r} m_i = p. Then, the general solution is\n\n\\rho(h) = \\alpha_1^h p_1(h) + \\ldots + \\alpha_r^h p_r(h),\n\nwith p_j(h) being a polynomial of degree m_j - 1.\n\n92.7.2.2.1 Example: AR(1)\nWe already know that for h \\ge 0, \\rho(h) = \\phi^h. Using the result above, we have\n\n\\rho(h) = a \\phi^h,\n\nand so to find a, we take \\rho(0) = 1 = a \\phi^0, hence a = 1.\n\n\n92.7.2.2.2 Example: AR(2)\nSimilarly, using the result above in the case of two complex-valued reciprocal roots, we have\n\n\\rho(h) = a \\alpha_1^h + b \\alpha_2^h = c r^h \\cos(\\omega h + d).\n\n\n\n\n92.7.2.3 PACF of AR(p)\nWe can use the Durbin-Levinson recursion to obtain the PACF of an AR(p).\nUsing the same representation but substituting the true autocovariances and autocorrelations with their sampled versions, we can also obtain the sample PACF.\nIt is possible to show that the PACF of an AR(p) is equal to zero for h &gt; p.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L05.html",
    "href": "C4-L05.html",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "",
    "text": "92.1 Bayesian inference in the AR(p): Reference prior, conditional likelihood 🎥\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#rcode-maximum-likelihood-estimation-arp-conditional-likelihood",
    "href": "C4-L05.html#rcode-maximum-likelihood-estimation-arp-conditional-likelihood",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.2 Rcode: Maximum likelihood estimation, AR(p), conditional likelihood 📖",
    "text": "92.2 Rcode: Maximum likelihood estimation, AR(p), conditional likelihood 📖\n\n  set.seed(2021)\n# Simulate 300 observations from an AR(2) with one pair of complex-valued reciprocal roots \nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Compute the MLE for phi and the unbiased estimator for v using the conditional likelihood\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"Estimate for v: \", s2, \"\\n\")\n\n\n MLE of conditional likelihood for phi:  1.65272 -0.9189823 \n Estimate for v:  0.9901292",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#model-order-selection",
    "href": "C4-L05.html#model-order-selection",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.3 Model order selection 🎥",
    "text": "92.3 Model order selection 🎥\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#example-bayesian-inference-in-the-arp-conditional-likelihood",
    "href": "C4-L05.html#example-bayesian-inference-in-the-arp-conditional-likelihood",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.4 Example: Bayesian inference in the AR(p), conditional likelihood 🎥",
    "text": "92.4 Example: Bayesian inference in the AR(p), conditional likelihood 🎥\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#sec-arp-bayesian-inference",
    "href": "C4-L05.html#sec-arp-bayesian-inference",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.5 R code: Bayesian inference, AR(p), conditional likelihood 📖",
    "text": "92.5 R code: Bayesian inference, AR(p), conditional likelihood 📖\n\n# Simulate 300 observations from an AR(2) with one pair of complex-valued roots \nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \npar(mfrow=c(1,1), mar = c(3, 4, 2, 1) )\nplot(yt)\n\n\n\n\n\n\n\n## Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\n#####################################################################################\n### Posterior inference, conditional likelihood + reference prior via \n### direct sampling                 \n#####################################################################################\n\nn_sample=1000 # posterior sample size\nlibrary(MASS)\n\n## step 1: sample v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)\n\n## step 2: sample phi conditional on v from normal distribution\nphi_sample=matrix(0, nrow = n_sample, ncol = p)\nfor(i in 1:n_sample){\n  phi_sample[i, ]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)\n}\n\npar(mfrow = c(2, 3), mar = c(3, 4, 2, 1),  cex.lab = 1.3)\n## plot histogram of posterior samples of phi and v\n\nfor(i in 1:2){\n  hist(phi_sample[, i], xlab = bquote(phi), \n       main = bquote(\"Histogram of \"~phi[.(i)]),col='lightblue')\n  abline(v = phi[i], col = 'red')\n}\n\nhist(v_sample, xlab = bquote(nu), main = bquote(\"Histogram of \"~v),col='lightblue')\nabline(v = sd, col = 'red')\n\n#####################################################\n# Graph posterior for modulus and period \n#####################################################\nr_sample=sqrt(-phi_sample[,2])\nlambda_sample=2*pi/acos(phi_sample[,1]/(2*r_sample))\nhist(r_sample,xlab=\"modulus\",main=\"\",col='lightblue')\nabline(v=0.95,col='red')\nhist(lambda_sample,xlab=\"period\",main=\"\",col='lightblue')\nabline(v=12,col='red')\n\n\n\n\n\n\n\n\n\n92.5.1 R code: Model order selection 📖\n\n###################################################\n# Simulate data from an AR(2)\n###################################################\nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n#############################################################################\n######   compute AIC and BIC for different AR(p)s based on simulated data ###\n#############################################################################\npmax=10 # the maximum of model order\nXall=t(matrix(yt[rev(rep((1:pmax),T-pmax)+rep((0:(T-pmax-1)),\n              rep(pmax,T-pmax)))], pmax, T-pmax));\ny=rev(yt[(pmax+1):T])\nn_cond=length(y) # (number of total time points - the maximum of model order)\n\n## compute MLE\nmy_MLE &lt;- function(y, Xall, p){\n  n=length(y)\n  x=Xall[,1:p]\n  a=solve(t(x) %*%x)\n  a=(a + t(a))/2 # for numerical stability \n  b=a%*%t(x)%*%y # mle for ar coefficients\n  r=y - x%*%b # residuals \n  nu=n - p # degrees freedom\n  R=sum(r*r) # SSE\n  s=R/nu #MSE\n  return(list(b = b, s = s, R = R, nu = nu))\n}\n\n\n## function for AIC and BIC computation \nAIC_BIC &lt;- function(y, Xall, p){\n  ## number of time points\n  n &lt;- length(y)\n  \n  ## compute MLE\n  tmp=my_MLE(y, Xall, p)\n  \n  ## retrieve results\n  R=tmp$R\n  \n  ## compute likelihood\n  likl= n*log(R)\n  \n  ## compute AIC and BIC\n  aic =likl + 2*(p)\n  bic =likl + log(n)*(p)\n  return(list(aic = aic, bic = bic))\n}\n# Compute AIC, BIC \naic =numeric(pmax)\nbic =numeric(pmax)\n\nfor(p in 1:pmax){\n  tmp =AIC_BIC(y,Xall, p)\n  aic[p] =tmp$aic\n  bic[p] =tmp$bic\n  print(c(p, aic[p], bic[p])) # print AIC and BIC by model order\n}\n\n[1]    1.000 2166.793 2170.463\n[1]    2.000 1635.816 1643.156\n[1]    3.000 1637.527 1648.536\n[1]    4.000 1639.059 1653.738\n[1]    5.000 1640.743 1659.093\n[1]    6.000 1641.472 1663.491\n[1]    7.000 1643.457 1669.147\n[1]    8.000 1645.370 1674.729\n[1]    9.000 1646.261 1679.290\n[1]   10.000 1647.915 1684.614\n\n## compute difference between the value and its minimum\naic =aic-min(aic) \nbic =bic-min(bic) \n\n## draw plot of AIC, BIC, and the marginal likelihood\npar(mfrow = c(1, 1), mar = c(3, 4, 2, 1) )\nmatplot(1:pmax,matrix(c(aic,bic),pmax,2),ylab='value',\n        xlab='AR order p',pch=\"ab\", col = 'black', main = \"AIC and BIC\")\n# highlight the model order selected by AIC\ntext(which.min(aic), aic[which.min(aic)], \"a\", col = 'red') \n# highlight the model order selected by BIC\ntext(which.min(bic), bic[which.min(bic)], \"b\", col = 'red') \n\n\n\n\n\n\n\n########################################################\np &lt;- which.min(bic) # We set up the moder order\nprint(paste0(\"The chosen model order by BIC: \", p))\n\n[1] \"The chosen model order by BIC: 2\"\n\n\n\n\n92.5.2 Spectral representation of the AR(p) 🎥\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\n\n\n92.5.3 Spectral representation of the AR(p): Example 🎥\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\n\n\n92.5.4 Rcode: Spectral density of AR(p) 📖\n\n### Simulate 300 observations from an AR(2) prcess with a pair of complex-valued roots \nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]&lt;- 2*r*cos(2*pi/lambda) \nphi[2] &lt;- -r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# sample from the AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n# Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood \np=2\ny=rev(yt[(p+1):T])\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\n# Obtain 200 samples from the posterior distribution under the conditional likelihood and the reference prior \nn_sample=200 # posterior sample size\nlibrary(MASS)\n\n## step 1: sample v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)\n\n## step 2: sample phi conditional on v from normal distribution\nphi_sample=matrix(0, nrow = n_sample, ncol = p)\nfor(i in 1:n_sample){\n  phi_sample[i,]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)\n}\n\n\n### using spec.ar to draw spectral density based on the data assuming an AR(2)\nspec.ar(yt, order = 2, main = \"yt\")\n\n\n\n\n\n\n\n### using arma.spec from astsa package to draw spectral density\nlibrary(\"astsa\")\n\n## plot spectral density of simulated data with posterior sampled \n## ar coefficients and innvovation variance\npar(mfrow = c(1, 1), mar = c(3, 4, 2, 1) )\n#result_MLE=arma.spec(ar=phi_MLE, var.noise = s2, log='yes',main = '')\nresult_MLE=arma.spec(ar=phi_MLE, var.noise = s2, main = '')\n\n\n\n\n\n\n\nfreq=result_MLE$freq\n  \nspec=matrix(0,nrow=n_sample,ncol=length(freq))\n\nfor (i in 1:n_sample){\nresult=arma.spec(ar=phi_sample[i,], var.noise = v_sample[i],# log='yes',\n                 main = '',plot=FALSE)\nspec[i,]=result$spec\n}\n\nplot(2*pi*freq,log(spec[1,]),type='l',ylim=c(-3,12),ylab=\"log spectra\",\n     xlab=\"frequency\",col=0)\n#for (i in 1:n_sample){\nfor (i in 1:2){\nlines(2*pi*freq,log(spec[i,]),col='darkgray')\n}\nlines(2*pi*freq,log(result_MLE$spec))\nabline(v=2*pi/12,lty=2,col='red')\n\n\n\n\n\n\n\n\n\n\n92.5.5 Quiz: Spectral representation of the AR(p)\nOmitted due to Coursera’s Honor Code\n\n\n92.5.6 Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p)\nThe dataset below corresponds to a portion of an electroencephalogram (EEG) recorded in a particular location on the scalp of an individual. The original EEG dataset was originally recorded at 256Hz but was then subsampled every sixth observations, so the resulting sampling rate is about 42.7 observations per second. The dataset below has 400 observations corresponding approximately to 9.36 seconds.\nYou will use an AR(8) to model this dataset and obtain maximum likelihood estimation and Bayesian inference for the parameters of the model. For this you will need to do the following:\n\nDownload the dataset, and plot it in R. Upload a picture of your graph displaying the data and comment on the features of the data. Does it present any trends or quasi-periodic behavior?\nModify the code below to obtain the maximum likelihood estimators (MLEs) for the AR coefficients under the conditional likelihood. For this you will assume an autoregressive model of order p=8. The parameters of the model are \\phi=(\\phi_1, \\ldots \\phi_8)' snf v. You will compute the MLE of \\phi denoted as \\hat\\phi. ​\nObtain an unbiased estimator for the observational variance of the AR(8). You will compute the unbiased estimator for v denoted as s^2.\nModify the code below to obtain 500 samples from the posterior distribution of the parameters \\phi=(\\phi_1, \\ldots \\phi_8)' and v under the conditional likelihood and the reference prior. You will assume an autoregressive model of order v. Once you obtain samples from the posterior distribution you will compute the posterior means of \\phi and v, denoted as \\hat\\phi. and \\hat v, respectively.\n\nModify the code below to use the function polyroot and obtain the moduli and periods of the reciprocal roots of the AR polynomial evaluated at the posterior mean \\hat\\phi.\n\nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \npar(mfrow=c(1,1), mar = c(3, 4, 2, 1) )\nplot(yt)\n\n\n\n\n\n\n\n## Case 1: Conditional likelihood\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"Estimate for v: \", s2, \"\\n\")\n\n\n MLE of conditional likelihood for phi:  1.65272 -0.9189823 \n Estimate for v:  0.9901292 \n\n#####################################################################################\n##  AR(2) case \n### Posterior inference, conditional likelihood + reference prior via \n### direct sampling                 \n#####################################################################################\n\nn_sample=1000 # posterior sample size\nlibrary(MASS)\n\n## step 1: sample v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)\n\n## step 2: sample phi conditional on v from normal distribution\nphi_sample=matrix(0, nrow = n_sample, ncol = p)\nfor(i in 1:n_sample){\n  phi_sample[i, ]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)\n}\n\n## plot histogram of posterior samples of phi and nu\npar(mfrow = c(1, 3),  mar = c(3, 4, 2, 1), cex.lab = 1.3)\nfor(i in 1:2){\n  hist(phi_sample[, i], xlab = bquote(phi), \n       main = bquote(\"Histogram of \"~phi[.(i)]))\n  abline(v = phi[i], col = 'red')\n}\n\nhist(v_sample, xlab = bquote(nu), main = bquote(\"Histogram of \"~v))\nabline(v = sd, col = 'red')",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#arima-processes",
    "href": "C4-L05.html#arima-processes",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.6 ARIMA processes 📖",
    "text": "92.6 ARIMA processes 📖\n\n\n\n\n\n\nNoteARMA Model Definition\n\n\n\nA time series process is a zero-mean autoregressive moving average process if it is given by\n\ny_t = \\textcolor{red}\n                {\\underbrace{\\sum_{i=1}^{p} \\phi_i y_{t-i}}_{AR(P)}}\n      +\n      \\textcolor{blue}{\\underbrace{\\sum_{j=1}^{q} \\theta_j \\epsilon_{t-j}}_{MA(Q)}} + \\epsilon_t \\qquad \\text{(ARMA(p, q))}\n\\tag{92.1}\nwith \\epsilon_t \\sim N(0, v).\n\nFor q = 0, we get an AR(p) process.\nFor p = 0, we get a MA(q) i.e. moving average process of order q.\n\n\n\nNext we will define the notions of stability and invertibility of an ARMA process.\n\n92.6.0.1 Stability Definition\nAn ARMA process is stable if the roots of the AR characteristic polynomial stable\n\n\\Phi(u) = 1 - \\phi_1 u - \\phi_2 u^2 - \\ldots - \\phi_p u^p\n\nlie outside the unit circle, i.e., for all u such that \\Phi(u) = 0, |u| &gt; 1.\nEquivalently, this happens when the reciprocal roots of the AR polynomial have moduli smaller than 1.\nThis condition implies stationarity.\n\n\n92.6.0.2 Invertible ARMA Definition\nAn ARMA process is invertible if the roots of the MA characteristic polynomial given by invertible\n\n\\Theta(u) = 1 + \\theta_1 u + \\ldots + \\theta_q u^q,\n\\tag{92.2}\nlie outside the unit circle.\n\nNote that \\Phi(B) y_t = \\Theta(B) \\epsilon_t.\n\nWhen an ARMA process is stable, it can be written as an infinite order moving average process.\nWhen an ARMA process is invertible, it can be written as an infinite order autoregressive process.\n\n\n92.6.0.3 ARIMA Processes\nAn autoregressive integrated moving average process with orders p, d, and q is a process that can be written as\n\n(1 - B)^d y_t = \\sum_{i=1}^{p} \\phi_i y_{t-i} + \\sum_{j=1}^{q} \\theta_j \\epsilon_{t-j} + \\epsilon_t,\n\\tag{92.3}\nwhere B is the backshift operator, d is the order of integration, and \\epsilon_t \\sim N(0, v).\nin other words, y_t follows an ARIMA(p, d, q) if the d difference of y_t follows an ARMA(p, q).\n\nEstimation in ARIMA processes can be done via least squares, maximum likelihood, and also in a Bayesian way. We will not discuss Bayesian estimation of ARIMA processes in this course.\n\n92.6.1 Spectral Density of ARMA Processes\nFor a given AR(p) process with AR coefficients \\phi_1, \\dots, \\phi_p and variance v, we can obtain its spectral density as\n\nf(\\omega) = \\frac{v}{2\\pi |\\Phi(e^{-i\\omega})|^2} = \\frac{v}{2\\pi |1 - \\phi_1 e^{-i\\omega} - \\ldots - \\phi_p e^{-ip\\omega}|^2},\n\\tag{92.4}\nwith \\omega a frequency in (0, \\pi).\nThe spectral density provides a frequency-domain representation of the process that is appealing because of its interpretability.\nFor instance, an AR(2) process that has one pair of complex-valued reciprocal roots with modulus 0.7 and a period of \\lambda = 12, will show a mode in the spectral density located at a frequency of 2\\pi/12. If we keep the period of the process at the same value of 12 but increase its modulus to 0.95, the spectral density will continue to show a mode at 2\\pi/12, but the value of f(2\\pi/12) will be higher, indicating a more persistent quasi-periodic behavior.\nSimilarly, we can obtain the spectral density of an ARMA process with AR characteristic polynomial \\Phi(u) = 1 - \\phi_1 u - \\ldots - \\phi_p u^p and MA characteristic polynomial \\Theta(u) = 1 + \\theta_1 u + \\ldots + \\theta_q u^q, and variance v as\n\nf(\\omega) = \\frac{v}{2\\pi} \\frac{|\\Theta(e^{-i\\omega})|^2}{|\\Phi(e^{-i\\omega})|^2}.\n\\tag{92.5}\nNote that if we have posterior estimates or posterior samples of the AR/ARMA coefficients and the variance v, we can obtain samples from the spectral density of AR/ARMA processes using the equations above.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L06.html",
    "href": "C4-L06.html",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "",
    "text": "94 The Normal Dynamic Linear Model: Definition, Model classes & The Superposition Principle\nNormal Dynamic Linear Models (NDLMs) are defined and illustrated in this module using several examples Model building based on the forecast function via the superposition principle is explained. Methods for Bayesian filtering, smoothing and forecasting for NDLMs in the case of known observational variances and known system covariance matrices are discussed and illustrated..\nThe Normal Dynamic Linear Model (DLM) is covered (R. Prado, Ferreira, and West 2023, 117–44)\nDynamic Linear Models (DLMs) extend classical linear regression to time-indexed data, introducing dependencies between observations through latent evolving parameters. A Normal DLM (NDLM) assumes Gaussian noise at both observation and system levels, enabling tractable Bayesian inference through the Kalman filter.\nWhile superficially complex, NDLMs are conceptually close to linear regression. Instead of I.I.D. observations indexed by i, we index data by time t and allow parameters to evolve with time, resulting in a two-level hierarchical model. At the top level is the observation equation. Below this there is the evolution equation(s) that can be understood as a latent state transition model that can capture trends, periodicity, and regression. The evolution equations can have more than one level however we will see that with some work these are summarized into a matrix form.\nTo make things simpler this is demonstrated using a white noise process and then a random walk model. What makes the NDLM somewhat different is that that there are two variance elements at two levels, necessitating learning more parameters. Once we cover these to models the instructor walks us though all the bits and pieces of the notation. Later we will see that we can add trends, periodicity, regression components in a more or less systematic way. However we need to pick and choose these components to get a suitable forecast function. This approach require an intimate familiarity with the data generating process to model.\nThis approach is Bayesian in that we draw our parameters from a multivariate normal and use updating to improve this initial estimate by incorporating the data and we end up with a posterior i.e. we have distributional view of the time series incorporating uncertainties. Additionally we have a number of Bayesian quantities that can be derived from the model, such as\nHowever the DLM framework is quite flexible and once you understand it it can ve adapted to support features like seasonality using the superposition principle. NDLMs don’t need to be non-stationary time series.\nAs far as I cen tell NDLMs are just DLM with their errors distributed normally at the different levels.",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#ndlm-definition",
    "href": "C4-L06.html#ndlm-definition",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "94.1 NDLM Definition 🎥",
    "text": "94.1 NDLM Definition 🎥\n\n\n\n\nNDLM Motivation\n\n\n\n\nNDLM general form\n\n\n\n\nthe forecast function\n\n\n\n\nIn this module, we will motivate and develop a class of models suitable for for analyzing and forecasting non-stationary time series called normal dynamic linear models . We will talk about Bayesian inference and forecasting within this class of models and describe model building as well.\n\n94.1.1 White Noise - A motivating example\nLet’s begin with a very simple model that has no temporal structure, just a mean value with some variation that is:\n\ny_t = \\mu + v_t \\qquad v_t \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, \\nu) \\qquad  \\text{(white noise model)}\n\\tag{94.1}\nwhere:\n\ny_t is the observed time series at time t,\n\\mu is the expected value of y_t this is characteristic we are interested in,\n\\nu_t is a white noise process as usual iid standard normal N(0,1).\n\nIf we plot this model we might see the following graph:\n\nset.seed(123)\nn &lt;- 100\nV &lt;- 1\nmu &lt;- 0\ny &lt;- mu + rnorm(n, 0, V)\nplot(y, type = \"l\", col = \"blue\", lwd = 2, xlab = \"Time\", ylab = \"y\", main = \"Model with no temporal structure\")\n\n\n\n\n\n\n\nFigure 94.1\n\n\n\n\n\nFor this model the mean of the time series is \\mu will be the the expected value of y_t, which is \\mu. And the variance of y_t is \\nu.\n\n\\mathbb{E}[y_t] = \\mu \\qquad \\text{and} \\qquad \\mathbb{V}ar[y_t] = \\nu \\qquad\n\\tag{94.2}\n\n\n94.1.2 A Random walk model with a slowly changing mean\nNext we incorporate some temporal structure, we allow the expected value of the time series, to change over time. To can achieve this, by update the model definition with a \\mu_t where the index indicates that it can change at every time step. And let us keep the noise unchanged. i.e. we set it to \\mu_t \\in N(0,\\nu).\nWe get the following model:\n\ny_t = \\mu_t + \\nu_t \\quad \\nu_t \\overset{\\text{iid}}{\\sim} N(0, V) \\qquad \\text{(radom walk model)}\n\\tag{94.3}\nTo complete this we need to also decide how to incorporate the the changes over time in the parameter \\mu_t. We might consider different options but we should pick the simplest possible to start with. One option is to assume that the expected value of \\mu_t is just the expected value of \\mu_{t-1} plus some noise.\nWe now have that random walk type of structure where \\mu_t can be written in terms of \\mu(t-1). The expected value of \\mu_t, we can think of it as \\mu_{t-1} + \\text{some noise}. This error is once again, assumed to be normally distributed random variable centered at zero and with variance W. Another assumption that we have made here is that the \\nu_t and \\omega_t, are also independent of each other.\nputting this together we get:\n\n\\begin{aligned}\ny_t &= \\mu_t + \\nu_t & \\nu_t & \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, V)  & \\text{(Observation eq.)} \\\\\n\\mu_t &= \\mu_{t-1} + \\omega_t  & \\omega_t & \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, W) & \\text{(System/evolution eq.)}\n\\end{aligned}\n\\tag{94.4}\nWith this model, what we are assuming is that the mean level of the series is changing over time. Note that this is an example of a Gaussian or Normal dynamic linear model.\nNDLMs are a two level hierarchical models where :\n\nAt the top is an observation level equation relating observations y at time t to some time dependent, (hidden) state parameters and some observation level iid distributed error.\nThe system evolution level equation describes the dynamics of parameters over time and incorporates some system iid distributed error.\nThese equations have a linear structure, in the sense that the expected value of y at time t is a linear function of the parameters.\nWe have the assumption of normality for the noise terms in both these equations as well as independence within and between levels.\n\nThis is our first example. Next we will be discuss the general class of models. Later we will consider how to incorporate different structures into the model, and how to perform Bayesian inference for filtering smoothing and forecasting.\n\n\n94.1.3 General form of the NDLM\nThe general class of dynamic linear models can be written as follows:\nWe are going to have two equations. One is the so-called observation equation that relates the observations to the parameters in the model, and the notation we are going to use is as follows.\n\\begin{aligned}\ny_t &= \\vec{F}_t' \\vec{\\theta}_t   + \\nu_t && \\nu_t \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, V_t) && \\text{(obs)} \\\\\n\\vec{\\theta}_t &= G_t \\vec{\\theta}_{t-1} + \\vec{\\omega}_t && \\vec{\\omega}_t \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, W_t) && \\text{(system)}\n\\end{aligned}\n\\tag{94.5}\nWhere:\n\ny_t a univariate observation at time t.\n\\vec{\\theta}_t the state vector is a k-dimensional vector of unknown parameters at time t.\n\\vec{F_t} the observation operator a k*1-dimensional vector at time t that transforms the state parameters into observations.\n\\nu_t is the observation noise at time t from a Normal distribution with variance V_t.\nG_t the state evolution operator is a k \\times k matrix (known)\n\\omega_t the innovation or state evolution noise at time t distributed as N(0,W_t)(known)\nthe noise at the observation level and the system level are each iid and mutually iid.\n\nWe also have the prior distribution for the state vector at time 0:\n\n\\vec{\\theta}_0 \\sim N(\\vec{m}_0,c_0) a prior k-dimensional Normal distribution.\n\nm_0 the mean in the prior is a k-dimensional vector of means. (known)\nc_0 is the covariance matrix k by k. (known)\n\n\n\n\n\n\n\n\nNoteSome Thoughts on NDLM the definition\n\n\n\n\n\nQ. Why are F_t and G_t a vector and a matrix respectively?\n\nIt may helps to think about F and G as follows:\nF_t' acts as a linear transformation that maps the latent state \\vec{\\theta}_t into the observation space, of y.\nG_t is a linear transformation that describes how the state vector evolves over time. I like to think about it as a Hidden Markov state transition matrix.\nIn other words, F_t takes the current hidden state \\theta_t and produces an observation y_t, while G_t takes the current state and produces the next state.\n\nQ. Why is this called a linear model?\n\nThis is because both the observation equation is a linear equation that relates the observations to the parameters in the model and the system equation is a linear equation that tells us how the time-varying parameter is going to be changing over time. This is why we call this a linear model.\n\nQ. Why are the noise terms \\nu_t and \\omega_t assumed to be normally distributed?\n\nThis is a common assumption in time series analysis. It is a convenient assumption that allows us to perform Bayesian inference and forecasting in a very simple way. And this is why we call this a normal dynamic linear model.\n\nQ. Isn’t this just a hierarchical model?\n\nIndeed, this is a hierarchical model. We have a model for the observations and a model for the system level. The system level is changing over time and the observations are related to the system level through the observation equation. And so it is possible to extend this model to more complex structures if we wish to do so by adding another level, etc… However adding more levels leads to extra dynamics that are captured in G without changing the overall framework!\n\n\n\n\n\n\n94.1.4 Inference in the NDLM\nIn terms of the inference, there are a few different kinds of densities and quantities that we are interested in:\n One of the distributions that we are interested in finding is the so-called filtering distribution. We may be interested here in finding what is the density of \\theta_t given all the observations that we have up to time t.Filtering distribution\n\n\\mathcal{D}_t= \\{\\mathcal{D}_0, y_{1:T}\\}\n\\tag{94.6}\nWe will denote information as \\mathcal{D}_t. Usually, it is all the information we have at time zero (i.e. our prior), coupled with all the data points I have up to time t.\nHere we conditioning on all the observed quantities and the prior information up to time t, and I may be interested in just finding what is the distribution for \\theta_t. This is called filtering.\n\n\\mathbb{P}r(\\theta_t \\mid \\mathcal{D}_t) \\qquad \\text{filtering distribution}\n\\tag{94.7}\nforecasting distribution\nAnother distribution that is very important in time series analysis is the forecasting distribution. We may be interested in the distribution of y{t+h}? where we consider h lags into the future and we have all the information \\mathcal{D}_t, up to time t. We want to do a predictions here\n\n\\mathbb{P}r(y_{t+h} \\mid \\mathcal{D}_t) \\qquad \\text{forecasting distribution}\n\\tag{94.8}\n Another important quantity or an important set of distributions is what we call the smoothing distribution. Usually, you have a time series, when you get your data, you observe, I don’t know, 300 data points. As you go with the filtering, you are going to start from zero all the way to 300 and you’re going to update these filtering distributions as you go and move forward. We may want instead to revisit the parameter at time 10, for example, given that you now have observed all these 300 observations. In that case, you’re interested in densities that are of the form. Let’s say that you observe capital T in your process and now you are going to revisit that density for \\theta_t. This is now in the past. Here we assume that t&lt;T. This is called smoothing.Smoothing Distribution\nSo you have more observation once you have seen the data. We will talk about how to perform Bayesian inference to obtain all these distributions under this model setting.\n\n\\mathbb{P}r(\\theta_t \\mid \\mathcal{D}_T)  \\qquad t &lt; T \\qquad \\text{smoothing distribution}\n\\tag{94.9}\n\n\n94.1.5 The forecast function for the NDLM\nIn addition to all the structure that we described before and all the densities that we are interested in finding, we also have as usual, the so-called forecast function, which instead of being the density is just \\mathbb{E}[y(t+h)\\mid \\mathcal{D}_t] i.e. expected value of y at time t given all the information we have before time t.\n\n\\mathbb{E}[y(t+h)\\mid \\mathcal(D_t)] = F'_{t+h} G_{t+h} \\ldots G_{t+1} \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t]\n\\tag{94.10}\nThis is the form of the forecast function.\nThere are particular cases and particular models that we will be discussing in which the F_t=F, i.e. constant and also G_t = G is also constant for all t. In these cases, the forecast function can be simplified and written as:\n\nf_t(h) = \\mathbb{E}(y_{t+h} \\mid D_t) = F'G^h \\mathbb{E}(\\theta_t \\mid \\mathcal{D}_t)\n\\tag{94.11}\nOne thing that we will learn is that the eigen-structure of this matrix is very important to define the form of the forecast function, and it’s very important for model building and for adding components into your model.\n\n\n94.1.6 NDLM short form notation\nFinally, just in terms of short notation, we can always write down when we’re working with normal dynamic linear models, we may be referring to the model instead of writing the two equations, the system and the observation equation. I can just write all the components that define my model. \n\\{F_t, G_t, v_t, W_t\\}\n\\tag{94.12}\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\nIn this part of the course, I will discuss the class of normal dynamic linear models for analyzing and forecasting non-stationary time series. We will talk about Bayesian inference and forecasting within this class of models and describe model building as well.\n\n94.1.7 Motivating example\nI want to begin first with a motivating example. Suppose you have a model that is very simple and has no temporal structure here, just a model that looks like this. You have your time series y_t. Then you’re interested in just thinking about what is the mean level of that time series. That mean level, I’m going to call it \\mu and then I have some noise and the noise is normally distributed. They are all independent, identically distributed normal random variables \\mathcal{N}(0,v). Again, I can think of my time series. Suppose that I have my time series here, and then I’m plotting y_t. Then I have something that looks like this. In this model that \\mu is going to try to get the mean of that time series, this expected value of y_t, which is \\mu. The variance here of y_t is v under this model. What may happen in practice again, this model has no temporal structure, I may want to incorporate some temporal structure that says, well, I think that the level of this, the expected value of this time series, should be changing over time. If you were to do that, you will write down a model where the \\mu changes over time, so it’s indexed in time. Then you have still your same noise here. Let’s again assume \\mathcal{N}(0,v). I have now to make a decision on how I’m going to incorporate temporal structure by modeling the changes over time in this parameter \\mu_t. You could consider different options.\nThe simplest possible, probably that you can consider is something that looks like this. You have that random walk type of structure where \\mu_t is now going to be written as \\mu_{t-1}. The expected value of \\mu_t, you’ll think of it as \\mu_{t-1} plus some noise. That error here is going to be again, assume normally distributed random variable centered at zero and with variance w. There is another assumption that we can make here and is that the nu t and omega t here, are also independent of each other. When I have this model, what am assuming here is that the mean level of the series is changing over time.\nThese type of models have a few characteristics. This is an example of a normal dynamic linear model, as we will see later. In this models, we usually have a few things.\nThe first thing is we have two equations. One is the so-called observation equation that is relating your y_t, your observed process to some parameters in the model that are changing over time. The next equation is the so-called system level equation or evolution equation that tells me how that time varying parameter is going to be changing over time. The other thing you may notice is that we have a linear structure both in the observational level and in the system level. The linear structure, in the sense of the expected value of y_t is just a linear function of that \\mu_t. It happens to be \\mu_t in this particular case. In the second level, I can think of the expected value of \\mu_t as a linear function given \\mu_{t-1}, so it’s a function that is linear on \\mu_{t-1}. There is that linear structure. The other thing that we have here is at both levels, we have the assumption of normality for the noise terms in those equations. This is an example of a Gaussian or normal dynamic. These are time-varying parameters linear model. We will be discussing the general class of models. This is just an example. We will also discuss how to build different structures into the model, as well as how to perform Bayesian inference and forecasting.\n\n\n94.1.8 General form of the model\nThe general class of dynamic linear models can be written as follows. Again, we are going to have two equations. One is the so-called observation equation that relates the observations to the parameters in the model, and the notation we are going to use is as follows. Here, my observations are univariate. We are discussing models for univariate time series. I have that related to a vector of parameters, \\theta_t plus some noise here. This is the noise. The noise are assumed to be independent, identically distributed normal random variables, 0, V_t. Then I have another equation which is a system equation that has this form. There is a general G_t matrix. This is going to be depending on \\theta_{t-1}. This is a vector, and then I have again, these are iid multivariate \\mathcal{N}(0, W_t). This is the observation equation. This is the system equation or evolution equation. This defines a normal dynamic linear model. Here, we are going to say that F_t is a vector. The dimension of the vector is going to be the same as the number of parameters in the model. Let’s say we have k. This is a vector of known values. For each t, we are going to assume that we know what that vector is. Then we have the vector of parameters here is also of dimension k of parameters. The G is the next thing we need to define is a known matrix. That one is also assumed to be known, and then I have V_t is variance at the observational level. The W_t we are going to assume at the beginning that these two quantities are also known for all the values t. This is the variance-covariance matrix at the system level. Again, if we think about these two equations, we have the model defined in this way.\nThere is a next piece that we need to consider if we are going to perform based in inference for the model parameters. The next piece that we need to consider to just fully specify the model is what is the prior distribution. In a normal dynamic linear model, the prior distribution is assumed to be conjugate here. In the case again in which V_t and W_t are known, we are going to be assuming that, say that zero, the parameter vector before observing any data is going to be normally distributed Multivariate normal with M_0 and C_0. The mean is a vector, again of the same dimension as \\theta_0. Then I have k by k covariance matrix there as well. These are assumed to be also given to move forward with the model.\n\n\n94.1.9 Inference, forecasting, smoothing, and filtering.\nIn terms of the inference, there are different kinds of densities and quantities that we are interested in. One distribution that we are interested in finding is the so-called filtering distribution. We may be interested here in finding what is the density of \\theta_{t} given all the observations that we have up to time t. I’m going to call and all the information that I have up to time t. I’m going to call that D_t . It can also be, in some cases, I will just write down. So D_t, you can view with all the info up to time t. Usually, it is all the information I have at time zero. Then coupled, if there is no additional information that’s going to be coupled with all the data points I have up to that time. Here I’m conditioning on all the observed quantities and the prior information up to time t, and I may be interested in just finding what is the distribution for \\theta_{t}.\nThis is called filtering. Another quantity that is very important in time series analysis is forecasting.\nI may be interested in just what is the density, the distribution of y_{t+h} ? Again, the number of steps ahead here, here I’m thinking of h, given that I have all this information up to time t. I’m interested in predictions here. We will be talking about forecasting. Then another important quantity or an important set of distributions is what we call the smoothing distribution. Usually, you have a time series, when you get your data, you observe, I don’t know, 300 data points. As you go with the filtering, you are going to start from zero all the way to 300 and you’re going to update these filtering distributions as you go and move forward. But then you may want to revisit your parameter at time 10, for example, given that you now have observed all these 300 observations. In that case, you’re interested in densities that are of the form. Let’s say that you observe capital T in your process and now you are going to revisit that density for \\theta_t. This is now in the past. Here we assume that t is smaller than capital T. This is called smoothing. So you have more observation once you have seen the data. We will talk about how to perform Bayesian inference to obtain all these distributions under this model setting.\n\n\n94.1.10 The forecast function\nIn addition to all the structure that we described before and all the densities that we are interested in finding, we also have as usual, the so-called forecast function, which is just instead of being the density is just expected value of y(t+h) given all the information I have up to time t. In the case of a general normal dynamic linear model, we have the structure for these just using the equations, the observation and the system of equations.\nWe’re going to have here G_{t+h}. We multiply all these all the way to G_(t+1), and then we have the \\mathbb{E}[\\theta_{t}\\mid D_t]. This is the form of the forecast function. There are particular cases and particular models that we will be discussing in which the F_t is equal to F, so is constant for all t and G_t is also constant for all t. In those cases, the forecast function can be simplified and written as F'G^h expected value. One thing that we will learn is that the eigen-structure of this matrix is very important to define the form of the forecast function, and it’s very important for model building and for adding components into your model.\n\n\n94.1.11 Short-form notation\nFinally, just in terms of short notation, we can always write down when we’re working with normal dynamic linear models, we may be referring to the model instead of writing the two equations, the system and the observation equation. I can just write all the components that define my model. This fully specifies the model in terms of the two equations. If I know what Ft is, what Gt is, what Vt is, and the covariance at the system level. I sometimes will be just talking about a short notation like this for defining the model.",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#polynomial-trend-models",
    "href": "C4-L06.html#polynomial-trend-models",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "94.2 Polynomial Trend Models 🎥",
    "text": "94.2 Polynomial Trend Models 🎥\n\n\n\n\nfirst and second order polynomial model\n\n\n\n\np-order polynomial model\n\n\nWhile we haven’t talked about the superposition principle yet we start at looking at adding different components to the DLM.\nWe might :\n\nsetting a baseline mean and variance\nadding a random walk with its variance\nadd a trend\nadd a regression\nadd seasonality\n\nNext we want to extend the random walk model to include different types of trends and this will be covered by the polynomial trend models. These are models that are useful to model linear trends or polynomial trends in your time series. So if you have a data set, where you have an increasing trend, or a decreasing trend, you would use one of those components in your model. Also\n\n94.2.1 First order polynomial model\nThe first order model is developed at great detail in chapter In (West and Harrison 2013 ch. 2). I don’t know what to make of it, isn’t this a trivial white noise model?\nThe math for Bayesian updating is fairly straight forward and must be much more complex with more sophisticated dynamics. So this is used by the authors to introduce their DLM and an 30 pages of the book is dedicated to in depth analysis and Bayesian development of this specific model and different distribution of interests as well as including comparison to other models and a look at the signal to noise ratio in the model.\nIt is worthwhile pointing out that these models get their name from their forecast function which will takes the general form Equation 94.22\nThe first order polynomial model is a model that is useful to describe linear trends in your time series. If you have a data set where you have an increasing trend or a decreasing trend, you would use one of those components in your model.\nSo the all those can be incorporated using the general p-order polynomial model, so I will just describe the form of this model.\nA first order polynomial is of the form Ax+B where A is the slope and B is the intercept. This is the same random walk model we saw above.\n\n\\begin{aligned}\ny_t &= \\theta_t + \\nu_t, \\qquad & \\nu_t & \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, V_t) \\\\\n\\theta_t &= \\theta_{t-1} + \\omega_t, \\qquad & \\omega_t & \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, W_t) \\\\\n&\\{1,1,v_t,W_t\\} && \\text{(short form)} \\\\\nf_t(h) &= \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t] && \\text{(forecast fn)} \\\\\n\\end{aligned}\n\\tag{94.13}\nIn the observation equation, \\theta_{t} is the level of the series at time t and \\nu_t is the observation error. In the evolution equation we see the mean for this parameter changing over time as a random walk or a local constant mean with evolution noise \\omega_t.\n(West and Harrison 2013, sec. 2.1) gives the following representation of the model:\nIt is useful to think of \\theta_t as a smooth function of time \\theta(t) with an associated Taylor series representation\n\n\\theta(t + \\delta t) = \\theta(t) + \\text{higher-order terms}\n\\tag{94.14}\nwhere the higher-order terms are assumed to be zero-mean noise. This is a very important point, because it means that we are not trying to model the higher-order terms explicitly, but rather we are assuming that they are just noise.\nwith the model simply describing the higher-order terms as zero-mean noise.\nThis is the genesis of the first-order polynomial DLM: the level model is a locally constant (first-order polynomial) proxy for the underlying evolution.·\nWe can write it down in short form with the following quadruple/\n\n\\{1, 1, V_t, W_t\\} \\qquad f_t(h) = \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t] = k_t \\ \\forall   h&gt;0\n\\tag{94.15}\nNext we can write the forecast function f_t(h) of this model using the representation we gave in Equation 94.15.\nAgain, we’re going to have something of the form F transposed G to the power of h and then the expected value of that \\theta_t given \\mathcal{D}_t. F is 1, G is 1, therefore I’m going to end up having just expected value of \\theta_t given \\mathcal{D}_t.\nWhich depending on the data that you have is you’re just going to have something that is a value that depends on t and it doesn’t depend on h. What this model is telling you is that the forecast function, how you expect to see future values of the series h steps ahead is something that looks like the level that you estimated at time t.\n\n\n94.2.2 Second order Polynomial model AKA Linear Growth model\n(West and Harrison 2013, secs. 7.1–7.2) gives a detailed analysis of this model.\nNow we want to create a model in which captures things that has a linear trend either increasing or decreasing. To do thus we need to have two components in our parameter vector of the state vector. For this we will need two components in our parameter vector of the state vector1.\nSo we have again something that looks like in my observation equation. I’m going to have, I’m going to call it say \\theta_{t,1} \\sim  \\mathcal{N}(v_t), and then I’m going to have say \\theta_{t,1} is going to be of the form to \\theta_{t-1,1} and there is another component here. The other component enters this equation plus let’s call this \\theta_{t-1,2}. And then I have finally also I need an evolution for the second component of the process which is going to be again having a random walk type of behavior.\n\\begin{aligned}\n  y_t &= \\theta_{t,1} + \\nu_t \\quad &\\nu_t &\\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, v_t) \\\\\n  \\theta_{t,1} &= \\theta_{t-1,1} + \\theta_{t-1,2} + \\omega_{t,1} \\qquad &\\omega_{t,1} &\\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, w_{t,11}) \\\\\n  \\theta_{t,2} &= \\theta_{t-1,2} + \\omega_{t,2} \\qquad &\\omega_{t,2} &\\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, w_{t,22})\n\\end{aligned}\n\\tag{94.16}\nSo there are different ways in which you can interpret this two parameters but essentially:\n\n\\theta_{t-1,1} is related to the baseline level of the series\n\\theta_{t-1,2} is related to the rate of change of the of the series.\n\n\n\n\n\n\n\nTipShort form DLM notation\n\n\n\n\n\n\nHaving the short form notation makes the model easier to understand in relation to other DLM models.\nIt will soon be instrumental in communicating the model structure with different software packages.\n\n\n\n\nNext we should summarize this model using the familiar short form DLM representation, which requires a bit of creative algebra.\n\n\\mathbf{\\theta}_t = (\\theta_{t,1}, \\theta_{t,2}) \\qquad \\{\\mathbf{F}, \\mathbf{G}, V_t, \\mathbf{W}_t\\}\n\nFirst we collect the two variances for the evolution two components into the vector \\utilde{w}_t and then assume that this w_t is Normal. Now this is a bi-variate normal.\n\n\\utilde{\\omega}_t = (\\omega_{t,1},\\omega_{t,2})' \\qquad \\utilde{\\omega}_t \\sim  \\mathcal{N}(0,W_t)\n\nSo what would be my F and my G in this model? So again my theta vector has two components, thus my G, so my F is going to be a two dimensional. We can write down F transposed as the only component that appears at this level is the first component of the vector. I’m going to have 1 and then a zero for F transposed. c.f. Equation 94.17 And then my G here if you think about writing down \\theta_t times G say the t-1 + \\omega_t. Then you have that you’re G is going to have this form.\n\n\\begin{aligned}\n\\mathbf{F} &= (1,0)' & V_t &= v_t \\\\\n\\mathbf{G} &=\n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix}\n& \\mathbf{W}_t &=\n\\begin{pmatrix}\nw_{t,11} & 0 \\\\\n0 & w_{t,22}\n\\end{pmatrix}\n\\end{aligned}\n\\tag{94.17}\nthis is the form from the video \n\\begin{aligned}\n\\mathbf{F} &= (1,0)' & V_t &= v_t \\\\\n\\mathbf{G} &=\n\\begin{pmatrix} 1 & h \\\\\n0 & 1 \\end{pmatrix}\n& \\mathbf{W}_t &=\n\\begin{pmatrix}\nw_{t,11} & w_{t,12} \\\\\nw_{t,21} & w_{t,22}\n\\end{pmatrix}\n\\end{aligned}\n\\tag{94.18}\nthis is the more general form from the handout. Note that in this case we have w_{t,12}=w_{t,21} so there is just one extra parameter.\nThe lesson videos and the handouts differ in the form \\mathbf{W}_t. In the lecture we assumed zero covariance but in the handout the covariance was snuck in. This gives us a slightly more general model. The covariance though is symmetric so we get an extra parameter we need to infer and include in the prior. Anyhow I kept the more general form, though in most cases we will keep the off diagonal terms at zero.\nSo for the first component, I have past values of both components. That’s why I have a 1 and 1 here for the second component I only have the past value of the second component. So there is a zero and a 1. So this tells me what is the structure of this second order polynomial. If I think about how to obtain the forecast function for this second order polynomial is going to be very similar to what we did before. So you can write it down as F transposed G to the power of h, expected value of theta t given Dt. Now the expected value is going to be vector also with two components because theta_t is a two dimensional vector. The structure here if you look at what G is G to the power of h going to be a matrix, that is going to look like 1, h, 0 1. When you multiply that matrix time this times this F what you’re going to end up having is something that looks like 1 h times this expected value of theta t given Dt. So I can think of two components here, so this gives you a constant on h, this part is not going to depend on h. So I can write this down as k t 11 component multiplied by 1 and then I have another constant, multiplied by h. So you can see what happens now is that your forecast function has the form of a linear polynomial. So it’s just a linear function on the number of steps ahead. The slope and the intercept related to that linear function are going to depend on the expected value of, theta_t given the all the information I have up to time t. But essentially is a way to model linear trends. So this is what happens with the second order polynomial model.\nAs we included linear trends and constant values in the forecast function, we may want to also incorporate other kinds of trends, polynomial trends in the model. So you may want to have a quadratic form, the forecast function or a cubic forecast function as a function of h.\n\n\\theta_t = (\\theta_{t,1}, \\theta_{t,2})' \\qquad \\mathbf{G} = \\mathbf{J}_2(1) \\qquad \\mathbf{E}_2 = (1, 0)'\n\n\n\\mathbf{G^h} =\n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix}\n\n\n\\begin{aligned}\nf_t(h) &= F' G^h \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t] \\\\\n&= (1,h) \\mathbb{E}[\\theta_{t}\\mid D_t] \\\\\n&= (1,h)(K_{t,0}, K_{t,1})' \\\\\n&= (K_{t,0} + K_{t,1} h)\n\\end{aligned}\n\\tag{94.19}\n\n\\begin{aligned}\n\\mathbf{G^h} &=\n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix}\n\\end{aligned}\n\\tag{94.20}\n\n\n94.2.3 General p-th order polynomial model\nWe can consider a so called p-th order polynomial model. This model will have a state-space vector of dimension p and a polynomial of order p − 1 forecast function on h. The model can be written as\n\n\\{E_p, J_p(1), v_t, W_t\\}\n\nwith F_t = E_p = (1, 0, \\ldots, 0)′ and G_t = J_p(1), with\n\nJ_p(1) = \\begin{pmatrix}\n1 & 1 & 0 & \\cdots & 0 & 0  \\\\\n0 & 1 & 1 & \\cdots & 0 & 0  \\\\\n0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 1\n\\end{pmatrix}\n\\tag{94.21}\nThe forecast function is given by \nf_t(k) = a_{t_0} +  a_{t_1}k + \\ldots + a_{t_{n-1}} k^{n-1} \\qquad k \\in \\mathbb{N}\n\\tag{94.22}\nwhere a_{t_i} are the coefficients of the polynomial and k is the number of steps ahead we need in our forecast. There is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function given by \\{Ep, Lp, vt, W t\\}, with\n\nL_p = \\begin{pmatrix}\n1 & 1 & 1 & \\cdots & 1 & 1  \\\\\n0 & 1 & 1 & \\cdots & 1 & 1   \\\\\n0 & 0 & 1 & \\cdots & 1 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 1\n\\end{pmatrix}\n\\tag{94.23}\nAnd in this type of model, the forecast function is going to have order p-1. So the parameter vector is going to have dimension p. So you’re going to have \\theta_t =  \\theta_{t1:p}.\nThe observation operator F is just a constant and if we write it as a row vector we get F' as a p-dimensional vector with the one in the first entry and zeros everywhere else.\nThe dynamics matrix G may be written using either a J Jordan form Equation 94.21 or as a triangular form Equation 94.23. These result in different parameterization of this model and we will talk a little bit about this.\nIn the Equation 94.21 we have a matrix with ones on the diagonal and the super diagonal, the matrix is needs to be p \\times p i.e. with dimension p to be compatible with the dimension of the hidden state vector \\theta. So this matrix G is what we call a Jordan block of dimension p of 1. So here 1 is the number that appears in the diagonal. And then I have a p I_p matrix, I have ones in the upper diagonal part. So this is the form of the model, so once again I have the F the G, and the W_t. I have my model.\nThe forecast function in this case again can be written as F' G^h \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t]. And when you simplify times expected value of \\theta_t, given D_t. Once you simplify those functions you get something that is a polynomial of order p-1 in h. So I just can write this down as k_t + k_{t,1} h + k_{t, p-1} h^{p-1}, so that’s my forecast function.\nThere is an alternative parameterization of this model that has the same F and the same algebraic form of the forecast function, the same form of the forecast function. But instead of using Equation 94.21 form of the G matrix, it has a Equation 94.23 form that has ones in the diagonal and ones everywhere above the diagonal. So it’s an upper triangular matrix with ones in the diagonal and above the diagonal. That’s a different parameterization of the same model but it leads to the same general form of the forecast function just with a different parameterization.\nSo again, we can consider the way you think about these models?\n\nWhat is you think what kind of forecast function makes sense here ?\nWhat is the type of predictions that I expect to have in my model?\nIf they look like a linear trend, I use a second order polynomial.\nIf it looks like a quadratic trend in the forecast then I would use 3rd order polynomial model representation.\n\nNote that the third order polynomial model is covered in\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\n94.2.4 First order Polynomial Models\nI will begin describing the structure of a particular class of models now, the polynomial trend models. These are models that are useful to describe linear trends or polynomial trends in your time series. So if you have a data set, where you have an increasing trend, or a decreasing trend, you would use one of those components in your model.\nWe will begin with the first order polynomial model, which we have already described. It’s the one that has y_t is a single parameter, I’m going to call it just \\theta_t + \\nu_t. And then a random walk evolution for that single parameter, so that’s the mean level of the series. And then we assume that it changes as a random walk, so this is the first order polynomial model.\nSo in general, I’m going to begin with the first order polynomial model, which we have already described. It’s the one that has y_t is a single parameter, I’m going to call it just \\theta_t + \\nu_t. And then a random walk evolution for that single parameters, so that’s the mean level of the series. And then we assume that it changes As a random walk, so this is the first order polynomial model. In this model if I want to write it down in short form I would have a quadruple that looks like this. So the F here that goes F transposed times the parameter vector in this case we have a scalar vector, scalar parameter. It’s going to be 1 my G that goes next to the state of t-1 is going to also be 1. And then I have vt and Wt here. So this fully defines my model if I think about the forecast function of this model using the representation we had before. Again, we’re going to have something of the form F'G^h and then the expected value of that \\theta_t | \\mathcal{D}_t. F is 1, G is 1, therefore I’m going to end up having just expected value of \\theta_t | \\mathcal{D}_t. Which depending on the data that you have is you’re just going to have something that is a value that depends on t and it doesn’t depend on h.\nWhat this model is telling you is that the forecast function, how you expect to see future values of the series h steps ahead is something that looks like the level that you estimated at time t.\nSo that’s the forecast function, you is a first order is a zero order polynomial is a constant on h and it’s called the first order polynomial model.\n\n\n94.2.5 Second order Polynomial Models\nIn the case of a second order polynomial We are going to now think about about a model in which we want to capture things that are not a constant over time but may have an increasing or decreasing linear trend. In this case we’re going to need two components in your parameter vector in the state vector.\nSo we have again something that looks like in my observation equation. I’m going to have, I’m going to call it say theta{t,1} Normal vt, and then I’m going to have say theta_{t,1} is going to be of the form to theta_{t-1,1} and there is another component here. The other component enters this equation plus let’s call this And then I have finally also I need an evolution for the second component of the process which is going to be again having a random walk type of behavior. So there is different ways in which you can interpret this two parameters but essentially one of them is related to the baseline level of the series the other one is related to the rate of change of the of the series. So if you think about the dlm representation again, these two components, I can collect into the vector wt. and then assume that this wt Is normal. Now this is a bivariate normal. So what would be my F and my G in this model? So again my theta vector has two components My G, so my F is going to be a two dimensional vectors. So I can write down F transposed as the only component that appears at this level is the first component of the vector. I’m going to have 1 and then a zero for F transposed. And then my G here if you think about writing down theta t times G say the t -1 +wt. Then you have that you’re G is going to have this form. So for the first component, I have past values of both components. That’s why I have a 1 and 1 here for the second component I only have the past value of the second component. So there is a zero and a 1. So this tells me what is the structure of this second order polynomial. If I think about how to obtain the forecast function for this second order polynomial is going to be very similar to what we did before. So you can write it down as F transposed G to the power of h, expected value of theta t given Dt. Now the expected value is going to be vector also with two components because theta_t is a two dimensional vector. The structure here if you look at what G is G to the power of h going to be a matrix, that is going to look like 1, h, 0 1. When you multiply that matrix time this times this F what you’re going to end up having is something that looks like 1 h times this expected value of theta t given Dt. So I can think of two components here, so this gives you a constant on h, this part is not going to depend on h. So I can write this down as k t 11 component multiplied by 1 and then I have another constant, multiplied by h. So you can see what happens now is that your forecast function has the form of a linear polynomial. So it’s just a linear function on the number of steps ahead. The slope and the intercept related to that linear function are going to depend on the expected value of, theta_t given the all the information I have up to time t. But essentially is a way to model linear trends. So this is what happens with the second order polynomial model.\n\n\n94.2.6 P-th Order Polynomial Models\nAs we included linear trends and constant values in the forecast function, we may want to also incorporate other kinds of trends, polynomial trends in the model. So you may want to have a quadratic form, the forecast function or a cubic forecast function as a function of h. So the all those can be incorporated using the general p-order polynomial model, so I will just describe the form of this model. And in this type of model, the forecast function is going to have order p-1. So your parameter vector is going to have dimension p. So you’re going to have theta_t theta t1 to tp. Your F matrix is going to be constant if I write it as a row vector. F transpose is going to be a p dimensional vector with the one in the first entry and zeros everywhere else. My G matrix is going to have this form and there is different parameterizations of this model and I will talk a little bit about this. But one way to parameterize the model is something that looks like this. So you have ones in the diagonal of the matrix, the matrix is going to be a p by p has to be the dimension of the p compatible with the dimension of the state vector. And then you have zeros’s below the diagonal above that set of ones that are also ones above the diagonal. So this matrix G is what we call a Jordan block of dimension p of 1. So here 1 is the number that appears in the diagonal. And then I have a p Ip matrix, I have ones in the upper diagonal part. So this is the form of the model, so once again I have the F the G, and the wt. I have my model. The forecast function in this case again can be written as F transposed G to the power of h. And when you simplify times expected value of theta_t, given Dt. Once you simplify those functions you get something that is a polynomial of order p-1 in h. So I just can write this down as kt constant.Plus kt1 h + kt p- 1, h to the p -1, so that’s my forecast function. There is an alternative parameterization of this model that has the same F and the same algebraic form of the forecast function, the same form of the forecast function. But instead of having this G matrix, it has a matrix that has ones in the diagonal and ones everywhere above the diagonal. So it’s an upper triangular matrix with ones in the diagonal and above the diagonal. That’s a different parameterization of the same model is going to have the same general form of the forecast function is a different parameterization. So again, you can consider the way you think about these models is you think what kind of forecast function I want to have for my future? What is the type of predictions that I expect to have in my model? And if they look like a linear trend, I use a second order polynomial. If it looks like a quadratic trend in the forecast then I would use 3rd order polynomial model representation.",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#summary-of-polynomial-trend-models",
    "href": "C4-L06.html#summary-of-polynomial-trend-models",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "94.3 Summary of polynomial trend models 📖",
    "text": "94.3 Summary of polynomial trend models 📖\n\n94.3.1 Polynomial Trend Models\n\n94.3.1.1 First-Order Polynomial\n\n\\begin{aligned}\ny_t &= \\mu_t + \\nu_t, \\qquad & \\nu_t &\\sim  \\mathcal{N}(0, v_t) \\\\\n\\mu_t &= \\mu_{t-1} + \\omega_t, \\qquad & \\omega_t &\\sim  \\mathcal{N}(0, w_t)\n\\end{aligned}\n\nIn this case, we have:\n\\theta_t = \\mu_t \\quad \\forall t\n\nF_t = 1 \\quad \\forall t \\qquad G_t = 1 \\quad \\forall t\n\nresulting in:\n\n\\{1, 1, v_t, w_t\\} \\qquad \\text{(short notation)}\n\nThe forecast function is:\n\nf_t(h) = E(\\mu_t \\mid \\mathcal{D}_t) = k_t, \\quad \\forall h &gt; 0.\n\n\n\n94.3.1.2 Second-Order Polynomial\n\\begin{aligned}\n  y_t &= \\theta_{t,1} + \\nu_t, \\quad &\\nu_t &\\sim  \\mathcal{N}(0, v_t) \\\\\n  \\theta_{t,1} &= \\theta_{t-1,1} + \\theta_{t-1,2} + \\omega_{t,1}, \\qquad &\\omega_{t,1} &\\sim  \\mathcal{N}(0, w_{t,11}) \\\\\n  \\theta_{t,2} &= \\theta_{t-1,2} + \\omega_{t,2}, \\qquad &\\omega_{t,2} &\\sim  \\mathcal{N}(0, w_{t,22}),\n\\end{aligned}\n\nwhere we can also have:\n\n\\mathbb{C}ov(\\theta_{t,1}, \\theta_{t,2} ) = w_{t,12} = w_{t,21}\n\nThis can be written as a DLM with the state-space vector \\theta_t = (\\theta_{t,1}, \\theta_{t,2})', and\n\n\\{\\mathbf{F}, \\mathbf{G}, v_t, \\mathbf{W}_t\\}  \\qquad \\text{(short notation)}\n\nwith \\mathbf{F} = (1, 0)' and\n\n\\mathbf{G} =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\quad \\mathbf{W}_t =\n\\begin{pmatrix}\nw_{t,11} & w_{t,12} \\\\\nw_{t,21} & w_{t,22}\n\\end{pmatrix}.\n\nNote that\n\n\\mathbf{G}^2 =\n\\begin{pmatrix}\n1 & 2 \\\\\n0 & 1\n\\end{pmatrix}, \\quad \\mathbf{G}^h =\n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix},\n\nand so:\n\nf_t(h) = (1, h) E(\\mathbf{\\theta}_t \\mid \\mathcal{D}_t) = (1, h) (k_{t,0}, k_{t,1})' = (k_{t,0} + h k_{t,1}).\n\nHere \\mathbf{G} = \\mathbf{J}_2(1) (see below).\nAlso, we denote \\mathbf{E}_2 = (1, 0)', and so the short notation for this model is\n\n\\{E_2, J_2(1), \\cdot, \\cdot\\}\n\n\n\n94.3.1.3 General p-th Order Polynomial Model\nWe can consider a p-th order polynomial model. This model will have a state-space vector of dimension p and a polynomial of order p-1 forecast function on h. The model can be written as\n\\{E_p, J_p(1), v_t, W_t\\}  \\qquad \\text{(short notation)}\n\nwith \\mathbf{F}_t = \\mathbf{E}_p = (1, 0, \\dots, 0)' and \\mathbf{G}_t = \\mathbf{J}_p(1), with\n\n\\mathbf{J}_p(1) =\n\\begin{pmatrix}\n1 & 1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n0 & 1 & 1 & \\cdots & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0 & 1\n\\end{pmatrix}.\n\nThe forecast function is given by\n\nf_t(h) = k_{t,0} + k_{t,1} h + \\dots + k_{t,p-1} h^{p-1}.\n\nThere is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function, given by \\{E_p, L_p, v_t, W_t\\}, with\n\nL_p =\n\\begin{pmatrix}\n1 & 1 & 1 & \\cdots & 1 \\\\\n0 & 1 & 1 & \\cdots & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{pmatrix}.",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#regression-models",
    "href": "C4-L06.html#regression-models",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "94.4 Regression models 🎥",
    "text": "94.4 Regression models 🎥\n\n\n\n\nRegression models\n\n\n94.4.1 Simple dynamic regression\n\n\\begin{aligned}\ny_t &= \\beta_{t,0} + \\beta_{t,1}x_t + ν_t \\\\\n\\beta_{t,0} &= \\beta_{t−1,0} + \\omega_{t,0} \\\\\n\\beta_{t,1} &= \\beta_{t−1,1} + \\omega_{t,1}\n\\end{aligned}\n\\tag{94.24}\nand so \\theta_t = (\\beta_t,0, \\beta_{t,1})′, F_t = (1, x_t)′ and G = I_2.\nThis results in a forecast function of the form\n\nf_t(h) = k_{t,0} + k_{t,1}x_{t+h}\n\\tag{94.25}\nwhere k_{t,0} = \\mathbb{E}[\\beta_{t,0} \\mid \\mathcal{D}_t] and k_{t,1} = \\mathbb{E}[\\beta_{t,1} \\mid \\mathcal{D}_t].\n\n\n94.4.2 General dynamic regression\n\n\\begin{aligned}\ny_t &= \\beta_{t,0} + \\beta_{t,1}x_{t,1} + \\ldots \\beta_{t,M} x_{t,M} + ν_t \\\\\n\\beta_{t,m} &= \\beta_{t−1,m} + \\omega_{t,m,} & m = 0 : M.\n\\end{aligned}\n\\tag{94.26}\nThen, \\theta = (\\beta_t,0, \\ldots , \\beta_{t,M} )′, F_t = (1, x_{t,1}, \\ldots , x_{t,M} )′ and G = I_M . The forecast function is given by\n\nf_t(h) = k_{t,0} + k_{t,1}x_{t+h,1} + \\ldots + k_{t+h,M}x_{t+h,M}\n\\tag{94.27}\nA particular case is of dynamic regressions is the case of time-varying auto-regressions (TVAR) with\n\n\\begin{aligned}\ny_t &= \\varphi_{t,1}y_{t−1} + \\varphi_{t,2}y_{t−2} + \\ldots + \\varphi_{t,p} y_{t−p} + ν_t,\\\\\n\\varphi_{t,m} &= \\varphi_{t−1,m} + \\omega_{t,m,} & m = 1 : p\n\\end{aligned}\n\\tag{94.28}\nThere is a paper (Raquel Prado, Huerta, and West 2000) on TVAR models that is a good reference for this model.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\n94.4.3 Regression Models\nIn regression models, we may also have additional covariates that are also measured sequentially over time. We may want to regress the y_t time series and see what relationships they have with other covariates that are also measured over time. The simplest possible case is the dynamic simple regression model. In this case, I can write down. I have a single covariate, that covariate is X_t that is observed here, and then I have the usual. In this case, I have an intercept and a slope, and this is representing my simple linear regression. It’s just the regression where both the intercept and the slope are time-varying. I can define the variation. I need to specify what’s the evolution of the two components, and we are going to use this random walk. We could use other structures, but again, in the normal linear case, we are going to be using these evolution equations. Then I collect here my W’s as a single vector. The \\omega_t is going to have the two components in here. These are normally distributed zero and variance covariance matrix W_t, that is a two-by-two matrix. This is the case of the simple regression model. In the case of this model, we have F now is time-varying. This is going to change depending on the value of X_t. I can write Ft transpose as one and X_t. My Theta vector. Again, if I think about what it is, is just Beta t, 0 Beta t, 1. I have those two components.\nThe G matrix is going to be the identity, and you can see that essentially the first component is related to the first component in t minus one, and the second component at time t is related to the second component at time t minus 1. So the identity matrix will be the G. Therefore, if I think about my forecast function in the simple linear regression case, this is going to be my F transpose, which is 1 xt times the G, the G is the identity, times the expected value of Theta t, given Dt. For the expected value of Theta t given Dt, This is a two-dimensional vector, so I’m going to have components in there. I can write this down as K_t0 plus K_t1 Xt. We can see that the forecast function is again has that form that depends on that covariate at the time. This should be t plus h because we are evaluating this at t plus h. You need to have the covariate evaluated at t plus h here.\n\n\n94.4.4 General Dynamic Regression Model\n In the case of general dynamic regression model, we’re going to have a set of covariates. We can have, let’s say k of those covariates or p of those covariates, X_t1. This is my observation equation. Instead of having a single covariate, now I’m going to have p of them. I’m going to have coefficients that go with each of those and I may have the Beta t0 coefficient. My G matrix now, if I think about my parameter vector is just p plus 1 dimensional, p plus 1. Yeah, so that I have the 0 and then the p values, so is a p plus 1 vector. Then my G is the identity. My F_t is going to be a vector, is also p plus 1 dimension. The first entry is one, the second is X_t1 X_tp. My forecast function is going to be similar to this, but now we are going to have more than one covariate, so we end up with a forecast function that has this form, p. This is the case for the dynamic regression.\n\n\n94.4.5 TVAR\n In the case of dynamic regression, we can also have a time-varying autoregressive process. This is a particular case of dynamic regression where the covariates are just past values of the time series itself. In this case, we can think about the observation equation as being a linear combination of past values of the time series. One particular example of dynamic regression model is the case of a time-varying autoregressive process. This brings us back to those autoregressive processes that we were discussing earlier in the course. When you you’re regressing each of the X’s correspond to pass values, you have a regression model that we call a time-varying ARP. In this case, your observation equation is going to have the AR coefficients, but the AR coefficients are going to be varying over time. If we assume that we put all the coefficients together and have a random walk evolution equation for those. If I said, I call Phi_t the vector that contains all the components with all the coefficients from one to p, then I can now define this evolution equation. Then my Omega_t here is a p-dimensional vector, and I have Omega t, normal zero, WT, and my epsilon t normal 0 vt.\nThis defines a time-varying AR. It’s the same structure that we had before. The only difference is my covariates are just past values of the time series. Therefore my forecast function for the time-varying AR is going to have this form where every_thing is going to depend on past values of the time series. We will study this model in particular and make connections with the AR that we studied earlier in the class.",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#summary-of-regression-models",
    "href": "C4-L06.html#summary-of-regression-models",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "94.5 Summary of Regression Models 📖",
    "text": "94.5 Summary of Regression Models 📖\n\n94.5.1 Dynamic Regression Models\n\n94.5.1.1 Simple Dynamic Regression\n\n\\begin{aligned}\n  y_t &= \\beta_{t,0} + \\beta_{t,1} x_t + \\nu_t \\\\\n  \\beta_{t,0} &= \\beta_{t-1,0} + \\omega_{t,0} \\\\\n  \\beta_{t,1} &= \\beta_{t-1,1} + \\omega_{t,1}\n\\end{aligned}\n\nThus:\n\n\\theta_t = (\\beta_{t,0}, \\beta_{t,1})'\n\n\nF_t = (1, x_t)'\n\nand\n\nG = I_2\n\nThis results in a forecast function of the form\n\nf_t(h) = k_{t,0} + k_{t,1} x_{t+h}.\n\n\n\n94.5.1.2 General Dynamic Regression\n\n\\begin{aligned}\ny_t &= \\beta_{t,0} + \\beta_{t,1} x_{t,1} + \\dots + \\beta_{t,M} x_{t,M} + \\nu_t \\\\\n\\beta_{t,m} &= \\beta_{t-1,m} + \\omega_{t,m}, \\quad &m = 0:M.\n\\end{aligned}\n\\tag{94.29}\nThen,\n\\theta_t = (\\beta_{t,0}, \\dots, \\beta_{t,M})',\n\\mathbf{F}_t = (1, x_{t,1}, \\dots, x_{t,M})' and\n\\mathbf{G} = \\mathbf{I}_M.\nThe forecast function is given by:\n\nf_t(h) = k_{t,0} + k_{t,1} x_{t+h,1} + \\dots + k_{t,M} x_{t+h,M}.\n\\tag{94.30}\nA particular case of dynamic regressions is the case of time-varying autoregressive (TVAR) with time-varying autoregressive (TVAR)\n\n\\begin{aligned}\n  y_t &= \\phi_{t,1} y_{t-1} + \\phi_{t,2} y_{t-2} + \\dots + \\phi_{t,p} y_{t-p} + \\nu_t \\\\\n  \\phi_{t,m} &= \\phi_{t-1,m} + \\omega_{t,m}, \\quad m = 1:p.\n\\end{aligned}",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#the-superposition-principle",
    "href": "C4-L06.html#the-superposition-principle",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "94.6 The superposition principle 🎥",
    "text": "94.6 The superposition principle 🎥\n\n\n\n\nThe superposition principle\n\nWe can use the superposition principle to build models that have different kinds of components. The main idea is to think about what is the general structure we want for the forecast function and then isolate the different components of the forecast function and think about the classes of dynamic linear models that are represented in each of those components. Each of those components has a class and then we can build the general dynamic linear model with all those pieces together using this principle.\nTwo references for the Superposition principle are\n\n(West and Harrison 2013, sec. 3.1 p. 98)\n(R. Prado, Ferreira, and West 2023, sec. 4.2.1 p. 136)\n\n\n\n\n\n\n\nImportant 94.1: Superposition Principle\n\n\n\nIn the first the author state:\n\nConditional independence also features strongly in initial model building and in choosing an appropriate parametrization. For example, the linear superposition principle states that any linear combination of deterministic linear models is a linear model. This extends to a normal linear superposition principle:\nAny linear combination of independent normal DLMs is a normal DLM. - &gt; – (West and Harrison 2013, sec. 3.1 p. 98)\n\n\n\nWe will illustrate how to do that with an example:\nLet’s say that we want to create a model here with a forecast function that has a linear trend component. Let’s say we have a linear function as a function of the number of steps ahead that you want to consider. Then suppose you also have a covariate here that you want to include in your model as a regression component. \nf_t(h) = \\underbrace{(k_{t,0} + k_{t,1}\\; h)}_{\\text{linear trend component}} + \\underbrace{(k_{t,2}\\; x_{t+h})}_{\\text{regression component}}\n\\tag{94.31}\nwhere:\n\nf_t(h) is our forecast function.\nk_{t,0}, k_{t,1} and k_{t,2} are just constants (that we index using time t and a second subscript).\nx_{t+h} is a time dependent regression covariate.\n\nWhen we look at the forecast function, we can isolate a linear trend and a regression components as indicated. Each of these can be set in terms of two forecast functions]{.mark}. I’m going to call the forecast function f_{1,t}(h), this is just the first piece.\n\n\\begin{aligned}\nf_t(h) &= f_{1,t}(h) + f_{2,t}(h) \\\\\nf_{1,t}(h) &= k_{t,0} + k_{t,1} & \\text{(linear trend component)} \\\\\nf_{2,t}(h) &= k_{t,2}x_{t+h} & \\text{(regression component)}\n\\end{aligned}\n\\tag{94.32}\nWe know how to represent forecast function f_{1,t} and f_{2,t} in terms of dynamic linear models.\nFor the linear trend component, f_{1,t}(h) , we have a 2-dimensional state vector, \\theta_t = (\\theta_{t,1}, \\theta_{t,2})', which yields the following DLM shortform:\n\n\\{F_1, G_1, \\cdot, \\cdot\\}  \\qquad \\text{(short notation)}\n\\tag{94.33}\n\nWhere we don’t explicitly specify the observational and system variances, V and W\nThe important bit are F and G. The forecast function is given by:\n\n\nF_{1} = E_2 = (1, 0)'\n\\tag{94.34}\n\nG_{1} =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n\\tag{94.35}\nfor the regression component f_{2,t}(h) we have the following DLM representation:\n\n\\{F_2,t, G_2, \\cdot, \\cdot\\}  \\qquad \\text{(short notation)}\n\\tag{94.36}\nwhere we have F_{2t} is X_t and my G is simply going to be 1. This is a one-dimensional vector in terms of the state parameter vector.\n\nF_{2,t} = x_{t+h}\n\\tag{94.37}\n\nG_{2} = 1\n\\tag{94.38}\nOnce we have these, we can assemble them into our final model. \\{F_t, G, \\cdot, \\cdot\\}\nWe care more about F, G, and less about the observational variance and some covariance also for the system where the\nF is going to be an F that has, you just concatenate the two Fs. You’re going to get 1, 0 and then you’re going to put the next component here. Again, this one is dependent on time because this component is time dependent and\nThe model with forecast function f_t(h) above is a model with a 3-dimensional state vector with\n\nF_t = (F_1', F_{2,t})' = (1, 0, x_t)'\n\nThen the G, you can create it just taking a block diagonal structure by concatenating G_1 and G_2. though formally there must be a better term for this operation.\n\nG = \\text{blockdiag}[G_1, G_2] =\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n\nThis gives us the full G dynamics matrix for the model. A model with this F_t and this G that is constant over time will give us this particular forecast function Equation 94.31 we started with.\nWe used the superposition principle to build this model. If we need additional components, we will learn how to incorporate seasonal components, regression components, trend components. One can build a fairly sophisticated model with different structures into this particular model using the superposition principle.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\n94.6.1 The superposition principle\nWe can use the superposition principle to build models that have different kinds of components. The main idea is to think about what is the general structure we want for the forecast function and then isolate the different components of the forecast function and think about the classes of dynamic linear models that are represented in each of those components. Each of those components has a class and then we can build the general dynamic linear model with all those pieces together using this principle. I will illustrate how to do that with an example. Let’s say that you want to create a model here with a forecast function that has a linear trend component. Let’s say we have a linear function as a function of the number of steps ahead that you want to consider. Then suppose you also have a covariate here that you want to include in your model as a regression component. Let’s say we have a K_t2 and then we have X_t plus h, this is my covariate. Again, the k’s here are just constants, as of constants in terms of h, they are dependent on time here. This is the general structure we want to have for the forecast function. Now you can see that when I look at the forecast function, I can isolate here and separate these two components. I have a component that looks like a linear trend and then I have a component that is a regression component. Each of this can be set in terms of two forecast functions. I’m going to call the forecast function F_1t h, this is just the first piece. Then I have my second piece here. I’m going to call it F_2t, is just this piece here with the regression component. We know how to represent this forecast function in terms of a dynamic linear model. I can write down a model that has an F, G, and some V, and some W that I’m going to just leave here and not specify them explicitly because the important components for the structure of the model are the F and the G. If you’ll recall the F in the case of a forecast function with a linear trend like this, is just my E_2 vector, which is a two-dimensional vector. The first entry is one, and the second one is a zero. Then the G in this case is just this upper triangular matrix that has 1, 1 in the first row and 0, 1 in the second one. Remember, in this case we have a two-dimensional state vector where one of the components in the vector is telling me information about the level of the time series, the other component is telling me about the rate of change in that level. This is a representation that corresponds to this forecast function. For this other forecast function, we have a single covariate, it’s just a regression and I can represent these in terms of an F_2, G_2, and then some observational variance and some system variance here in the case of a single covariate and this one depends on t. We have F_2t is X_t and my G here is simply going to be one. This is a one-dimensional vector in terms of the state parameter vector. We have a single state vector and it’s just going to tell me about the changes, the coefficient that goes with the X_t covariate. Once I have these, I can create my final model and I’m going to just say that my final model is F, G, and then I have some observational variance and some covariance also for the system where the F is going to be an F that has, you just concatenate the two Fs. You’re going to get 1, 0 and then you’re going to put the next component here. Again, this one is dependent on time because this component is time dependent and then the G, you can create it just taking a block diagonal structure, G_1 and G_2. You just put together, the first one is 1, 1, 0, 1 and then I concatenate this one as a block diagonal. This should be one. This gives me the full G function for the model. Now a model with this F_t and this G that is constant over time will give me this particular forecast function. I’m using the superposition principle to build this model. If you want additional components, we will learn how to incorporate seasonal components, regression components, trend components. You can build a fairly sophisticated model with different structures into this particular model using the superposition principle.",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#superposition-principle-general-case",
    "href": "C4-L06.html#superposition-principle-general-case",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "94.7 Superposition principle: General case 📖",
    "text": "94.7 Superposition principle: General case 📖\nYou can build dynamic models with different components, for example, a trend component plus a regression component, by using the principle of superposition. The idea is to think about the general form of the forecast function you want to have for prediction. You then write that forecast function as a sum of different components where each component corresponds to a class of DLM with its own state-space representation. The final DLM can then be written by combining the pieces of the different components.\nFor example, suppose you are interested in a model with a forecast function that includes a linear polynomial trend and a single covariate x_t, i.e.,\n\nf_t(h) = k_{t,0} + k_{t,1}h + k_{t,3}x_{t+h}.\n\nThis forecast function can be written as f_t(h) = f_{1,t}(h) + f_{2,t}(h), with\n\nf_{1,t}(h) = (k_{t,0} + k_{t,1}h), \\quad f_{2,t}(h) = k_{t,3}x_{t+h}.\n\nThe first component in the forecast function corresponds to a model with a 2-dimensional state vector, F_{1,t} = F_1 = (1, 0)',\n\nG_{1,t} = G_1 =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n\nThe second component corresponds to a model with a 1-dimensional state vector, F_{2,t} = x_t, G_{2,t} = G_2 = 1.\nThe model with forecast function f_t(h) above is a model with a 3-dimensional state vector with F_t = (F_1', F_{2,t})' = (1, 0, x_t)' and\n\nG_t = \\text{blockdiag}[G_1, G_2] =\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n\n\n94.7.1 General Case\nThe general case wasn’t covered in the video and we didn’t have a proper statement of the superposition principle. However, in Important 94.1 I extracted the statement of the principle above. This statement clarifies that the principle arises via conditional independence, a tool we also used extensively in the previous course on mixture models. Now let us consider the general case from the handout.\nAssume that you have a time series process y_t with a forecast function\n\nf_t(h) = \\sum_{i=1}^{m} f_{i,t}(h),\n\nwhere each f_{i,t}(h) is the forecast function of a DLM with representation \\{F_{i,t}, G_{i,t}, v_{i,t}, W_{i,t}\\}.\nThen, f_t(h) has a DLM representation \\{F_t, G_t, v_t, W_t\\} with\n\nF_t = (F_{1,t}', F_{2,t}', \\dots, F_{m,t}')',\n\n\nG_t = \\text{blockdiag}[G_{1,t}, \\dots, G_{m,t}],\n\n\nv_t = \\sum_{i=1}^{m} v_{i,t},\n\nand\n\nW_t = \\text{blockdiag}[W_{1,t}, \\dots, W_{m,t}].\n\n\n\n\n\n\n\nPrado, Raquel, Gabriel Huerta, and Mike West. 2000. “Bayesian Time-Varying Autoregressions: Theory, Methods and Applications.” Resenhas Do Instituto de Matemática e Estatı́stica Da Universidade de São Paulo 4 (4): 405–22. https://www2.stat.duke.edu/~mw/MWextrapubs/Prado2001.pdf.\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#footnotes",
    "href": "C4-L06.html#footnotes",
    "title": "93  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "",
    "text": "the state makes it’s appearance↩︎",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L07.html",
    "href": "C4-L07.html",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "",
    "text": "95\nWe will now delve into Bayesian inference in the case of the normal dynamic linear model where both the observational variance and the system variance are known. We will talk about filtering equations, smoothing equations and also forecasting in this setting using Bayesian approach",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#filtering",
    "href": "C4-L07.html#filtering",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.1 Filtering 🎥",
    "text": "95.1 Filtering 🎥\n\n\n\n\nDerivation for the Prior and Forecast at Time t\n\n\n\n\nDerivation of the Posterior at Time t\n\n\nRecall we are working in a Bayesian setting where a NDLM model with a normal prior would like this:\n\n\\begin{aligned}\n  y_t &= F_t' \\theta_t + \\nu_t & \\nu_t &\\sim \\mathcal{N}(0, v_t) & \\text{(observation)} \\\\\n  \\theta_t &= G_t \\theta_{t-1} + \\omega_t & \\omega_t &\\sim  \\mathcal{N}(0, W_t) & \\text{(evolution)} \\\\\n  & &(\\theta_0 \\mid \\mathcal{D}_0) & \\sim  \\mathcal{N}(m_0, C_0) & \\text{(prior)}\n\\end{aligned}\n\\tag{95.1}\n\nIn the prior \\mathcal{D}_0 stands for the information that we have before collecting any data and\nWe are assuming \\theta_0 follows a normal distribution with\nm_0 mean\nC_0 variance covariance matrix.\n\nSince we are doing filtering which is a retrospective analysis, of past states we assume that we know m_0, C_0, \\nu_t, \\omega_t, F_t, G_t \\qquad \\forall t.\nHowever, there is often great interest in looking back in time in order to get a clearer picture of what happened.\nWe are interested in performing Bayesian inference in this setting and we talked about different kinds of distributions.\n\nOne is the filtering distribution that allows us to update the distribution of \\theta_t as we receive observations and information over time.\nThe other one is smoothing equations that allows us to just revisit the past once we have observed a chunk of data.\n\nIn a Bayesian setting, you have to set a prior distribution. We will work with the prior distribution that is conjugate.\nIn this case we have to begin with a distribution at time zero for \\theta_0. So before we have seen any data at all, I have this prior distribution.\nWe also assume a prior distribution of the form:\n\n(\\theta_{t} \\mid \\mathcal{D}_{t-1}) \\sim \\mathcal{N}(m_{t-1}, C_{t-1}).\n\\tag{95.2}\nWe assume that this the filtering distribution follows this normal distribution based on\n\nthe prior in Equation 95.9 being conjugate of the normal and\n\nthe linearity of the model in Equation 95.9.\n\nThese result in updates to the model parameters and uncertainty, at each time step, preserving the normal structure from the prior.\nThen, we can obtain the following distributions:\n\nPrior at Time t\n\n\n  (\\theta_t \\mid \\mathcal{D}_{t-1}) \\sim  \\mathcal{N}(a_t, R_t) \\qquad \\text{(prior at time t)} \\qquad\n   \\tag{95.3}\nwith\n \\begin{aligned}\n  a_t \\doteq& \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_{t-1}] =& G_t  \\mathbb{E}[G_t \\theta_{t-1} \\mid \\mathcal{D}_{t-1} ] =& G_t m_{t-1} \\\\\n  R_t \\doteq& \\mathbb{V}ar[\\theta_t \\mid \\mathcal{D}_{t-1}] =& G_t \\mathbb{V}ar[\\theta_t \\mid \\mathcal{D}_{t-1}] =& G_t C_{t-1} G_t' + W_t.\n  \\end{aligned}\n   \\tag{95.4}\nWhere we simply took the first and second moments of the system equation from Equation 95.9 conditioned on our information set \\mathcal{D}_{t-1}\n\nOne-Step Forecast\n\n\n  (y_t \\mid \\mathcal{D}_{t-1}) \\sim  \\mathcal{N}(f_t, q_t) \\qquad \\text{(one step forecast fn)} \\qquad\n   \\tag{95.5}\nwith\n\\begin{aligned}\n  f_t\n     & \\doteq \\mathbb{E}[ y_t \\mid \\mathcal{D}_{t-1} ]\n     & = F_t' \\mathbb{E}[ y_t \\mid \\mathcal{D}_{t-1} ]\n     & = F_t' a_t \\\\\n  q_t\n     & \\doteq \\mathbb{V}ar[y_t \\mid \\mathcal{D}_{t-1}]\n     & = F_t' \\mathbb{V}ar[y_t \\mid \\mathcal{D}_{t-1}]  \n     & = F_t' R_t F_t + v_t\n  \\end{aligned}\n   \\tag{95.6}\nWhere we took the first moments on the observation equation conditioned on the information set \\mathcal{D}_t and substituted Equation 95.5\n\nPosterior at Time t\n\n\n  (\\theta_t \\mid \\mathcal{D}_t) \\sim  \\mathcal{N}(m_t, C_t)\n  \nwith\n\\begin{aligned}\n  m_t &= a_t + R_t F_t q_t^{-1} (y_t - f_t), \\\\\n  C_t &= R_t - R_t F_t q_t^{-1} F_t' R_t.\n  \\end{aligned}\n   \\tag{95.7}\nThese can be derived via Normal theory or via the Multivariate Bayes’ theorem. The background for both seems to be provided in (West and Harrison 2013, secs. 17.2.3 p.639)\nNow, denoting e_t = (y_t - f_t) and A_t = R_t F_t q_t^{-1}, we can rewrite the equations above as:\nIt follows that\n\n\\begin{pmatrix}Y \\\\ \\theta\\end{pmatrix} \\sim \\mathcal{N}\n\\left(\n  \\begin{pmatrix}F'a \\\\ a \\end{pmatrix},\n  \\begin{pmatrix} F'RF + V & F'R \\\\ RF & R \\end{pmatrix}\n\\right)\n\nTherefore, identifying Y with X_1 and \\theta with X_2 in the partition of X in 17.2.2, we have RF R )]\nTherefore, identifying Y with X1 and θ with X2 in the partition of X in 17.2.2, we have \nY \\sim \\mathcal{N}[F'a, F'RF + V]\n\n\n(\\theta \\mid Y) \\sim \\mathcal{N}[m, C],\n\nwhere\n\nm = a + RF[F′RF + V]−1[Y − F′a]\n\nand \nC = R − RF[F′RF + V]−1F′R.\n\n\n  \\begin{aligned}\n  \\theta \\mid \\mathcal{D}_t &\\sim \\mathcal{N}(m_t,C_t)\\\\\n  m_t &\\doteq a_t + A_t e_t, \\\\\n  C_t &\\doteq R_t - A_t q_t A_t'\n  \\end{aligned}\n\\tag{95.8}\nEquation 95.4 , Equation 95.6 and Equation 95.8 are often referred to as the Kalman filtering equations.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\nI will now discuss Bayesian inference in the case of the normal dynamic linear model where both the observational variance and the system variance are known. We will talk about filtering equations, smoothing equations and also forecasting in this setting using Bayesian approach.\nSo recall we are working with a model that looks like this: … And then this is my first equation, the observation equation and I have a system equation that looks like this.\nWe are going to assume that V_t and W_t are known for every t. And we also know what the F_t’s and the G_t’s are here. So the response is a uni-dimensional y_t and then I have, say, \\theta_t is a vector of a given dimension, depending on the structure of the model.\nWe are interested in performing Bayesian inference in this setting and we talked about different kinds of distributions\n\nOne is the filtering distribution that allows us to update the distribution of \\theta_t as we receive observations and information over time.\nThe other one is smoothing equations that allows us to just revisit the past once we have observed a chunk of data.\n\nSo I will be talking about those and also smoothing.\nIn a Bayesian setting, you have to set a prior distribution. We will work with the prior distribution that is conjugate.\nIn this case I have to begin with distribution at time zero. So before I know, I have seen any data at all, I have this prior distribution. D_0 stands for the information that I have before collecting any data. And we are going to assume, That this \\theta_0 follows a normal distribution with m_0 mean and variance covariance matrix C_0. So these are also specified when you’re working with this model.\nSo we assume that this m_0 and C_0 is known.\nOnce we have this setting using these equations, we can obtain the filtering equations.\nSo the first assumption is going to be that we have, a structure.\nSo for \\theta_{t -1} \\mid \\mathcal{D}_{t-1} is going to have this normal structure which is going to happen basically because we’re using this conjugate prior. And because we have normal structure in the model, is going to lead to the following distribution. So the first one is the prior at time t.\nSo if I want to think about why my distribution for the t is given the information I have up to t-1, I can look at the equations of the model and use this second equation. And by looking at this equation, if I condition on the information I have up to t-1, I can see that, say, \\theta_t is written as a linear function of, \\theta_{t -1} and I have the assumption of normality here.\nTherefore, say, \\theta_t going to follow a normal distribution with some mean and some variance. So now we’re going to compute this mean and this variance using this equation. So if you think about the expected value of \\theta_t, given D_{t -1}, that’s just going to be G_t is a constant here. So I have my G_t and then I have expected value of \\theta_{t -1} given G_{t -1} plus expect the value of this \\omega_t.\nBut \\omega_t is a zero mean, normally distributed quantity, so it’s just going to be zero. Using the assumption that I have this structure, then I have that the \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_{t -1}] = G_t \\times m_{t-1}. We’re going to call this quantity a_t, so we have here a_t. For the variance covariance matrix, then we just have to compute, do the same type of operation. And again, we can use this equation and see that we obtain this G_t variance of \\theta_{t-1} \\mid \\mathcal{D}_{t -1} G_t'. And then we have now the variance of the omega, the variance of the omega is just W_t. So we have G_t = C_{t -1} G_t' + W_t. So we can call this quantity R_t and just have the form of this prior distribution at time t.\nI can now think about another distribution which is the distribution of y_t \\mid \\mathcal{D}_{t-1}. So this is the so called one-step ahead, Forecast, And in the one-step ahead forecast again is a similar type of structure. So now we’re going to use the first equation rather than the second equation and we see that y_t is written in terms of a linear function of \\theta_t. And we have also the Gaussian in assumption here. So again the y_t is going to be normally distributed, And we just have to compute the mean and the variance for this y_t. So using the first equation, we have the expected value of y_t given D_{t -1} is just F_t' \\mathbb{E}[\\theta_t \\mid D_{t -1}]. And we computed this before, so this is, again, the expected value of \\theta_t given D_{t -1} is what we computed here. So this is to be F_t' a_t. And we are going to call this little f_t. Then, for the variance, Again, we use this equation, we have this component, so we are going to get F_t' R_t F_t + D_t. And I’m going to call this q_t. So my final distribution, the one-step ahead forecast distribution, tells me that this follows a normal f_t q_t. The next equations we are going to discuss are the equations that tell me about what is the distribution of \\theta_t once we incorporate the information provided by y_t. The next distribution is the posterior of \\theta_t given D_t. So that’s, \\theta_t given D_t. And we can write D_t as whatever information we have at time t- 1. And the new data point with this just y_t. So we just want to update the distribution of \\theta_t given that we have received this additional data point at time t. There are two ways of computing this distribution. One uses normal theory, the other one uses Bayes’ theorem. And you obtain that the distribution of \\theta_t given D_t is going to be a normal, with mean we call it m_t and variance C_t. We will see how to obtain this distribution or the moments of this distribution using normal theory.\n\n\nSo, again, we can write down, if we think about just combining the vector \\theta_t with the observation\n\n\nY_t given D_{t -1}, right? We have information about \\theta_t \\mid t-1. That’s the prior for \\theta_{ta,t}, based on the information at t -1. And then we also computed before the one step ahead forecast distribution for y_t| \\mathcal{D}_{t -1}. So we know that when we combine these two in a single vector, we’re going to have a multivariate normal distribution and the first component is going to be a_t. The second component is what we have called F_t, so that’s the mean. And then for the covariance matrix. We’re going to have now, what goes here is just the variance of \\theta_t given D_{t -1}, which we have called R_t. What goes here is the variance of y_t \\mid \\mathcal{D}_{t -1} and we have called this q_t. And now we have to compute the covariance between \\theta_t and y_t, and that goes here. And the covariance between y_t and \\theta_t, which is just the transpose of that, is going to go here. So if I think about computing the covariance of \\theta_t and y_t \\mid \\mathcal{D}_{t -1}, I can write y_t using the first equation here as a function of \\theta_t. That’s going to give us, F_t' \\theta_t + v_t given D_{t -1}. And in this one we can see that this is going to give us basically the variance of \\theta_t given D_{t -1} and then multiplied by F_t' F_t which gives me the F_t. So this is going to be variance of \\theta_t given D_{t -1} times F_t. And then there is a term that combines the \\theta_t with the noise but they are independent, so the covariance is going to be zero. So this one is simply going to be my R_t F_t, so this goes here, And what goes here is just the covariance of y_t with \\theta_t or the transpose of this. So this is going to give me F_t' R_t', but R_t is a covariance matrix, so R_t' = R_t. So now I have my full multivariate distribution and I can use properties of the multivariate distribution to compute the distribution of, \\theta_t, given y_t and D_{t -1}. So that’s going to be a conditional distribution, I’m going to condition on the y_t. And when I combine y_t and D_{t -1} that gives me just the information up to time t. So we are interested in just finding, say, \\theta_t given y_t and D_{t -1} which is the same as \\theta_t given D_t. We partition the normal distribution in this way, so I can just think about this is the first component and then I have these different pieces in my covariance matrix. And we know from normal theory that if we have a distribution, if we have a vector that is partitioned into vectors here where they are normally distributed. And I have my mean partition here and let’s say I have one component here, Then we know that if I wanted to compute the distribution of X_1 conditional on X_2, that’s going to give me normal, let’s say \\alpha^*. And let’s call this one the \\sigma^*, where \\alpha^* is going to be my \\alpha_1 + \\sigma_{12}^{-1}. And then I have _1 - \\alpha_2 and then I have my \\sigma^*. And this one gives me my \\sigma_{11} - \\sigma_{21}. So this is a result from normal theory. So if I want my conditional distribution of X_1 given X_2 I can apply these equations. So we notice we have the same type of structure here. If I partition my vector and in \\theta_t and y_t. And now I condition on, I take the distribution of \\theta_t conditioning on y_t. I’m going to have that same structure where this is normal, m_t C_t. And my m_t using normal theory, again, is going to be a_t + \\sigma_{22}^{-1}. And then I have y_t - f_t. So that’s my mean and my covariance matrix. It’s going to be R_t - q_t^{-1} and then I have this transpose again. So if we simplify things a bit here and we call e_t, it’s just the error that we make when we compare y_t, which is the observation with the prediction, right? And then I also use the notation I call a_t, let’s call here A_t R_t F_t q_t^{-1}. Then we can write this down, to mean, we can write as a_t + A_t. And the covariance matrix. We can write it as R_t, A_t q_t A_t'. So this gives me the posterior mean after receiving this y_t observation. And you can see that you can write down the posterior mean, has this usual form of the prior plus something that relates to the error that I make with the prediction.\nSo the y_t appears there and then is weighted by this quantity that we just call a_t.\nAnd for the covariance structure, we are also incorporating information about the prior and what the y_t observation provides. So this gives us our filtering equation for \\theta_t given D_t. And now we can apply all these equations as we receive observations from t = 1 all the way to T. If we happen to have T observations in the time series, we can do this filtering process and obtain these distributions as we receive information.",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#summary-of-filtering-distributions",
    "href": "C4-L07.html#summary-of-filtering-distributions",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.2 Summary of filtering distributions 📖",
    "text": "95.2 Summary of filtering distributions 📖\n\n95.2.1 Bayesian Inference in NDLM: Known Variances\nConsider an NDLM given by:\n\n\\begin{aligned}\ny_t &= F_t' \\theta_t + \\nu_t, \\quad \\nu_t \\sim  \\mathcal{N}(0, v_t), \\\\\n\\theta_t &= G_t \\theta_{t-1} + \\omega_t, \\quad \\omega_t \\sim  \\mathcal{N}(0, W_t),\n\\end{aligned}\n\\tag{95.9}\nwith F_t, G_t, v_t, and W_t known. We also assume a prior distribution of the form (\\theta_0 \\mid \\mathcal{D}_0) \\sim  \\mathcal{N}(m_0, C_0), with m_0, C_0 known.\n\n95.2.1.1 Filtering\nWe are interested in finding \\mathbb{P}r(\\theta_t \\mid \\mathcal{D}_t) for all t. Assume that the posterior at t-1 is such that:\n\n(\\theta_{t-1} \\mid \\mathcal{D}_{t-1}) \\sim  \\mathcal{N}(m_{t-1}, C_{t-1}).\n\\tag{95.10}\nThen, we can obtain the following:\n\nPrior at Time t\n\n\n(\\theta_t \\mid \\mathcal{D}_{t-1}) \\sim  \\mathcal{N}(a_t, R_t),\n\nwith\n\na_t = G_t m_{t-1} \\qquad R_t = G_t C_{t-1} G_t' + W_t.\n\n\nOne-Step Forecast\n\n\n(y_t \\mid D_{t-1}) \\sim  \\mathcal{N}(f_t, q_t),\n\nwith\n\nf_t = F_t' a_t, \\quad q_t = F_t' R_t F_t + v_t.\n\n\nPosterior at Time t: (\\theta_t \\mid \\mathcal{D}_t) \\sim  \\mathcal{N}(m_t, C_t) with\n\n\\begin{aligned}\nm_t &= a_t + R_t F_t q_t^{-1} (y_t - f_t), \\\\\nC_t &= R_t - R_t F_t q_t^{-1} F_t' R_t.\n\\end{aligned}\n\nNow, denoting e_t = (y_t - f_t) and A_t = R_t F_t q_t^{-1}, we can rewrite the equations above as:\n\\begin{aligned}\nm_t &= a_t + A_t e_t, \\\\\nC_t &= R_t - A_t q_t A_t'\n\\end{aligned}",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#rcode-filtering-in-the-ndlm-example",
    "href": "C4-L07.html#rcode-filtering-in-the-ndlm-example",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.3 Rcode Filtering in the NDLM: Example 📖",
    "text": "95.3 Rcode Filtering in the NDLM: Example 📖\n\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices &lt;- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states &lt;- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter &lt;- function(data, matrices, initial_states){\n  ## retrieve dataset\n  y_t &lt;- data$y_t\n  T &lt;- length(y_t)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF &lt;- matrices$FF  \n  GG &lt;- matrices$GG\n  VV &lt;- matrices$VV\n  WW &lt;- matrices$WW\n  \n  ## retrieve initial states\n  m0 &lt;- initial_states$m0\n  C0 &lt;- initial_states$C0\n  \n  ## create placeholder for results\n  d &lt;- dim(GG)[1]\n  at &lt;- matrix(NA, nrow=T, ncol=d)\n  Rt &lt;- array(NA, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(NA, nrow=T, ncol=d)\n  Ct &lt;- array(NA, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(m0)\n      Rt[, , i] &lt;- GG %*% C0 %*% t(GG) + WW\n      Rt[, , i] &lt;- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }else{\n      at[i, ] &lt;- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[, , i] &lt;- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(FF) %*% (at[i, ]) \n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% FF / Qt[i]\n    et[i] &lt;- y_t[i] - ft[i]\n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[, , i] &lt;- 0.5*Ct[, , i]+ 0.5*t(Ct[, , i])\n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = \n                Rt, ft = ft, Qt = Qt))\n}\n\nforecast_function &lt;- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  WW &lt;- matrices$WW\n  VV &lt;- matrices$VV\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state-space parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[, , i] &lt;- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }else{\n      at[i, ] &lt;- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[, , i] &lt;- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }\n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval &lt;- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile &lt;- qnorm(quantile)\n  bound &lt;- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] &lt;- mu + \n    z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] &lt;- mu + \n    z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron,main=\"Lake Huron Data\",\n     ylab=\"level in feet\") # Total of 98 observations \n\n\n\n\n\n\n\nk=4\nT=length(LakeHuron)-k # We take the first 94 observations \n                      # only as our data\nts_data=LakeHuron[1:T]\nts_validation_data &lt;- LakeHuron[(T+1):98]\n\ndata &lt;- list(y_t = ts_data)\n\n# First order polynomial model \n\n## set up the DLM matrices \nFF &lt;- as.matrix(1)\nGG &lt;- as.matrix(1)\nVV &lt;- as.matrix(1)\nWW &lt;- as.matrix(1)\nm0 &lt;- as.matrix(570)\nC0 &lt;- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices &lt;- set_up_dlm_matrices(FF, GG, VV, WW)\ninitial_states &lt;- set_up_initial_states(m0, C0)\n\n## filtering\nresults_filtered &lt;- forward_filter(data, matrices, \n                                   initial_states)\n\nForward filtering is completed!\n\nnames(results_filtered)\n\n[1] \"mt\" \"Ct\" \"at\" \"Rt\" \"ft\" \"Qt\"\n\nci_filtered &lt;- get_credible_interval(results_filtered$mt, \n                                     results_filtered$Ct)\n\n## forecasting \nresults_forecast &lt;- forecast_function(results_filtered,k, \n                                      matrices)\n\nForecasting is completed!\n\nci_forecast &lt;- get_credible_interval(results_forecast$ft, \n                                     results_forecast$Qt)\n\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\nindex_forecast=index[(T+1):98]\n\nplot(index, LakeHuron, ylab = \"level\", \n     main = \"Lake Huron Level\",type='l',\n     xlab=\"time\",lty=3,ylim=c(574,584))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered$mt, type='l',\n      col='red',lwd=2)\nlines(index_filt, ci_filtered[, 1], type='l', \n      col='red', lty=2)\nlines(index_filt, ci_filtered[, 2], type='l', col='red', lty=2)\n\n\nlines(index_forecast, results_forecast$ft, type='l',\n      col='green',lwd=2)\nlines(index_forecast, ci_forecast[, 1], type='l',\n      col='green', lty=2)\nlines(index_forecast, ci_forecast[, 2], type='l',\n      col='green', lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"forecast\"),\n       col = c(\"red\", \"green\"), lty=c(1, 1))\n\n\n\n\n\n\n\n#Now consider a 100 times smaller signal to noise ratio \nVV &lt;- as.matrix(1)\nWW &lt;- as.matrix(0.01)\nmatrices_2 &lt;- set_up_dlm_matrices(FF,GG, VV, WW)\n\n## filtering\nresults_filtered_2 &lt;- forward_filter(data, matrices_2, \n                                     initial_states)\n\nForward filtering is completed!\n\nci_filtered_2 &lt;- get_credible_interval(results_filtered_2$mt, \n                                       results_filtered_2$Ct)\n\nresults_forecast_2 &lt;- forecast_function(results_filtered_2, \n                             length(ts_validation_data), \n                             matrices_2)\n\nForecasting is completed!\n\nci_forecast_2 &lt;- get_credible_interval(results_forecast_2$ft, \n                                       results_forecast_2$Qt)\n\n\nplot(index, LakeHuron, ylab = \"level\", \n     main = \"Lake Huron Level\",type='l',\n     xlab=\"time\",lty=3,ylim=c(574,584))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered_2$mt, type='l', \n      col='magenta',lwd=2)\nlines(index_filt, ci_filtered_2[, 1], type='l', \n      col='magenta', lty=2)\nlines(index_filt, ci_filtered_2[, 2], type='l', \n      col='magenta', lty=2)\n\nlines(index_forecast, results_forecast_2$ft, type='l', \n      col='green',lwd=2)\nlines(index_forecast, ci_forecast_2[, 1], type='l', \n      col='green', lty=2)\nlines(index_forecast, ci_forecast_2[, 2], type='l', \n      col='green', lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"forecast\"),\n       col = c(\"magenta\", \"green\"), lty=c(1, 1))\n\n\n\n\n\n\n\nplot(index_filt,results_filtered$mt,type='l',col='red',lwd=2,\n     ylim=c(574,584),ylab=\"level\")\nlines(index_filt,results_filtered_2$mt,col='magenta',lwd=2)\npoints(index,LakeHuron,pch=20)\nlines(index,LakeHuron,lty=2)",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#smoothing-and-forecasting",
    "href": "C4-L07.html#smoothing-and-forecasting",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.4 Smoothing and forecasting 🎥",
    "text": "95.4 Smoothing and forecasting 🎥\n\n\n\n\nSmoothing\n\n\n\n\nForecasting\n\n\nWe now discuss the smoothing equations for the case of the NDLM, where we are assuming that the variance at the observation level \\nu_t and the covariance matrix at the system level \\mathbf{W}_t are both known.\n \\begin{aligned}\ny_t &= \\mathbf{F}_t' \\mathbf{\\theta}_t + \\nu_t, &\\nu_t &\\sim \\mathcal{N} (0, v_t), & \\text{(observation)} \\\\\n\\mathbf{\\theta}_t & = \\mathbf{G}_t \\mathbf{\\theta}_{t-1} + \\mathbf{\\omega}_t, &\\mathbf{\\omega}_t & \\sim \\mathcal{N} (0, \\mathbf{W}_t), & \\text{(evolution)} \\\\\n&\\{ \\mathbf{F}_t, \\mathbf{G}_t, v_t, \\mathbf{W}_t \\}  &(\\mathbf{\\omega}_0 \\mid \\mathcal{D}_0) & \\sim \\mathcal{N}(\\mathbf{m}_0, \\mathbf{C}_0) & \\text{(prior)}\n\\end{aligned}\n\\tag{95.11}\nwith F_t, G_t, v_t, W_t, m_0 and C_0 known.\nWe have discussed the filtering equations, i.e. the process for obtaining the distributions of \\theta_t \\mid \\mathcal{D}_t, as we collect observations over time, called filtering.\nWe do this by updating the distribution of \\theta_t given the data we have collected step by step, as we move forward in time - updating the from the prior distribution.\nNow we will discuss what happens when we do smoothing, meaning when we revisit the distributions of \\theta_t, given now that we have received a set of observations.\n\n95.4.0.1 Smoothing\nFor t &lt; T, we have that:\n\n(\\theta_t \\mid D_T) \\sim  \\mathcal{N}(a_T(t - T), R_T(t - T)),\n\nwhere\n\na_T(t - T) = m_t - B_t [a_{t+1} - a_T(t - T + 1)],\n\n\nR_T(t - T) = C_t - B_t [R_{t+1} - R_T(t - T + 1)] B_t',\n\nfor t = (T - 1), (T - 2), \\dots, 0, with B_t = C_t G_t' R_{t+1}^{-1}, and a_T(0) = m_T, R_T(0) = C_T. Here a_t, m_t, R_t, and C_t are obtained using the filtering equations as explained before.\n\n\n95.4.0.2 Forecasting\nFor h \\geq 0, it is possible to show that:\n\n(\\theta_{t+h} \\mid D_t) \\sim  \\mathcal{N}(a_t(h), R_t(h)),\n\n\n(y_{t+h} \\mid D_t) \\sim  \\mathcal{N}(f_t(h), q_t(h)),\n\nwith\n\na_t(h) = G_{t+h} a_t(h - 1),\n\n\nR_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},\n\n\nf_t(h) = F_{t+h}' a_t(h),\n\n\nq_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},\n\nand\n\na_t(0) = m_t, \\quad R_t(0) = C_t.\n\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\nSmoothing\n\nWe know, we now discuss the smoothing equations for the case of the normal dynamic linear model. When we are assuming that both the variance at the observation level is known and the covariance matrix at the system level is also known.\n\n\n\nthe NDLM we will be inferring\n\nRecall we have two equations here, we have the observation equation, where y_t is modeled as F_t'\\theta_t + \\text{noise} the noise is \\mathcal{N}(0,\\nu_t). And we’re assuming that the vt is given. We are also assuming that we know Ft for all t. And then in the evolution equation we have \\theta_t= G_t \\theta(t-1) + noise. And then again, the assumption for the w_t is here is that they are normally distributed with mean zero, and these variants co variance matrix, capital W_t. So we can summarize the model in terms of F_t, G_t, Vt and W_t, that are given for all t. We have discussed the filtering equations.\n\n\n\nRecall what is Filtering?\n\nSo the process for obtaining the distributions of \\theta_t \\mid \\mathcal{D}_t, as we collect observations over time is called filtering.\n\n\n\nRecall what is Smoothing?\n\nNow we will discuss what happens when we do smoothing, meaning when we revisit the distributions of \\theta_t, given now that we have received a set of observations.\n\n\n\nFiltering illustrated\n\nSo Just to illustrate the process, we have here, \\theta_0,\\theta_1 all way up to \\theta_4. And we can assume just for the sake of the example, that we are going to receive three observations. So we are going to proceed with the filtering, and then once we receive the last observation at time three, we’re going to go backwards and we’re going to revisit the distributions for the state parameters.\nSo just to remind you how the filtering works, we move forward, before we receive any observations. In the NDLM, when we have all the variances known. The conjugate prior distribution is a \\mathcal{N}(m_0,C_0), and this is specified by the user, before collecting any observations.\nWe can then use the structure of the model, meaning the system equation and the observation equation to obtain the distribution of \\theta_t \\mid \\mathcal{D}_0. Before observing the first y. This gives us first the distribution of \\theta_t, \\theta_1 \\mid \\mathcal{D}_0, which is \\mathcal{N}(a_1, R_1). And then we can also get the one step ahead forecast distribution for y_1 \\mid  \\mathcal{D}_0, which is a \\mathcal{N}(f_1, q_1). And we have discussed how to obtain these moments using the filtering equations.\nThen we received the first observation, and the first observation can allows us to update the distribution of . So we obtain now the distribution of \\theta1 \\mid y_1, and whatever information we have at \\mathcal{D}_0. So this gives us \\mathcal{N}(m_1, C_1). And using again the structure of the model, we can get the prior distribution for \\theta_2 given the one and that’s a \\mathcal{N}(a_2, R_2). And then the one step ahead forecast distribution now for y_2 \\mid \\mathcal{D}_1 and that’s a \\mathcal{N}(f_2, q_2). So we can receive y_2 update the distribution of and we can continue this process, now get the priors at T=3. And then once we get the observation at T=3, we update the distribution. And we can continue like this with the prior for \\theta_4 and so on. Let’s say that we stop here, at T=3.\n\n\nAnd now we are interested in answering the question. Well, what is the distribution for example of \\theta_2 given that, now, I obtain not only y_1 and y_2, but also y_3. I want to revisit that distribution using all that information. Same thing for say, the distribution of \\theta_0 \\mid D_0, y_1, y_2, y_3. So that’s what it’s called smoothing.\nSo the smoothing equations, allow us to obtain those distributions. So just to talk a little bit about the notation again, in the normal dynamic linear model where v_t and w_t are known for all t’s. We have that this is a normal, so the notation here, the T &gt;t, here. So we’re looking at the distribution of \\theta_t, now in the past and that one follows a normal distribution with mean aT(t-T). So the notation here for the subscript T means that I’m conditioning on all the information I have to T. And then the variance covariance matrix is given by this, RT(t-T). So this is just going to indicate how many steps I’m going to go backwards as you will see in the example.\n\n\nSo we have some recursions in the same way that we have the filtering equations. Now we have the smoothing equations. And for these smoothing equations we have that the mean. You can see here, that whenever you’re computing a particular step t- T, you’re going to need a quantity that you computed in the previous step, t-T+1. So you’re going to need that, is a recursion, but you’re also going to need mt and and at+1. So those are quantities that you computed using the filtering equations. So in order to get the smoothing equations, you first have to proceed with the filtering. Similarly for RT(t-T), you have also that depends on something you previously obtained. And then you also have the Ct, the Rt+1 and so on. So those quantities you computed when you were updating the filtering equations. The recursion begins with aT(0) meaning that you are not going to go backwards any points in time. So that is precisely the mean is going to be whatever you computed with the filtering equations of up to T, that’s mT. And then RT(0) is going to be CT. So just to again illustrate how this would work in the example, if we start here right? If we condition, so the first step would be to compute again to initialize using the distribution of given D3. And that is a normal with mean a3(0) and variance covariance matrix R3(0), But those are precisely m3 and C3 respectively. Then we go backwards one step. And if we want to look at what is the distribution of \\theta^2, now conditional on D3. That’s a normal with mean a3(-1) and variance covariance matrix R3(-1). So if you look at the equations down here, you will see that, in order to compute a3 (-1), and R3(-1). You’re going to need m2,C2, a3,R3 and then what you computed here these moments in the previous step, a3(0) and R3(0). Then you obtain that distribution and you can now look at the distribution of given D3, that’s the normal a3(-2), R3(-2). And once again, to compute these moments, you’re going to need m1,C1,a2,R2 and then you’re going to need a3(-1),R3(-1). And you can continue all the way down to given D3 using these recursions. So the smoothing equations allow us to, just compute all these distributions. And the important equations work basically because of the linear and Gaussian structure in the normal dynamic linear model.\n\n\n\n95.4.1 Forecasting\n\nIn a similar way, we can compute the forecasting distributions. Now we are going to be looking forward, and in the case of forecasting, we are interested in the distribution of \\theta(t+h) given D_t. And now h is a positive lag. So here we assume that is h≥0. So we are going to have the recursion is a N(a_t(h), R_t(h)). The mean is a_t(h) and we are going to use the structure of the model to obtain these recursions, again. So here we are using the system equation, and the moment at(h) depends on what you computed at a_t(h-1) the previous lag, times G_{t+h}. And then, would you initialize the recursion with a_t(0)=m_t.\n\n\nSimilarly, for the covariance matrix h steps ahead, you’re going to have a recursion that depends on Rt(h-1). And then you’re going to need to input also G_{t+h} and W_{t+h}. To initialize, the recursion with Rt(0)= Ct. So you can see that in order to compute these moments, you’re going to need mt and Ct to start with. And then you’re also going to have to input all the G’s and the W’s for the number of steps ahead that you require.\n\n\nSimilarly, you can compute the distribution, the h steps ahead distribution of y_t+h given Dt. And that one also follows a normal, with mean f_t(h), q_t(h). And now we also have a recursion here, ft(h) depends on at(h) and as we said, a_t(h) depends on a_t(h-1) and so on. And q_t(h) is just given by these equations. So once again, you have to have access to F_{t+h} for all the h, a number of steps ahead that you are trying to compute this distribution. And then you also have to provide the observational variance for every h value. So that you get v_{t+h}. So this is specified in the modeling framework as well. If you want proceed with the forecasting distributions.",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#summary-of-the-smoothing-and-forecasting-distributions",
    "href": "C4-L07.html#summary-of-the-smoothing-and-forecasting-distributions",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.5 Summary of the smoothing and forecasting distributions 📖",
    "text": "95.5 Summary of the smoothing and forecasting distributions 📖\n\n\n95.5.1 Bayesian Inference in NDLM: Known Variances\nConsider the NDLM given by:\n \\begin{aligned}\ny_t &= \\mathbf{F}_t' \\mathbf{\\theta}_t + \\nu_t, &\\nu_t &\\sim \\mathcal{N} (0, v_t), \\\\\n\\mathbf{\\theta}_t &= \\mathbf{G}_t \\mathbf{\\theta}_{t-1} + \\mathbf{\\omega}_t, &\\mathbf{\\omega}_t &\\sim \\mathcal{N} (0, \\mathbf{W}_t), \\\\\n&\\{ \\mathbf{F}_t, \\mathbf{G}_t, v_t, \\mathbf{W}_t \\}  &(\\mathbf{\\omega}_0 \\mid \\mathcal{D}_0) &\\sim  \\mathcal{N}(\\mathbf{m}_0, \\mathbf{C}_0)\n\\end{aligned}\n\\tag{95.12}\nwith F_t, G_t, v_t, and W_t known.\nWe also assume a prior distribution of the form (\\theta_0 \\mid D_0) \\sim  \\mathcal{N}(m_0, C_0), with m_0 and C_0 known.\n\n95.5.1.1 Smoothing\nFor t &lt; T, we have that:\n\n(\\theta_t \\mid D_T) \\sim  \\mathcal{N}(a_T(t - T), R_T(t - T)),\n\nwhere\n\na_T(t - T) = m_t - B_t [a_{t+1} - a_T(t - T + 1)],\n\n\nR_T(t - T) = C_t - B_t [R_{t+1} - R_T(t - T + 1)] B_t',\n\nfor t = (T - 1), (T - 2), \\dots, 0, with B_t = C_t G_t' R_{t+1}^{-1}, and a_T(0) = m_T, R_T(0) = C_T. Here a_t, m_t, R_t, and C_t are obtained using the filtering equations as explained before.\n\n\n95.5.1.2 Forecasting\nFor h \\geq 0, it is possible to show that:\n\n(\\theta_{t+h} \\mid D_t) \\sim  \\mathcal{N}(a_t(h), R_t(h)),\n\n\n(y_{t+h} \\mid D_t) \\sim  \\mathcal{N}(f_t(h), q_t(h)),\n\nwith\n\na_t(h) = G_{t+h} a_t(h - 1),\n\n\nR_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},\n\n\nf_t(h) = F_{t+h}' a_t(h),\n\n\nq_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},\n\nand\n\na_t(0) = m_t, \\quad R_t(0) = C_t.",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#smoothing-in-the-ndlm-example",
    "href": "C4-L07.html#smoothing-in-the-ndlm-example",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.6 Smoothing in the NDLM, Example 🎥",
    "text": "95.6 Smoothing in the NDLM, Example 🎥",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#sec-smoothing-in-the-NDLM",
    "href": "C4-L07.html#sec-smoothing-in-the-NDLM",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.7 R code: Smoothing in the NDLM, Example 📖",
    "text": "95.7 R code: Smoothing in the NDLM, Example 📖\n\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices &lt;- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states &lt;- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter &lt;- function(data, matrices, initial_states){\n  ## retrieve dataset\n  y_t &lt;- data$y_t\n  T &lt;- length(y_t)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF &lt;- matrices$FF  \n  GG &lt;- matrices$GG\n  VV &lt;- matrices$VV\n  WW &lt;- matrices$WW\n  \n  ## retrieve initial states\n  m0 &lt;- initial_states$m0\n  C0 &lt;- initial_states$C0\n  \n  ## create placeholder for results\n  d &lt;- dim(GG)[1]\n  at &lt;- matrix(NA, nrow=T, ncol=d)\n  Rt &lt;- array(NA, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(NA, nrow=T, ncol=d)\n  Ct &lt;- array(NA, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  \n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(m0)\n      Rt[, , i] &lt;- GG %*% C0 %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] &lt;- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(FF) %*% (at[i, ]) \n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% FF / Qt[i]\n    et[i] &lt;- y_t[i] - ft[i]\n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[,,i] &lt;- 0.5*Ct[,,i] + 0.5*t(Ct[,,i]) \n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = Rt, \n              ft = ft, Qt = Qt))\n}\n\n\nforecast_function &lt;- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  WW &lt;- matrices$WW\n  VV &lt;- matrices$VV\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] &lt;- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval &lt;- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile &lt;- qnorm(quantile)\n  bound &lt;- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] &lt;- mu + z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] &lt;- mu + z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n### smoothing equations ###\nbackward_smoothing &lt;- function(data, matrices, \n                               posterior_states){\n  ## retrieve data \n  y_t &lt;- data$y_t\n  T &lt;- length(y_t) \n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  \n  ## retrieve matrices\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  at &lt;- posterior_states$at\n  Rt &lt;- posterior_states$Rt\n  \n  ## create placeholder for posterior moments \n  mnt &lt;- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt &lt;- array(NA, dim = dim(Ct))\n  fnt &lt;- numeric(T)\n  Qnt &lt;- numeric(T)\n  for(i in T:1){\n    # moments for the distributions of the state vector given D_T\n    if(i == T){\n      mnt[i, ] &lt;- mt[i, ]\n      Cnt[, , i] &lt;- Ct[, , i]\n      Cnt[, , i] &lt;- 0.5*Cnt[, , i] + 0.5*t(Cnt[, , i]) \n    }else{\n      inv_Rtp1&lt;-solve(Rt[,,i+1])\n      Bt &lt;- Ct[, , i] %*% t(GG) %*% inv_Rtp1\n      mnt[i, ] &lt;- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n      Cnt[, , i] &lt;- Ct[, , i] + Bt %*% (Cnt[, , i + 1] - Rt[, , i+1]) %*% t(Bt)\n      Cnt[,,i] &lt;- 0.5*Cnt[,,i] + 0.5*t(Cnt[,,i]) \n    }\n    # moments for the smoothed distribution of the mean response of the series\n    fnt[i] &lt;- t(FF) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] &lt;- t(FF) %*% t(Cnt[, , i]) %*% FF\n  }\n  cat(\"Backward smoothing is completed!\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron,main=\"Lake Huron Data\",ylab=\"level in feet\") \n\n\n\n\n\n\n\n# 98 observations total \nk=4\nT=length(LakeHuron)-k # We take the first 94 observations \n                     #  as our data\nts_data=LakeHuron[1:T]\nts_validation_data &lt;- LakeHuron[(T+1):98]\n\ndata &lt;- list(y_t = ts_data)\n\n## set up matrices\nFF &lt;- as.matrix(1)\nGG &lt;- as.matrix(1)\nVV &lt;- as.matrix(1)\nWW &lt;- as.matrix(1)\nm0 &lt;- as.matrix(570)\nC0 &lt;- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices &lt;- set_up_dlm_matrices(FF,GG,VV,WW)\ninitial_states &lt;- set_up_initial_states(m0, C0)\n\n## filtering\nresults_filtered &lt;- forward_filter(data, matrices, \n                                   initial_states)\n\nForward filtering is completed!\n\nci_filtered&lt;-get_credible_interval(results_filtered$mt,\n                                   results_filtered$Ct)\n## smoothing\nresults_smoothed &lt;- backward_smoothing(data, matrices, \n                                       results_filtered)\n\nBackward smoothing is completed!\n\nci_smoothed &lt;- get_credible_interval(results_smoothed$mnt, \n                                     results_smoothed$Cnt)\n\n\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\n\nplot(index, LakeHuron, main = \"Lake Huron Level \",type='l',\n     xlab=\"time\",ylab=\"level in feet\",lty=3,ylim=c(575,583))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered$mt, type='l', \n      col='red',lwd=2)\nlines(index_filt, ci_filtered[,1], type='l', col='red',lty=2)\nlines(index_filt, ci_filtered[,2], type='l', col='red',lty=2)\n\nlines(index_filt, results_smoothed$mnt, type='l', \n      col='blue',lwd=2)\nlines(index_filt, ci_smoothed[,1], type='l', col='blue',lty=2)\nlines(index_filt, ci_smoothed[,2], type='l', col='blue',lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"smoothed\"),\n       col = c(\"red\", \"blue\"), lty=c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#sec-second-order-polynomial",
    "href": "C4-L07.html#sec-second-order-polynomial",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.8 Second order polynomial: Filtering and smoothing example 🎥",
    "text": "95.8 Second order polynomial: Filtering and smoothing example 🎥\nIn this video walk through the code provided in the section below the comment\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\nWe now consider another example where instead of fitting a first order polynomial we’re fitting a second order polynomial DLM. So I just want to show you how to set up the structure of the model in a case in which you have a state parameter vector. That is of dimension larger than one in this particular case we have a bivariate state parameter vector. So once again we are going to source this file that has all the DLM functions for the case in which the F, G, V and W are known. So we’re just assuming that this is the case and then we’re assuming that F, G, V and W are constant over time in these examples. So we just I’m going to use a new data set which is also data set available in R this data set corresponds to the atmospheric CO2 concentrations in parts per million in the location of Mauna Loa. And this is monthly data so I’m just plotting the data here. If you look at the data you can see that it has two important features. One of them is an increasing trend as the time increases the concentration increases. And then the other very specific feature that you can see in this data set is this seasonal behavior. So right now what I’m going to do with this example is we are going to ignore the seasonal behavior, and we are going to try to fit the model that captures the linear increasing trend using a second order polynomial model.\nSo I’m going to just specify everything here. We are going to use the entire data set here. We’re going to analyze the entire data. We are going to read in this into a list and then we’re going to set up the DLM in matrices. So here because the model it’s a second order polynomial we are going to have a state vector. That is of dimension two the F matrix is going to be, so it’s a vector that has 1 in the first entry and 0 in the second one. And then G is this upper triangular matrix that has 1s in the diagonal and 1 above the diagonal as well. So the two parameters that we’re fitting here one of them you can view the two components in the state of theta_t parameter vector. The first component corresponds to the baseline of the level and then the second component corresponds to the rate of growth in that level that we are fitting. So just defining the F and G like that. And then V the observational variance I’m just going to set it at 10. You can play with different numbers here, and the W is a diagonal matrix with .0001 in each of the elements in the diagonal. So these models are not as flexible as the ones that we are going to consider later. So in particular we are using an assumption that the two components in the state sector are independent over time which is usually not very realistic. And we can consider more flexible models later but just to show you here how to fit these models, for the prior distribution I have again two components. So I’m going to say that a priori my baseline is 315 parts per million. And then for the second, the rate of growth is going to be 0 a priori. And then I have C0 which is this 10 times the diagonal of dimension 2 so this is an identity matrix. So is we have a diagonal with the elements in the diagonal equal to 10. So we wrap up all the DLM matrices with the functions that we defined before. And then we proceed with the filtering equations just using the forward filter function. We can obtain credible intervals for the expected value of y_t via this filtering equations.\nSo the reason why I’m calling it the expected value of y_t via filtering it’s just the first component of the say that theta_t vectors. So that corresponds to the level of the series, the expected value of that y_t. And then, I can compute the smoothing equations using the backward smoothing. And again I have to pass the data, the structure of the model in terms of the matrices and the results that I obtained via the filtering equations. And I can compute credible intervals for this expected value via smoothing and as we mentioned before, it has the same structure the smoothing and the filtering is just that, we call the mean and the variance mt and Ct. In the case of the filtering equations for the smoothing equations we just call them mnt and Cnt. So now we can plot all the results here. I’m just going to plot the results that correspond to the smoothing distributions just for you to see. And we can see here that is this trend that is estimated here is capturing the structure of this linear increasing trend. And you can play with different values of the signal to noise ratio. So different values of the V and the W. And if you change the values so that there is more or less signal to noise ratio, you will see that you will capture more of the seasonal structure and less of this linear trend structure. If you were to change those values. So if I go back a little bit here you can see that I have a very low signal to noise ratio and I picked this on purpose, because I didn’t want to capture any of the seasonal behavior that I observe in the series through these parameters. So I’m assuming that a lot of the variation that I see now I’m just keeping it in the noise. Just because I want to just get a very smooth estimate for this linear trend through a second order polynomial model. In practice what we’re going to do later is we really want to construct a model in which we have a component for the linear trend using the second order polynomial model. And then we add another component that will allow us to capture also the seasonal behavior that we observe in this series using a Fourier component model. So we will illustrate that later, in a separate example here is just again to show you how to use the code for specifying a second order polynomial.",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#using-the-dlm-package-in-r",
    "href": "C4-L07.html#using-the-dlm-package-in-r",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.9 Using the dlm package in R 🎥",
    "text": "95.9 Using the dlm package in R 🎥\nThe dlm package in R is a powerful tool for working with dynamic linear models. The package provides a wide range of functions for filtering, smoothing, forecasting, and parameter estimation in DLMs. In this video, we walk through the code provided in Listing 95.7.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\nSo here I’m going to show you how to use the dlm package to fit these dynamic linear models as well. So the dlm is package that is available from Cran. And it allows you to compute the filtering smoothing and forecasting equations for dynamic linear models. So I’m just going to show you how to do the same thing we’ve been doing with the code that I provided just using the dlm package. So I’m going to just run here the first examples that we ran. And I’m going to show you how to do the same again. So here, I’m just going through the Lake Huron data. So just setting up every_thing as we did before. And then going through the filtering and smoothing equations. And so we can now plot the results and just want to have all the results here. So we have the red line corresponds to the posterior mean for the distribution of \\theta_t given the Dt using a first order polynomial model to fit the data. And the blue line corresponds to the smoothing mean. So the mean of the posterior distribution of the smoothing equations here. So now we can look at how to fit this with the dlm package. So you have to call, install the package if you don’t have it installed. And then just call that library once you have installed the package. And the dlm package has a different set of functions to construct the model first.\nSo I’m going to use the function that is called the dlmModPoly, which allows you to fit polynomial models. So it constructs the polynomial models. The default function as you can see here is a function in that assumes that the polynomial model is of order 2. So here I want to polynomial model of all the 1. And then I’m going to specify the variance at the observational level, which is called dV in that package. dW is the variance at the evolution level. And then I have my prior mean for theta and the prior variance. I’m just using exactly the same prior distribution. And the package provides two functions of the dlm filter function allows you to providing the data. And the model that you just define computes the filtering recursions here. And then there is another function that is called the dlmSmooth that you essentially pass the results of the filtering equations. And then you obtain the smoothing distributions. So we’re just going to do that. And now I’m going to plot the results that I obtained from those filtering equations. One thing that you can see here, if I do names of, let’s say results_filter_dlm. You can see that the way in which the dlm functions from the dlm package keep the results. It has a particular format. So in the case of the dlm package, you’re going to have the information about what model you fitted. Then you have the mean of theta_t given Dt is kept in this m object. And then you have a is the prior mean of theta_t, given the t -1. And then f is the mean of the one step ahead forecast distribution. And then you have these U.C, D.C, U.R, D.R, those are just decompositions of the C variance matrix. So each of the Cs at time t. And then if you have also the composition of the R matrices. So the model, the way in which the functions are implemented in this dlm package. Assume used an SVD decomposition of all the matrices. So you have to keep in mind if you’re going to recover the structure here for the different components in the model. You have to keep this in mind. So for the filtering results, this is the structure. If you do names of the results, smooth, with the dlm package. You’re going to have again, here is the mean here that is called S and then you have the decomposition of the matrix as well. So, I’m just going to plot now for the filtering results. I’m just going to plot the mean here. And then for the smoothing distribution, I’m also going to plot that means. In this case, we’re working with the first order polynomial. So the dimension of the state vector is 1. So you can see that we obtain exactly the same results. And you can compare them numerically. The upper plot corresponds to the results we get with the code that we’ve been using. And the second block corresponds to just using the code from the dlm package. We can also run the example with the second order polynomial. So again, if I use the specification of the model that we use before with the functions that we described. I can keep my results there. And if I use the dlm package, I can use again, this is a second order polynomial model. I say that the order of the polynomial is 2, I use this dlmModPoly function. I specify the observational variance, the system variance m0 and C0. So I’m using exactly the same priors in this case. And then I use the dlm filter function and the dlm smooth just to compute the moments of the filtering and smoothing distributions. And then I can plot every_thing here. We are plotting just the first component here. The posterior distribution for the first component of the theta vector. Which also corresponds to the expected value of the y_t. And then if I do the same with the dlm package, you can see that you obtain the same results. So again, the upper plot corresponds to the results that we get from the code that we’ve been using. And then the bottom plot corresponds to the results that we get from the dlm package. So I just wanted to illustrate this. You’re welcome to always use the dlm package. Just keep in mind the structure in which the matrices are kept is a little bit different than what we have been discussing. Because the dlm package uses and SVD decomposition of the covariance matrices and keeps every_thing like that. So there are some differences. But you can also use this package to obtain inference in the case of dynamic linear models.",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#sec-dlm-package",
    "href": "C4-L07.html#sec-dlm-package",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.10 R code: Using the dlm package in R 📖",
    "text": "95.10 R code: Using the dlm package in R 📖\n\n\n\n\nListing 95.1: Using the dlm package for dynamic linear models\n\n\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices &lt;- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states &lt;- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter &lt;- function(data, matrices, initial_states){\n  ## retrieve dataset\n  y_t &lt;- data$y_t\n  T &lt;- length(y_t)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF &lt;- matrices$FF  \n  GG &lt;- matrices$GG\n  VV &lt;- matrices$VV\n  WW &lt;- matrices$WW\n  \n  ## retrieve initial states\n  m0 &lt;- initial_states$m0\n  C0 &lt;- initial_states$C0\n  \n  ## create placeholder for results\n  d &lt;- dim(GG)[1]\n  at &lt;- matrix(NA, nrow=T, ncol=d)\n  Rt &lt;- array(NA, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(NA, nrow=T, ncol=d)\n  Ct &lt;- array(NA, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  \n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(m0)\n      Rt[, , i] &lt;- GG %*% C0 %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] &lt;- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(FF) %*% (at[i, ]) \n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% FF / Qt[i]\n    et[i] &lt;- y_t[i] - ft[i]\n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[,,i] &lt;- 0.5*Ct[,,i] + 0.5*t(Ct[,,i]) \n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = Rt, \n              ft = ft, Qt = Qt))\n}\n\nforecast_function &lt;- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  WW &lt;- matrices$WW\n  VV &lt;- matrices$VV\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] &lt;- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval &lt;- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile &lt;- qnorm(quantile)\n  bound &lt;- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] &lt;- mu + z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] &lt;- mu + z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n### smoothing equations ###\nbackward_smoothing &lt;- function(data, matrices, \n                               posterior_states){\n  ## retrieve data \n  y_t &lt;- data$y_t\n  T &lt;- length(y_t) \n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  \n  ## retrieve matrices\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  at &lt;- posterior_states$at\n  Rt &lt;- posterior_states$Rt\n  \n  ## create placeholder for posterior moments \n  mnt &lt;- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt &lt;- array(NA, dim = dim(Ct))\n  fnt &lt;- numeric(T)\n  Qnt &lt;- numeric(T)\n  for(i in T:1){\n    # moments for the distributions of the state vector given D_T\n    if(i == T){\n      mnt[i, ] &lt;- mt[i, ]\n      Cnt[, , i] &lt;- Ct[, , i]\n      Cnt[, , i] &lt;- 0.5*Cnt[, , i] + 0.5*t(Cnt[, , i]) \n    }else{\n      inv_Rtp1&lt;-solve(Rt[,,i+1])\n      Bt &lt;- Ct[, , i] %*% t(GG) %*% inv_Rtp1\n      mnt[i, ] &lt;- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n      Cnt[, , i] &lt;- Ct[, , i] + Bt %*% (Cnt[, , i + 1] - Rt[, , i+1]) %*% t(Bt)\n      Cnt[,,i] &lt;- 0.5*Cnt[,,i] + 0.5*t(Cnt[,,i]) \n    }\n    # moments for the smoothed distribution of the mean response of the series\n    fnt[i] &lt;- t(FF) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] &lt;- t(FF) %*% t(Cnt[, , i]) %*% FF\n  }\n  cat(\"Backward smoothing is completed!\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n\n\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron) # 98 observations total \n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 95.2: Using the dlm package for dynamic linear models\n\n\nk=4\nT=length(LakeHuron)-k # We take the first \n                      # 94 observations only as our data\nts_data=LakeHuron[1:T]\nts_validation_data &lt;- LakeHuron[(T+1):98]\n\ndata &lt;- list(y_t = ts_data)\n\n## set up dlm matrices\nGG &lt;- as.matrix(1)\nFF &lt;- as.matrix(1)\nVV &lt;- as.matrix(1)\nWW &lt;- as.matrix(1)\nm0 &lt;- as.matrix(570)\nC0 &lt;- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices &lt;- set_up_dlm_matrices(FF, GG, VV, WW)\ninitial_states &lt;- set_up_initial_states(m0, C0)\n\n## filtering and smoothing \nresults_filtered &lt;- forward_filter(data, matrices, \n                                   initial_states)\n\n\n\n\nForward filtering is completed!\n\n\n\n\nListing 95.3: Using the dlm package for dynamic linear models\n\n\nresults_smoothed &lt;- backward_smoothing(data, matrices, \n                                       results_filtered)\n\n\n\n\nBackward smoothing is completed!\n\n\n\n\nListing 95.4: Using the dlm package for dynamic linear models\n\n\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\n\n\npar(mfrow=c(2,1), mar = c(3, 4, 2, 1))\nplot(index, LakeHuron, main = \"Lake Huron Level \",type='l',\n     xlab=\"time\",ylab=\"feet\",lty=3,ylim=c(575,583))\npoints(index,LakeHuron,pch=20)\nlines(index_filt, results_filtered$mt, type='l', \n      col='red',lwd=2)\nlines(index_filt, results_smoothed$mnt, type='l', \n      col='blue',lwd=2)\n\n\n# Now let's look at the DLM package \nlibrary(dlm)\nmodel=dlmModPoly(order=1,dV=1,dW=1,m0=570,C0=1e4)\nresults_filtered_dlm=dlmFilter(LakeHuron[1:T],model)\nresults_smoothed_dlm=dlmSmooth(results_filtered_dlm)\n\nplot(index_filt, LakeHuron[1:T], ylab = \"level\", \n     main = \"Lake Huron Level\",\n     type='l', xlab=\"time\",lty=3,ylim=c(575,583))\npoints(index_filt,LakeHuron[1:T],pch=20)\nlines(index_filt,results_filtered_dlm$m[-1],col='red',lwd=2)\nlines(index_filt,results_smoothed_dlm$s[-1],col='blue',lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 95.5: Using the dlm package for dynamic linear models\n\n\n# Similarly, for the second order polynomial and the co2 data:\nT=length(co2)\ndata=list(y_t = co2)\n\nFF &lt;- (as.matrix(c(1,0)))\nGG &lt;- matrix(c(1,1,0,1),ncol=2,byrow=T)\nVV &lt;- as.matrix(200)\nWW &lt;- 0.01*diag(2)\nm0 &lt;- t(as.matrix(c(320,0)))\nC0 &lt;- 10*diag(2)\n\n## wrap up all matrices and initial values\nmatrices &lt;- set_up_dlm_matrices(FF,GG, VV, WW)\ninitial_states &lt;- set_up_initial_states(m0, C0)\n\n## filtering and smoothing \nresults_filtered &lt;- forward_filter(data, matrices, \n                                   initial_states)\n\n\n\n\nForward filtering is completed!\n\n\n\n\nListing 95.6: Using the dlm package for dynamic linear models\n\n\nresults_smoothed &lt;- backward_smoothing(data, matrices, \n                                       results_filtered)\n\n\n\n\nBackward smoothing is completed!\n\n\n\n\nListing 95.7: Using the dlm package for dynamic linear models\n\n\n#### Now, using the DLM package: \nmodel=dlmModPoly(order=2,dV=200,dW=0.01*rep(1,2),\n                 m0=c(320,0),C0=10*diag(2))\n# filtering and smoothing \nresults_filtered_dlm=dlmFilter(data$y_t,model)\nresults_smoothed_dlm=dlmSmooth(results_filtered_dlm)\n\npar(mfrow=c(2,1), mar = c(3, 4, 2, 1))\nplot(as.vector(time(co2)),co2,type='l',xlab=\"time\",\n     ylim=c(300,380))\nlines(as.vector(time(co2)),results_filtered$mt[,1],\n      col='red',lwd=2)\nlines(as.vector(time(co2)),results_smoothed$mnt[,1],\n      col='blue',lwd=2)\n\nplot(as.vector(time(co2)),co2,type='l',xlab=\"time\",\n     ylim=c(300,380))\nlines(as.vector(time(co2)),results_filtered_dlm$m[-1,1],\n      col='red',lwd=2)\nlines(as.vector(time(co2)),results_smoothed_dlm$s[-1,1],\n      col='blue',lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#practice-graded-assignment-ndlm-sensitivity-to-the-model-parameters",
    "href": "C4-L07.html#practice-graded-assignment-ndlm-sensitivity-to-the-model-parameters",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.11 Practice Graded Assignment: NDLM – sensitivity to the model parameters",
    "text": "95.11 Practice Graded Assignment: NDLM – sensitivity to the model parameters\n\n\n\n\n\n\nCaution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#homework---ndlm-part-i-review",
    "href": "C4-L07.html#homework---ndlm-part-i-review",
    "title": "94  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "95.12 Homework - NDLM, Part I: Review",
    "text": "95.12 Homework - NDLM, Part I: Review\n\n\n\n\n\n\nCaution\n\n\n\nSection omitted to comply with the Honor Code\n\n\n\n\n\n\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L08.html",
    "href": "C4-L08.html",
    "title": "95  Seasonal NDLMs M4L8",
    "section": "",
    "text": "95.1 Seasonal NDLMs",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#seasonal-ndlms",
    "href": "C4-L08.html#seasonal-ndlms",
    "title": "95  Seasonal NDLMs M4L8",
    "section": "",
    "text": "NoteLearning Objectives\n\n\n\n\n\n\nUse R for analysis and forecasting of time series using the NDLM (cases of known or unknown observational variance and unknown system variance specified using a discount factor)\nDerive the equations to obtain posterior inference and forecasting in the NDLM with unknown observational variance and system variance specified via discount factors\nDefine seasonal NDLMs\nApply the NDLM superposition principle and explain the role of the forecast function",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#fourier-representation",
    "href": "C4-L08.html#fourier-representation",
    "title": "95  Seasonal NDLMs M4L8",
    "section": "95.2 Fourier representation 🎥",
    "text": "95.2 Fourier representation 🎥\nTranscript:\n\nI will now describe how to incorporate seasonal components in a normal dynamic linear model. What we will do is we will first talk about the so-called single Fourier component representation . Just in case you have a single frequency and how to incorporate that single frequency in your model for the seasonality. Then using the superposition principle, you can incorporate several frequencies or a single frequency and the corresponding harmonics in your model.single Fourier component representation\nThere are other seasonal representations as well clarification needed. We will focus on the Fourier representation as is is flexible without needing too many parameters. E.g. if you want to consider, a fundamental frequency but you don’t want all the harmonics of that frequency. The Fourier representation, if you happen to have a single frequency.\nWe will discuss two cases with different component representations:\n\n\\omega \\in (0,\\pi)\n\\omega = \\pi \\implies \\{ 1,1,\\cdot, \\cdot\\}\n\nIn the case of any frequency \\omega \\in (0,\\pi), we will have a DLM that has this structure:\n\n\\{ \\underbrace {E_2}_{F},  \\underbrace {J_2(1,\\omega)}_{G}, \\underbrace{\\cdot}_{v_t}, \\underbrace{\\cdot}_{W_t}\\}\n\\tag{95.1}\nWe will have the F vector the 2-dimensional vector: \nE_2=(1,0)'\n\\tag{95.2}\nAs usual and the G matrix will be the 2 by 2 matrix:\n\nJ_2(1, \\omega) =\n\\begin{pmatrix}\n\\cos(\\omega) & \\sin(\\omega) \\\\\n-\\sin(\\omega) & \\cos(\\omega)\n\\end{pmatrix},\n\\tag{95.3}\nwhere \\omega is the frequency that we are considering.\nSince this is a 2 by 2 matrix the our state parameter vector will also be a vector of dimension 2.\nIf we think about the forecast function, f_t(h) h-steps ahead, (you are at time t and you want to look for h steps ahead).\nLet’s recall: the way we work with this is F* G^h * a_t\ngoing to be your E_2', then you have to take this G matrix, which is just this J_2(1,\\omega)^h, and then you have a vector, I’m going to call a_t and b_t, which is just going to be this vector value of your Theta t vector given the information up to the time t. It’s going to have two components, I’m just going to generically call them a_t and b_t. When you take this to the power of h using just trigonometric results, you’re going to get that J_2(1,\\omega)^h, is just going to give you cosine of Omega h sine of Omega h minus sine of Omega h cosine of Omega h. When you look at this expression, you get something that looks like this, and then you have, again, times these a_t, b_t.\n\n\\begin{aligned}\nf_t(h) &= E_2' [J_2(1, \\omega)]^h \\underbrace{\\begin{pmatrix} a_t \\\\\nb_t \\end{pmatrix}}_{\\mathbb{E}[\\theta\\mid \\mathcal{D}]} \\\\\n&= (1,0) \\begin{pmatrix} \\cos(\\omega h) & \\sin(\\omega h) \\\\\n-\\sin(\\omega h) & \\cos(\\omega h) \\end{pmatrix} \\begin{pmatrix} a_t \\\\\nb_t \\end{pmatrix} \\\\\n&= a_t \\cos(\\omega h) + b_t \\sin(\\omega h) \\\\\n&= A_t \\cos(\\omega h + B_t).\n\\end{aligned}\n\\tag{95.4}\n\nYou’re going to have the cosine and sine only multiplied by this. In the end, you’re going to have something that looks like this.\nYou have this sinusoidal form with the period Omega in your forecast function. You can also write this down in terms of an amplitude that I’m going to call A_t and then a phase that is B_t. Here again, you have your periodicity that appears in this cosine wave. This is again for the case in which you have a single frequency and the frequencies in this range. There was a second case that I mentioned, and that case is the case in which the Omega is exactly Pi. In this case, your Fourier representation is going to be your model that has a state vector that is just one dimensional. In the case where Omega is between zero and Pi, you have a two-dimensional state, vector here you’re going to have a one-dimensional state vector.\nThis is going to be your F and your G. Then you have again whatever you want to put here as your v_t and W_t. This gives me, if I think about the forecast function, h steps ahead is just going to be something that has the form -1^h \\times a_t. Now I have a single component here, is uni-dimensional. This is going to have an oscillatory behavior between a_t and -a_t if I were to look h steps ahead forward when I’m at time t. These two forms give me the single component Fourier representation and using the superposition principle, we will see that we can combine a single frequency and the corresponding harmonics or several different frequencies just using the superposition principle in the normal dynamic linear model. You can also incorporate more than one component in a full Fourier representation. Usually the way this works is you have a fundamental period, let’s say p. For example, if you are recording monthly data, p could be 12 and then you are going to incorporate in the model the fundamental frequency, and then all the harmonics that go with that fundamental frequency related to the period p.\n\n\n\n\n\nslide 1\n\n\nHere p, is the period and in this case, we are going to discuss essentially two different situations. One is when p is an odd number, the other one is when p is an even number. Let’s begin with the case of p is odd and in this particular scenario, we can write down p as 2 times m minus 1 for some value of m. This gives me a period that is odd. How many frequencies I’m going to incorporate in this model? I’m going to be able to write down \\omega_j = 2 \\pi \\times j / p, which is the fundamental period. j here goes from one all the way to m minus 1. Now we can use the superposition principle thinking we have a component DLM representation for each of these frequencies. They are all going to be between 0 and Pi. For each of them I’m going to have that two-dimensional DLM representation in terms of the state vector and then I can use the superposition principle to concatenate them all and get a model that has all these frequencies, the one related to the fundamental period and all the harmonics for that. Again, if I think about what is my F and my G here, I’m not writing down the t because both F and G are going to be constant over time. So my F is going to be again, I concatenate as many E_2 as I have frequencies in here. I’m going to have E_2 transpose and so on and I’m going to have m minus one of those. Times 2 gives me the dimension of \\theta_t. The vector here is 2 times m minus 1 dimensional vector.\nMy G is going to have that block diagonal structure where we are going to just have all those J_{2,1} \\omega_1, all the way down to the last harmonic. Each of these blocks is a two-by-two matrix and I’m going to put them together in a block diagonal form. This gives me the representation when the period is odd, what is the structure of the forecast function? Again, using the superposition principle, the forecast function is going to be just the sum of m minus 1 components, where each of those components is going to have an individual forecast function that has that cosine wave representation that we discussed before. Again, if I think about the forecast function at time t h steps ahead, I will be able to write it down like this.\nThis should be a B. B_{t,j}. Again here, I have an amplitude for each of the components and a phase for each of the components so it depends on time but does not depend on h. The h enters here, and this is my forecast function. In the case of P even the situation is slightly different. But again, it’s the same in terms of using the superposition principle. In this case, we can write down P as 2 times m because it’s an even number. Now I can write down these Omega j’s as a function of the fundamental period. Again, this goes from 1 up to m minus 1. But there is a last frequency here. When j is equal to m, this simplifies to be the Nyquist frequency. In this case, I have my Omega is equal to Pi. In this particular case, when I concatenate everything, I’m going to have again an F and a G that look like this. Once again, I concatenate all of these up to the component m minus 1. Then I have this 1 for the last frequency. Then my G is going to be the block diagonal.\nFor the last frequency I have that minus 1. This determines the dimension of the state vector, in this case I’m going to have 2 times m minus 1 plus 1.\nMy f function, my forecast function, is again a function of the number of steps ahead. I’m going to have the same structure I had before for the m minus 1 components. Then I have to add one more component that corresponds to the frequency Pi. This one appears with the power of h. As you can see, I’m using once again the superposition principle to go from component representation to the full Fourier representation. In practice, once we set the period, we can use a model that has the fundamental period and all the harmonics related to that fundamental period. We could also use, discard some of those harmonics and use a subset of them. This is one of the things that the Fourier representation allows. It allows you to be flexible in terms of how many components you want to add in this model. There are other representations that are also used in practice. One of them is the seasonal factors representation. In that case, you’re going to have a model in which the state vector has dimension p for a given period. It uses a G matrix that is a permutation matrix. There is a correspondence between this parameterization using the Fourier representation and that other parameterization. If you want to use that parameterization, the way to interpret the components of this state vector, since you have P of those, is going to be a representation in terms of factors. For example, if you think about monthly data, you will have the say January factor, February factor, March factor, and so on. You could think about those effects and do a correspondence with this particular model. We will always work in this class with these representations because it’s more flexible. But again, you can go back and forth between one and the other.\n\n\n\n\n\nslide 2",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#fourier-representation-example-1",
    "href": "C4-L08.html#fourier-representation-example-1",
    "title": "95  Seasonal NDLMs M4L8",
    "section": "95.3 Fourier Representation: Example 1 📖",
    "text": "95.3 Fourier Representation: Example 1 📖\n\n95.3.1 Seasonal Models\nExample: Full Fourier Model with p=5\nIn this case the Fourier frequencies are\n\nω_1 = 2π/5 and\nω_2 = 4π/5 and so\np = 2 × 3 − 1. Then,\nm = 3 and\n\\theta_t = (\\theta_{t,1}, \\ldots , \\theta_{t,4})′,\nF = (1, 0, 1, 0),\nG is given by:\n\n\nG = \\begin{bmatrix}\n\\cos(2\\pi/5) & \\sin(2\\pi/5) & 0 & 0 \\\\\n\\cos(4\\pi/5) & \\sin(4\\pi/5) & 0 & 0 \\\\\n0 & 0 & \\cos(2\\pi/5) & \\sin(2\\pi/5) \\\\\n0 & 0 & \\cos(4\\pi/5) & \\sin(4\\pi/5) \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\nand the forecast function is:\n\nf_t(h) = A_{t,1} \\cos(2\\pi h/5 + \\gamma_t) + A_{t,2} \\cos(4\\pi h /5 + \\gamma_{t,2}) \\qquad\n\\tag{95.5}",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#building-ndlms-with-multiple-components-examples",
    "href": "C4-L08.html#building-ndlms-with-multiple-components-examples",
    "title": "95  Seasonal NDLMs M4L8",
    "section": "95.4 Building NDLMs with multiple components: Examples 🎥",
    "text": "95.4 Building NDLMs with multiple components: Examples 🎥\n\n\n\n\ntwo component model\n\n\nIn this second example, we are going to have two components; a linear trend plus a seasonal component where the fundamental period is four. The way to build this model, again, is using the superposition principle.\nFirst we need to think “what structure do we need, to get a linear trend in the forecast function?”\nThe linear trend is a linear function on the number of steps ahead.\nWhenever you have that structure, you will get a DLM that is the so-called polynomial model of order 2. So let’s discuss first the linear. Let’s say the linear trend part, and in this case, we have an F and a G, I’m going to call them 1, F_1 and G_1 to denote that this is the first component in the model.\nF_1 is just going to be 1, 0 transpose, and the G_1 is that upper triangular matrix, it’s a 2 by 2 matrix that has 1, 1 in the first row, 0, 1 in the second row, so this gives me a linear trend.\nMy forecast function, let’s call it f_{1,t} in terms of the number of steps ahead is just a linear function on h, is a linear polynomial order 1. Let’s say it’s a constant of K but depends on t0 plus K_{t_1}^h. This is the structure of the first component. Then I have to think about the seasonal component with period of four. If we are going to incorporate all the harmonics, we have to think again, is this an even period or a not period? In this example, this is an even period. I can write p, which is 4, as 2 times 2, so this gives me that m. I’m going to have one frequency, the first one, Omega 1, is related to the fundamental period of 4, so is 2 Pi over 4, which I can simplify and write down this as Pi over 2. This is the first frequency. The last one is going to correspond to the Nyquist.\nWe could obtain that doing 4Pi over 4, which is just Pi. As you remember, this component is going to require a two-dimensional DLM component model, this one is going to require a one-dimensional DLM component model in terms of the dimension here is the dimension of the state vectors. When we build this concatenating these components, we are going to have, again, let’s call it F_2 and G_2 for this particular component. I had called this here a, let’s call this b. My F_2 has that E_2 transpose and a 1, which gives me just 1, 0, 1. My G matrix is going to be a 3 by 3 matrix. The first component is\nthe component associated to that fundamental period. It’s a block diagonal again, and I’m going to have that J_2, 1 Omega 1, and then I have my minus 1 here. What this means is if I write this down as a matrix, let me write it here, G_2 is going to be cosine of that Pi halves,\nand then I have zeros here, I have my minus 1 here, 0, and 0. I can further simplify these to have this structure. The cosine of Pi halves is 0, the sine is 1, so I can write this down as 0, 1, 0, minus 1, 0, 0, and 0, 0 minus 1. Now if I want to go back to just having a model that has both components, I use the superposition principle again and combine this component with this component. The linear plus seasonal\nis a model that is going to have the representation F, G, with F is going to be just concatenate F_1 and F_2. G now has that block diagonal form again.\nIf I look at what I have, I have this block that is a 2 by 2, this block that is a 3 by 3. Therefore my model is going to be a five-dimensional model in terms of the state parameter vector, so this G is a 5 by 5, and this one is also a five-dimensional vector. Finally, if I think about the forecast function in this case, if I call here the forecast function f_{2,t} for the component that is seasonal, I’m going to have my A_t1 cosine of Pi halves h plus B_{t,1}, and then I have my A_{t,2} minus 1^h. My forecast function for the final model is going to be just the sum of these two components.\nYou can see how I can now put together all these blocks, so I have a block that is seasonal and a block that is a linear polynomial model, and I can put them together in a single model just to create a more flexible structure. You could add regression components, you could add autoregressive components and put together as many components as you need for the forecast function to have the form that you expect it to have. All of these models are using, again, the superposition principle and the fact that we’re working with a linear and Gaussian structure in terms of doing the posterior inference later.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#summary-dlm-fourier-representation",
    "href": "C4-L08.html#summary-dlm-fourier-representation",
    "title": "95  Seasonal NDLMs M4L8",
    "section": "95.5 Summary: DLM Fourier representation 📖",
    "text": "95.5 Summary: DLM Fourier representation 📖\n\n95.5.1 Seasonal Models: Fourier Representation\nFor any frequency \\omega \\in (0, \\pi), a model of the form \\{E_2, J_2(1, \\omega), \\cdot, \\cdot\\} with a 2-dimensional state vector \\theta_t = (\\theta_{t,1}, \\theta_{t,2})' and\n\nJ_2(1, \\omega) =\n\\begin{pmatrix}\n\\cos(\\omega) & \\sin(\\omega) \\\\\n-\\sin(\\omega) & \\cos(\\omega)\n\\end{pmatrix},\n\nhas a forecast function\n\n\\begin{aligned}\nf_t(h) &= (1, 0) J_2^h(1, \\omega) (a_t, b_t) \\\\\n       &= a_t \\cos(\\omega h) + b_t \\sin(\\omega h) \\\\\n       &= A_t \\cos(\\omega h + B_t).\n\\end{aligned}\n\nFor \\omega = \\pi, the NDLM is \\{1, -1, \\cdot, \\cdot\\} and has a forecast function of the form\n\nf_t(h) = (-1)^h m_t\n\nThese are component Fourier models. Now, for a given period p, we can build a model that contains components for the fundamental period and all the harmonics of such a period using the superposition principle as follows:\n\n\n95.5.2 Case: p = 2m - 1 (odd)\nLet \\omega_j = 2\\pi j / p for j = 1 : (m - 1), F a (p - 1)-dimensional vector, or equivalently, a 2(m - 1)-dimensional vector, and G a (p - 1) \\times (p - 1) matrix with F = (E_2', E_2', \\dots, E_2')',\n\nG = \\text{blockdiag}[J_2(1, \\omega_1), \\dots, J_2(1, \\omega_{m-1})].\n\n\n\n95.5.3 Case: p = 2m (even)\nIn this case, F is again a (p - 1)-dimensional vector (or equivalently a (2m - 1)-dimensional vector), and G is a (p - 1) \\times (p - 1) matrix such that F = (E_2', \\dots, E_2', 1)' and\n\nG = \\text{blockdiag}[J_2(1, \\omega_1), \\dots, J_2(1, \\omega_{m-1}), -1].\n\nIn both cases, the forecast function has the general form:\n\nf_t(h) = \\sum_{j=1}^{m-1} A_{t,j} \\cos(\\omega_j h + \\gamma_{t,j}) + (-1)^h A_{t,m},\n\nwith A_{t,m} = 0 if p is odd.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#examples",
    "href": "C4-L08.html#examples",
    "title": "95  Seasonal NDLMs M4L8",
    "section": "95.6 Examples",
    "text": "95.6 Examples\n\n95.6.1 Fourier Representation, p = 12:\nIn this case, p = 2 \\times 6 so \\theta_t is an 11-dimensional state vector,\n\nF = (1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1)',\n\nthe Fourier frequencies are \\omega_1 = 2\\pi/12, \\omega_2 = 4\\pi/12 = 2\\pi/6, \\omega_3 = 6\\pi/12 = 2\\pi/4, \\omega_4 = 8\\pi/12 = 2\\pi/3, \\omega_5 = 10\\pi/12 = 5\\pi/6, and \\omega_6 = 12\\pi/12 = \\pi (the Nyquist frequency).\n\nG = \\text{blockdiag}(J_2(1, \\omega_1), \\dots, J_2(1, \\omega_5), 1)\n\nand the forecast function is given by:\n\nf_t(h) = \\sum_{j=1}^{5} A_{t,j} \\cos(2\\pi j / 12 + \\gamma_{t,j}) + (-1)^h A_{t,6}.\n\n\n\n95.6.2 Linear Trend + Seasonal Component with p = 4\nWe can use the superposition principle to build more sophisticated models. For instance, assume that we want a model with the following 2 components:\n\nLinear trend: \\{F_1, G_1, \\cdot, \\cdot\\} with F_1 = (1, 0)',\n\n\nG_1 = J_2(1) =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n\n\nFull seasonal model with p = 4: \\{F_2, G_2, \\cdot, \\cdot\\}, p = 2 \\times 2 so m = 2 and \\omega = 2\\pi / 4 = \\pi / 2,\n\n\nF_2 = (1, 0, 1)',\n\nand\n\nG_2 =\n\\begin{pmatrix}\n\\cos(\\pi / 2) & \\sin(\\pi / 2) & 0 \\\\\n-\\sin(\\pi / 2) & \\cos(\\pi / 2) & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 1 & 0 \\\\\n-1 & 0 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}.\n\nThe resulting DLM is a 5-dimensional model \\{F, G, \\cdot, \\cdot\\} with\n\nF = (1, 0, 1, 0, 1)',\n\nand\n\nG =\n\\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & -1\n\\end{pmatrix}.\n\nThe forecast function is:\n\nf_t(h) = (k_{t,1} + k_{t,2} h) + k_{t,3} \\cos(\\pi h / 2) + k_{t,4} \\sin(\\pi h / 2) + k_{t,5} (-1)^h.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#quiz-seasonal-models-and-superposition",
    "href": "C4-L08.html#quiz-seasonal-models-and-superposition",
    "title": "95  Seasonal NDLMs M4L8",
    "section": "95.7 Quiz: Seasonal Models and Superposition",
    "text": "95.7 Quiz: Seasonal Models and Superposition\nThis is omitted due to the Coursera honor code.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L09.html",
    "href": "C4-L09.html",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "",
    "text": "96.1 Filtering, Smoothing and Forecasting: Unknown observational variance 🎥\nIn this video we cover the following material also provided as a handout:\nInference in the NDLM with unknown but constant observational variance:\nLet v_t = v for all t, with v unknown and consider a DLM with the following structure: \n\\begin{aligned}\ny_t &= \\mathbf{F}_t' \\mathbf{\\theta}_t + \\nu_t, &\\nu_t &\\sim N (0, v)\\\\\n\\mathbf{\\theta}_t &= \\mathbf{G}_t \\mathbf{\\theta}_{t-1} + \\mathbf{\\omega}_t, & \\mathbf{\\omega}_t &\\sim N (0, v \\mathbf{W}^*_t)\n\\end{aligned}\n\\tag{96.1}\nwith conjugate prior distributions: \n(\\mathbf{\\theta}_0 \\mid D_0, v) \\sim N (\\mathbf{m}_0, v\\mathbf{C}^*_0), \\qquad (v \\mid D_0) \\sim IG(\\frac{n_0}{2}, \\frac{d_0}{2}),\n\\tag{96.2} and d_0 = n_0s_0",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#filtering-smoothing-and-forecasting-unknown-observational-variance",
    "href": "C4-L09.html#filtering-smoothing-and-forecasting-unknown-observational-variance",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "",
    "text": "96.1.1 Filtering\nAssuming (\\theta_{t-1} \\mid D_{t-1}, v) \\sim N (m_{t-1}, vC^*_{t-1}), we have the following results:\n\n(\\theta_t \\mid D_{t-1}, v) \\sim N (a_t, vR^*_t) with a_t = G_t m_{t-1} and R^*_t = G_t C^*_{t-1} G'_t + W^*_t, and unconditional on v, (\\theta_t \\mid D_{t-1}) \\sim T_{n_{t-1}} (a_t, R_t), with R_t = s_{t-1} R^*_t. The expression for s_t for all t is given below.\n(y_t \\mid D_{t-1}, v) \\sim N (f_t, vq^*_t), with f_t = F'_t a_t, and q^*_t = (1 + F'_t R^*_t F_t) and unconditional on v we have (y_t \\mid D_{t-1}) \\sim T_{n_{t-1}} (f_t, q_t), with q_t = s_{t-1} q^*_t.\n(v \\mid D_t) \\sim \\mathcal{IG}(n_t/2, s_t/2), with n_t = n_{t-1} + 1 and \ns_t = s_{t-1} + \\frac{s_{t-1}}{n_t} \\left ( \\frac{e^2_t}{q^*_t} - 1 \\right ),\n\\tag{96.3}\nwhere e_t = y_t - f_t\nθt|Dt, v) ∼ N (mt, vC∗ t ), with mt = at + Atet, and C∗ t = R∗ t − AtA′ tq∗ t . Similarly, unconditional on v we have (θt|Dt) ∼ Tnt (mt, Ct), with Ct = stC∗ t",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#summary-of-filtering-smoothing-and-forecasting-distributions-ndlm-unknown-observational-variance",
    "href": "C4-L09.html#summary-of-filtering-smoothing-and-forecasting-distributions-ndlm-unknown-observational-variance",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "96.2 Summary of Filtering, Smoothing and Forecasting Distributions, NDLM unknown observational variance 📖",
    "text": "96.2 Summary of Filtering, Smoothing and Forecasting Distributions, NDLM unknown observational variance 📖",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#specifying-the-system-covariance-matrix-via-discount-factors",
    "href": "C4-L09.html#specifying-the-system-covariance-matrix-via-discount-factors",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "96.3 Specifying the system covariance matrix via discount factors 🎥",
    "text": "96.3 Specifying the system covariance matrix via discount factors 🎥",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#ndlm-unknown-observational-variance-example",
    "href": "C4-L09.html#ndlm-unknown-observational-variance-example",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "96.4 NDLM, Unknown Observational Variance: Example 🎥",
    "text": "96.4 NDLM, Unknown Observational Variance: Example 🎥\nThis is a walk though of the R code for the example bellow.",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#rcode-ndlm-unknown-observational-variance-example",
    "href": "C4-L09.html#rcode-ndlm-unknown-observational-variance-example",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "96.5 Rcode: NDLM, Unknown Observational Variance Example 📖",
    "text": "96.5 Rcode: NDLM, Unknown Observational Variance Example 📖\nThis code allows time-varying F_t, G_t and W_t matrices and assumes an unknown but constant \\nu. It also allows the user to specify W_t using a discount factor \\delta \\in (0,1] or assume W_t known.\n\n## create list for matrices\nset_up_dlm_matrices_unknown_v &lt;- function(Ft, Gt, Wt_star){\n  if(!is.array(Gt)){\n    Stop(\"Gt and Ft should be array\")\n  }\n  if(missing(Wt_star)){\n    return(list(Ft=Ft, Gt=Gt))\n  }else{\n    return(list(Ft=Ft, Gt=Gt, Wt_star=Wt_star))\n  }\n}\n\n\n## create list for initial states\nset_up_initial_states_unknown_v &lt;- function(m0, C0_star, n0, S0){\n  return(list(m0=m0, C0_star=C0_star, n0=n0, S0=S0))\n}\n\nforward_filter_unknown_v &lt;- function(data, matrices, \n                              initial_states, delta){\n  ## retrieve dataset\n  yt &lt;- data$yt\n  T&lt;- length(yt)\n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  if(missing(delta)){\n    Wt_star &lt;- matrices$Wt_star\n  }\n  \n  ## retrieve initial state\n  m0 &lt;- initial_states$m0\n  C0_star &lt;- initial_states$C0_star\n  n0 &lt;- initial_states$n0\n  S0 &lt;- initial_states$S0\n  C0 &lt;- S0*C0_star\n  \n  ## create placeholder for results\n  d &lt;- dim(Gt)[1]\n  at &lt;- matrix(0, nrow=T, ncol=d)\n  Rt &lt;- array(0, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(0, nrow=T, ncol=d)\n  Ct &lt;- array(0, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  nt &lt;- numeric(T)\n  St &lt;- numeric(T)\n  dt &lt;- numeric(T)\n  \n  # moments of priors at t\n  for(i in 1:T){\n    if(i == 1){\n      at[i, ] &lt;- Gt[, , i] %*% m0\n      Pt &lt;- Gt[, , i] %*% C0 %*% t(Gt[, , i])\n      Pt &lt;- 0.5*Pt + 0.5*t(Pt)\n      if(missing(delta)){\n        Wt &lt;- Wt_star[, , i]*S0\n        Rt[, , i] &lt;- Pt + Wt\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }else{\n        Rt[, , i] &lt;- Pt/delta\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }\n      \n    }else{\n      at[i, ] &lt;- Gt[, , i] %*% t(mt[i-1, , drop=FALSE])\n      Pt &lt;- Gt[, , i] %*% Ct[, , i-1] %*% t(Gt[, , i])\n      if(missing(delta)){\n        Wt &lt;- Wt_star[, , i] * St[i-1]\n        Rt[, , i] &lt;- Pt + Wt\n        Rt[,,i]=0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }else{\n        Rt[, , i] &lt;- Pt/delta\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }\n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(Ft[, , i]) %*% t(at[i, , drop=FALSE]) \n    Qt[i] &lt;- t(Ft[, , i]) %*% Rt[, , i] %*% Ft[, , i] + \n      ifelse(i==1, S0, St[i-1])\n    et[i] &lt;- yt[i] - ft[i]\n    \n    nt[i] &lt;- ifelse(i==1, n0, nt[i-1]) + 1\n    St[i] &lt;- ifelse(i==1, S0, \n                    St[i-1])*(1 + 1/nt[i]*(et[i]^2/Qt[i]-1))\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% Ft[, , i] / Qt[i]\n    \n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- St[i]/ifelse(i==1, S0, \n                  St[i-1])*(Rt[, , i] - Qt[i] * At %*% t(At))\n    Ct[,,i] &lt;- 0.5*Ct[,,i]+0.5*t(Ct[,,i])\n  }\n  cat(\"Forward filtering is completed!\\n\")\n  return(list(mt = mt, Ct = Ct,  at = at, Rt = Rt, \n              ft = ft, Qt = Qt,  et = et, \n              nt = nt, St = St))\n}\n\n### smoothing function ###\nbackward_smoothing_unknown_v &lt;- function(data, matrices, \n                                posterior_states,delta){\n  ## retrieve data \n  yt &lt;- data$yt\n  T &lt;- length(yt) \n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  \n  ## retrieve matrices\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  Rt &lt;- posterior_states$Rt\n  nt &lt;- posterior_states$nt\n  St &lt;- posterior_states$St\n  at &lt;- posterior_states$at\n  \n  ## create placeholder for posterior moments \n  mnt &lt;- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt &lt;- array(NA, dim = dim(Ct))\n  fnt &lt;- numeric(T)\n  Qnt &lt;- numeric(T)\n  \n  for(i in T:1){\n    if(i == T){\n      mnt[i, ] &lt;- mt[i, ]\n      Cnt[, , i] &lt;- Ct[, , i]\n    }else{\n      if(missing(delta)){\n        inv_Rtp1 &lt;- chol2inv(chol(Rt[, , i+1]))\n        Bt &lt;- Ct[, , i] %*% t(Gt[, , i+1]) %*% inv_Rtp1\n        mnt[i, ] &lt;- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n        Cnt[, , i] &lt;- Ct[, , i] + Bt %*% (Cnt[, , i+1] - \n                                    Rt[, , i+1]) %*% t(Bt)\n        Cnt[,,i] &lt;- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])\n      }else{\n        inv_Gt &lt;- solve(Gt[, , i+1])\n        mnt[i, ] &lt;- (1-delta)*mt[i, ] + \n                delta*inv_Gt %*% t(mnt[i+1, ,drop=FALSE])\n        Cnt[, , i] &lt;- (1-delta)*Ct[, , i] + \n                delta^2*inv_Gt %*% Cnt[, , i + 1]  %*% t(inv_Gt)\n        Cnt[,,i] &lt;- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])\n      }\n    }\n    fnt[i] &lt;- t(Ft[, , i]) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] &lt;- t(Ft[, , i]) %*% t(Cnt[, , i]) %*% Ft[, , i]\n  }\n  for(i in 1:T){\n     Cnt[,,i]=St[T]*Cnt[,,i]/St[i] \n     Qnt[i]=St[T]*Qnt[i]/St[i]\n  }\n  cat(\"Backward smoothing is completed!\\n\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n\n## Forecast Distribution for k step\nforecast_function_unknown_v &lt;- function(posterior_states, k, \n                                        matrices, delta){\n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  if(missing(delta)){\n    Wt_star &lt;- matrices$Wt_star\n  }\n  \n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  St &lt;- posterior_states$St\n  at &lt;- posterior_states$at\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- Gt[, , T+i] %*% t(mt[T, , drop=FALSE])\n      \n      if(missing(delta)){\n       Rt[, , i] &lt;- Gt[, , T+i] %*% Ct[, , T] %*% \n         t(Gt[, , T+i]) + St[T]*Wt_star[, , T+i]\n      }else{\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Ct[, , T] %*% \n          t(Gt[, , T+i])/delta\n      }\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      \n    }else{\n      at[i, ] &lt;- Gt[, , T+i] %*% t(at[i-1, , drop=FALSE])\n      if(missing(delta)){\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Rt[, , i-1] %*% \n          t(Gt[, , T+i]) + St[T]*Wt_star[, , T + i]\n      }else{\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Rt[, , i-1] %*% \n          t(Gt[, , T+i])/delta\n      }\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n    }\n    \n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(Ft[, , T+i]) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(Ft[, , T+i]) %*% Rt[, , i] %*% Ft[, , T+i] + \n      St[T]\n  }\n  cat(\"Forecasting is completed!\\n\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval_unknown_v &lt;- function(ft, Qt, nt, \n                                   quantile = c(0.025, 0.975)){\n  bound &lt;- matrix(0, nrow=length(ft), ncol=2)\n\n  if ((length(nt)==1)){\n   for (t in 1:length(ft)){\n      t_quantile &lt;- qt(quantile[1], df = nt)\n      bound[t, 1] &lt;- ft[t] + t_quantile*sqrt(as.numeric(Qt[t])) \n  \n  # upper bound of 95% credible interval\n      t_quantile &lt;- qt(quantile[2], df = nt)\n      bound[t, 2] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t]))}\n  }else{\n  # lower bound of 95% credible interval\n    for (t in 1:length(ft)){\n      t_quantile &lt;- qt(quantile[1], df = nt[t])\n      bound[t, 1] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t])) \n  \n  # upper bound of 95% credible interval\n      t_quantile &lt;- qt(quantile[2], df = nt[t])\n      bound[t, 2] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t]))}\n  }\n  return(bound)\n\n}\n\n\n\n## Example: Nile River Level (in 10^8 m^3), 1871-1970 \n## Model: First order polynomial DLM\nplot(Nile) \n\n\n\n\n\n\n\nn=length(Nile) #n=100 observations \nk=5\nT=n-k\ndata_T=Nile[1:T]\ntest_data=Nile[(T+1):n]\ndata=list(yt = data_T)\n\n\n## set up matrices for first order polynomial model \nFt=array(1, dim = c(1, 1, n))\nGt=array(1, dim = c(1, 1, n))\nWt_star=array(1, dim = c(1, 1, n))\nm0=as.matrix(800)\nC0_star=as.matrix(10)\nn0=1\nS0=10\n\n## wrap up all matrices and initial values\nmatrices = set_up_dlm_matrices_unknown_v(Ft, Gt, Wt_star)\ninitial_states = set_up_initial_states_unknown_v(m0, \n                                      C0_star, n0, S0)\n\n## filtering \nresults_filtered = forward_filter_unknown_v(data, matrices, \n                                            initial_states)\n\nForward filtering is completed!\n\nci_filtered=get_credible_interval_unknown_v(results_filtered$mt, \n                                    results_filtered$Ct, \n                                     results_filtered$nt)\n\n## smoothing\nresults_smoothed=backward_smoothing_unknown_v(data, matrices, \n                                             results_filtered)\n\nBackward smoothing is completed!\n\nci_smoothed=get_credible_interval_unknown_v(results_smoothed$mnt, \n                                         results_smoothed$Cnt, \n                                         results_filtered$nt[T])\n\n## one-step ahead forecasting\nresults_forecast=forecast_function_unknown_v(results_filtered, \n                                                k,  matrices)\n\nForecasting is completed!\n\nci_forecast=get_credible_interval_unknown_v(results_forecast$ft, \n                                          results_forecast$Qt, \n                                     results_filtered$nt[T])\n\n\n## plot results\nindex=seq(1871, 1970, length.out = length(Nile))\nindex_filt=index[1:T]\nindex_forecast=index[(T+1):(T+k)]\n\nplot(index, Nile, main = \"Nile River Level \",type='l',\n     xlab=\"time\",ylab=\"feet\",lty=3,ylim=c(400,1500))\npoints(index,Nile,pch=20)\n\nlines(index_filt,results_filtered$mt, type='l', col='red',lwd=2)\nlines(index_filt,ci_filtered[, 1], type='l', col='red', lty=2)\nlines(index_filt,ci_filtered[, 2], type='l', col='red', lty=2)\nlines(index_filt,results_smoothed$mnt, type='l', col='blue',lwd=2)\nlines(index_filt, ci_smoothed[, 1], type='l', col='blue', lty=2)\nlines(index_filt, ci_smoothed[, 2], type='l', col='blue', lty=2)\n\nlines(index_forecast, results_forecast$ft, type='l', \n      col='green',lwd=2)\nlines(index_forecast, ci_forecast[, 1], type='l', \n      col='green', lty=2)\nlines(index_forecast, ci_forecast[, 2], type='l', \n      col='green', lty=2)",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#practice-graded-assignment-ndlm-data-analysis",
    "href": "C4-L09.html#practice-graded-assignment-ndlm-data-analysis",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "96.6 Practice Graded Assignment: NDLM data analysis",
    "text": "96.6 Practice Graded Assignment: NDLM data analysis\nThis peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you’ve learned in R and prepare you for your data analysis project in week 5.\nThe R code below fits a Normal Dynamic Linear Model to the monthly time series of Google trends “hits” for the term “time series”. The model has two components: (a) a polynomial model of order 2 and (b) a seasonal component with 4 frequencies: ω_1=2π/12, (annual cycle) ω_2=2π/6 (6 months cycle), ω_3=2π/4 and ω_4=2π/3. The model assumes that the observational variance v is unknown and the system variance-covariance matrix W_t is specified using a single discount factor. The discount factor is chosen using an optimality criterion as explained in the course.\nYou will be asked to modify the code in order to consider a DLM with two components: (a) a polynomial model of order 1 and (b) a seasonal component that contains a fundamental period of p = 12 and 2 additional harmonics for a total of 3 frequencies: ω1=2π/12, ω2=2π/6 and ω3=2π/4. You will also need to optimize the choice of the discount factor for this model. You will be asked to upload pictures summarizing your results.\nR code to fit the model: requires R packages gtrends,and dlm as well as the files “all_dlm_functions_unknown_v.R” and “discountfactor_selection_functions.R” also provided below.\n#| label: code-gtrendsR-data-analysis\n# download data \nlibrary(gtrendsR)\ntimeseries_data &lt;- gtrends(\"time series\",time=\"all\")\nplot(timeseries_data)\nnames(timeseries_data)\n\ntimeseries_data=timeseries_data$interest_over_time\ndata=list(yt=timeseries_data$hits)\n\nlibrary(dlm)\nmodel_seasonal=dlmModTrig(s=12,q=4,dV=0,dW=1)\nmodel_trend=dlmModPoly(order=2,dV=10,dW=rep(1,2),m0=c(40,0))\nmodel=model_trend+model_seasonal\nmodel$C0=10*diag(10)\nn0=1\nS0=10\nk=length(model$m0)\nT=length(data$yt)\n\nFt=array(0,c(1,k,T))\nGt=array(0,c(k,k,T))\nfor(t in 1:T){\n   Ft[,,t]=model$FF\n   Gt[,,t]=model$GG\n}\n\nsource('all_dlm_functions_unknown_v.R')\nsource('discountfactor_selection_functions.R')\n\nmatrices=set_up_dlm_matrices_unknown_v(Ft=Ft,Gt=Gt)\ninitial_states=set_up_initial_states_unknown_v(model$m0,\n                                               model$C0,n0,S0)\n\ndf_range=seq(0.9,1,by=0.005)\n\n## fit discount DLM\n## MSE\nresults_MSE &lt;- adaptive_dlm(data, matrices, \n               initial_states, df_range,\"MSE\",forecast=FALSE)\n\n## print selected discount factor\nprint(paste(\"The selected discount factor:\",results_MSE$df_opt))\n\n## retrieve filtered results\nresults_filtered &lt;- results_MSE$results_filtered\nci_filtered &lt;- get_credible_interval_unknown_v(\n  results_filtered$ft,results_filtered$Qt,results_filtered$nt)\n\n## retrieve smoothed results\nresults_smoothed &lt;- results_MSE$results_smoothed\nci_smoothed &lt;- get_credible_interval_unknown_v(\n  results_smoothed$fnt, results_smoothed$Qnt, \n  results_filtered$nt[length(results_smoothed$fnt)])\n\n## plot smoothing results \npar(mfrow=c(1,1), mar = c(3, 4, 2, 1))\nindex &lt;- timeseries_data$date\nplot(index, data$yt, ylab='Google hits',\n     main = \"Google Trends: time series\", type = 'l',\n     xlab = 'time', lty=3,ylim=c(0,100))\nlines(index, results_smoothed$fnt, type = 'l', col='blue', \n      lwd=2)\nlines(index, ci_smoothed[, 1], type='l', col='blue', lty=2)\nlines(index, ci_smoothed[, 2], type='l', col='blue', lty=2)\n\n# Plot trend and rate of change \npar(mfrow=c(2,1), mar = c(3, 4, 2, 1))\nplot(index,data$yt,pch=19,cex=0.3,col='lightgray',xlab=\"time\",\n     ylab=\"Google hits\",main=\"trend\")\nlines(index,results_smoothed$mnt[,1],lwd=2,col='magenta')\nplot(index,results_smoothed$mnt[,2],col='darkblue',lwd=2,\n     type='l', ylim=c(-0.6,0.6), xlab=\"time\",\n     ylab=\"rate of change\")\nabline(h=0,col='red',lty=2)\n\n# Plot seasonal components \npar(mfrow=c(2,2), mar = c(3, 4, 2, 1))\nplot(index,results_smoothed$mnt[,3],lwd=2,col=\"darkgreen\",\n     type='l', xlab=\"time\",ylab=\"\",main=\"period=12\",\n     ylim=c(-12,12))\nplot(index,results_smoothed$mnt[,5],lwd=2,col=\"darkgreen\",\n     type='l',xlab=\"time\",ylab=\"\",main=\"period=6\",\n     ylim=c(-12,12))\nplot(index,results_smoothed$mnt[,7],lwd=2,col=\"darkgreen\",\n     type='l', xlab=\"time\",ylab=\"\",main=\"period=4\",\n     ylim=c(-12,12))\nplot(index,results_smoothed$mnt[,9],lwd=2,col=\"darkgreen\",\n     type='l', xlab=\"time\",ylab=\"\",main=\"period=3\",\n     ylim=c(-12,12))\n\n#Estimate for the observational variance: St[T]\nresults_filtered$St[T]\n\n96.6.1 All dlm functions unknown v\n\n## create list for matrices\nset_up_dlm_matrices_unknown_v &lt;- function(Ft, Gt, Wt_star){\n  if(!is.array(Gt)){\n    Stop(\"Gt and Ft should be array\")\n  }\n  if(missing(Wt_star)){\n    return(list(Ft=Ft, Gt=Gt))\n  }else{\n    return(list(Ft=Ft, Gt=Gt, Wt_star=Wt_star))\n  }\n}\n\n\n## create list for initial states\nset_up_initial_states_unknown_v &lt;- function(m0, C0_star, n0, S0){\n  return(list(m0=m0, C0_star=C0_star, n0=n0, S0=S0))\n}\n\nforward_filter_unknown_v &lt;- function(data, matrices, \n                              initial_states, delta){\n  ## retrieve dataset\n  yt &lt;- data$yt\n  T&lt;- length(yt)\n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  if(missing(delta)){\n    Wt_star &lt;- matrices$Wt_star\n  }\n  \n  ## retrieve initial state\n  m0 &lt;- initial_states$m0\n  C0_star &lt;- initial_states$C0_star\n  n0 &lt;- initial_states$n0\n  S0 &lt;- initial_states$S0\n  C0 &lt;- S0*C0_star\n  \n  ## create placeholder for results\n  d &lt;- dim(Gt)[1]\n  at &lt;- matrix(0, nrow=T, ncol=d)\n  Rt &lt;- array(0, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(0, nrow=T, ncol=d)\n  Ct &lt;- array(0, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  nt &lt;- numeric(T)\n  St &lt;- numeric(T)\n  dt &lt;- numeric(T)\n  \n  # moments of priors at t\n  for(i in 1:T){\n    if(i == 1){\n      at[i, ] &lt;- Gt[, , i] %*% m0\n      Pt &lt;- Gt[, , i] %*% C0 %*% t(Gt[, , i])\n      Pt &lt;- 0.5*Pt + 0.5*t(Pt)\n      if(missing(delta)){\n        Wt &lt;- Wt_star[, , i]*S0\n        Rt[, , i] &lt;- Pt + Wt\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }else{\n        Rt[, , i] &lt;- Pt/delta\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }\n      \n    }else{\n      at[i, ] &lt;- Gt[, , i] %*% t(mt[i-1, , drop=FALSE])\n      Pt &lt;- Gt[, , i] %*% Ct[, , i-1] %*% t(Gt[, , i])\n      if(missing(delta)){\n        Wt &lt;- Wt_star[, , i] * St[i-1]\n        Rt[, , i] &lt;- Pt + Wt\n        Rt[,,i]=0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }else{\n        Rt[, , i] &lt;- Pt/delta\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }\n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(Ft[, , i]) %*% t(at[i, , drop=FALSE]) \n    Qt[i] &lt;- t(Ft[, , i]) %*% Rt[, , i] %*% Ft[, , i] + \n      ifelse(i==1, S0, St[i-1])\n    et[i] &lt;- yt[i] - ft[i]\n    \n    nt[i] &lt;- ifelse(i==1, n0, nt[i-1]) + 1\n    St[i] &lt;- ifelse(i==1, S0, \n                    St[i-1])*(1 + 1/nt[i]*(et[i]^2/Qt[i]-1))\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% Ft[, , i] / Qt[i]\n    \n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- St[i]/ifelse(i==1, S0, \n                  St[i-1])*(Rt[, , i] - Qt[i] * At %*% t(At))\n    Ct[,,i] &lt;- 0.5*Ct[,,i]+0.5*t(Ct[,,i])\n  }\n  cat(\"Forward filtering is completed!\\n\")\n  return(list(mt = mt, Ct = Ct,  at = at, Rt = Rt, \n              ft = ft, Qt = Qt,  et = et, \n              nt = nt, St = St))\n}\n\n### smoothing function ###\nbackward_smoothing_unknown_v &lt;- function(data, matrices, \n                                posterior_states,delta){\n  ## retrieve data \n  yt &lt;- data$yt\n  T &lt;- length(yt) \n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  \n  ## retrieve matrices\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  Rt &lt;- posterior_states$Rt\n  nt &lt;- posterior_states$nt\n  St &lt;- posterior_states$St\n  at &lt;- posterior_states$at\n  \n  ## create placeholder for posterior moments \n  mnt &lt;- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt &lt;- array(NA, dim = dim(Ct))\n  fnt &lt;- numeric(T)\n  Qnt &lt;- numeric(T)\n  \n  for(i in T:1){\n    if(i == T){\n      mnt[i, ] &lt;- mt[i, ]\n      Cnt[, , i] &lt;- Ct[, , i]\n    }else{\n      if(missing(delta)){\n        inv_Rtp1 &lt;- chol2inv(chol(Rt[, , i+1]))\n        Bt &lt;- Ct[, , i] %*% t(Gt[, , i+1]) %*% inv_Rtp1\n        mnt[i, ] &lt;- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n        Cnt[, , i] &lt;- Ct[, , i] + Bt %*% (Cnt[, , i+1] - \n                                    Rt[, , i+1]) %*% t(Bt)\n        Cnt[,,i] &lt;- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])\n      }else{\n        inv_Gt &lt;- solve(Gt[, , i+1])\n        mnt[i, ] &lt;- (1-delta)*mt[i, ] + \n                delta*inv_Gt %*% t(mnt[i+1, ,drop=FALSE])\n        Cnt[, , i] &lt;- (1-delta)*Ct[, , i] + \n                delta^2*inv_Gt %*% Cnt[, , i + 1]  %*% t(inv_Gt)\n        Cnt[,,i] &lt;- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])\n      }\n    }\n    fnt[i] &lt;- t(Ft[, , i]) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] &lt;- t(Ft[, , i]) %*% t(Cnt[, , i]) %*% Ft[, , i]\n  }\n  for(i in 1:T){\n     Cnt[,,i]=St[T]*Cnt[,,i]/St[i] \n     Qnt[i]=St[T]*Qnt[i]/St[i]\n  }\n  cat(\"Backward smoothing is completed!\\n\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n\n## Forecast Distribution for k step\nforecast_function_unknown_v &lt;- function(posterior_states, k, \n                                        matrices, delta){\n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  if(missing(delta)){\n    Wt_star &lt;- matrices$Wt_star\n  }\n  \n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  St &lt;- posterior_states$St\n  at &lt;- posterior_states$at\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- Gt[, , T+i] %*% t(mt[T, , drop=FALSE])\n      \n      if(missing(delta)){\n       Rt[, , i] &lt;- Gt[, , T+i] %*% Ct[, , T] %*% \n         t(Gt[, , T+i]) + St[T]*Wt_star[, , T+i]\n      }else{\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Ct[, , T] %*% \n          t(Gt[, , T+i])/delta\n      }\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      \n    }else{\n      at[i, ] &lt;- Gt[, , T+i] %*% t(at[i-1, , drop=FALSE])\n      if(missing(delta)){\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Rt[, , i-1] %*% \n          t(Gt[, , T+i]) + St[T]*Wt_star[, , T + i]\n      }else{\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Rt[, , i-1] %*% \n          t(Gt[, , T+i])/delta\n      }\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n    }\n    \n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(Ft[, , T+i]) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(Ft[, , T+i]) %*% Rt[, , i] %*% Ft[, , T+i] + \n      St[T]\n  }\n  cat(\"Forecasting is completed!\\n\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval_unknown_v &lt;- function(ft, Qt, nt, \n                                   quantile = c(0.025, 0.975)){\n  bound &lt;- matrix(0, nrow=length(ft), ncol=2)\n\n  if ((length(nt)==1)){\n   for (t in 1:length(ft)){\n      t_quantile &lt;- qt(quantile[1], df = nt)\n      bound[t, 1] &lt;- ft[t] + t_quantile*sqrt(as.numeric(Qt[t])) \n  \n  # upper bound of 95% credible interval\n      t_quantile &lt;- qt(quantile[2], df = nt)\n      bound[t, 2] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t]))}\n  }else{\n  # lower bound of 95% credible interval\n    for (t in 1:length(ft)){\n      t_quantile &lt;- qt(quantile[1], df = nt[t])\n      bound[t, 1] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t])) \n  \n  # upper bound of 95% credible interval\n      t_quantile &lt;- qt(quantile[2], df = nt[t])\n      bound[t, 2] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t]))}\n  }\n  return(bound)\n\n}\n\n\n\n96.6.2 Discount factor selection functions\n\n##################################################\n##### using discount factor ##########\n##################################################\n## compute measures of forecasting accuracy\n## MAD: mean absolute deviation\n## MSE: mean square error\n## MAPE: mean absolute percentage error\n## Neg LL: Negative log-likelihood of disc,\n##         based on the one step ahead forecast distribution\nmeasure_forecast_accuracy &lt;- function(et, yt, Qt=NA, nt=NA, type){\n  if(type == \"MAD\"){\n    measure &lt;- mean(abs(et))\n  }else if(type == \"MSE\"){\n    measure &lt;- mean(et^2)\n  }else if(type == \"MAPE\"){\n    measure &lt;- mean(abs(et)/yt)\n  }else if(type == \"NLL\"){\n    measure &lt;- log_likelihood_one_step_ahead(et, Qt, nt)\n  }else{\n    stop(\"Wrong type!\")\n  }\n  return(measure)\n}\n\n\n## compute log likelihood of one step ahead forecast function\nlog_likelihood_one_step_ahead &lt;- function(et, Qt, nt){\n  ## et:the one-step-ahead error\n  ## Qt: variance of one-step-ahead forecast function\n  ## nt: degrees freedom of t distribution\n  T &lt;- length(et)\n  aux=0\n  for (t in 1:T){\n    zt=et[t]/sqrt(Qt[t])\n    aux=(dt(zt,df=nt[t],log=TRUE)-log(sqrt(Qt[t]))) + aux \n  } \n  return(-aux)\n}\n\n## Maximize log density of one-step-ahead forecast function to select discount factor\nadaptive_dlm &lt;- function(data, matrices, initial_states, df_range, type, \n                         forecast=TRUE){\n  measure_best &lt;- NA\n  measure &lt;- numeric(length(df_range))\n  valid_data &lt;- data$valid_data\n  df_opt &lt;- NA\n  j &lt;- 0\n  ## find the optimal discount factor\n  for(i in df_range){\n    j &lt;- j + 1\n    results_tmp &lt;- forward_filter_unknown_v(data, matrices, initial_states, i)\n     \n    measure[j] &lt;- measure_forecast_accuracy(et=results_tmp$et, yt=data$yt,\n                                  Qt=results_tmp$Qt, \n                                  nt=c(initial_states$n0,results_tmp$nt), type=type)\n    \n    \n    if(j == 1){\n      measure_best &lt;- measure[j]\n      results_filtered &lt;- results_tmp\n      df_opt &lt;- i\n    }else if(measure[j] &lt; measure_best){\n      measure_best &lt;- measure[j]\n      results_filtered &lt;- results_tmp\n      df_opt &lt;- i\n    }\n  }\n  results_smoothed &lt;- backward_smoothing_unknown_v(data, matrices, results_filtered, delta = df_opt)\n  if(forecast){\n    results_forecast &lt;- forecast_function(results_filtered, length(valid_data), \n                                          matrices, df_opt)\n    return(list(results_filtered=results_filtered, \n                results_smoothed=results_smoothed, \n                results_forecast=results_forecast, \n                df_opt = df_opt, measure=measure))\n  }else{\n    return(list(results_filtered=results_filtered, \n                results_smoothed=results_smoothed, \n                df_opt = df_opt, measure=measure))\n  }\n  \n}\n\n\n\n96.6.3 Grading Criteria\nThe assignment will be graded based on the uploaded pictures summarizing the results. Estimates of some of the model parameters and additional discussion will also be requested.",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#eeg-data",
    "href": "C4-L09.html#eeg-data",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "97.1 EEG data",
    "text": "97.1 EEG data",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#google-trends",
    "href": "C4-L09.html#google-trends",
    "title": "96  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "97.2 Google Trends",
    "text": "97.2 Google Trends",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L10.html",
    "href": "C4-L10.html",
    "title": "97  Final Project",
    "section": "",
    "text": "In this final project you will use normal dynamic linear models to analyze a time series dataset downloaded from Google trend.\n\n\n\n\n\n\nNoteObjectives\n\n\n\n\nUse R for analysis and forecasting of time series using NDLM (case of known observational and system variances)\nUse R for analysis and forecasting of time series using the NDLM (cases of known or unknown observational variance and unknown system variance specified using a discount factor)\n\n\n\n\n\n\n\n\n\nNoteInstructions\n\n\n\nSo far in this course, we have discussed the following aspects of Bayesian time series models:\n\nConcepts of stationarity, the autocorrelation function, definition and properties of autoregressive (AR) models;\nMaximum likelihood and Bayesian conjugate analysis of AR models;\nDetermination of the order of AR models using AIC or BIC as criteria;\nDefinition of Normal Dynamic Linear Models (NDLMs);\nNDLM building using polynomial trend, seasonal and regression components via the superposition principle;\nBayesian filtering, smoothing and forecasting in the NDLM with known observational variances and known system covariance matrices;\nBayesian filtering, smoothing and forecasting in the NDLM with unknown but constant observational variance and known system covariance matrix;\nBayesian filtering, smoothing and forecasting in the NDLM with known observational variances and unknown system covariance matrices using discount factors;\nBayesian filtering, smoothing and forecasting in the NDLM with unknown but constant observational variance and unknown system covariance matrices using discount factors.\n\nIn this project, you will download a dataset from Google trends. In order to do this you can type a term/terms of interest in Google trends, just like we did with the example with the term “time series” analyzed in the course. You could use any term such as “flu”, “cranberry” or any other term(s). Here is a tutorial on how to download data from Google trends:",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "C4-L11.html",
    "href": "C4-L11.html",
    "title": "98  Week 0: Feynman Notebook on Bayesian Analysis",
    "section": "",
    "text": "98.1 A Feynman Notebook - For Bayesian Time Series Analysis",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Week 0: Feynman Notebook on Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "C4-L11.html#a-feynman-notebook---for-bayesian-time-series-analysis",
    "href": "C4-L11.html#a-feynman-notebook---for-bayesian-time-series-analysis",
    "title": "98  Week 0: Feynman Notebook on Bayesian Analysis",
    "section": "",
    "text": "ImportantTS Questions — A Feynman Notebook\n\n\n\nHere is where I can collect questions about TS analysis that I have as I go through the course. Hopefully I will be better equipped to answer many of these them by the end of the course.\n\n\n\nHow does Bayesian TS analysis differ from regular TS analysis?\n\nIn the NDLM we supply a Normal Prior.\nWe use Bayesian updating to update the model parameters from the data.\nRather than using a point estimate, we maintain a distributional view of the parameters. We can propagate these uncertainties into our forecasting, smoothing, and filtering distributions.\n\nFourier analysis is a powerful tool for time series analysis. How does it relate to Bayesian time series analysis?\n\nIt’s easier to discuss periodicity and frequency components rather than the fourier analysis which is a technique. However, Fourier analysis lets us consider the time series in the time domain and in the frequency domain.\nWe see in the intro that\nin a Bayesian framework. We can incorporate prior beliefs about the frequency of events.\nWe can incorporate the outcomes to incorporate seasonal elements into an NDLM\n\nHow and what type of prior knowledge into time series analysis?\n\nIn (West and Harrison 2013) they authors discuss both is the actual prior.\nBut they also talk about supporting interventions. E.g. when a major competitor goes out of business.The model should be able to handle this information and they make a big issues of how we need to incorporate into the next time step both new expected demand as well as an estimate of its variance which give better estimates of required production.\n\nAre there models that are unique to Bayesian time series analysis?\n\nHard to say but DLM seem to be.\n\nHow does distributional thinking affect time series analysis?\n\nIt gives us confidence bounds on future estimates.\n\nHow do we represent uncertainty in Bayesian time series analysis?\n\nWe have distribution and we can derive for any point estimate a corresponding credible interval.\nWe can use smoothing to try and reason about trend or seasonality separately.\n\nWill we learn about Gaussian Processes/Neural Networks in this course?\n\nThis is a type of Bayesian Non-parametric and we don’t cover these in the specialization. However Abel Rodriguez, the instructor of the third course on mixture model has a short course\nHerbert Lee wrote a Bayesian Nonparametrics via Neural Networks on the subject.\n\nWhat BTS models are most useful in economics and finance?\nIs there a cleanup procedure for time series data?\n\nUsing exponential smoothing\nUsing weighted averaging going back and forward enough steps can smooth seasonal effects.\nMore generally this is handled by smoothing\nGoing backwards this is can be done using filtering.\n\nIs there an Bayesian EDA for time series?\n\nwe can use differencing to make the time series stationary\nwe can use the ACF and PACF\nwe can decompose the time series into trend, seasonal, and residual components\nwe can visualize autocorrelation using a correlogram\nwe can visualize periodicity using a periodogram and spectral density.\nsee (Nielsen 2019)\n\nHow do we handle missing data in time series?\nHow do we handle non-stationary time series?\n\nBy applying differencing we can make the time series stationary.\n\nAre there processes for long term memory in time series?\n\nsee (Prado, Ferreira, and West 2023, 124)\nthe book also touches on EKF and MKF\n\nAre there processes for power laws.\n\n\nsee https://wiki.santafe.edu/images/5/52/Powerlaws.pdf\n\n\nCan BTS handle dynamic systems in time series?\nCan we model time series with multiple scales?\nWhat TS models are useful for regime switching?\n\nI recall this came up in Davidson-Pilon (2015)\nThis is also covered in module 3 of the course.\n\nHow can we simulate time series data?\nHow can we forecast time series data?\nHow can we find periodicity in time series data?\nIs the Kalman Filter a state-space or dynamic linear model\n\nsee (Prado, Ferreira, and West 2023, 141)\nthe book also touches on EKF and MKF\n\nParticle filters AKA Sequential Monte Carlo methods in the book.\n\nsee (Prado, Ferreira, and West 2023, 205)\n\nAre there Bayesian time series models of contagion?\nAre there Bayesian time series models of epidemics?\nWhat are Seasonal adjustments?\nHow to do a seasonal adjustment?\nWhat are the tools for wrangling and cleaning TS data.\n\nData Engineering using Wrangling and Cleaning c.f. (Woodward, Sadler, and Robertson 2022, sec. 1.4.1.3)\n\nHow to use the frequency domain as a key part of understanding and analyzing data?\nHow to assess whether an existing trend should be predicted to continue or whether caution should be used in forecasts?\nDo you really understand AR models or are they just a black box?\nWhat is a unit root?\nHow to use tools that would help you decide whether to use a seasonal model and/or include a unit root in models?\nHow to use predictor variables to help forecast some variable such as sales? (i.e. regression on time series data)\n\n\n98.1.1 Co-integration\n\nWhat are Cointegrated time series?\n\nCo-integration  is a technique used to find a long term correlation between time series processes that was introduced in 1987 by Nobel laureates Robert Engle and Clive Granger\nalso see (Pfaff 2008)\n\nWhat are some test for co-integration:\n\nEngle-Granger,\nJohansen Test,\nthe Phillips-Ouliaris test.\n\n\nmagic trick\n\n98.1.2 NN and Deep learning\n\nHow to use neural network methods to forecast time series?\n\nRNNs\nlstms\nconvolutions\nGRUs\ntransformers\nTS foundations models\nNeural Prophet citation needed\n\nDeep learning foundation models citation needed pre-train NN model with many time series. Is this a form of Bayesian time series analysis?\nHow does this BTS relate to deep learning?\n\nDiffusion models in DL are autoregressive citation needed\nthe recently the mamba architecture has been proposed which is an autoregressive state space model. citation needed\n\n\n\n\n98.1.3 Web scraping\n\nAny tips on scaraping time series data?\n\n\nWeb Scraping using Bots c.f (Woodward, Sadler, and Robertson 2022, sec. 1.4.1.4)\n\n\n\n98.1.4 Filtering & Smoothing\n\nWhat is smoothing in BTS\n\ndecomposing the series as a sum of two components: a smooth component, plus another component that includes all the features that are unexplained by the smooth component.\none way is to use a moving average.\nin the Bayesian context smoothing is the process of estimating the hidden states of a system given the observed data.\n\nWhat is filtering in Bayesian Time Series?\n\nin the Bayesian context filtering is the process of estimating the previous hidden states of a system given the observed data. I.e. a retrospective analysis to understand the process better\nwe want to sample \\mathbb{P}r(\\theta_t,k \\mid \\mathcal{D}_t)\n\nWhat is the Kalman filter?\n\nsee (Prado, Ferreira, and West 2023, 141)\nthe book also toches on EKF and MKF\n\nWhat is a particle filter?\n\nsee (Prado, Ferreira, and West 2023, sec. 6.2.2) on the The Auxiliary Particle Filter\n\nWhat is the Butterworth filter?\n\nThe Butterworth filter is a signal processing filter that has a flat response in passband (or as close as possible to flat) - making it good for cleaning up noise !?\n\n\n\n\n98.1.5 Models:\n\nWhite noise\nWiener process (random walk)\nAR(1): Autoregressive process order 1\nOrnstein–Uhlenbeck process a continuous-time version of the AR(1) process\nAR(p): Autoregressive process order p\nMA(q): Moving average process order q\nARMA(p,q): Autoregressive moving average\nSARMA: Seasonal ARMA\nARIMA: Autoregressive integrated moving average\nSARIMA: Seasonal ARIMA\nVAR: Vector autoregressive\nSVAR: structural vector autoregressive models (SVAR).\nVECM: Vector error correction models (VECM).\nGARCH: Generalized autoregressive conditional heteroskedasticity\nARCH: Autoregressive conditional heteroskedasticity\nSMC: Sequential Monte Carlo\nMDM: Multi-regression dynamic models\nLTMs: latent threshold models\nFFBS: Forward Filtering Backward Sampling\nDLM: Dynamic Linear Models\nBSTS: Bayesian Structural Time Series\n\nhttps://drive.google.com/file/d/14US56VzanuLt03XBkoAGzLy0gDEreZUc/view\n\n\nTVAR: Time-varying autoregressive models\nDGLM: Dynamic Generalized Linear Models\n\n\nWhat is the relation between Dynamic linear time series models (DLTS) models and Bayesian structural time series models (BSTS) models?\n\nDynamic Linear Time Series (DLTS) models and Bayesian Structural Time Series (BSTS) models are both frameworks for modeling time series data, and they share a strong connection, particularly in the way they approach model formulation and uncertainty. Here’s a breakdown of their relationship:\n\n\nDynamic Linear Time Series (DLTS) Models:\n\n\nDLTS models, often referred to as Dynamic Linear Models (DLMs), are a class of models where the parameters (such as the intercept or slope) evolve over time according to a stochastic process. They can be written in a state-space form, consisting of:\nObservation Equation: Relates the observed data to the hidden state.\nState Equation: Describes how the hidden state evolves over time.\nThese models use Kalman filtering for inference and prediction.\nDLTS models are flexible in handling non-stationarity and time-varying parameters.\n\n\nIn the course we primarily learned to use Bayesian methods to estimate the parameters of the model.\n\n\nBayesian Structural Time Series (BSTS) Models:\n\n\nBSTS models are a Bayesian approach to time series modeling, which generalizes the DLTS framework.\nLike DLTS, they use a state-space form, where the time series is decomposed into different components (e.g., trend, seasonality, regression effects).\nBSTS explicitly incorporates Bayesian inference, where prior distributions are placed on the model components and parameters, and inference is conducted using MCMC or other Bayesian methods.\nOne of the key advantages of BSTS is its ability to incorporate model uncertainty, allowing the user to specify structural components (such as trend or seasonality) with uncertainty about their presence or importance in the data.\n\n\nRelation Between DLTS and BSTS:\n\n\nBayesian Extension of DLTS: BSTS can be seen as a Bayesian extension of DLTS models. While DLTS uses Kalman filtering for deterministic inference, BSTS uses Bayesian methods to quantify and propagate uncertainty in model components and parameters.\nComponent Decomposition: Both models can represent the time series in terms of structural components (like trends, seasonal patterns, or covariates), but BSTS allows for more flexible modeling of these components using Bayesian priors and hierarchical structures.\nHandling of Uncertainty: DLTS models provide point estimates for parameters using Kalman filters, while BSTS incorporates full probabilistic estimates, enabling better uncertainty quantification in the presence of small data, model misspecification, or structural breaks.\nModel Complexity: BSTS models can handle more complex scenarios where the structure of the time series isn’t fully known (e.g., unknown seasonality or trends), whereas DLTS is typically used when the structure of the model (e.g., presence of trend or seasonality) is more defined.\nfor more information on BSTS",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Week 0: Feynman Notebook on Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "C4-L11.html#software",
    "href": "C4-L11.html#software",
    "title": "98  Week 0: Feynman Notebook on Bayesian Analysis",
    "section": "98.2 Software",
    "text": "98.2 Software\n\nWhat are the most useful packages in R for time series analysis?\n\nBOA Bayesian Output Analysis (BOA, Smith 2007) citation needed and\nCODA Convergence Diagnosis and Output Analysis for MCMC (CODA, Plummer, Best, Cowles, and Vines 2006). citation needed\nURCA Unit Root and Cointegration Tests for Time Series Data (URCA, Pfaff 2008)citation needed.\nVars VAR Modelling (Vars, Pfaff 2008). citation needed\nBSTS Bayesian Structural Time Series (BSTS, Scott and Varian 2014). citation needed\nCausalImpact Causal Impact Analysis (CausalImpact, Brodersen, Gallusser, Koehler, Remy, and Scott 2015). citation needed builds on BSTS and methods from this Inferring causal impact using Bayesian structural time-series models\nKFAS Kalman Filter and Smoother for Exponential Family State Space Models (KFAS, Helske 2017). citation needed\nMARSS Multivariate Autoregressive State-Space Models (MARSS, Holmes, Ward, and Scheuerell 2012). citation needed\nMCMCpack Markov Chain Monte Carlo (MCMCpack, Martin, Quinn, and Park 2011). citation needed\nMCMCglmm Markov Chain Monte Carlo Generalized Linear Mixed Models (MCMCglmm, Hadfield 2010). citation needed\nR-INLA Integrated Nested Laplace Approximations (R-INLA, Rue, Martino, and Chopin 2009). citation needed used approximate Bayesian inference for Latent Gaussian Models that can be expressed as latent Gaussian Markov random fields (GMRF)\n\nWhat about in python?\n\n\n\n\n\n\n\nDavidson-Pilon, C. 2015. Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference. Addison-Wesley Data & Analytics Series. Pearson Education. https://books.google.co.il/books?id=rMKiCgAAQBAJ.\n\n\nNielsen, A. 2019. Practical Time Series Analysis: Prediction with Statistics and Machine Learning. O’Reilly Media. https://books.google.co.il/books?id=xNOwDwAAQBAJ.\n\n\nPfaff, B. 2008. Analysis of Integrated and Cointegrated Time Series with r. Use r! Springer New York. https://books.google.co.il/books?id=ca5MkRbF3fYC.\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.\n\n\nWoodward, W. A., B. P. Sadler, and S. Robertson. 2022. Time Series for Data Science: Analysis and Forecasting. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=_W16EAAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Week 0: Feynman Notebook on Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "A01.html",
    "href": "A01.html",
    "title": "99  Appendix: Notation",
    "section": "",
    "text": "99.1 The argmax function\nParameters describe the population and are usually designated with Greek letters and the preferred letter is \\theta\n\\theta\n\\tag{99.1}\nother common parameters are: \n\\mu, \\sigma^2, \\alpha, \\beta\n\\tag{99.2}\nwhere \\mu is the mean, \\sigma^2 is the variance, \\alpha is the intercept, and \\beta is the slope in a linear regression context.\nStatistics are population estimates of parameters and are usually designated with Latin letters, such as \\hat{\\theta}.\n\\hat{p}\n\\tag{99.3}\nthe Certain event\n\\Omega\nprobability of RV X taking value x\n\\mathbb{P}r(X=x)\nodds\n\\mathcal{O}(X)\nrandom variables \nX\nX is distributed as\nX\n\\sim N(\\mu, \\sigma^2)\nX is proportional to\nX\\propto N(\\mu, \\sigma^2)\nProbability of A and B\n\\mathbb{P}r(X \\cap Y)\nConditional probability\n\\mathbb{P}r(X \\mid Y)\nJoint probability\n\\mathbb{P}r(X,Y)\nY_i \\stackrel{iid}\\sim N(\\mu, \\sigma^2)\nApproximately distributed as (say using the CLT)\nY_i \\stackrel{.}\\sim N(\\mu, \\sigma^2)\n\\mathbb{E}[X_i]\nThe expected value of an RV X set to 0 (A.K.A. a fair bet)\n\\mathbb{E}[X_i]  \\stackrel{set} = 0\nThe variance of an RV\n\\mathbb{V}ar[X_i]\nlogical implication\n\\implies\nif and only if\n\\iff\ntherefore\n\\therefore\nindependence\nA \\perp \\!\\!\\! \\perp B \\iff \\mathbb{P}r(A \\mid B) = \\mathbb{P}r(A)\nIndicator function \n\\mathbb{I}_{\\{A\\}} = \\begin{cases}\n1 & \\text{if } A \\text{ is true} \\\\ 0 & \\text{otherwise} \\end{cases}\nDirichlet function\nThis is a continuous version of the indicator function, defined as a limit.\n\\delta(x) = \\lim_{\\epsilon \\to 0} \\frac{1}{2\\epsilon} \\mathbb{I}_{\\{|x| &lt; \\epsilon\\}}\nThe Dirichlet function is used to represent a point mass at a point, often used as a component for zero inflated mixtures.\nThe following are from course 4\nWhen we want to maximize a function f(x), there are two things we may be interested in:",
    "crumbs": [
      "<span class='chapter-number'>99</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A01.html#sec-argmax-function",
    "href": "A01.html#sec-argmax-function",
    "title": "99  Appendix: Notation",
    "section": "",
    "text": "The value f(x) achieves when it is maximized, which we denote \\max_x f(x).\nThe x-value that results in maximizing f(x), which we denote \\hat x = \\arg \\max_x f(x). Thus \\max_x f(x) = f(\\hat x).",
    "crumbs": [
      "<span class='chapter-number'>99</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A01.html#sec-indicator-functions",
    "href": "A01.html#sec-indicator-functions",
    "title": "99  Appendix: Notation",
    "section": "99.2 Indicator Functions",
    "text": "99.2 Indicator Functions\nThe concept of an indicator function is a really useful one. This is a function that takes the value one if its argument is true, and the value zero if its argument is false. Sometimes these functions are called Heaviside functions or unit step functions. I write an indicator function as \\mathbb{I}_{A}(x), although sometimes they are written \\mathbb{1}_{A}(x). If the context is obvious, we can also simply write I{A}.\nExample:\n\n    \\mathbb{I}_{x&gt;3}(x)=\n    \\begin{cases}\n      0, & \\text{if}\\ x \\le 3 \\\\\n      1, & \\text{otherwise}\n    \\end{cases}\n\nNote: Indicator functions are easy to implement in code using a lambda function. They can be combined using a dictionary.\nBecause 0 · 1 = 0, the product of indicator functions can be combined into a single indicator function with a modified condition.\n\n99.2.1 Products of Indicator Functions:\n\n    \\mathbb{I}{x&lt;5} \\cdot \\mathbb{I}{x≥0} = \\mathbb{I}{0≤x&lt;5}\n\n\n    \\prod_{i=1}^N \\mathbb{I}_{(x_i&lt;2)} = \\mathbb{I}_{(x_i&lt;2) \\forall i} = \\mathbb{I}_{\\max_i x_i&lt;2}",
    "crumbs": [
      "<span class='chapter-number'>99</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A02.html",
    "href": "A02.html",
    "title": "100  Appendix: Discrete Distributions",
    "section": "",
    "text": "100.1 Discrete Uniform",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-discrete-uniform",
    "href": "A02.html#sec-discrete-uniform",
    "title": "100  Appendix: Discrete Distributions",
    "section": "",
    "text": "100.1.1 Stories\n\n\n\n\n\n\nNoteDiscrete Uniform Finite set Parametrization\n\n\n\nLet C be a finite, nonempty set of numbers and X random variable associated with the event of choosing one of these numbers uniformly at random, that is all values being equally likely X(x=c)\nThen X is said to have the Discrete Uniform distribution with parameter C.\nWe denote this by X ∼ DUnif(C).\n\n\n\n\n\n\n\n\nNoteDiscrete Uniform with Lower and Upper bound Parametrization\n\n\n\nWhen the set C above is C=\\{c \\in \\mathbb{Z} \\mid a \\le c \\le b\\ \\}.\nThen X is said to have the Discrete Uniform distribution with lower bound parameter a and upper bound parameter b.\nWe denote this by X ∼ DUnif(a,b).\n\n\n\n\n\n\n\n\nNoteUrn Model\n\n\n\nSuppose we have an urn with n balls labeled with the numbers a 1, \\dots, a_n . One drawing from the urn produces a discrete uniform random variable on the set \\{a_1, \\dots, a_n \\}.\n\n\n\n\n100.1.2 Moments\n\n\\begin{aligned}\n    \\phi_X(t)&={\\displaystyle {\\frac {e^{at}-e^{(b+1)t}}{n(1-e^{t})}}}  && \\text{(MGF)}\\\\  \n    \\mathbb{E}[X] &= \\frac{a + b}{2} && \\text{(Expectation)} \\\\\n    \\mathbb{V}ar[X] &= \\frac{(b - a + 1)^2 - 1}{12} && \\text{(Variance)}\n\\end{aligned}\n\\tag{100.1}\n\n\n100.1.3 Probability mass function (PMF)\n\nf(x \\mid a, b) = \\frac{1}{b - a + 1}\n\n\n\n100.1.4 Cumulative distribution function (CDF)\n\nF(x \\mid a, b) = \\frac{\\lfloor x \\rfloor - a - 1}{b - a + 1} \\\\\n\\text{where} \\lfloor x \\rfloor \\text{ is the floor function (rounds down reals to nearest smaller integer)}\n\n\n\n100.1.5 Prior\nSince a number of families have the uniform as a special case we can use them as priors when we want a uniform prior:",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-bernoulli-distribution",
    "href": "A02.html#sec-bernoulli-distribution",
    "title": "100  Appendix: Discrete Distributions",
    "section": "100.2 Bernoulli Distribution",
    "text": "100.2 Bernoulli Distribution\n. \n\n100.2.1 Stories\n\n\n\n\n\n\nNote\n\n\n\nThe Bernoulli distribution arises when modeling the outcome of a binary event called a Bernoulli trial.\nLet X be the indicator variable corresponding to the success of getting “heads” in a “coin toss”, with a coin that has probability of success p for getting “heads”.\nThen X has a Bernoulli Distribution with parameter p\nWe denote this as X \\sim Bern(p)\n\n\n\n\n100.2.2 Parameters\nBecause of this story, the parameter p is often called the success probability of the Bern(p) distribution.\n\n\n100.2.3 Examples\n\n\n\n\n\n\nNote\n\n\n\n\nfair coin toss\nunfair coin toss\nad click\nweb site conversion\ndeath or survival of a patient in a medical trial\nindicator random variable\n\n\n\n\n\n100.2.4 Checklist\n\n\n\n\n\n\nNote\n\n\n\n\nDiscrete data\nA single trial\nOnly two trial outcomes: success and failure (These do not need to literally represent successes and failures, but this shorthand is typically used.)\n\n\n\n\n\\begin{aligned}\nX &\\sim Bernoulli(p)\\\\\n  & \\sim Bern(p)\\\\\n  & \\sim B(p)  \n\\end{aligned}\n\\tag{100.2}\n\n\n100.2.5 Moments\n\nM_X(t)=q+pe^{t} \\qquad \\text{(MGF)}\n\\tag{100.3}\n\n\\mathbb{E}[X]= p \\qquad \\text{(Expectation)}\n\\tag{100.4}\n\n\\mathbb{V}ar[x]= \\mathbb{P}r(1-p) \\qquad \\text{(Variance)}\n\\tag{100.5}\n\n\n100.2.6 PMF\nWhere parameter p is the probability of getting heads.\nThe probability for the two events is:\n\n\\mathbb{P}r(X=1) = p \\qquad \\mathbb{P}r(X=0)=1-p\n\n\n{\\displaystyle {\\begin{cases}1-p&{\\text{if }}k=0 \\\\\np&{\\text{if }}k=1\\end{cases}}}  \\qquad \\text{(PMF)}\n\\tag{100.6}\n\n\n100.2.7 CDF\n\n{\\displaystyle {\\begin{cases}0&{\\text{if }}k&lt;0  \\\\\n1-p&{\\text{if }}0\\leq k&lt;1 \\\\\n1&{\\text{if }}k\\geq 1\\end{cases}}} \\qquad \\text{(CDF)}\n\\tag{100.7}\n\n\n100.2.8 Likelihood\n\nL(\\theta) = \\prod p^x(1-p)^{1-x} \\mathbb{I}_{[0,1]}(x)  \\qquad \\text{(Likelihood)}\n\\tag{100.8}\n\n\\mathcal{L}(\\theta) =log(p) \\sum x + log(1-p)\\sum (1-x)  \\qquad \\text{(Log Likelihood)}\n\\tag{100.9}\n\n\n100.2.9 Entropy and Information\n\n\\mathbb{H}(x)= -q \\ln(q)- p \\ln(p) \\qquad \\text{(Entropy)}\n\\tag{100.10}\n\n\\mathcal{I}[X]\\frac{1}{\\mathbb{P}r(1-p)} \\qquad \\text{(Fisher Information)}\n\\tag{100.11}\n\nBeta(x) \\qquad \\text{(Conjugate Prior)}\n\\tag{100.12}\n\n\n100.2.10 Usage\n\n\n\nTable 100.1: Usage of Bernoulli\n\n\n\n\n\nPackage\nSyntax\n\n\n\n\nNumPy\nrg.choice([0, 1], p=[1-theta, theta])\n\n\nSciPy\nscipy.stats.bernoulli(theta)\n\n\nStan\nbernoulli(theta)\n\n\n\n\n\n\n\n\n100.2.11 Plots\n\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=0.30, var=0.21, skew=0.87, kurt=-1.24\n\nx = np.arange(bernoulli.ppf(0.01, p),\n              bernoulli.ppf(0.99, p))\nax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\n\nrv = bernoulli(p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n## Generate random numbers\nr = bernoulli.rvs(p, size=10)\nr\n\narray([0, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n\n\n\n\n\n\nA Swiss stamp issueed in 1994 depicting Mathematician Jakob Bernouilli and the formula and diagram of the law of large numbers\n\n\n\n\n\n\n\nTipBiographical note on Jacob Bernoulli\n\n\n\n\nIt seems that to make a correct conjecture about any event whatever, it is necessary to calculate exactly the number of possible cases and then to determine how much more likely it is that one case will occur than another. (Bernoulli 1713)\n\nThe Bernoulli distribution as well as The Binomial distribution are due to Jacob Bernoulli (1655-1705) who was a prominent mathematician in the Bernoulli family. He discovered the fundamental mathematical constant e. With his brother Johann, he was among the first to develop Leibniz’s calculus, introducing the word integral and applying it to polar coordinates and the study of curves such as the catenary, the logarithmic spiral and the cycloid\nHis most important contribution was in the field of probability, where he derived the first version of the law of large numbers (LLN). The LLN is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and will tend to become closer to the expected value as more trials are performed. A special form of the LLN (for a binary random variable) was first proved by Jacob Bernoulli. It took him over 20 years to develop sufficiently rigorous mathematical proof.\n\nFor a more extensive biography visit the following link\n\n\nThe Bernoulli distribution is built on a trial of a coin toss (possibly biased).\n\nWe use the Bernoulli distribution to model a random variable for the probability of such a coin toss trial.\nWe use the Binomial distribution to model a random variable for the probability of getting k heads in N independent trails.",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-binomial-distribution",
    "href": "A02.html#sec-binomial-distribution",
    "title": "100  Appendix: Discrete Distributions",
    "section": "100.3 Binomial distribution",
    "text": "100.3 Binomial distribution\n\n100.3.1 Stories\n\n\n\n\n\n\nNote\n\n\n\n\n\\overbrace{\\underbrace{\\fbox{0}\\ \\ldots \\fbox{0}}_{N_0}\\ \\underbrace{\\fbox{1}\\ \\ldots \\fbox{1}}_{N_1}}^N\n\\tag{100.13}\nThe Binomial distribution arises when we conduct multiple independent Bernoulli trials and wish to model X the number of successes in Y_i\\mid \\theta identically distributed Bernoulli trials with the same probability of success \\theta. If n independent Bernoulli trials are performed, each with the same success probability p. The distribution of X is called the Binomial distribution with parameters n and p. We write X \\sim \\text{Bin}(n, p) to mean that X has the Binomial distribution with parameters n and p, where n is a positive integer and 0 &lt; p &lt; 1.\n\n\n\n\n100.3.2 Parameters\n\n\\theta - the probability of success in the Bernoulli trials\nN - the total number of trials being conducted\n\n\n\n100.3.3 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nDiscrete data\nTwo possible outcomes for each trial\nEach trial is independent and\nThe probability of success/failure is the same in each trial\nThe outcome is the aggregate number of successes\n\n\n\n\n\n100.3.4 Examples\n\nto model the aggregate outcome of clinical drug trials,\nto estimate the proportion of the population voting for each political party using exit poll data (where there are only two political parties).\n\n\nX \\sim Bin[n,p]\n\\tag{100.14}\n\nf(X=x \\mid \\theta) = {n \\choose x} \\theta^x(1-\\theta)^{n-x}\n\\tag{100.15}\n\nL(\\theta)=\\prod_{i=1}^{n} {n\\choose x_i}  \\theta ^ {x_i} (1− \\theta) ^ {(n−x_i)}\n\\tag{100.16}\n\n\\begin{aligned}\n\\ell( \\theta) &= \\log \\mathcal{L}( \\theta) \\\\\n              &= \\sum_{i=1}^n \\left[\\log {n\\choose x_i} + x_i \\log  \\theta + (n-x_i)\\log (1- \\theta) \\right]\n\\end{aligned}\n\\tag{100.17}\n\n\\mathbb{E}[X]= N \\times  \\theta\n\\tag{100.18}\n\n\\mathbb{V}ar[X]=N \\cdot \\theta \\cdot (1-\\theta)\n\\tag{100.19}\n\n\\mathbb{H}(X) = \\frac{1}{2}\\log_2 \\left (2\\pi n \\theta(1 - \\theta)\\right) + O(\\frac{1}{n})\n\\tag{100.20}\n\n\\mathcal{I}(\\theta)=\\frac{n}{ \\theta \\cdot (1- \\theta)}\n\\tag{100.21}\n\n\n100.3.5 Usage\n\n\n\nTable 100.2: Usage of Binomial\n\n\n\n\n\nPackage\nSyntax\n\n\n\n\nNumPy\nrg.binomial(N, theta)\n\n\nSciPy\nscipy.stats.binom(N, theta)\n\n\nStan\nbinomial(N, theta)\n\n\n\n\n\n\n\n\n100.3.6 Relationships\n\n\n\nbinomial distribution relations\n\n\n The Binomial Distribution is related to\n\nThe Binomial is a special case of the Multinomial distribution with K =2 (two categories).\nthe Poisson distribution distribution. If X \\sim Binomial(n, p) rv and Y \\sim Poisson(np) distribution then \\mathbb{P}r(X = n) ≈ \\mathbb{P}r(Y = n) for large n and small np.\nThe Bernoulli distribution is a special case of the the Binomial distribution \n\\begin{aligned}\nX & \\sim \\mathrm{Binomial}(n=1, p) \\\\\n\\implies X &\\sim \\mathrm{Bernoulli}(p)\n\\end{aligned}\n\nthe Normal distribution If X \\sim \\mathrm{Binomial}(n, p) RV and Y \\sim \\mathcal{N}(\\mu=np,\\sigma=n\\mathbb{P}r(1-p)) then for integers j and k, \\mathbb{P}r(j ≤ X ≤ k) ≈ \\mathbb{P}r(j – 1/2 ≤ Y ≤ k + 1/2). The approximation is better when p ≈ 0.5 and when n is large. For more information, see normal approximation to the Binomial\nThe Binomial is a limit of the Hypergeometric. The difference between a binomial distribution and a hypergeometric distribution is the difference between sampling with replacement and sampling without replacement. As the population size increases relative to the sample size, the difference becomes negligible. So If X \\sim \\mathrm{Binomial}(n, p) RV and Y \\sim \\mathrm{HyperGeometric}(N,a,b) then\n\n\\lim_{n\\to \\infty} X = Y\n\n\n\n100.3.7 Plots\n\nimport numpy as np\nfrom scipy.stats import binom\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\nn, p = 5, 0.4\nmean, var, skew, kurt = binom.stats(n, p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=2.00, var=1.20, skew=0.18, kurt=-0.37\n\nx = np.arange(binom.ppf(0.01, n, p),\n              binom.ppf(0.99, n, p))\nax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\nax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\nrv = binom(n, p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n## generate random numbers\nr = binom.rvs(n, p, size=10)\nr\n\narray([1, 1, 2, 2, 4, 1, 3, 2, 1, 2])\n\n\nfrom __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport numpy as np\nimport scipy\nfrom scipy.special import gamma, factorial, comb\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\n#pyo.init_notebook_mode()\nINTERACT_FLAG=False\ndef binomial_vector_over_y(theta, n):\n    total_events = n\n    y =  np.linspace(0, total_events , total_events + 1)\n    p_y = [comb(int(total_events), int(yelem)) * theta** yelem * (1 - theta)**(total_events - yelem) for yelem in y]\n\n    fig = px.line(x=y, y=p_y, color_discrete_sequence=[\"steelblue\"], \n                  height=600, width=800, title=\" Binomial distribution for theta = %lf, n = %d\" %(theta, n))\n    fig.data[0].line['width'] = 4\n    fig.layout.xaxis.title.text = \"y\"\n    fig.layout.yaxis.title.text = \"P(y)\"\n    fig.show()\n    \nif(INTERACT_FLAG):    \n    interact(binomial_vector_over_y, theta=0.5, n=15)\nelse:\n    binomial_vector_over_y(theta=0.5, n=10)\n\n\n\n\n\n\n\nVideo 100.1: Binomial Distribution",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-hypergeometric-distribution",
    "href": "A02.html#sec-hypergeometric-distribution",
    "title": "100  Appendix: Discrete Distributions",
    "section": "100.4 Hypergeometric distribution",
    "text": "100.4 Hypergeometric distribution\n\n100.4.1 story 1 - Urn Model\nThe beta-binomial distribution with parameters \\alpha success rate and \\beta failure and n the number of trials can be motivated by an Pólya urn model.\nImagine a trial in which a ball is drawn without replacement from urn containing \\alpha white balls and \\beta black balls. If this is repeated n times, then the probability of observing x white balls follows a hypergeometric distribution with parameters n, \\alpha and \\beta.\nNote: is we used a\nIf the random draws are with simple replacement (no balls over and above the observed ball are added to the urn), then the distribution follows a binomial distribution and if the random draws are made without replacement, the distribution follows a hypergeometric distribution.\n\n\n\n\n\n\n\nVideo 100.2: The Hypergeometric Distribution\n\n\n\n\n100.4.2 Examples\n\nk white balls from an in Urn without replacement\ncapture-recapture\nAces in a poker hand\n\n\n\n100.4.3 Story\nConsider an urn with w white balls and b black balls. We draw n balls out of the urn at random without replacement, such that all w+b samples are equally likely. Let X be the number of white balls in n the sample. Then X is said to have the Hypergeometric distribution with parameters w, b, and n; we denote this by X \\sim \\mathcal{HG}(w, b, n) or X \\sim \\mathrm{HyperGeom}(w, b, n) or",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-poisson-distribution",
    "href": "A02.html#sec-poisson-distribution",
    "title": "100  Appendix: Discrete Distributions",
    "section": "100.5 Poisson distribution",
    "text": "100.5 Poisson distribution\n\n100.5.1 Stories\n\n\n\n\n\n\nNotePoisson Parametrization\n\n\n\nThe Poisson distribution arises when modeling the number of successes of independent and identically distributed (IID) events in a fixed interval of time or space, occurring at a constant rate \\lambda. Let X represent the count of the number of phone calls received at a call center in a given interval, such as an hour, with the parameter \\lambda corresponding to the average rate at which events occur in that interval. Then X is said to have the Poisson distribution with parameter \\lambda, and we denote this as X \\sim \\mathrm{Pois}(\\lambda).\n\nX \\sim \\mathrm{Pois}(\\lambda)\n\\tag{100.22}\n\n\n\n\n\n\n\n\n\nVideo 100.3: The Poisson Distribution\n\n\n\n\n100.5.2 Checklist\n\nCount of discrete events\nIndividual events occur at a given rate and independently of other events\nFixed amount of time or space in which the events can occur\n\n\n\n100.5.3 Examples\n\nThe number of emails you receive in an hour. There are a lot of people who could potentially email you at that hour, but it is unlikely that any specific person will actually email you at that hour. Alternatively, imagine subdividing the hour into milliseconds. There are 3.6×106 seconds in an hour, but in any specific millisecond, it is unlikely that you will get an email.\nThe number of chips in a chocolate chip cookie. Imagine subdividing the cookie into small cubes; the probability of getting a chocolate chip in a single cube is small, but the number of cubes is large.\nThe number of earthquakes in a year in some regions of the world. At any given time and location, the probability of an earthquake is small, but there are a large number of possible times and locations for earthquakes to occur over the course of the year.\nCount of component failures per week\nestimating the failure rate of artificial heart valves,\nestimating the prevalence of violent crimes in different districts,\napproximating the binomial which is, itself, being used to explain the prevalence of autism in the UK.\n\n\n\n100.5.4 Moments\n\n\\mathrm{E}(X) = \\lambda\n\n\n\\mathrm{V}ar(X) = \\lambda\n\n\n\n100.5.5 Probability mass function (PMF)\n\nf(x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\tag{100.23}\n\n\n100.5.6 Cumulative distribution function (CDF)\n\nF(x \\mid \\lambda) = \\frac{\\Gamma(\\lfloor x+1\\rfloor,\\lambda)}{\\lfloor x \\rfloor !} \\qquad \\text{CDF}\n\\tag{100.24}\n\n\\text{where }\\Gamma(u,v)=\\int_{v}^{\\infty}t^{u-1}e^{-t} \\mathrm{d}t \\text{ is the upper incomplete gamma function}\n\\tag{100.25}\n\n\\text{and } \\lfloor x \\rfloor \\text{ is the floor function (rounds down reals to nearest smaller integer)}\n\\tag{100.26}",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-geometric-distribution",
    "href": "A02.html#sec-geometric-distribution",
    "title": "100  Appendix: Discrete Distributions",
    "section": "100.6 Geometric distribution",
    "text": "100.6 Geometric distribution\n\n100.6.1 Stories\n\n\n\n\n\n\nNoteGeometric Distribution Failures before success\n\n\n\nConsider a sequence of independent Bernoulli trials, each with the same success probability p \\in (0, 1), with trials performed until a success occurs. Let X be the number of failures before the first successful trial. Then X has the Geometric distribution with parameter p; we denote this by X \\sim Geom(p).\nFor example, if we flip a fair coin until it lands Heads for the first time, then the number of Tails before the first occurrence of Heads is distributed as Geom(1/2).\nTo get the Geometric PMF from the story, imagine the Bernoulli trials as a string of 0’s (failures) ending in a single 1 (success). Each 0 has probability q = 1 − p and the final 1 has probability p, so a string of k failures followed by one success has probability q^kp.\n\n\n\n\n\n\n\n\nNoteGeometric distribution Failures and success\n\n\n\nConsider a sequence of independent Bernoulli trials, each with the same success probability p \\in (0, 1), with trials performed until a success occurs. Let X be the number of failures before the first successful trial. Then X has the Geometric distribution with parameter p; we denote this by X \\sim Geom(p).\nFor example, if we flip a fair coin until it lands Heads for the first time, then the number of Tails before the first occurrence of Heads is distributed as Geom(1/2).\nTo get the Geometric PMF from the story, imagine the Bernoulli trials as a string of 0’s (failures) ending in a single 1 (success). Each 0 has probability q = 1 − p and the final 1 has probability p, so a string of k failures followed by one success has probability q^kp.\n\n\n\n\n100.6.2 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nDiscrete data\nTwo possible outcomes for each trial\nEach trial is independent and\nThe probability of success/failure is the same in each trial\nThe outcome is the count of failures before the first success\n\n\n\n\n\n100.6.3 Examples\n\nConsider polymerization of an actin filament. At each time step, an actin monomer may add to the end of the filament (“failure”), or an actin monomer may fall off the end (“success”) with (usually very low) probability θ. The length of actin filaments, measured in a number of constitutive monomers, is Geometrically distributed.\n\nThe Geometric distribution arises when we want to know “What is the number of Bernoulli trials required to get the first success?”, i.e., the number of Bernoulli events until a success is observed, such as the probability of getting the first head when flipping a coin. It takes values on the positive integers starting with one (since at least one trial is needed to observe a success).\n\n\n\n\n\n\n\nVideo 100.4: The Geometric Distribution\n\n\n\nX \\sim Geo(p)\n\\tag{100.27}\n\n\n100.6.4 Moments\n\n\\mathbb{M}_X[t] = \\frac{pe^t}{1-(1-p)e^t} \\qquad t&lt;-ln(1-p)\n\\tag{100.28}\n\n\\mathbb{E}[X] = \\frac{1}{p}\n\\tag{100.29}\n\n\\mathbb{V}ar[X]=\\frac{1-p}{p^2}\n\\tag{100.30}\n\n\n100.6.5 PMF\n\n\\mathbb{P}r(X = x \\mid p) = \\mathbb{P}r(1-p)^{x-1} \\qquad \\forall x \\in N;\\quad 0\\le p \\le 1\n\\tag{100.31}\n\n\n100.6.6 CDF\n\n1-(1-p)^{\\lfloor x\\rfloor } \\qquad x&lt;1\n\\tag{100.32}\n\n\n100.6.7 Memoryless property\n\nThe geometric distribution is based on geometric series.\nThe geometric distribution has the memoryless property:\n\nP (X &gt; s \\mid X &gt;  t) = P (X &gt; s − t)\n\nOne can say that the distribution “forgets” what has occurred, so that The probability of getting an additional s − t failures, having already observed t failures, is the same as the probability of observing s − t failures at the start of the sequence. In other words, the probability of getting a run of failures depends only on the length of the run, not on its position.\nY=X-1 is the \\text{negative binomial}(1,p)\n\n\n100.6.8 Worked out Examples\n\nExample 100.1 (Geometric Distribution) The Geometric distribution arises when we consider how long we will have to “wait for a success” during repeated Bernoulli trials.\nWhat is the probability that we flip a fair coin four times and don’t see any heads?\nThis is the same as asking what is \\mathbb{P}r(X &gt; 4) where X ∼ Geo(1/2).\n\n  \\begin{aligned}\n    \\mathbb{P}r(X &gt; 4) &= 1 − \\mathbb{P}r(X =1)−\\mathbb{P}r(X = 2)−\\mathbb{P}r(X = 3)−\\mathbb{P}r(X = 4) \\\\\n    &= 1−(\\frac{1}{2})−(\\frac{1}{2})(\\frac{1}{2})−(\\frac{1}{2})(\\frac{1}{2})^2−(\\frac{1}{2})(\\frac{1}{2})^3  \\\\\n   &= \\frac{1}{16}\n    \\end{aligned}\n\nOf course, we could also have just computed it directly, but here we see an example of using the geometric distribution and we can also see that we got the right answer.\n\n\n\n100.6.9 Plots\n\nimport numpy as np\nfrom scipy.stats import geom\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\np = 0.5\nmean, var, skew, kurt = geom.stats(p,moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=2.00, var=2.00, skew=2.12, kurt=6.50\n\nx = np.arange(geom.ppf(0.01, p),\n              geom.ppf(0.99, p))\nax.plot(x, geom.pmf(x, p), 'bo', ms=8, label='geom pmf')\nax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=5, alpha=0.5)\n\nrv = geom(p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\nr = geom.rvs(p,size=10)\nr\n\narray([ 1, 10,  2,  2,  1,  2,  3,  1,  1,  2])",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-negative-binomial-distribution",
    "href": "A02.html#sec-negative-binomial-distribution",
    "title": "100  Appendix: Discrete Distributions",
    "section": "100.7 Negative Binomial Distribution",
    "text": "100.7 Negative Binomial Distribution\n\n100.7.1 Story\n\n\n\n\n\n\nNote\n\n\n\nIn a sequence of independent Bernoulli trials with success probability p, if X is the number of failures before the rth success, then X is said to have the Negative Binomial distribution with parameters r and p, denoted X \\sim NBin(r, p).\n\n\nBoth the Binomial and the Negative Binomial distributions are based on independent Bernoulli trials; they differ in the stopping rule and in what they are counting.\nThe Binomial counts the number of successes in a fixed number of trials; the Negative Binomial counts the number of failures until a fixed number of successes.\nIn light of these similarities, it comes as no surprise that the derivation of the Negative Binomial PMF bears a resemblance to the corresponding derivation for the Binomial.\n\n\n100.7.2 Parameters\n\nr the number of successes.\np the probability of the Bernoulli trial.\n\n\n\n100.7.3 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nCount of discrete events\nNon-independent events; it is sometimes said that the events can exhibit contagion, meaning that if one event occurs, it is more likely that another will also occur\nCan model a data-generating process where the variance exceeds the mean\nFixed amount of time or space in which the events can occur\n\n\n\n\n\n100.7.4 Examples\n\nStamp collection - Suppose there are n types of stamps, which you are collecting one by one, with the goal of getting a complete set. When collecting stamps, the stamp types are random. Assume that each time you collect a stamp, it is equally likely to be any of the n types. What is the expected number of toys needed until you have a complete set?\neverything the Poisson can do and more,\nto model the number of measles cases that occur on an island,\nthe number of banks that collapse in a financial crisis.\nthe length of a hospital stay\nthe probability you will have to visit Y houses if you must sell r cookies before returning home\n\n\n\n100.7.5 Moments\n\n\\mathrm{E}(X) = \\lambda\n\n\nvar(X) = \\lambda + \\frac{\\lambda^2}{\\kappa}\n\n\n\n100.7.6 Probability mass function (PMF)\n\nf(x \\mid \\lambda,\\kappa) = \\frac{\\Gamma(x+\\kappa)}{x!\\Gamma(\\kappa+1)}\\left(\\frac{\\lambda}{\\lambda+\\kappa}\\right)^x \\left(\\frac{\\kappa}{\\lambda+\\kappa}\\right)^\\kappa\n\n\n\n100.7.7 Cumulative distribution function (CDF)\n\nF(x \\mid \\lambda,\\kappa) =\n\\begin{cases}\n  I_{\\frac{\\kappa}{\\kappa+\\lambda}}(\\kappa,1+\\lfloor x \\rfloor), & x \\ge q 0 \\\\\n  0,                                                             & \\text{Otherwise}\n\\end{cases}\n\n\n\\text{where } I_w(u,v) \\text{ is the regularised incomplete beta function: }\nI_w(u,v) = \\frac{B(w; u, v)}{B(u,v)}\n\n\n\\text{where } B(w; u,v)=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the incomplete beta function and }\\\\\nB(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the complete beta function}",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-multinomial-distribution",
    "href": "A02.html#sec-multinomial-distribution",
    "title": "100  Appendix: Discrete Distributions",
    "section": "100.8 Multinomial Distribution",
    "text": "100.8 Multinomial Distribution\nThe Multinomial distribution is a generalization of the Binomial. Whereas the Binomial distribution counts the successes in a fixed number of trials that can only be categorized as success or failure, the Multinomial distribution keeps track of trials whose outcomes can fall into multiple categories, such as excellent, adequate, poor; or red, yellow, green, blue.\n\n100.8.1 Story\nMultinomial distribution. Each of N objects is independently placed into one of k categories. An object is placed into category j with probability p_j ,P where the p_j are non-negative and \\sum^k_{j=1} p_j = 1. Let X_1 be the number of objects in category 1, X_2 the number of objects in category 2, etc., so that X_1 + \\dots + X_k = n. Then X = (X_1 , \\dots , X_k ) is said to have the Multinomial distribution with parameters n and p = (p_1 , \\dots , p_k ). We write this as X \\sim Mult_k(n, p).\nWe call X a random vector because it is a vector of random variables. The joint PMF of X can be derived from the story.\n\n\n100.8.2 Examples\n\nBlood type counts across n individuals\nNumbers of people voting for each party in a sample\n\n\n\n100.8.3 Moments\n\n\\mathrm{E}(X_i) = n p_i \\text{, }\\forall i\n\n\nvar(X_i) = n p_i (1-p_i) \\text{, }\\forall i\n\n\ncov(X_i,X_j) = -n p_i p_j \\text{, }\\forall i\\neq j\n\n\n\n100.8.4 Probability Mass Function (PMF)\n\nf(x_1,x_2,\\dots,x_d \\mid n,p_1,p_2,\\dots,p_d) = \\frac{n!}{x_1 ! x_2 ! \\dots x_d !} p_1^{x_1} p_2^{x_2}\\dots p_d^{x_d}",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#beta-binomial",
    "href": "A02.html#beta-binomial",
    "title": "100  Appendix: Discrete Distributions",
    "section": "100.9 Beta Binomial",
    "text": "100.9 Beta Binomial\n\n100.9.1 Story 1 - Polya Urn Model\nThe beta-binomial distribution with parameters \\alpha success rate and \\beta failure and n the number of trials can be motivated by an Pólya urn model.\nImagine an urn containing \\alpha red balls and \\beta black balls, where random draws are made. If a red ball is observed, then two red balls are returned to the urn. Likewise, if a black ball is drawn, then two black balls are returned to the urn. If this is repeated n times, then the probability of observing x red balls follows a beta-binomial distribution with parameters n, \\alpha and \\beta.\nIf the random draws are with simple replacement (no balls over and above the observed ball are added to the urn), then the distribution follows a binomial distribution and if the random draws are made without replacement, the distribution follows a hypergeometric distribution.\n\n\n100.9.2 Story 2 compound distribution\nThe Beta distribution is a conjugate distribution of the binomial distribution. This fact leads to an analytically tractable compound distribution constructed in a hierarchical fashion where one can think of the p parameter in the binomial distribution as being randomly drawn from a beta distribution.\nSuppose we were interested in predicting the number of heads, x in n future trials. This is given by\n\n{\\displaystyle {\n    \\begin{aligned}\n        f(x\\mid n,\\alpha ,\\beta )&=\\int _{0}^{1}\\mathrm {Bin} (x \\mid n,p)\\mathrm {Beta} (p\\mid \\alpha ,\\beta )\\,dp\\\\\n                            [6pt]&={n \\choose x}{\\frac {1}{\\mathrm {B} (\\alpha ,\\beta )}}\\int _{0}^{1}p^{x+\\alpha -1}(1-p)^{n-x+\\beta -1}\\,dp \\\\\n                            [6pt]&={n \\choose x}{\\frac {\\mathrm {B} (x+\\alpha ,n-x+\\beta )}{\\mathrm {B} (\\alpha ,\\beta )}}.\n    \\end{aligned}}}\n\n\n{\\displaystyle f(x\\mid n,\\alpha ,\\beta )={\\frac {\\Gamma (n+1)}{\\Gamma (x+1)\\Gamma (n-x+1)}}{\\frac {\\Gamma (x+\\alpha )\\Gamma (n-x+\\beta )}{\\Gamma (n+\\alpha +\\beta )}}{\\frac {\\Gamma (\\alpha +\\beta )}{\\Gamma (\\alpha )\\Gamma (\\beta )}}.}\n\n\n\n100.9.3 Moments\n\n\\mathrm{E}(X) = \\frac{n\\alpha}{\\alpha+\\beta}\n \nvar(X) = \\frac{n\\alpha\\beta(\\alpha+\\beta+n)}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\n\n\n\n100.9.4 Probability mass function (PMF)\n\nf(x \\mid n,\\alpha,\\beta) = \\binom{n}{x}\\frac{B(x+\\alpha,n-x+\\beta)}{B(\\alpha,\\beta)}\n\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the (complete) beta function }\n\n\n\n100.9.5 Cumulative distribution function (CDF)\n\nF(x\\mid n,\\alpha,\\beta) = \\begin{cases}\n0, & x&lt;0 \\\\\n\\binom{n}{x}\\frac{B(x+\\alpha,n-x+\\beta)}{B(\\alpha,\\beta)} {}_{3}F_2(1,-x,n-x+\\beta;n-x-1,1-x-\\alpha;1), & 0\\leq x \\leq n \\\\\n1, & x&gt;n \\end{cases}\n\n\n\\text{where } {}_{3}F_2(a,b,x) \\text{ is the generalised hypergeometric function}\n\n\n\n100.9.6 Relations\n\nThe Pascal distribution (after Blaise Pascal) is special cases of the negative binomial distribution. Used with an integer-valued stopping-time parameter r\nThe Pólya distribution (for George Pólya) is special cases of the negative binomial distribution. Used with a real-valued-valued stopping-time parameter r\n\n\n\n\n\nA photo of Hungarian Mathematician George Pólya\n\n\n\n\n\n\n\nTipBiographical note on George Pólya\n\n\n\n\nThe cookbook gives a detailed description of ingredients and procedures but no proofs for its prescriptions or reasons for its recipes; the proof of the pudding is in the eating … Mathematics cannot be tested in exactly the same manner as a pudding; if all sorts of reasoning are debarred, a course of calculus may easily become an incoherent inventory of indigestible information. (Polya 1945)\n\nPólya was arguably the most influential mathematician of the 20th century. His basic research contributions span complex analysis, mathematical physics, probability theory, geometry, and combinatorics. He was a teacher par excellence who maintained a strong interest in pedagogical matters throughout his long career.\nHe was awarded a doctorate in mathematics having studied, essentially without supervision, a problem in the theory of geometric probability. Later Pólya looked at the Fourier transform of a probability measure, showing in 1923 that it was a characteristic function. He wrote on the normal distribution and coined the term “central limit theorem” in 1920 which is now standard usage.\nIn 1921 he proved his famous theorem on random walks on an integer lattice. He considered a d-dimensional array of lattice points where a point moves to any of its neighbors with equal probability. He asked whether given an arbitrary point A in the lattice, a point executing a random walk starting from the origin would reach A with probability 1. Pólya’s surprising answer was that it would for d=1 and for d=2, but it would not for d\\ge 3. In later work he looked at two points executing independent random walks and also at random walks satisfying the condition that the moving point never passed through the same lattice point twice.\nOne of Pólya’s notable achievements was his collaboration with the economist Abraham Wald during World War II. They developed statistical techniques to solve military problems, including estimating enemy troop movements and predicting the effectiveness of bombing missions. These contributions played a vital role in aiding the Allies during the war.\nHis book “How to Solve It,” published in 1945, presented problem-solving heuristics applicable to various mathematical domains, including probability and statistics. This influential work emphasized the importance of understanding the problem, devising a plan, executing the plan, and reflecting on the results. Pólya’s problem-solving strategies continue to be widely taught and practiced.\n\nFor a more extensive biography visit the following link\n\n\n\n\n\n\n\n\nBernoulli, J. 1713. Ars Conjectandi [the Art of Conjecturing]. Impensis Thurnisiorum. https://books.google.co.il/books?id=Ba5DAAAAcAAJ.\n\n\nPolya, G. 1945. How to Solve It. Princeton University Press. https://doi.org/10.1515/9781400828678.",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html",
    "href": "A03.html",
    "title": "101  Appendix: Continuous Distributions",
    "section": "",
    "text": "101.1 The Continuous Uniform\nFollowing a subjective view of distribution, which is more amenable to reinterpretation I use an indicator function to place restrictions on the range of parameter of the PDF.",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-continuous-uniform",
    "href": "A03.html#sec-continuous-uniform",
    "title": "101  Appendix: Continuous Distributions",
    "section": "",
    "text": "101.1.1 Stories\n\n\n\n\n\n\nNoteDiscrete Uniform Finite set Parametrization\n\n\n\n\n\n\n\nX \\sim U[\\alpha,\\beta]\n\\tag{101.1}\n\n\n101.1.2 Moments\n\n\\mathbb{E}[X]=\\frac{(\\alpha+\\beta)}{2}\n\\tag{101.2}\n\n\\mathbb{V}ar[X]=\\frac{(\\beta-\\alpha)^2}{12}\n\\tag{101.3}\n\n\n101.1.3 Probability mass function (PDF)\n\nf(x)= \\frac{1}{\\alpha-\\beta} \\mathbb{I}_{\\{\\alpha \\le x \\le \\beta\\}}(x)\n\\tag{101.4}\n\n\n101.1.4 Cumulative distribution function (CDF)\n\nF(x\\mid \\alpha,\\beta)=\\begin{cases}\n  0,  & \\text{if }x &lt; \\alpha \\\\\n  \\frac{x-\\alpha}{\\beta-\\alpha}, & \\text{if } x\\in [\\alpha,\\beta]\\\\\n  1, & \\text{if } x &gt; \\beta\n  \\end{cases}\n\\tag{101.5}\n\n\n101.1.5 Prior\nSince a number of families have the uniform as a special case we can use them as priors when we want a uniform prior:\nNormal(0,1)= Beta(1,1)",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-beta-distribution",
    "href": "A03.html#sec-the-beta-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.2 The Beta Distribution",
    "text": "101.2 The Beta Distribution\n\n\n101.2.1 Story\nThe Beta distribution is used for random variables which take on values between 0 and 1. For this reason (and other reasons we will see later in the course), the Beta distribution is commonly used to model probabilities.\n\nX \\sim Beta(\\alpha, \\beta)\n\\tag{101.6}\n\n\n101.2.2 PDF & CDF\n\nf(x \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha−1}(1 − x)^{\\beta−1}\\mathbb{I}_{x\\in(0,1)}\\mathbb{I}_{\\alpha\\in\\mathbb{R}^+}\\mathbb{I}_{\\beta\\in\\mathbb{R}^+} \\qquad \\text{(PDF)}\n\\tag{101.7}\n\n\\begin{aligned}\n                 & F(x \\mid \\alpha,\\beta) &= I_x(\\alpha,\\beta) && \\text{(CDF)} \\\\\n   \\text{where } & I_w(u,v) & &&\\text{ is the regularized beta function: } \\\\\n                 & I_w(u,v) &= \\frac{B(w; u, v)}{B(u,v)} \\\\\n    \\text{where }& B(w; u,v) &=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t  && \\text{ is the incomplete beta function  }\\\\\n    \\text{and }  & B(u,v)& && \\text{ is the (complete) beta function}\n\\end{aligned}\n\\tag{101.8}\n\n\n101.2.3 Moments\n\n\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha + \\beta} \\qquad (\\text{expectation})\n\\tag{101.9}\n\n\\mathbb{V}ar[X] = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)} \\qquad (\\text{variance})\n\\tag{101.10}\n\n\\mathbb{M}_X(t) = 1+ \\sum^\\infty_{i=1} \\left ( {\\prod^\\infty_{j=0} \\frac{\\alpha+j}{\\alpha + \\beta + j} } \\right ) \\frac{t^i}{i!}\n\\tag{101.11}\nwhere \\Gamma(·) is the Gamma function introduced with the gamma distribution.\nNote also that \\alpha &gt; 0 and \\beta &gt; 0.\n\n\n101.2.4 Relations\n\n\n\nRelations of the Beta distribution\n\n\nThe standard Uniform(0, 1) distribution is a special case of the beta distribution with \\alpha = \\beta = 1.\n\nUniform(0, 1) = Beta(1,1)\n\\tag{101.12}\n\n\n101.2.5 As a prior\nThe Beta distribution is often used as a prior for parameters that are probabilities,since it takes values from 0 and 1.\nDuring prior elicitation the parameters can be set using\n\nthe mean: \\alpha \\over \\alpha +\\beta which I would interpret here as count of successes over trials prior to seeing the data.\nvariance: Equation 101.10 or\nThe effective sample size which is \\alpha+\\beta (see course 1 lesson 7.3 for the derivation).",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-cauchy",
    "href": "A03.html#sec-cauchy",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.3 The Cauchy Distribution",
    "text": "101.3 The Cauchy Distribution\n\n\n101.3.1 PDF\n\n\\text{Cauchy}(y\\mid\\mu,\\sigma) = \\frac{1}{\\pi \\sigma} \\\n\\frac{1}{1 + \\left((y - \\mu)/\\sigma\\right)^2} \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{101.13}\n\n\n101.3.2 CDF\n\nF(x \\mid \\mu, \\sigma) = \\frac{1}{2} + \\frac{1}{\\pi}\\text{arctan}\\left(\\frac{x-\\mu}{\\sigma}\\right) \\qquad \\text{(CDF)}\n\\tag{101.14}\n\n\\mathbb{E}(X) = \\text{ undefined}\n\n\n\\mathbb{V}ar[X] = \\text{ undefined}\n\n\n\n101.3.3 As a prior\n\nThe Cauchy despite having no mean or variance is recommended as a prior for regression coefficients in Logistic regression. see (Gelman et al. 2008) this is analyzed and discussed in (Ghosh, Li, and Mitra 2018)",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-double-exponential",
    "href": "A03.html#sec-double-exponential",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.4 Double Exponential Distribution (Laplace)",
    "text": "101.4 Double Exponential Distribution (Laplace)\n\n\n\\text{DoubleExponential}(y \\mid \\mu,\\sigma) =\n\\frac{1}{2\\sigma} \\exp \\left( - \\, \\frac{\\|y - \\mu\\|}{\\sigma} \\right)\n\\qquad \\text (PDF)\n\\tag{101.15}",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-gamma-distribution",
    "href": "A03.html#sec-the-gamma-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.5 The Gamma Distribution",
    "text": "101.5 The Gamma Distribution\n\n\n101.5.1 Story\nIf X_1, X_2, ..., X_n are independent (and identically distributed \\mathrm{Exp}(\\lambda)) waiting times between successive events, then the total waiting time for all n events to occur Y = \\sum X_i will follow a gamma distribution with shape parameter \\alpha = n and rate parameter \\beta = \\lambda:\nWe denote this as:\n\nY =\\sum^N_{i=0} \\mathrm{Exp}_i(\\lambda) \\sim \\mathrm{Gamma}(\\alpha = N, \\beta = \\lambda)\n\\tag{101.16}\n\n\n101.5.2 PDF\n\nf(y \\mid \\alpha , \\beta) = \\frac{\\beta^\\alpha} {\\Gamma(\\alpha)} y^{\\alpha−1} e^{− \\beta y} \\mathbb{I}_{y \\ge \\theta }(y)\n\\tag{101.17}\n\n\n101.5.3 Moments\n\n\\mathbb{E}[Y] = \\frac{\\alpha}{ \\beta}\n\\tag{101.18}\n\n\\mathbb{V}ar[Y] = \\frac{\\alpha}{ \\beta^2}\n\\tag{101.19}\nwhere \\Gamma(·) is the gamma function, a generalization of the factorial function which can accept non-integer arguments. If n is a positive integer, then \\Gamma(n) = (n − 1)!.\nNote also that \\alpha &gt; 0 and $ &gt; 0$.\n\n\n101.5.4 Relations\n\n\n\nRelations of the Gamma Distribution\n\n\nThe exponential distribution is a special case of the Gamma distribution with \\alpha = 1. The gamma distribution commonly appears in statistical problems, as we will see in this course. It is used to model positive-valued, continuous quantities whose distribution is right-skewed. As \\alpha increases, the gamma distribution more closely resembles the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#inverse-gamma-distribution",
    "href": "A03.html#inverse-gamma-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.6 Inverse Gamma Distribution",
    "text": "101.6 Inverse Gamma Distribution\n\n\n101.6.1 PDF\n\n\\mathcal{IG}(y\\mid\\alpha,\\beta) =\n\\frac{1} {\\Gamma(\\alpha)}\\frac{\\beta^{\\alpha}}{y^{\\alpha + 1}}  e^{- \\frac{ \\beta}{y}}\n   \\ \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{\\beta \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)\n\\tag{101.20}\n\n\n101.6.2 Moments\n\n\\mathbb{E}[X]=\\frac{\\beta}{\\alpha - 1} \\qquad \\text{Expectation}\n\\tag{101.21}\n\n\\mathbb{V}ar[X]=\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}\\qquad \\text{Variance}\n\\tag{101.22}",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#the-z-or-standard-normal-distribution",
    "href": "A03.html#the-z-or-standard-normal-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.7 The Z or Standard normal distribution",
    "text": "101.7 The Z or Standard normal distribution\n· The Standard normal distribution is given by:\n\nZ \\sim \\mathcal{N}[1,0]\n\\tag{101.23}\n\nf(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}}\n\\tag{101.24}\n\n\\mathcal{L}(\\mu,\\sigma)=\\prod_{i=1}^{n}{1 \\over 2 \\pi \\sigma}e^{−(x_i−\\mu)^2 \\over 2 \\sigma^2}\n\\tag{101.25}\n\n\\begin{aligned}\n\\ell(\\mu, \\sigma) &= \\log \\mathcal{L}(\\mu, \\sigma) \\\\\n&= -\\frac{n}{2}\\log(2\\pi) - n\\log\\sigma - \\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\n\\end{aligned}\\sigma^2\n\\tag{101.26}\n\n\\begin{aligned}\n  \\mathbb{E}(Z)&= 0 \\quad \\text{(Expectation)} \\qquad  \\mathbb{V}ar(Z)&= 1 \\quad \\text{(Variance)}\n\\end{aligned}\n\\tag{101.27}",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-normal-distribution",
    "href": "A03.html#sec-the-normal-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.8 The Normal Distribution",
    "text": "101.8 The Normal Distribution\n The normal, or Gaussian distribution is one of the most important distributions in statistics.\nIt arises as the limiting distribution of sums (and averages) of random variables. This is due to the Section 104.1. Because of this property, the normal distribution is often used to model the “errors,” or unexplained variations of individual observations in regression models.\nNow consider X = \\sigma Z+\\mu where \\sigma &gt; 0 and \\mu is any real constant. Then \\mathbb{E}[X] = \\mathbb{E}[\\sigma Z+\\mu] = \\sigma E[Z] + \\mu = \\sigma_0 + \\mu = \\mu and \\mathbb{V}ar[X] = Var(\\sigma^2 Z + \\mu) = \\sigma^2 Var(Z) + 0 = \\sigma^2 \\cdot 1 = \\sigma^2\nThen, X follows a normal distribution with mean \\mu and variance \\sigma^2 (standard deviation \\sigma) denoted as\n\nX \\sim N[\\mu,\\sigma^2]\n\\tag{101.28}\n\n101.8.1 PDF\n\nf(x\\mid\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}  e^{-\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}(x-\\mu)^2}\n\\tag{101.29}\n\n\n101.8.2 Moments\n\n\\mathbb{E}(x)= \\mu\n\\tag{101.30}\n\nVar(x)= \\sigma^2\n\\tag{101.31}\n\nThe normal distribution is symmetric about the mean \\mu and is often described as a bell-shaped curve.\nAlthough X can take on any real value (positive or negative), more than 99% of the probability mass is concentrated within three standard deviations of the mean.\n\nThe normal distribution has several desirable properties.\nOne is that if X_1 \\sim N(\\mu_1, \\sigma^2_1) and X_2 ∼ N(\\mu_2, \\sigma^2_2) are independent, then X_1+X_2 \\sim N(\\mu_1+\\mu_2, \\sigma^2_1+\\sigma^2_2).\nConsequently, if we take the average of n Independent and Identically Distributed (IID) Normal random variables we have:\n\n\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim N(\\mu, \\frac{\\sigma^2}{n})\n\\tag{101.32}\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = norm.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=0.00, var=1.00, skew=0.00, kurt=0.00\n\nx = np.linspace(norm.ppf(0.01),\n                norm.ppf(0.99), 100)\nax.plot(x, norm.pdf(x),\n       'r-', lw=5, alpha=0.6, label='norm pdf')\n\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nr = norm.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n(array([0.00737665, 0.00368832, 0.01106497, 0.05901319, 0.06270151,\n       0.10696141, 0.12909135, 0.23236443, 0.25080606, 0.35776746,\n       0.31719589, 0.35039081, 0.36145579, 0.37989741, 0.32088422,\n       0.2581827 , 0.20285784, 0.12909135, 0.05532487, 0.03688324,\n       0.02581827, 0.0147533 , 0.00368832, 0.00737665, 0.00368832]), array([-3.19698904, -2.92586321, -2.65473738, -2.38361155, -2.11248572,\n       -1.84135989, -1.57023406, -1.29910823, -1.0279824 , -0.75685656,\n       -0.48573073, -0.2146049 ,  0.05652093,  0.32764676,  0.59877259,\n        0.86989842,  1.14102425,  1.41215008,  1.68327591,  1.95440174,\n        2.22552757,  2.4966534 ,  2.76777923,  3.03890506,  3.31003089,\n        3.58115672]), [&lt;matplotlib.patches.Polygon object at 0x73ba9cfc1e40&gt;])\n\nax.set_xlim([x[0], x[-1]])\n\n(-2.3263478740408408, 2.3263478740408408)\n\nax.legend(loc='best', frameon=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-t-distribution",
    "href": "A03.html#sec-the-t-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.9 The t-Distribution",
    "text": "101.9 The t-Distribution\n If we have normal data, we can use (Equation 101.32) to help us estimate the mean \\mu. Reversing the transformation from the previous section, we get:\n\n\\frac {\\hat X - \\mu}{\\sigma / \\sqrt(n)} \\sim N(0, 1)\n\\tag{101.33}\nHowever, we may not know the value of \\sigma. If we estimate it from data, we can replace it with S = \\sqrt{\\sum_i \\frac{(X_i-\\hat X)^2}{n-1}}, the sample standard deviation. This causes the expression (Equation 101.33) to no longer be distributed as a Standard Normal; but as a standard t-distribution with ν = n − 1 degrees of freedom\n\nX \\sim t[\\nu]\n\\tag{101.34}\nf(t\\mid\\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}}\\left (1 + \\frac{t^2}{\\nu}\\right)^{-(\\frac{\\nu+1}{2})}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{101.35}\n\n\\text{where }\\Gamma(w)=\\int_{0}^{\\infty}t^{w-1}e^{-t}\\mathrm{d}t \\text{ is the gamma function}\n\nf(t\\mid\\nu)={\\frac {1}{{\\sqrt {\\nu }}\\,\\mathrm {B} ({\\frac {1}{2}},{\\frac {\\nu }{2}})}}\\left(1+{\\frac {t^{2}}{\\nu }}\\right)^{-(\\nu +1)/2}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{101.36}\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the beta function}\n\n\\begin{aligned}\n&& F(t)&=\\int _{-\\infty }^{t}f(u)\\,du=1-{\\tfrac {1}{2}}I_{x(t)}\\left({\\tfrac {\\nu }{2}},{\\tfrac {1}{2}}\\right) &&\\text{(CDF)} \\\\\n   \\text{where } && I_{x(t)}&= \\frac{B(x; u, v)}{B(u,v)}                 &&\\text{is the regularized Beta function} \\\\\n   \\text{where } && B(w; u,v)&=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t &&  \\text{ is the incomplete Beta function } \\\\\n   \\text {and }  && B(u,v)&=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t    && \\text{ is the (complete) beta function}\n\\end{aligned}\n\\tag{101.37}\n\\int _{-\\infty }^{t}f(u)\\,du={\\tfrac {1}{2}}+t{\\frac {\\Gamma \\left({\\tfrac {1}{2}}(\\nu +1)\\right)}{{\\sqrt {\\pi \\nu }}\\,\\Gamma \\left({\\tfrac {\\nu }{2}}\\right)}}\\,{}_{2}F_{1}\\left({\\tfrac {1}{2}},{\\tfrac {1}{2}}(\\nu +1);{\\tfrac {3}{2}};-{\\tfrac {t^{2}}{\\nu }}\\right)\n\n\n\\mathcal{L}(\\mu, \\sigma, \\nu) = \\prod_{i=1}^n \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right)^{-\\frac{\\nu+1}{2}}\n\\tag{101.38}\n\n\\begin{aligned}\n\\ell(\\mu, \\sigma, \\nu) &= \\log \\mathcal{L}(\\mu, \\sigma, \\nu) \\\\\n&= \\sum_{i=1}^n \\left[\\log\\Gamma\\left(\\frac{\\nu+1}{2}\\right) - \\log\\sqrt{\\nu\\pi} - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\frac{\\nu+1}{2}\\log\\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right)\\right] \\\\\n&= n\\log\\Gamma\\left(\\frac{\\nu+1}{2}\\right) - n\\log\\sqrt{\\nu\\pi} - n\\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\frac{\\nu+1}{2}\\sum_{i=1}^n\\log\\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right).\n\\end{aligned}\n\\tag{101.39}\n\n\\mathbb{E}[Y] = 0 \\qquad \\text{ if } \\nu &gt; 1\n\\tag{101.40}\n\n\\mathbb{V}ar[Y] = \\frac{\\nu}{\\nu - 2} \\qquad \\text{ if } \\nu &gt; 2\n\\tag{101.41}",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#location-scale-parametrization-t-distribution",
    "href": "A03.html#location-scale-parametrization-t-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.10 Location Scale Parametrization t-distribution",
    "text": "101.10 Location Scale Parametrization t-distribution\n\nX=\\mu+\\sigma T\n\nThe resulting distribution is also called the non-standardized Student’s t-distribution.\nthis is another parameterization of the student-t with:\n\nlocation \\mu \\in \\mathbb{R}^+\nscale \\sigma \\in \\mathbb{R}^+\ndegrees of freedom \\nu \\in \\mathbb{R}^+\n\n\nf(x \\mid \\mu, \\sigma, \\nu) = \\frac{\\left(\\frac{\\nu }{\\nu +\\frac{(x-\\mu )^2}{\\sigma ^2}}\\right)^{\\frac{\\nu+1}{2}}}{\\sqrt{\\nu } \\sigma  B\\left(\\frac{\\nu }{2},\\frac{1}{2} \\right)}\n\\tag{101.42}\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the beta function}\n\n\nF(\\mu, \\sigma, \\nu) =\n\\begin{cases}\n\\frac{1}{2} I_{\\frac{\\nu  \\sigma ^2}{(x-\\mu )^2+\\nu  \\sigma  ^2}}\\left(\\frac{\\nu }{2},\\frac{1}{2}\\right),                & x\\leq \\mu  \\\\\n\\frac{1}{2} \\left(I_{\\frac{(x-\\mu )^2}{(x-\\mu )^2+\\nu  \\sigma   ^2}}\\left(\\frac{1}{2},\\frac{\\nu }{2}\\right)+1\\right), & \\text{Otherwise}\n\\end{cases}\n\\tag{101.43}\nwhere I_w(u,v) is the regularized incomplete beta function:\n\nI_w(u,v) = \\frac{B(w; u, v)}{B(u,v)}\n\nwhere B(w; u,v) is the incomplete beta function:\n\nB(w; u,v) =\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t\n\nAnd B(u,v) is the (complete) beta function\n\n\\mathbb{E}[X] = \\begin{cases}\n  \\mu,               & \\text{if }\\nu &gt; 1  \\\\\n  \\text{undefined} & \\text{ otherwise}\n\\end{cases}\n\\tag{101.44}\n\n\\mathbb{V}ar[X] = \\frac{\\nu \\sigma^2}{\\nu-2}\n\\tag{101.45}\nThe t distribution is symmetric and resembles the Normal Distribution but with thicker tails. As the degrees of freedom increase, the t distribution looks more and more like the standard normal distribution.\n\n\n\n\n\n\n\nFigure 101.1: William Sealy Gosset AKA Student\n\n\n\n\n\n\n\n\nTipHistorical Note on The William Sealy Gosset A.K.A Student\n\n\n\n The student-t distribution is due to Gosset, William Sealy (1876-1937) who was an English statistician, chemist and brewer who served as Head Brewer of Guinness and Head Experimental Brewer of Guinness and was a pioneer of modern statistics. He is known for his pioneering work on small sample experimental designs. Gosset published under the pseudonym “Student” and developed most famously Student’s t-distribution – originally called Student’s “z” – and “Student’s test of statistical significance”.\nHe was told to use a Pseudonym and choose ‘Student’ after a predecessor at Guinness published a paper that leaked trade secrets. Gosset was a friend of both Karl Pearson and Ronald Fisher. Fisher suggested a correction to the student-t using the degrees of freedom rather than the sample size. Fisher is also credited with helping to publicize its use.\nfor a full biography see (Pearson et al. 1990)",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-exponential-distribution",
    "href": "A03.html#sec-the-exponential-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.11 The Exponential Distribution",
    "text": "101.11 The Exponential Distribution\n\n\n101.11.1 Story\nThe Exponential distribution models the waiting time between events for events with a rate lambda. Those events, typically, come from a Poisson process\nThe exponential distribution is often used to model the waiting time between random events. Indeed, if the waiting times between successive events are independent then they form an Exp(λ) distribution. Then for any fixed time window of length t, the number of events occurring in that window will follow a Poisson distribution with mean tλ.\n\nX \\sim Exp[\\lambda]\n\\tag{101.46}\n\n\n101.11.2 PDF\n\nf(x \\mid \\lambda) = \\frac{1}{\\lambda} e^{- \\frac{x}{\\lambda}}(x)\\mathbb{I}_{\\lambda\\in\\mathbb{R}^+ } \\mathbb{I}_{x\\in\\mathbb{R}^+_0 } \\quad \\text{(PDF)}\n\\tag{101.47}\n\n\n101.11.3 CDF\n\nF(x \\mid \\lambda) = 1 - e^{-\\lambda x} \\qquad \\text{(CDF)}\n\n\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda x_i}\n\\tag{101.48}\n\n\\begin{aligned} \\ell(\\lambda) &= \\log \\mathcal{L}(\\lambda) \\\\\n&= \\sum_{i=1}^n \\log(\\lambda) - \\lambda x_i \\\\\n&= n\\log(\\lambda) - \\lambda\\sum_{i=1}^n x_i \\end{aligned}\n\\tag{101.49}\n\n\n101.11.4 Moments\n\n\\mathbb{E}(x)= \\lambda\n\\tag{101.50}\n\n\\mathbb{V}ar[X]= \\lambda^2\n\\tag{101.51}\n\n\\mathbb{M}_X(t)= \\frac{1}{1-\\lambda t} \\qquad t &lt; \\frac{1}{\\gamma}\n\\tag{101.52}\n\n\n101.11.5 Special cases:\n \n\nWeibull Y = X^{\\frac{1}{\\gamma}}\nRayleigh Y = \\sqrt{\\frac{2X}{\\lambda}}\nGumbel Y=\\alpha - \\gamma \\log(\\frac{X}{\\lambda})\n\n\n\n101.11.6 Properties:\n\nmemoryless\n\n\nimport numpy as np\nfrom scipy.stats import expon\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = expon.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=1.00, var=1.00, skew=2.00, kurt=6.00\n\nx = np.linspace(expon.ppf(0.01), expon.ppf(0.99), 100)\nax.plot(x, expon.pdf(x), 'r-', lw=5, alpha=0.6, label='expon pdf')\n\nrv = expon()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\nr = expon.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n(array([0.79628554, 0.68381583, 0.60283764, 0.44088126, 0.37789822,\n       0.32391276, 0.29692003, 0.15745759, 0.15745759, 0.12596607,\n       0.10347213, 0.05848425, 0.06298304, 0.04498788, 0.08547698,\n       0.02249394, 0.02249394, 0.02699273, 0.0404891 , 0.00899758,\n       0.        , 0.01349637, 0.00899758, 0.00449879, 0.        ,\n       0.00449879, 0.01349637, 0.        , 0.        , 0.        ,\n       0.00899758, 0.        , 0.        , 0.00449879]), array([2.39229972e-03, 2.24674371e-01, 4.46956443e-01, 6.69238515e-01,\n       8.91520587e-01, 1.11380266e+00, 1.33608473e+00, 1.55836680e+00,\n       1.78064887e+00, 2.00293095e+00, 2.22521302e+00, 2.44749509e+00,\n       2.66977716e+00, 2.89205923e+00, 3.11434130e+00, 3.33662338e+00,\n       3.55890545e+00, 3.78118752e+00, 4.00346959e+00, 4.22575166e+00,\n       4.44803373e+00, 4.67031581e+00, 4.89259788e+00, 5.11487995e+00,\n       5.33716202e+00, 5.55944409e+00, 5.78172617e+00, 6.00400824e+00,\n       6.22629031e+00, 6.44857238e+00, 6.67085445e+00, 6.89313652e+00,\n       7.11541860e+00, 7.33770067e+00, 7.55998274e+00]), [&lt;matplotlib.patches.Polygon object at 0x73ba9cedc940&gt;])\n\nax.set_xlim([x[0], x[-1]])\n\n(0.010050335853501442, 4.605170185988091)\n\nax.legend(loc='best', frameon=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-lognormal-distribution",
    "href": "A03.html#sec-lognormal-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.12 LogNormal Distribution",
    "text": "101.12 LogNormal Distribution\nThe long normal arises when the a log transform is applied to the normal distribution.\n\n\n\\text{LogNormal}(y\\mid\\mu,\\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\ \\sigma} \\, \\frac{1}{y} \\ \\exp \\! \\left( - \\, \\frac{1}{2} \\, \\left( \\frac{\\log y - \\mu}{\\sigma} \\right)^2 \\right) \\ \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)\n\\tag{101.53}",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-pareto-distribution",
    "href": "A03.html#sec-pareto-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.13 Pareto Distribution",
    "text": "101.13 Pareto Distribution\n\n\n\\text{Pareto}(y|y_{\\text{min}},\\alpha) = \\frac{\\displaystyle\n\\alpha\\,y_{\\text{min}}^\\alpha}{\\displaystyle y^{\\alpha+1}} \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{y_{min} \\in \\mathbb{R}^+}\\mathbb{I}_{y\\ge y_{min} \\in \\mathbb{R}^+}\n\\qquad \\text (PDF)\n\\tag{101.54}\n\n\\mathrm{Pareto\\_Type\\_2}(y|\\mu,\\lambda,\\alpha) = \\\n\\frac{\\alpha}{\\lambda} \\, \\left( 1+\\frac{y-\\mu}{\\lambda}\n\\right)^{-(\\alpha+1)} \\! \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\lambda \\in \\mathbb{R}^+}\\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{y\\ge \\mu \\in \\mathbb{R}}\n\\qquad \\text (PDF)\n\\tag{101.55}\n\n\\mathbb{E}[X]=\\displaystyle{\\frac{\\alpha y_\\mathrm{min}}{\\alpha - 1}}\\mathbb{I}_{\\alpha&gt;1} \\qquad \\text (expectation)\n\\tag{101.56}\n\n\\mathbb{V}ar[X]=\\displaystyle{\\frac{\\alpha y_\\mathrm{min}^2}{(\\alpha - 1)^2(\\alpha - 2)}}\\mathbb{I}_{\\alpha&gt;2} \\qquad \\text (variance)\n\\tag{101.57}",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-weibull-distribution",
    "href": "A03.html#sec-weibull-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.14 Weibull Distribution",
    "text": "101.14 Weibull Distribution\n\n\n101.14.1 PDF\n\n\\text{Weibull}(y|\\alpha,\\sigma) =\n\\frac{\\alpha}{\\sigma} \\, \\left( \\frac{y}{\\sigma} \\right)^{\\alpha - 1} \\, e^{ - \\left( \\frac{y}{\\sigma} \\right)^{\\alpha}} \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-chi-squared-distribution",
    "href": "A03.html#sec-chi-squared-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.15 Chi Squared Distribution",
    "text": "101.15 Chi Squared Distribution\n\nThe chi squared distribution is a special case of the gamma. It is widely used in hypothesis testing and the construction of confidence intervals. It is parameterized using parameter \\nu for the degrees of predom\n\n101.15.1 PDF:\n\n\\text{ChiSquare}(y\\mid\\nu) = \\frac{2^{-\\nu/2}}     {\\Gamma(\\nu / 2)} \\,\ny^{\\nu/2 - 1} \\, \\exp \\! \\left( -\\, \\frac{1}{2} \\, y \\right) \\mathbb{I}_{\\nu \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}} \\qquad \\text (PDF)\n\\tag{101.58}\n\n\n101.15.2 CDF:\n\n{\\frac {1}{\\Gamma (\\nu/2)}}\\;\\gamma \\left({\\frac {\\nu}{2}},\\,{\\frac {x}{2}}\\right) \\qquad \\text (CDF)\n\\tag{101.59}\n\n\n101.15.3 MOMENTS\n\n\\mathbb{E}[X]=\\nu\n\\tag{101.60}\n\n\\mathbb{V}ar[X] = 2\\nu\n\\tag{101.61}",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-logistic-distribution",
    "href": "A03.html#sec-logistic-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.16 Logistic Distribution",
    "text": "101.16 Logistic Distribution\n\n\\text{Logistic}(y|\\mu,\\sigma) = \\frac{1}{\\sigma} \\\n\\exp\\!\\left( - \\, \\frac{y - \\mu}{\\sigma} \\right) \\ \\left(1 + \\exp\n\\!\\left( - \\, \\frac{y - \\mu}{\\sigma} \\right) \\right)^{\\!-2} \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}}  \\qquad \\text (PDF)\n\\tag{101.62}",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-f-distribution",
    "href": "A03.html#sec-f-distribution",
    "title": "101  Appendix: Continuous Distributions",
    "section": "101.17 F Distribution",
    "text": "101.17 F Distribution\n   The F-distribution or F-ratio, arises frequently as the null distribution of a test statistic, in the analysis of variance (ANOVA) and other F-tests.F DistributionF-ratio\n\n101.17.1 PDF\n\n\\frac {\\sqrt {\\frac {(d_{1}x)^{d_{1}}d_{2}^{d_{2}}}{(d_{1}x+d_{2})^{d_{1}+d_{2}}}}}{x\\,\\mathrm {B} \\!\\left({\\frac {d_{1}}{2}},{\\frac {d_{2}}{2}}\\right)}\n\\tag{101.63}\n\n\n101.17.2 CDF\n\n\\mathbb{I}_{\\frac {d_{1}x}{d_{1}x+d_{2}}}\\left({\\tfrac {d_{1}}{2}},{\\tfrac {d_{2}}{2}}\\right)\n\\tag{101.64}\n\n\n101.17.3 Moments\n\n\\mathbb{E}[X]=\\frac {d_{2}}{d_{2}-2}\n\\tag{101.65}\n\n\\mathbb{V}ar[X] = {\\frac {2\\,d_{2}^{2}\\,(d_{1}+d_{2}-2)}{d_{1}(d_{2}-2)^{2}(d_{2}-4)}}\n\\tag{101.66}\n\n\n\n\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” The Annals of Applied Statistics 2 (4). https://doi.org/10.1214/08-aoas191.\n\n\nGhosh, Joyee, Yingbo Li, and Robin Mitra. 2018. “On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression.” Bayesian Analysis 13 (2). https://doi.org/10.1214/17-ba1051.\n\n\nPearson, E. S., W. S. Gosset, R. L. Plackett, and G. A. Barnard. 1990. Student: A Statistical Biography of William Sealy Gosset. Clarendon Press. https://books.google.co.il/books?id=LBDvAAAAMAAJ.",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A05.html",
    "href": "A05.html",
    "title": "102  Appendix: Exponents & Logarithms",
    "section": "",
    "text": "102.1 Exponents\nExponents are of the form a^x where:\nRecall that a^0 = 1. Exponents have the following useful properties\nNote: that the first property requires that both terms have the same base a.\nWe cannot simplify a^x ·b^y if a \\ne b.",
    "crumbs": [
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A05.html#sec-exponents",
    "href": "A05.html#sec-exponents",
    "title": "102  Appendix: Exponents & Logarithms",
    "section": "",
    "text": "a (called the base) and\nx (called the exponent) is any real number.\n\n\n\na^x· a^y = a^{x+y}\n(a^x)^y = a^{x·y}\n\n\n\n\nOne common base is the number e which is approximately equal to 2.7183.\nThe function e^x is so common in mathematics and has its own symbol e^x = \\exp(x).\nBecause e &gt; 0 we have e^x &gt; 0 for all real numbers x\n\\lim_{x \\to \\infty} x = e^{−x} = 0.",
    "crumbs": [
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A05.html#sec-natural-logarithms",
    "href": "A05.html#sec-natural-logarithms",
    "title": "102  Appendix: Exponents & Logarithms",
    "section": "102.2 Natural Logarithms",
    "text": "102.2 Natural Logarithms\nWe will need to manipulate long products of probabilities. Since there often comprise small fractions, their calculation on computers can be problematic due to the underflow of floats. We will therefore prefer to convert these products into sums of logarithms.\n\nDefinition 102.1 (The Logarithm) A log is the inverse of a power. We can use (Equation 102.1).\n\ny = a^x \\implies log_a(y) = x\n\\tag{102.1}\n\n\nDefinition 102.2 (The Natural log) The natural logarithm function has base e and is written without the subscript\n\nlog_e(y) = log(y)\n\\tag{102.2}\n\n\nTheorem 102.1 (Logs take positive values) logs only exist for values greater than 0\n\n\\forall x(e^x &gt; 0) \\implies \\exists \\log(y) \\iff {y &gt; 0}\n\n\nWe can use the properties of exponents from the previous section to obtain some important properties of logarithms:\n\nDefinition 102.3 (Log of a product) we can use Equation 102.3 to convert a log of a product to a sum of logs.\n\n\\log(x·y) = \\log(x) + \\log(y)\n\\tag{102.3}\n\n\nDefinition 102.4 (Log of a quotient) we can use Equation 102.4 to convert a log of a quotient to a difference of logs.\n\n\\log(\\frac{x}{y}) = log(x) − log(y)\n\\tag{102.4}\n\n\nDefinition 102.5 (Log of a power) we can use Equation 102.5 to convert a log of a variable raised to a power into the product.\n\n    \\log(x^b) = b \\cdot log(x)\n\\tag{102.5}\n\n\nDefinition 102.6 (Log of one) we can use (Equation 102.6) to replace a log of 1 with zero since $x(x^0 = 1) $\n\n    \\log(1)=0\n\\tag{102.6}\n\n\n102.2.1 Log of exponent\nwe can use (Equation 102.7) to cancel a log of an exponent since the log is the inverse function of the exponent.\n\nexp(log(y)) = log(exp(y)) = y\n\\tag{102.7}\n\nExample 102.1 (Logarithm) \n    \\begin{aligned}\n    log \\frac{5^2}{10}= 2 log(5) − log(10) ≈ 0.916.\n    \\end{aligned}\n\n\n\nDefinition 102.7 (Change of base for a log) we can use (Equation 102.8) to change the base of a logarithm.\n\n    \\log_b(a)=\\frac{\\log_c(a)}{\\log_c(n)}\n\\tag{102.8}\n\n\nDefinition 102.8 (Derivative of a Log) we can use (Equation 102.9) to differentiate a log.\n\n    \\frac{d}{dx} \\log_(x)=\\frac{1}{x}\n\\tag{102.9}\n\n\nBecause the natural logarithm is a monotonically increasing one-to-one function, finding the x which maximizes any (positive-valued function) f(x) is equivalent to maximizing log(f(x)).\nThis is useful because we often take derivatives to maximize functions.\nIf f(x) has product terms, then log(f(x)) will have summation terms, which are usually simpler when taking derivatives.",
    "crumbs": [
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A07.html",
    "href": "A07.html",
    "title": "103  Appendix: The Law of Large Numbers",
    "section": "",
    "text": "103.1 Law of large numbers\nSuppose we observe data D=\\{x_1, \\ldots, x_n\\} with each x_i \\sim F .\nBy the strong law of large numbers the empirical distribution \\hat{F}_n based on data D=\\{x_1, \\ldots, x_n\\} converges to the true underlying distribution F as n \\rightarrow \\infty almost surely:\n\\hat{F}_n\\overset{a. s.}{\\to} F\nThe Glivenko–Cantelli asserts that the convergence is uniform. Since the strong law implies the weak law we also have convergence in probability:\n\\hat{F}_n\\overset{P}{\\to} F\nCorrespondingly, for n \\rightarrow \\infty the average \\text{E}_{\\hat{F}_n}(h(x)) = \\frac{1}{n} \\sum_{i=1}^n h(x_i) converges to the expectation \\text{E}_{F}(h(x)) .",
    "crumbs": [
      "<span class='chapter-number'>103</span>  <span class='chapter-title'>Appendix: The Law of Large Numbers</span>"
    ]
  },
  {
    "objectID": "A08.html",
    "href": "A08.html",
    "title": "104  Appendix: The Central Limit Theorem",
    "section": "",
    "text": "104.1 Central Limit Theorem\nThe Central Limit Theorem is one of the most important results in statistics, stating that with sufficiently large sample sizes, the sample average approximately follows a normal distribution. This underscores the importance of the normal distribution, as well as most of the methods commonly used which make assumptions about the data being normally distributed.\nLet’s first stop and think about what it means for the sample average to have a distribution. Imagine going to the store and buying a bag of your favorite brand of chocolate chip cookies. Suppose the bag has 24 cookies in it. Will each cookie have the exact same number of chocolate chips in it? It turns out that if you make a batch of cookies by adding chips to dough and mixing it really well, then putting the same amount of dough onto a baking sheet, the number of chips per cookie closely follows a Poisson distribution. (In the limiting case of chips having zero volume, this is exactly a Poisson process.) Thus we expect there to be a lot of variability in the number of chips per cookie. We can model the number of chips per cookie with a Poisson distribution. We can also compute the average number of chips per cookie in the bag. For the bag we have, that will be a particular number. But there may be more bags of cookies in the store. Will each of those bags have the same average number of chips? If all of the cookies in the store are from the same industrial-sized batch, each cookie will individually have a Poisson number of chips. So the average number of chips in one bag may be different from the average number of chips in another bag. Thus we could hypothetically find out the average number of chips for each bag in the store. And we could think about what the distribution of these averages is, across the bags in the store, or all the bags of cookies in the world. It is this distribution of averages that the central limit theorem says is approximately a normal distribution, with the same mean as the distribution for the individual cookies, but with a standard deviation that is divided by the square root of the number of samples in each average (i.e., the number of cookies per bag).",
    "crumbs": [
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>Appendix: The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "A08.html#sec-cl-theorem",
    "href": "A08.html#sec-cl-theorem",
    "title": "104  Appendix: The Central Limit Theorem",
    "section": "",
    "text": "Theorem 104.1 (Central Limit Theorem) Let X_1, ..., X_n be independent and identically distributed (IID) with \\mathbb{E}(X_i) = \\mu and Var(X_i) = \\sigma^2 &lt;\\infty\nThen:\n\n\\lim_{n\\to\\infty} \\sqrt{n} \\sum_{i=0}^{n} \\frac{1}{n}\\frac{(X_i-\\mu)}{\\sigma} = \\sum_{i=0}^{n} \\frac{X_i-\\mu}{\\sqrt{n} \\sigma} = N(0, 1)\n\nThat is, \\hat{X_n} is approximately normally distributed with mean µ and variance \\frac{\\sigma}{2/n} or standard deviation \\frac{\\sigma}{\\sqrt{n}}.",
    "crumbs": [
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>Appendix: The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "A09.html",
    "href": "A09.html",
    "title": "105  Appendix: Conjugate Priors",
    "section": "",
    "text": "105.1 Conjugate Priors",
    "crumbs": [
      "<span class='chapter-number'>105</span>  <span class='chapter-title'>Appendix: Conjugate Priors</span>"
    ]
  },
  {
    "objectID": "A09.html#sec-conjugate-priors",
    "href": "A09.html#sec-conjugate-priors",
    "title": "105  Appendix: Conjugate Priors",
    "section": "",
    "text": "Table 105.1: Conjugate prior\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\nConjugate prior\nPosterior\nPosterior predictive\n\n\n\n\n\\text{Bernoulli}(p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left( \\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}\\right)}\n{\\displaystyle \\mathbb{P}r({\\tilde {x}}=1)={\\frac {\\alpha '}{\\alpha '+\\beta '}}}\n\n\n\\text{Binomial}(trials=m,p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left(\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}\\right)}\n{\\displaystyle \\operatorname {BetaBin} ({\\tilde {x}}|\\alpha ',\\beta ')}\n\n\n\\text{NegBinomial}(fails=r)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left( \\alpha +rn,\\beta + \\sum _{i=1}^{n} x_{i}\\right)}\n{\\displaystyle \\operatorname {BetaNegBin} ({\\tilde {x}}|\\alpha ',\\beta ')}\n\n\n\\text{Poisson}(rate=\\lambda)\n\\text{Gamma}(k,\\theta)\n{\\displaystyle \\text{Gamma}\\left( k+\\sum _{i=1}^{n}x_{i},\\ {\\frac {\\theta }{n\\theta +1}}\\!\\right)}\n{\\displaystyle \\operatorname {NB} \\left({\\tilde {x}}\\mid k',{\\frac {1}{\\theta '+1}}\\right)}\n\n\n\\text{Poisson}(rate=\\lambda)\n\\text{Gamma}(\\alpha,\\beta)\n{\\displaystyle\\text{Gamma}\\left( \\alpha +\\sum _{i=1}^{n}x_{i},\\ \\beta +n\\!\\right)}\n{\\displaystyle \\operatorname {NB} \\left({\\tilde {x}}\\mid \\alpha ',{\\frac {\\beta '}{1+\\beta '}}\\right)}\n\n\n\\text{Categorical}(probs=p,cats=k)\n\\text{Dir}(\\alpha_k)\\mathbb{I}_{k\\ge1}\n{\\displaystyle \\text{Dir}\\left({ {\\boldsymbol {\\alpha }}+(c_{1},\\ldots ,c_{k})}\\right)}\n{\\displaystyle {\\begin{aligned}\\mathbb{P}r({\\tilde {x}}=i)&={\\frac {{\\alpha _{i}}'}{\\sum _{i}{\\alpha _{i}}'}} \\\\ &={\\frac {\\alpha _{i}+c_{i}}{\\sum _{i}\\alpha _{i}+n}}\\end{aligned}}}\n\n\n\\text{Multinomial}(probs=p,cats=k)\n\\text{Dir}(\\alpha_k)\\mathbb{I}_{k\\ge1}\n{\\displaystyle \\text{Dir}\\left({ {\\boldsymbol {\\alpha }}+\\sum _{i=1}^{n}\\mathbf {x} _{i}\\!}\\right)}\n{\\displaystyle \\operatorname {DirMult} ({\\tilde {\\mathbf {x} }}\\mid {\\boldsymbol {\\alpha }}')}\n\n\n\\text{Hypergeometric}(pop=n)\n\\text{BetaBinomial}(\\alpha,\\beta,n=N)\n{\\displaystyle \\text{BetaBinomial}\\left({\\displaystyle \\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}}\\right)}\n\n\n\n\\text{Geometric}(p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle\\text{Beta}\\left( \\alpha +n,\\,\\beta +\\sum _{i=1}^{n}x_{i}\\right)}",
    "crumbs": [
      "<span class='chapter-number'>105</span>  <span class='chapter-title'>Appendix: Conjugate Priors</span>"
    ]
  },
  {
    "objectID": "A10.html",
    "href": "A10.html",
    "title": "106  Appendix: Link Function",
    "section": "",
    "text": "106.1 Link functions\nIn statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\nThe link function provides the relationship between the linear predictor and the mean of the distribution function. There are many commonly used link functions, and their choice is informed by several considerations. There is always a well-defined canonical link function which is derived from the exponential of the response’s density function. However, in some cases, it makes sense to try to match the domain of the link function to the range of the distribution function’s mean, or use a non-canonical link function for algorithmic purposes, for example Bayesian probit regression.\nWhen using a distribution function with a canonical parameter \\theta , the canonical link function is the function that expresses \\theta in terms of \\mu i.e. \\theta =b(\\mu) . For the most common distributions, the mean μ is one of the parameters in the standard form of the distribution’s Density function, and then b(\\mu ) is the function as defined above that maps the density function into its canonical form. When using the canonical link function, b(\\mu )=\\theta =\\mathbf {X} {\\boldsymbol {\\beta }} , which allows \\mathbf{X}^{T}\\mathbf{Y} to be a sufficient statistic for \\beta .\nFollowing is a table of several exponential-family distributions in common use and the data they are typically used for, along with the canonical link functions and their inverses (sometimes referred to as the mean function, as done here).\nIn the cases of the exponential and gamma distributions, the domain of the canonical link function is not the same as the permitted range of the mean. In particular, the linear predictor may be positive, which would give an impossible negative mean. When maximizing the likelihood, precautions must be taken to avoid this. An alternative is to use a non-canonical link function.\nIn the case of the Bernoulli, binomial, categorical and multinomial distributions, the support of the distributions is not the same type of data as the parameter being predicted. In all of these cases, the predicted parameter is one or more probabilities, i.e. real numbers in the range [0,1]. The resulting model is known as Logistic regression (or Multinomial logistic regression in the case that K-way rather than binary values are being predicted).\nFor the Bernoulli and binomial distributions, the parameter is a single probability, indicating the likelihood of occurrence of a single event. The Bernoulli still satisfies the basic condition of the generalized linear model in that, even though a single outcome will always be either 0 or 1, the expected value will nonetheless be a real-valued probability, i.e. the probability of occurrence of a “yes” (or 1) outcome. Similarly, in a binomial distribution, the expected value is Np, i.e. the expected proportion of “yes” outcomes will be the probability to be predicted.\nFor categorical and multinomial distributions, the parameter to be predicted is a K-vector of probabilities, with the further restriction that all probabilities must add up to 1. Each probability indicates the likelihood of occurrence of one of the K possible values. For the multinomial distribution, and for the vector form of the categorical distribution, the expected values of the elements of the vector can be related to the predicted probabilities similarly to the binomial and Bernoulli distributions.",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A10.html#sec-link-function",
    "href": "A10.html#sec-link-function",
    "title": "106  Appendix: Link Function",
    "section": "",
    "text": "Common distributions with typical uses and canonical link functions\n\n\n\n\n\n\n\n\n\n\nDistribution\nSupport\nUses\nLink name\nLink fn\nMean fn\n\n\n\n\nNormal\n\\mathbb{R}\n\nIdentity\n\\mathbf {X} {\\boldsymbol {\\beta }}=\\mu\n\n\n\nExpoential\n\\mathbb{R}^+_0\n\nNegative inverse\n\\mathbf {X} {\\boldsymbol {\\beta }}=-\\mu^{-1}\n\n\n\nGamma\n\\mathbb{R}^+_0\n\nNegative inverse\n\\mathbf {X} {\\boldsymbol {\\beta }}=-\\mu^{-1}\n\n\n\nInverse Gamma\n\\mathbb{R}^+_0\n\nInverse squared\n\\mathbf {X} {\\boldsymbol {\\beta }}=\\mu^{-2}\n\n\n\nPoisson\n\\mathbb{N}_0\n\nLog\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\mu)\n\n\n\nBernuolli\n\\{0,1\\}\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})\n\n\n\nBinomial\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{n-\\mu})\n\n\n\nCategorical\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})\n\n\n\nMultinomial\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A10.html#credits",
    "href": "A10.html#credits",
    "title": "106  Appendix: Link Function",
    "section": "Credits:",
    "text": "Credits:\nThis page is based on the Generalized linear model article on Wikipedia, which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. By Wikimedia contributors, available under CC BY-SA 3.0.\nThe text has been modified for clarity and conciseness.",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A11.html",
    "href": "A11.html",
    "title": "107  Appendix: Bayes by backprop",
    "section": "",
    "text": "107.1 Introduction\nThis appendix reviews of a method to introduce weight uncertainty into neural networks called the “Bayes by Backprop” method introduced in (Blundell et al. 2015). where, the main question is how to determine the parameters of the distribution for each network weight. I learned about it from Probabilistic Deep Learning with TensorFlow 2 by Dr Kevin Webster, S reading from based this on\nThe authors note that prior work which considered uncertainty at the hidden unit (H_i) an approach that allows to state the uncertainty with respect to a particular observation and which is an easier problem since the number of weight is greater by two orders of magnitude. But considering the uncertainty in the weights is complementary, in the sense that it captures uncertainty about which neural network is appropriate, leading to regularization of the weights and model averaging. This weight uncertainty can be used to drive the exploration/exploitation in contextual bandit problems using Thompson sampling . Weights with greater uncertainty introduce more variability into the decisions made by the network, naturally leading to exploration. As more data are observed, the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the environment is better understood.\nIn a traditional neural network, as shown in the upper left, each weight has a single value. But in the true value for these weights is not certain. Much of this uncertainty comes from imperfect training data, which is an approximation of the the distribution of the data generating process from which the data were sampled. Recall that this is called epistemic uncertainty, which we expect to decrease as the amount of training data increases.\nIn this method, we want to include such uncertainty in deep learning models. This is done by changing each weight from a single deterministic value to a probability distribution. We then learn the parameters of this distribution. Consider a neural network weight w_i . In a standard (deterministic) neural network, this has a single value \\hat{w}_i , learnt via backpropagation. In a neural network with weight uncertainty, each weight is represented by a probability distribution, and the parameters of this distribution are learned via backpropagation. Suppose, for example, that each weight has a normal distribution. This has two parameters: a mean \\mu_i and a standard deviation \\sigma_i .\nSince the weights are uncertain, the feedforward value of some input x_i is not constant. A single feedforward value is determined in two steps: 1. Sample each network weight from their respective distributions – this gives a single set of network weights. 2. Use these weights to determine a feedforward value \\hat{y}_i .\nHence, the key question is how to determine the parameters of the distribution for each network weight. The paper introduces exactly such a scheme, called Bayes by Backprop.",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#introduction",
    "href": "A11.html#introduction",
    "title": "107  Appendix: Bayes by backprop",
    "section": "",
    "text": "Fig. 1 from (Blundell et al. 2015) contrasting traditional and Bayesian neural networks\n\n\n\n\n\nClassic deterministic NN: w_i = \\hat{w}_i\nNN with weight uncertainty represented by normal distribution: w_i \\sim N(\\hat{\\mu}_i, \\hat{\\sigma}_i) .",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#bayesian-learning",
    "href": "A11.html#bayesian-learning",
    "title": "107  Appendix: Bayes by backprop",
    "section": "107.2 Bayesian learning",
    "text": "107.2 Bayesian learning\nNote: We use the notation P to refer to a probability density. For simplicity, we’ll only consider continuous distributions (which have a density). In the case of discrete distributions, P would represent a probability mass and integrals should be changed to sums. However, the formulae are the same.\nWhat you need to know now is that Bayesian methods can be used to calculate the distribution of a model parameter given some data. In the context of weight uncertainty in neural networks, this is convenient, since we are looking for the distribution of weights (model parameters) given some (training) data. The key step relies on Bayes’ theorem. This theorem states, in mathematical notation, that\n\n\\mathbb{P}r(w \\mid D) = \\frac{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)}{\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'}\n\nwhere the terms mean the following:\n\nD is some data, e.g. x and y value pairs: D = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\} . This is sometimes called the evidence.\nw is the value of a model weight.\n\\mathbb{P}r(w) is called the prior. This is our “prior” belief on the probability density of a model weight, i.e. the distribution that we postulate before seeing any data.\n\\mathbb{P}r(D \\mid w) is the likelihood of having observed data D given weight w . It is precisely the same likelihood used to calculate the negative log-likelihood.\n\\mathbb{P}r(w \\mid D) is the posterior density of the distribution of the model weight at value w , given our training data. It is called posterior since it represents the distribution of our model weight after taking the training data into account.\n\nNote that the term {\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'} = \\mathbb{P}r(D) does not depend on w (as the w' is an integration variable). It is only a normalization term. For this reason, we will from this point on write Bayes’ theorem as\n\n\\mathbb{P}r(w \\mid D) = \\frac{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)}{\\mathbb{P}r(D)}.\n\nBayes’ theorem gives us a way of combining data with some “prior belief” on model parameters to obtain a distribution for these model parameters that considers the data, called the posterior distribution.",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#bayesian-neural-network-with-weight-uncertainty-in-principle",
    "href": "A11.html#bayesian-neural-network-with-weight-uncertainty-in-principle",
    "title": "107  Appendix: Bayes by backprop",
    "section": "107.3 Bayesian neural network with weight uncertainty – in principle",
    "text": "107.3 Bayesian neural network with weight uncertainty – in principle\nThe above formula gives a way to determine the distribution of each weight in the neural network:\n\nPick a prior density \\mathbb{P}r(w) .\nUsing training data D , determine the likelihood \\mathbb{P}r(D \\mid w) .\nDetermine the posterior density \\mathbb{P}r(w \\mid D) using Bayes’ theorem.\n\nThis is the distribution of the NN weight.\nWhile this works in principle, in many practical settings it is difficult to implement. The main reason is that the normalization constant {\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'} = \\mathbb{P}r(D) may be very difficult to calculate, as it involves solving or approximating a complicated integral. For this reason, approximate methods, such as Variational Bayes described below, are often employed.",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#variational-bayes",
    "href": "A11.html#variational-bayes",
    "title": "107  Appendix: Bayes by backprop",
    "section": "107.4 Variational Bayes",
    "text": "107.4 Variational Bayes\n Variational Bayes methods approximate the posterior distribution with a second function, called a variational posterior. This function has a known functional form, and hence avoids the need to determine the posterior \\mathbb{P}r(w \\mid D) exactly. Of course, approximating a function with another one has some risks, since the approximation may be very bad, leading to a posterior that is highly inaccurate. In order to mediate this, the variational posterior usually has a number of parameters, denoted by \\theta , that are tuned so that the function approximates the posterior as well as possible. Let’s see how this works below.\nInstead of \\mathbb{P}r(w \\mid D) , we assume the network weight has density q(w \\mid \\theta) , parameterized by \\theta . q(w \\mid \\theta) is known as the variational posterior . We want q(w \\mid \\theta) to approximate \\mathbb{P}r(w \\mid D) , so we want the “difference” between q(w \\mid \\theta) and \\mathbb{P}r(w \\mid D) to be as small as possible. This “difference” between the two distributions is measured by the Kullback-Leibler divergence D_{\\text{KL}} (note that this is unrelated to the D we use to denote the data). The Kullback-Leibler divergence between two distributions with densities f(x) and g(x) respectively is defined as\n\nD_{KL} (f(x) \\parallel g(x)) = \\int f(x) \\log \\left( \\frac{f(x)}{g(x)} \\right) \\text{d} x\n\nNote that this function has value 0 (indicating no difference) when f(x) \\equiv g(x) , which is the result we expect. We use the convention that \\frac{0}{0} = 1 here.\n Viewing the data D as a constant, the Kullback-Leibler divergence between q(w \\mid \\theta) and \\mathbb{P}r(w \\mid D) is hence:\n\n\\begin{aligned}\n  D_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D)) &= \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta)}{\\mathbb{P}r(w \\mid D)} \\right) \\text{d} w \\\\\n&= \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta) \\mathbb{P}r(D)}{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)} \\right) \\text{d} w \\\\\n&= \\int q(w \\mid \\theta) \\log \\mathbb{P}r(D) \\text{d} w + \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta)}{\\mathbb{P}r(w)} \\right) \\text{d} w - \\int q(w \\mid \\theta) \\log \\mathbb{P}r(D \\mid w) \\text{d} w \\\\\n&= \\log \\mathbb{P}r(D) + D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w))\n\\end{aligned}\n\nwhere, in the last line, we have used\n\n\\int q(w \\mid \\theta) \\log \\mathbb{P}r(D) \\text{d}w = \\log \\mathbb{P}r(D) \\int q(w \\mid \\theta) \\text{d} w = \\log \\mathbb{P}r(D)\n\nsince q(w \\mid \\theta) is a probability distribution and hence integrates to 1. If we consider the data D to be constant, the first term is a constant also, and we may ignore it when minimizing the above. Hence, we are left with the function\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w))\n\\end{aligned}\n\nNote that this function depends only on \\theta and D , since w is an integration variable. This function has a nice interpretation as the sum of: - The Kullback-Leibler divergence between the variational posterior q(w \\mid \\theta) and the prior \\mathbb{P}r(w) . This is called the complexity cost, and it depends on \\theta and the prior but not the data D . - The expectation of the negative log likelihood \\log \\mathbb{P}r(D \\mid w) under the variational posterior q(w \\mid \\theta) . This is called the likelihood cost and it depends on \\theta and the data but not the prior.\nL(\\theta \\mid D) is the loss function that we minimize to determine the parameter \\theta . Note also from the above derivation, that we have\n\n\\begin{aligned}\n\\log \\mathbb{P}r(D) &= \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) - D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) + D_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D))\\\\\n&\\ge \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) - D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) =: ELBO\n\\end{aligned}\n\nwhich follows because\n\nD_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D))\n\nis non negative. The final expression on the right hand side is therefore a lower bound on the log-evidence, and is called the evidence lower bound, often shortened to ELBO. The ELBO is the negative of our loss function, so minimizing the loss function is equivalent to maximizing the ELBO.\nMaximizing the ELBO requires a trade off between the KL term and expected log-likelihood term. On the one hand, the divergence between q(w \\mid \\theta) and \\mathbb{P}r(w) should be kept small, meaning the variational posterior shouldn’t be too different to the prior. On the other, the variational posterior parameters should maximize the expectation of the log-likelihood \\log \\mathbb{P}r(D \\mid w) , meaning the model assigns a high likelihood to the data.",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#a-backpropagation-scheme",
    "href": "A11.html#a-backpropagation-scheme",
    "title": "107  Appendix: Bayes by backprop",
    "section": "107.5 A backpropagation scheme",
    "text": "107.5 A backpropagation scheme\n\n107.5.1 The idea\nWe can use the above ideas to create a neural network with weight uncertainty, which we will call a Bayesian neural network. From a high level, this works as follows. Suppose we want to determine the distribution of a particular neural network weight w .\n\nAssign the weight a prior distribution with density \\mathbb{P}r(w) , which represents our beliefs on the possible values of this network before any training data. This may be something simple, like a unit Gaussian. Furthermore, this prior distribution will usually not have any trainable parameters.\nAssign the weight a variational posterior with density q(w \\mid \\theta) with some trainable parameter \\theta .\nq(w \\mid \\theta) is the approximation for the weight’s posterior distribution. Tune \\theta to make this approximation as accurate as possible as measured by the ELBO.\n\nThe remaining question is then how to determine \\theta . Recall that neural networks are typically trained via a backpropagation algorithm, in which the weights are updated by perturbing them in a direction that reduces the loss function. We aim to do the same here, by updating \\theta in a direction that reduces L(\\theta \\mid D) .\nHence, the function we want to minimise is\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) \\\\\n&= \\int q(w \\mid \\theta) ( \\log q(w \\mid \\theta) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w) ) \\text{d}w.\n\\end{aligned}\n\nIn principle, we could take derivatives of L(\\theta \\mid D) with respect to \\theta and use this to update its value. However, this involves doing an integral over w , and this is a calculation that may be impossible or very computationally expensive. Instead, we want to write this function as an expectation and use a Monte Carlo approximation to calculate derivatives. At present, we can write this function as\n\nL(\\theta \\mid D) = \\mathbb{E}_{q(w \\mid \\theta)} ( \\log q(w \\mid \\theta) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w) )\n\nHowever, taking derivatives with respect to \\theta is difficult because the underlying distribution the expectation is taken with respect to depends on \\theta . One way we can handle this is with the reparameterization trick.\n\n\n107.5.2 The reparameterization trick\nThe reparameterization trick is a way to move the dependence on \\theta around so that an expectation may be taken independently of it. It’s easiest to see how this works with an example. Suppose q(w \\mid \\theta) is a Gaussian, so that \\theta = (\\mu, \\sigma) . Then, for some arbitrary f(w; \\mu, \\sigma) , we have\n\n\\begin{aligned}\n\\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\int q(w \\mid \\mu, \\sigma) f(w; \\mu, \\sigma) \\text{d}w \\\\\n&= \\int \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{1}{2 \\sigma^2} (w - \\mu)^2 \\right) f(w; \\mu, \\sigma) \\text{d}w \\\\\n&= \\int \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left( -\\frac{1}{2} \\epsilon^2 \\right) f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right) \\text{d}\\epsilon \\\\\n&= \\mathbb{E}_{\\epsilon \\sim N(0, 1)} (f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right) )\n\\end{aligned}\n\nwhere we used the change of variable w = \\mu + \\sigma \\epsilon . Note that the dependence on \\theta = (\\mu, \\sigma) is now only in the integrand and we can take derivatives with respect to \\mu and \\sigma:\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{\\epsilon \\sim N(0, 1)} (f \\left( w; \\mu, \\sigma \\right) ) = \\mathbb{E}_{\\epsilon \\sim N(0, 1)} \\frac{\\partial}{\\partial \\mu} f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right)\n\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\sigma} \\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\frac{\\partial}{\\partial \\sigma} \\mathbb{E}_{\\epsilon \\sim N(0, 1)} (f \\left( w; \\mu, \\sigma \\right) ) = \\mathbb{E}_{\\epsilon \\sim N(0, 1)} \\frac{\\partial}{\\partial \\sigma} f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right)\n\\end{aligned}\n\nFinally, note that we can approximate the expectation by its Monte Carlo estimate:\n\n\\begin{aligned}\n\\mathbb{E}_{\\epsilon \\sim N(0, 1)}  \\frac{\\partial}{\\partial \\theta} f \\left( \\mu + \\sigma \\epsilon; \\mu, \\sigma \\right) \\approx \\sum_{i}  \\frac{\\partial}{\\partial \\theta} f \\left( \\mu + \\sigma \\epsilon_i; \\mu, \\sigma \\right),\\qquad \\epsilon_i \\sim N(0, 1).\n\\end{aligned}\n\nThe above reparameterization trick works in cases where we can write the w = g(\\epsilon, \\theta) , where the distribution of the random variable \\epsilon is independent of \\theta .\n\n\n107.5.3 Implementation\nPutting this all together, for our loss function L(\\theta \\mid D) \\equiv L(\\mu, \\sigma \\mid D) , we have\n\nf(w; \\mu, \\sigma) = \\log q(w \\mid \\mu, \\sigma) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w)\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu} L(\\mu, \\sigma \\mid D) \\approx \\sum_{i} \\left( \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\mu} \\right)\n\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\sigma} L(\\mu, \\sigma \\mid D) \\approx \\sum_{i} \\left( \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} \\epsilon_i + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\sigma} \\right)\n\\end{aligned}\n\n\nf(w; \\mu, \\sigma) = \\log q(w \\mid \\mu, \\sigma) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w)\n\nwhere w_i = \\mu + \\sigma \\epsilon_i, \\, \\epsilon_i \\sim N(0, 1) . In practice, we often only take a single sample \\epsilon_1 for each training point. This leads to the following backpropagation scheme:\n\nSample \\epsilon_i \\sim N(0, 1) . 2. Let w_i = \\mu + \\sigma \\epsilon_i\nCalculate\n\n\n\\nabla_{\\mu}f = \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\mu} \\hspace{3em} \\nabla_{\\sigma}f = \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} \\epsilon_i + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\sigma}\n\n\nUpdate the parameters with some gradient-based optimizer using the above gradients.\n\nThis is how we learn the parameters of the distribution for each neural network weight.\n\n\n107.5.4 Minibatches\nNote that the loss function (or negative of the ELBO) is\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) \\\\\n& = D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\sum_{j=1}^N \\log \\mathbb{P}r(y_j, x_j \\mid w_j)\n\\end{aligned}\n\nwhere j runs over all the data points in the training data (N in total) and w_j = \\mu + \\sigma \\epsilon_j is sampled using \\epsilon_j \\sim N(0, 1) (we assume a single sample from the approximate posterior per data point for simplicity).\nIf training occurs in minibatches of size B , typically much smaller than N , we instead have a loss function\n\n\\begin{aligned}\nD_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\sum_{j=1}^{B} \\log \\mathbb{P}r(y_j, x_j \\mid w_j).\n\\end{aligned}\n\nNote that the scaling factors between the first and second terms have changed, since before the sum ran from 1 to N , but it now runs from 1 to B . To correct for this, we should add a correction factor \\frac{N}{B} to the second term to ensure that its expectation is the same as before. This leads to the loss function, after dividing by N to take the average per training value, of\n\n\\begin{aligned}\n\\frac{1}{N} D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\frac{1}{B} \\sum_{j=1}^{B} \\log \\mathbb{P}r(y_j, x_j \\mid w_j).\n\\end{aligned}\n\nBy default, when Tensorflow calculates the loss function, it calculates the average across the minibatch. Hence, it already uses the factor \\frac{1}{B} present on the second term. However, it does not, by default, divide the first term by N . In an implementation, we will have to specify this. You’ll see in the next lectures and coding tutorials how to do this.\n\n\n107.5.5 Conclusion\nWe introduced the Bayes by Backpropagation method, which can be used to embed weight uncertainty into neural networks. Good job getting through it, as the topic is rather advanced. This approach allows the modelling of epistemic uncertainty on the model weights. We expect that, as the number of training points increases, the uncertainty on the model weights decreases. This can be shown to be the case in many settings. In the next few lectures and coding tutorials, you’ll learn how to apply these methods to your own models, which will make the idea much clearer.\n\n\n107.5.6 Further reading and resources\n\nBayes by backprop paper (Blundell et al. 2015)\nWikipedia article on Bayesian inference\n\n\n\n\n\n\n\nBlundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015. “Weight Uncertainty in Neural Networks.” https://doi.org/10.48550/ARXIV.1505.05424.",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A12.html",
    "href": "A12.html",
    "title": "108  Bayesian Books in R & Python",
    "section": "",
    "text": "108.1 Introduction to Probability\nA number of books on Bayesian statistics and time series analysis are available in both R and Python. A number of these are introduced in the specilization and many others are worth mentioning.\nThere are many books in R and Python that can help you learn more about these languages and how to use them for data analysis.\nHere are some of the most popular books on R and Python:",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#introduction-to-probability",
    "href": "A12.html#introduction-to-probability",
    "title": "108  Bayesian Books in R & Python",
    "section": "",
    "text": "Introduction to Probability by Dennis L. Sun",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#sec-bayesian-books",
    "href": "A12.html#sec-bayesian-books",
    "title": "108  Bayesian Books in R & Python",
    "section": "108.2 Books in R",
    "text": "108.2 Books in R\n\nR for Data Science by Hadley Wickham & Garrett Grolemund\nAdvanced R by Hadley Wickham\nggplot2: Elegant Graphics for Data Analysis (3e)\nR Graphics Cookbook, 2nd edition\nAn Introduction to Statistical Learning\nEngineering Production-Grade Shiny Apps\nForecasting: Principles and Practice (3rd ed)\nExploratory Data Analysis with R Roger D. Peng\nModern R with the tidyverse by Bruno Rodrigues\nModern Statistics with R by Benjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton\nMastering Shiny by Hadley Wickham, Winston Chang, and Joe Cheng\nLearning Statistics with R by Danielle Navarro\nText Mining with R by Julia Silge and David Robinson",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#books-in-python",
    "href": "A12.html#books-in-python",
    "title": "108  Bayesian Books in R & Python",
    "section": "108.3 Books in Python",
    "text": "108.3 Books in Python\n\n(James et al. 2013) An Introduction to Statistical Learning with python by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\n(McKinney 2022) Python for Data Analysis by Wes McKinney of Pandas infamy parquet and Apache Arrow\n(VanderPlas 2016) Python Data Science Handbook by Jake VanderPlas\nThink Stats by Allen B. Downey\nThink Bayes by Allen B. Downey\nProbabilistic Programming & Bayesian Methods for Hackers by Cameron Davidson-Pilon",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#bayesian-statistics-techniques-and-models",
    "href": "A12.html#bayesian-statistics-techniques-and-models",
    "title": "108  Bayesian Books in R & Python",
    "section": "108.4 Bayesian Statistics: Techniques and Models",
    "text": "108.4 Bayesian Statistics: Techniques and Models\nAt the end of the course there is a handout called further reading. In this course the following titles are recommended for further reading and reference, unfortunately some of the links were out of date.\n\n\n\n\n\n\n\nFigure 108.1: Doing Bayesian Data Analysis\n\n\n(Kruschke 2011) Doing Bayesian Data Analysis this is the favorite on Bayesian statistics. It is a great book for learning Bayesian statistics and covers a wide range of topics in Bayesian modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. This text is recommended for further reading in the course 2\n\n\n\n\n\n\n\nFigure 108.2: Statistical Rethinking\n\n\n(McElreath 2015) Statistical Rethinking: A Bayesian Course with Examples in R and Stan by Richard McElreath is, the runner up in terms of a popular book on Bayesian Statistics. This book is a great resource for learning Bayesian statistics and covers a wide range of topics in Bayesian modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. McElreath is a gifted and entertaining explainer but he is not a full-fledged statistician. I find that in most cases the examples in this book are rather weak and challenging to adapt to real life settings. On the other hand his use of metaphors and his ability to pass on his intuition is unparalleled and has helped to shape my own thinking about statistics as is evidenced by the many references to his work in these notes. This text is recommended for further reading in the course 2\n\n\n\n\n\n\n\nFigure 108.3: Bayesian Data Analysis\n\n\n(Gelman et al. 2013) Bayesian Data Analysis by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, Donald B. Rubin. This book is considered the Graduate level text on Bayesian statistics and covers a wide range of topics in Bayesian data analysis. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. This book is harder to read than the previous two books. I’d recommend reading and viewing McElreath’s book and videos first and only then moving to this book later. This text is recommended for further reading in the course 2 but with a cautionary caveat that a solid background in calculus-based probability will be useful, as many details are left to the reader. I took an undergraduate course in probability and statistics that was based on measure theory but I’m not sure this was the kind of background these authors have in mind.\n\n\n\n\n\n\n\nFigure 108.4: The BUGS book\n\n\n(Lunn et al. 2012) The BUGS book: A Practical Introduction to Bayesian Analysis is a classic text on Bayesian statistics and covers a wide range of topics in Bayesian modeling, computation, and inference. IT focuses on simulation using BUGS. This text is recommended for further reading in the course 2\nSome addition books that were mentioned in the course bibliography:\n(Carlin and Louis 2008) Bayesian Methods for Data Analysis\n(Agresti 2012) This is a graduate level book on categorical data which often poses extra challenges for Bayesian statistics. Section 17.5 covers Bayesian methods for Categorical Data.\n(Spiegelhalter et al. 2002) “Measures of Model Complexity and Fit” by David Spiegelhalter, Andrew Thomas, Nicky Best, and W. Brian Gilks. This paper considers the problem of model selection in Bayesian Hierarchical Models and proposes a new measure of model complexity and fit, the Deviance Information Criterion (DIC).\n(Banerjee, Gelfand, and Carlin 2026) Hierarchical Modeling and Analysis for Spatial Data by Sudipto Banerjee, Bradley P. Carlin, and Alan E. Gelfand. This book is a comprehensive introduction to hierarchical modeling and analysis for spatial data and covers a wide range of topics in spatial statistics, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. data etc\n(Carlin and Louis 2008) Bayesian Methods for Data Analysis\n(Gelman and Hill 2006) This is too is a classic book on data analysis using regression and multilevel/hierarchical models. I read this one may years ago and is fairly approachable and recommended Unless the next edition has come out?",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#mixture-models",
    "href": "A12.html#mixture-models",
    "title": "108  Bayesian Books in R & Python",
    "section": "108.5 Mixture Models",
    "text": "108.5 Mixture Models\n\n\n\n\n\n\n\nFigure 108.5: Finite Mixture and Markov Switching Models\n\n\nThere were no books recommended in this course but (Frühwirth-Schnatter 2006) was in the bibliography from the previous course. Finite Mixture and Markov Switching Models by Sylvia Frühwirth-Schnatter. This book is a comprehensive introduction to finite mixture and Markov switching models and covers a wide range of topics in mixture modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\n(Fruhwirth-Schnatter, Celeux, and Robert 2019) Handbook of Mixture Analysis by Christian P. Robert, Gilles Celeux, Sylvia Fruhwirth-Schnatter\n(McLachlan and Peel 2004) Finite Mixture Models by Geoffrey J. McLachlan and David Peel. A comprehensive account of major issues in finite mixture modeling. Advanced text on finite mixture models. Lacks exercises and requires deep diving into papers.\n(Chen 2023) Statistical Inference Under Mixture Models by Jiahua Chen. A more recent introduction to mixture models with recent developments in testing hypothesis for the order of the mixture and insights on inference for mixture models.\n(Yao and Xiang 2024) Mixture Models: Parametric, Semiparametric, and New Directions by Weixin Yao and Sijia Xiang.\n(Visser and Speekenbrink 2022) Mixture and Hidden Markov Models with R by Visser, I. and Speekenbrink, M.",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#non-parametric-bayesian-statistics",
    "href": "A12.html#non-parametric-bayesian-statistics",
    "title": "108  Bayesian Books in R & Python",
    "section": "108.6 Non-parametric Bayesian Statistics",
    "text": "108.6 Non-parametric Bayesian Statistics\n(Bayesian Nonparametrics 2010) “Bayesian Nonparametrics” by Peter M. Hjort, Chris Holmes, Maria E. Müller, and Stephen G. Walker. This book is a comprehensive introduction to Bayesian nonparametric methods and covers a wide range of topics in nonparametric modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#bayesian-time-series-bibliography",
    "href": "A12.html#bayesian-time-series-bibliography",
    "title": "108  Bayesian Books in R & Python",
    "section": "108.7 Bayesian Time Series Bibliography",
    "text": "108.7 Bayesian Time Series Bibliography\nWe start with some books from the course, I collected here both the recommended books and some others that I found useful.\n\n\n\n\n\n\n\nFigure 108.6: Bayesian Forecasting and Dynamic Models\n\n\n(West and Harrison 2013) Bayesian Forecasting and Dynamic Models by Mike West and Jeff Harrison. This book is a classic text on Bayesian statistics and covers a wide range of topics in Bayesian forecasting and dynamic models. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. This is a much easier read than the following book and is a great introduction to Bayesian time series analysis. It soon became my goto reference for understanding the more challenging aspects of the course. It provides simple explanations with better motivations.\nI have some criticism. The authors sometimes become very meta discussing their models at very abstract level that is more relevant to philosophy majors than statistics. For example a dynamic model is a defined as a set of models whose parameters may change at any index of time series. That’s a very poor definition - its like asking us to use a 1:1 map for navigation. When I took a course on function approximation in RL we got a much neater exposition to parameterization of to very similar markov state space models. I found that the white noise model getting a very extensive treatment, perhaps, because it is easier to analyze. This is a random walk and the rudiments of which are easily taught in a single high-school lesson. They also digress into statistical war stories type anecdotes that rather than instil confidence that the authors know what they are talking about they are effectively making a case that they failed to communicate effectively with their clients with disastrous effect. Mathematically the book is very much self contained and this shows great integrity. I’ve seen some talks by Mike West in which he highlights research by his students and this are great if very challenging to follow.\nThe authors also published a second book [Applied Bayesian Forecasting and Time Series Analysis]](https://www2.stat.duke.edu/~mw/bats.html) this one is based on the BATS software package which is no longer maintained. However it does contain a large number of datasets\n\n\n\n\n\n\n\nFigure 108.7: Time Series: Modeling, Computation, and Inference\n\n\n(Prado, Ferreira, and West 2023) Time Series: Modeling, Computation, and Inference by course instructor Raquel Prado and Marco A. R. Ferreira, Mike West. This book, now in its second edition is a comprehensive introduction to time series analysis and covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\nWhile learning this course I found some of the material harder to follow than I expected. The book helped to clarify definitions and so on however the book is rather comprehensive and mathematically advanced unlike some other books on statistics.\nThe teacher frequently point out that many aspects of Times series and are beyond the scope of the course. Yet this book covers much more ground like unequally spaced time series and vector valued time series.\nFor example we look at EKG data which the authors have been working on for years. However we look at it in this course in terms of a univariate time series while in reality EKG is usually sampled at 12 sites simultaneously yielding a multi-variate time series.\nOnce this course is done I will probably want to dive deeper into the subject and try to devote more time to other models in the book.\nc.f. (Nielsen 2019)\n\n\n\n\n\n\n\nFigure 108.8: Practical Times Series Analysis\n\n\n(Nielsen 2019) Practical Time Series Analysis: Prediction with Statistics and Machine Learning by Aileen Nielsen. Is a good resource for practitioners getting started with time series analysis. I also recommend any videos by Aileen Nielsen on the subject. In many ways this is a recommended introductory book on time series analysis. It covers, finding and wrangle time series data. EDA for TS. How to store temporal data. Simulate time series data. Generate and select features for a time series. Estimate errors, evaluate accuracy and performance. Forecast and classify time series with machine or deep learning.\n\n\n\n\n\n\n\nFigure 108.9: Statistical Analysis in Climate Research\n\n\n(Storch and Zwiers 2002) Statistical Analysis in Climate Research I came across this book while looking into the Durban-Levinson recursion and the Yule-Walker equations. So far I haven’t had time to read it but it looks promising. Here is the description from the publisher:\n\nClimatology is, to a large degree, the study of the statistics of our climate. The powerful tools of mathematical statistics therefore find wide application in climatological research. The purpose of this book is to help the climatologist understand the basic precepts of the statistician’s art and to provide some of the background needed to apply statistical methodology correctly and usefully. The book is self contained: introductory material, standard advanced techniques, and the specialized techniques used specifically by climatologists are all contained within this one source. There are a wealth of real-world examples drawn from the climate literature to demonstrate the need, power and pitfalls of statistical analysis in climate research. Suitable for graduate courses on statistics for climatic, atmospheric and oceanic science, this book will also be valuable as a reference source for researchers in climatology, meteorology, atmospheric science, and oceanography.\n\n\nHans von Storch is Director of the Institute of Hydrophysics of the GKSS Research Centre in Geesthacht, Germany and a Professor at the Meteorological Institute of the University of Hamburg.\n\n\nFrancis W. Zwiers is Chief of the Canadian Centre for Climate Modelling and Analysis, Atmospheric Environment Service, Victoria, Canada, and an Adjunct Professor at the Department of Mathematics and Statistics of the University of Victoria.\n\n\n\n\n\n\n\n\nFigure 108.10: Bayesian Modeling and Computation in Python\n\n\n(Martin, Kumar, and Lao 2021) Bayesian Modeling and Computation in Python by Osvaldo Martin is a great book for learning Bayesian statistics and covers a wide range of topics in Bayesian modeling, computation, and inference. I found the chapter on state space modeling and the Kalman filter particularly useful. The book is a great resource for translating what we learned in the course to Python. The book is suitable for undergraduate students in statistics, computer science, and related fields.\n\n\n\n\n\n\n\nFigure 108.11: Introductory Time Series with R\n\n\n(Cowpertwait and Metcalfe 2009) Introductory Time Series with R by Cowpertwait and Metcalfe\n\nYearly global mean temperature and ocean levels, daily share prices, and the signals transmitted back to Earth by the Voyager space craft are all examples of sequential observations over time known as time series. This book gives you a step-by-step introduction to analyzing time series using the open source software R. Each time series model is motivated with practical applications, and is defined in mathematical notation. Once the model has been introduced it is used to generate synthetic data, using R code, and these generated data are then used to estimate its parameters. This sequence enhances understanding of both the time series model and the R function used to fit the model to data. Finally, the model is used to analyze observed data taken from a practical application. By using R, the whole procedure can be reproduced by the reader.\nAll the data sets used in the book are available on the website at datasets\nThe book is written for undergraduate students of mathematics, economics, business and finance, geography, engineering and related disciplines, and postgraduate students who may need to analyze time series as part of their taught programme or their research.\n\nPaul Cowpertwait is an associate professor in mathematical sciences (analytics) at Auckland University of Technology with a substantial research record in both the theory and applications of time series and stochastic models.\nAndrew Metcalfe is an associate professor in the School of Mathematical Sciences at the University of Adelaide, and an author of six statistics text books and numerous research papers. Both authors have extensive experience of teaching time series to students at all levels.\n\n\n\n\n\n\n\nFigure 108.12: Analysis of Integrated and Cointegrated Time Series with R\n\n\n(Pfaff 2008) “Analysis of Integrated and Cointegrated Time Series with R” by Bernhard Pfaff. Its been a long time since I read this book and rather than do it an injustice I direct you to the review by Dirk Eddelbuettel in the Journal of Statistical Software is available at review. Or the book’s website at Analysis of Integrated and Cointegrated Time Series with R.\n\nThe analysis of integrated and co-integrated time series can be considered as the main methodology employed in applied econometrics. This book not only introduces the reader to this topic but enables him to conduct the various unit root tests and co-integration methods on his own by utilizing the free statistical programming environment R. The book encompasses seasonal unit roots, fractional integration, coping with structural breaks, and multivariate time series models. The book is enriched by numerous programming examples to artificial and real data so that it is ideally suited as an accompanying text book to computer lab classes.\nThe second edition adds a discussion of vector auto-regressive, structural vector auto-regressive, and structural vector error-correction models.\n\n(Broemeling 2019) Bayesian Analysis of Time Series by Lyle D. Broemeling\nIn many branches of science relevant observations are taken sequentially over time. Bayesian Analysis of Time Series discusses how to use models that explain the probabilistic characteristics of these time series and then utilizes the Bayesian approach to make inferences about their parameters. This is done by taking the prior information and via Bayes theorem implementing Bayesian inferences of estimation, testing hypotheses, and prediction. The methods are demonstrated using both R and WinBUGS. The R package is primarily used to generate observations from a given time series model, while the WinBUGS packages allows one to perform a posterior analysis that provides a way to determine the characteristic of the posterior distribution of the unknown parameters.\nThe book covers pretty much the material in the course. It uses R and WinBUGS to demonstrate the models and methods. Models considered include: white noise, Wiener process (random walk), AR(p),ARMA(p,q), ARIMA, Regression, Regression with MA and Seasonal effects, DLM , TAR\n\n\n\n\n\n\n\nFigure 108.13: Bayesian Inference for Stochastic Processes by Lyle D. Broemeling\n\n\n(Broemeling 2019) Bayesian Inference for Stochastic Processes by Lyle D. Broemeling is a comprehensive introduction to the analysis of stochastic processes using Bayesian methods. The book covers a wide range of topics in stochastic process modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. The code for R and WinBUGS is available at code It is based on WinBUGS which is a bit dated but still useful and a bit dated but it covers a lot of the material in the course.\n\n\n\n\n\n\n\nFigure 108.14: Dynamic Time Series Models using R-INLA: An Applied Perspective\n\n\n(Ravishanker, Raman, and Soyer 2022) Dynamic Time Series Models using R-INLA: An Applied Perspective is a new book that covers the use of the R-INLA package for fitting dynamic time series models. The book is available online gitbook\nThis is a very interesting book which covers a new approach to fitting time series models using the R-INLA package. INLA stands for Integrated Nested Laplace Approximation and is a method for fitting Bayesian models that is faster than MCMC. The book covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\n\n\n\n\n\n\n\nFigure 108.15: Statistics for Spatio-Temporal Data\n\n\n(Cressie and Wikle 2011) Statistics for Spatio-Temporal Data is a book I came across when I tried to understand the NDLM model. NDLMs have a two level hierarchical form and it seems possible to extend this formulation will non-normally distributed shocks and possibly non linear relation. In this book the authors take an interesting approach of not only looking at NDLM as a hierarchical model but they also extend the time series model into a spatio-temporal model.\nThis book is a comprehensive introduction to the analysis of spatio-temporal data and covers a wide range of topics in spatio-temporal statistics. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\na newer title from the authors is:\n\n\n\n\n\n\n\nFigure 108.16: Spatio-Temporal Statistics with R\n\n\n(Wikle, Zammit-Mangion, and Cressie 2019) Spatio-Temporal Statistics with R\n\n\n\n\n\n\n\nFigure 108.17: Bayesian Analysis of Stochastic Process Models\n\n\n(Rios Insua, Ruggeri, and Wiper 2012) Bayesian Analysis of Stochastic Process Models by David Rios Insua, Fabrizio Ruggeri, Michael P. Wiper. This book is a comprehensive introduction to the analysis of stochastic process models using Bayesian methods. It covers a wide range of topics in stochastic process modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\nThere are also a number of books on NDLM that I’ve come across:\nDynamic linear model tutorial matlab\nForecasting, structural time series and the Kalman filter by Andrew C. Harvey\nDynamic Linear Models with R by Giovanni Petris Sonia Petrone Patrizia Campagnoli\nTime Series Analysis by State Space Methods by J. Durbin and S.J. Koopman\n\n\n\n\n\n\n\nFigure 108.18: Machine Learning: A Bayesian and Optimization Perspective\n\n\n(Theodoridis 2015) Machine Learning: A Bayesian and Optimization Perspective Has two chapters on bayesian learning which are summarized in this summer school slide deck. I came across this book while looking into the Durban-Levinson recursion and the Yule-Walker equations. So far I haven’t had time to read it but it looks like a good book on machine learning and the slide deck indicates it covers some of the essential material missing from this specialization like particle filtering.\n\n\n\n\n\n\nAgresti, A. 2012. Categorical Data Analysis. Wiley Series in Probability and Statistics. Wiley. https://books.google.co.il/books?id=UOrr47-2oisC.\n\n\nBanerjee, S., A. E. Gelfand, and B. P. Carlin. 2026. Hierarchical Modeling and Analysis for Spatial Data. CRC Press LLC. https://books.google.co.il/books?id=GRFT0QEACAAJ.\n\n\nBayesian Nonparametrics. 2010. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\n\n\nBroemeling, Lyle D. 2019. Bayesian Analysis of Time Series. CRC Press.\n\n\nCarlin, B. P., and T. A. Louis. 2008. Bayesian Methods for Data Analysis. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=GTJUt8fcFx8C.\n\n\nChen, J. 2023. Statistical Inference Under Mixture Models. ICSA Book Series in Statistics. Springer Nature Singapore. https://books.google.co.il/books?id=sBXlEAAAQBAJ.\n\n\nCowpertwait, P. S. P., and A. V. Metcalfe. 2009. Introductory Time Series with r. Use r! Springer New York. https://books.google.co.il/books?id=QFiZGQmvRUQC.\n\n\nCressie, N., and C. K. Wikle. 2011. Statistics for Spatio-Temporal Data. CourseSmart Series. Wiley. https://books.google.co.il/books?id=-kOC6D0DiNYC.\n\n\nFruhwirth-Schnatter, S., G. Celeux, and C. P. Robert. 2019. Handbook of Mixture Analysis. Chapman & Hall/CRC Handbooks of Modern Statistical Methods. CRC Press. https://books.google.co.il/books?id=N3yCDwAAQBAJ.\n\n\nFrühwirth-Schnatter, S. 2006. Finite Mixture and Markov Switching Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=f8KiI7eRjYoC.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Analytical Methods for Social Research. Cambridge University Press. https://books.google.co.il/books?id=c9xLKzZWoZ4C.\n\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning: With Applications in r. Springer Texts in Statistics. Springer New York. https://books.google.co.il/books?id=qcI_AAAAQBAJ.\n\n\nKruschke, John K. 2011. Doing Bayesian Data Analysis: A Tutorial with R and BUGS. Burlington, MA: Academic Press. http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855.\n\n\nLunn, D., C. Jackson, N. Best, A. Thomas, and D. Spiegelhalter. 2012. The BUGS Book: A Practical Introduction to Bayesian Analysis. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis. https://books.google.co.il/books?id=Cthz3XMa_VQC.\n\n\nMartin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. Bayesian Modeling and Computation in Python. Boca Raton.\n\n\nMcElreath, Richard. 2015. Statistical Rethinking, a Course in r and Stan.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and Jupyter. 3rd ed. O’Reilly Media.\n\n\nMcLachlan, G. J., and D. Peel. 2004. Finite Mixture Models. Wiley Series in Probability and Statistics. Wiley. https://books.google.co.il/books?id=c2_fAox0DQoC.\n\n\nNielsen, A. 2019. Practical Time Series Analysis: Prediction with Statistics and Machine Learning. O’Reilly Media. https://books.google.co.il/books?id=xNOwDwAAQBAJ.\n\n\nPfaff, B. 2008. Analysis of Integrated and Cointegrated Time Series with r. Use r! Springer New York. https://books.google.co.il/books?id=ca5MkRbF3fYC.\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nRavishanker, N., B. Raman, and R. Soyer. 2022. Dynamic Time Series Models Using r-INLA: An Applied Perspective. CRC Press.\n\n\nRios Insua, David, Fabrizio Ruggeri, and Michael P Wiper. 2012. Bayesian Analysis of Stochastic Process Models. John Wiley & Sons.\n\n\nSpiegelhalter, David J., Nicola G. Best, Bradley P. Carlin, and Angelika Van Der Linde. 2002. “Bayesian Measures of Model Complexity and Fit.” Journal of the Royal Statistical Society Series B: Statistical Methodology 64 (4): 583–639. https://doi.org/10.1111/1467-9868.00353.\n\n\nStorch, H. von, and F. W. Zwiers. 2002. Statistical Analysis in Climate Research. Cambridge University Press.\n\n\nTheodoridis, S. 2015. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. 1st ed. O’Reilly Media, Inc. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nVisser, I., and M. Speekenbrink. 2022. Mixture and Hidden Markov Models with r. Use r! Springer International Publishing. https://books.google.co.il/books?id=Eep3EAAAQBAJ.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.\n\n\nWikle, C. K., A. Zammit-Mangion, and N. Cressie. 2019. Spatio-Temporal Statistics with r. Chapman & Hall/CRC the r Series. CRC Press. https://books.google.co.il/books?id=FD-IDwAAQBAJ.\n\n\nYao, W., and S. Xiang. 2024. Mixture Models: Parametric, Semiparametric, and New Directions. Chapman & Hall/CRC Monographs on Statistics and Applied Probability. CRC Press. https://books.google.co.il/books?id=scP9EAAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A13.html",
    "href": "A13.html",
    "title": "109  Appendix: Yule-Walker Equations & Durbin-Levinson Recursion",
    "section": "",
    "text": "109.1 Durbin-Levinson recursion\nDurbin-Levinson recursion is a method in linear algebra for computing the solution to an equation involving a Toeplitz matrix AKA a diagonal-constant matrix where descending diagonals are constant. The recursion runs in O(n^2) time rather then O(n^3) time required by Gauss-Jordan elimination.\nIn the course on Bayesian time series analysis, the Professor mentions the Durbin-Levinson recursion several times without explaining what this is. It is a shame as it is a very elegant bit of linear algebra for solving the Yule-Walker equations more efficiently. I tried to find a good explanation in the context of the course, However I wrote some notes that can help you understand this topic. One final point is that Durbin-Levinson recursion is not the last word on solving this system of equations. There are today numerous improvements which are both faster and more numerically stable!\nLike me, you might be curious about the Durbin-Levinson recursion mentioned above. This is not covered in the course, and turned out to be an enigma wrapped in a mystery.\nI present my finding in the note below - much of it is due to (Wikipedia contributors 2024b) and (Wikipedia contributors 2024a)\nIn (Yule 1927) and (Walker 1931), Yule and Walker proposed a method for estimating the parameters of an autoregressive model. The method is based on the Yule-Walker equations which are a set of linear equations that can be used to estimate the parameters of an autoregressive model.\nDue to the autoregressive structure of the model, the matrix for these equations is sparse and of a well known form called a Toeplitz matrix. \n\\begin{array}{c} {\\displaystyle \\qquad {\\begin{bmatrix}a&b&c&d&e\\\\f&a&b&c&d\\\\g&f&a&b&c\\\\h&g&f&a&b\\\\i&h&g&f&a\\end{bmatrix}}.} \\end{array}\n\\tag{109.1}\nIn the 1930s, Yule and Walker would have had to solve these equations using Gauss-Jordan elimination which has an O(n^3) time complexity.\nThis is where Durbin and Levinson come in. A decade or two later in (Levinson 1946) and (Durbin 1960) the authors came up for with a weakly stable yet more efficient algorithm for solving these autocorrelated system of equations which requires only O(n^2) in time complexity. Later their work was further refined in (Trench 1964) and (Zohar 1969) to just 3\\times n^2 multiplication.\nA cursory search reveals that Toeplitz matrix inversion is still an area of active research with papers covering parallel algorithms and stability studies. Not surprising as most of the more interesting deep learning models, including LLMs are autoregressive!\nSo the Durbin-Levinson recursion is just an elegant bit of linear algebra for solving the Yule-Walker equations more efficiently.\nHere is what I dug up:",
    "crumbs": [
      "<span class='chapter-number'>109</span>  <span class='chapter-title'>Appendix: Yule-Walker Equations & Durbin-Levinson Recursion</span>"
    ]
  },
  {
    "objectID": "A13.html#sec-durbin-levinson",
    "href": "A13.html#sec-durbin-levinson",
    "title": "109  Appendix: Yule-Walker Equations & Durbin-Levinson Recursion",
    "section": "",
    "text": "109.1.1 Durbin-Levinson and the Yule-Walker equations (Off-Course Reading)\nThe Durbin-Levinson recursion is a method in linear algebra for computing the solution to an equation involving a Toeplitz matrix AKA a diagonal-constant matrix where descending diagonals are constant. The recursion runs in O(n^2) time rather then O(n^3) time required by Gauss-Jordan elimination.\nThe recursion can be used to compute the coefficients of the autoregressive model of a stationary time series. It is based on the Yule-Walker equations and is used to compute the PACF of a time series.\nThe Yule-Walker equations can be stated as follows for an AR(p) process:\n\n\\gamma_m = \\sum_{k=1}^p \\phi_k \\gamma_{m-k} + \\sigma_\\epsilon^2\\delta_{m,0} \\qquad \\text{(Yule-Walker equations)}\n\\tag{109.2}\nwhere:\n\n\\gamma_m is the autocovariance function of the time series,\n\\phi_k are the AR coefficients,\n\\sigma_\\epsilon^2 is the variance of the white noise process, and\n\\delta_{m,0} is the Kronecker delta function.\n\nwhen m=0 the equation simplifies to:\n\n\\gamma_0 = \\sum_{k=1}^p \\phi_k \\gamma_{-k} + \\sigma_\\epsilon^2 \\qquad \\text{(Yule-Walker equations for m=0)}\n\\tag{109.3}\nfor m &gt; 0 the equation simplifies to:\n\n  \\begin{bmatrix}\n    \\gamma_1 \\\\\n    \\gamma_2 \\\\\n    \\gamma_3 \\\\\n    \\vdots   \\\\\n    \\gamma_p\n  \\end{bmatrix} =  \n  \\begin{bmatrix}\n    \\gamma_0     & \\gamma_{-1}  & \\gamma_{-2}  & \\cdots \\\\\n    \\gamma_1     & \\gamma_0     & \\gamma_{-1}  & \\cdots \\\\\n    \\gamma_2     & \\gamma_1     & \\gamma_0     & \\cdots \\\\\n    \\vdots       & \\vdots       & \\vdots       & \\ddots \\\\\n    \\gamma_{p-1} & \\gamma_{p-2} & \\gamma_{p-3} & \\cdots\n\\end{bmatrix}  \n\\begin{bmatrix}\n    \\phi_{1} \\\\\n    \\phi_{2} \\\\\n    \\phi_{3} \\\\\n    \\vdots   \\\\\n    \\phi_{p}\n\\end{bmatrix}\n\n and since this matrix is Toeplitz, we can use Durbin-Levinson recursion to efficiently solve the system for \\phi_k \\forall k.\nOnce \\{\\phi_m \\qquad m=1,2, \\dots ,p \\} are known, we can consider m=0 and solved for \\sigma_\\epsilon^2 by substituting the \\phi_k into Equation 109.3 Yule-Walker equations.\nOf course the Durbin-Levinson recursion is not the last word on solving this system of equations. There are today numerous improvements which are both faster and more numerically stable.\nThe Yule-Walker equations are a set of p linear equations in the p unknowns \\phi_1, \\phi_2, \\ldots, \\phi_p that can be used to estimate the parameters of an autoregressive model of order p. The Yule-Walker equations are derived by setting the sample autocorrelation function equal to the theoretical autocorrelation function of an AR(p) model and then solving for the unknown parameters. The Yule-Walker equations are given by:\n\n\\begin{aligned}\n\\gamma(0) & = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + \\ldots + \\phi_p \\gamma(p) \\\\\n\\gamma(1) & = \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) + \\ldots + \\phi_p \\gamma(p-1) \\\\\n\\gamma(2) & = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0) + \\ldots + \\phi_p \\gamma(p-2) \\\\\n\\vdots \\\\\n\\gamma(p) & = \\phi_1 \\gamma(p-1) + \\phi_2 \\gamma(p-2) + \\ldots + \\phi_p \\gamma(0) \\\\\n\\end{aligned}\n\nwhere \\gamma(k) is the sample autocorrelation function at lag k. The Yule-Walker equations can be solved using matrix algebra to obtain the estimates of the AR parameters \\phi_1, \\phi_2, \\ldots, \\phi_p.\n\n\n\n\n\n\nDurbin, J. 1960. “The Fitting of Time-Series Models.” Revue de l’Institut International de Statistique / Review of the International Statistical Institute 28 (3): 233–44. http://www.jstor.org/stable/1401322.\n\n\nLevinson, Norman. 1946. “The Wiener (Root Mean Square) Error Criterion in Filter Design and Prediction.” Journal of Mathematics and Physics 25 (1-4): 261–78. https://doi.org/https://doi.org/10.1002/sapm1946251261.\n\n\nTrench, William F. 1964. “An Algorithm for the Inversion of Finite Toeplitz Matrices.” Journal of the Society for Industrial and Applied Mathematics 12 (3): 515–22. http://ramanujan.math.trinity.edu/wtrench/research/papers/TRENCH_RP_6.PDF.\n\n\nWalker, Gilbert Thomas. 1931. “On Periodicity in Series of Related Terms.” Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character 131 (818): 518–32. https://doi.org/10.1098/rspa.1931.0069.\n\n\nWikipedia contributors. 2024a. “Autoregressive Model — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Autoregressive_model&oldid=1233171855#Estimation_of_AR_parameters.\n\n\n———. 2024b. “Levinson Recursion — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Levinson_recursion&oldid=1229942891.\n\n\nYule, George Udny. 1927. “VII. On a Method of Investigating Periodicities Disturbed Series, with Special Reference to Wolfer’s Sunspot Numbers.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 226 (636-646): 267–98. https://doi.org/10.1098/rsta.1927.0007.\n\n\nZohar, Shalhav. 1969. “Toeplitz Matrix Inversion: The Algorithm of w. F. Trench.” J. ACM 16: 592–601. https://api.semanticscholar.org/CorpusID:3115290.",
    "crumbs": [
      "<span class='chapter-number'>109</span>  <span class='chapter-title'>Appendix: Yule-Walker Equations & Durbin-Levinson Recursion</span>"
    ]
  },
  {
    "objectID": "A14.html",
    "href": "A14.html",
    "title": "110  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "",
    "text": "110.1 Properties of transpose\nThe Moore-Penrose inversion and the Cholesky decomposition are two important methods in linear algebra for solving linear equations efficiently. They are widely used in various applications, including statistics, machine learning, and numerical analysis.\ntranspose of a row vector is a column vector and vice versa:\nkA^{T} = {kA}^T \\qquad \\text{scalar multiplication }\n\\tag{110.1}\n(A^{T})^T = A \\qquad \\text { involution }\n\\tag{110.2}\n(A+B)^{T} = A^T + B^T \\qquad\\text {  distributivity under addition }\n\\tag{110.3}\n(AB)^T = B^T A^T \\qquad\\text {   anti }\n\\tag{110.4}\nnote that we swap the order of the matrices in the product when taking the transpose.\nif A is a square matrix, then the following are equivalent: \nSq = A^t*A = A*A^t\n where Sq is a symmetric positive definite matrix.\nSk = A^t*A = A*A^t",
    "crumbs": [
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A14.html#sec-moore-penrose",
    "href": "A14.html#sec-moore-penrose",
    "title": "110  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "110.3 Generelised (Moore-Penrose) Inverse",
    "text": "110.3 Generelised (Moore-Penrose) Inverse\nThe Moore-Penrose inversion is a method for computing the pseudoinverse of a matrix. The pseudoinverse is a generalization of the inverse of a matrix that can be used to solve linear equations when the matrix is rectangular, not-invertible or even singular.\n\nDefinition 110.1 (Definition of the Moore-Penrose Inverse 1) The Moore–Penrose inverse of the m × n matrix A is the n × m matrix, denoted by A^+, which satisfies the conditions\n\nAA+ A = A\n\\tag{110.5}\n  \nA^+AA^+ = A^+\n\\tag{110.6}\n\n(AA^+ )' = AA^+\n\\tag{110.7}\n\n(A^+A)' = A^+A\n\\tag{110.8}\n\nAn important features of the Moore–Penrose inverse, is that it is uniquely defined.\nCorresponding to each m × n matrix A, one and only one n × m matrix A^+ exists satisfying conditions (Equation 110.5)–(Equation 110.8).\nDefinition Definition 110.1 is the definition of a generalized inverse given by Penrose (1955).\nThe following alternative definition, which we will find useful on some occasions, utilizes properties of the Moore–Penrose inverse that were first illustrated by Moore (1935).\n\nDefinition 110.2 (Definition of the Moore-Penrose Inverse 2) Let A be an m × n matrix. Then the Moore–Penrose inverse of A is the unique n × m matrix A^+ satisfying\n\nAA^+ = P_{R(A)}\n\\tag{110.9}\n\nA^+ A = P_{R(A^+)}\n\\tag{110.10}\nwhere P_{R(A)} and P_{R(A^+)} are the projection matrices of the range spaces of A and A^+, respectively.\n\n\nTheorem 110.1 (Properties of the Moore-Penrose inverse) Let A be an m \\times n matrix. Then:\n\n(αA)^+ = α^{-1} A^+ , \\text{ if } \\alpha \\ne 0 \\text{ is a scalar}\n(A^T)^+ = (A^+)^T\n(A^+)^+ = A\nA^+ = A^{-1} ,\\text{if A is square and nonsingular}\n(A^T A)^+ = A^+ A^T and (AA^T)^+ = A^T A^+\n(AA^+)^+ = AA^+ and (A^+ A)^+ = A^+ A\nA^+ = (A^T A)^+ A^T = A^T (AA^T)^+\nA^+ = (A^T A)^{-1} A^T and A^+ A = I_n , \\text{ if } rank(A) = n\nA^+ = A^T (AA^T)^{-1} and AA^+ = I_m , \\text{ if } rank(A) = m\nA^+ = A^T if the columns of A are orthogonal, that is, A^T A = I_n\n\n\n\nTheorem 110.2 (Rank of Moore-Penrose Inverse) For any m \\times n matrix A, \\text{rank}(A) = \\text{rank}(A^+) = \\text{rank}(AA^+) = \\text{rank}(A^+ A).\n\n\nTheorem 110.3 (Symmetric Moore-Penrose Inverse) Let A be an m × m symmetric matrix. Then a. A^+ is also symmetric, b. AA^+ = $A^+A, c. A^+ = A, if A is idempotent.\n\n\nThe Moore-Penrose inverse is particularly useful in maximum likelihood estimation (MLE) for linear models. In MLE, we often need to solve linear equations of the form Ax = b, where A is the design matrix and b is the response vector. If A is not full rank or is not square, we can use the Moore-Penrose inverse to find a solution that minimizes the residual sum of squares.\nIn the context of MLE, the Moore-Penrose inverse allows us to obtain parameter estimates even when the design matrix is singular or when there are more predictors than observations. This is achieved by projecting the response vector onto the column space of the design matrix, leading to a solution that is consistent and has desirable statistical properties.\nwe start with:\n\ny = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol\\epsilon \\qquad \\boldsymbol\\epsilon \\sim \\mathcal{N} (0, v\\mathbf{I})\n\nwe want MLE of \\boldsymbol{\\beta}, which is given by: \n\\hat{\\boldsymbol{\\beta}}_{NKE} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T y\n\nAlso in the baysian setting we can show that MLE is equivalent to minimizing the negative log-likelihood function, under a uniform prior on \\boldsymbol{\\beta}, which is equivalent to minimizing the residual sum of squares.\nwe can show that if we use least squares AKA l_2 norm minimization, we will end up with the Moore-Penrose inverse to find the solution:\nwe can write this explicitly as:\n\n\\mathbb{E}_{l_2}(\\boldsymbol{\\beta}) = \\frac{1}{2} \\sum (y - \\boldsymbol{\\beta}^T \\mathbf{X})^2",
    "crumbs": [
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A14.html#sec-cholesky",
    "href": "A14.html#sec-cholesky",
    "title": "110  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "110.4 Cholesky Decomposition",
    "text": "110.4 Cholesky Decomposition\nThe Cholesky decomposition is a method for factorizing a positive definite matrix into the product of a lower triangular matrix and its transpose. It is particularly useful for solving systems of linear equations and for generating samples from multivariate normal distributions.\n\nDefinition 110.3 (Definition of the Cholesky Decomposition) André-Louis Cholesky (1875–1918) was a cartographer in the French army, who introduced a method for decomposing a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose. This decomposition is known as the Cholesky decomposition.\nLet A be a symmetric positive definite matrix. The Cholesky decomposition of A is a factorization of the form: \nA = LL^T\n\\tag{110.11} where L is a lower triangular matrix with positive diagonal entries.\n\nThe Cholesky decomposition is unique for a given positive definite matrix, and it can be computed efficiently using algorithms such as the Doolittle algorithm or the Crout algorithm.\n\n110.4.1 Doolittle Algorithm\n\ndef doolittle(A):\n    \"\"\"Performs Doolittle LU decomposition: A = LU, with L unit lower triangular and U upper triangular.\"\"\"\n    n = len(A)\n    L = [[0.0]*n for _ in range(n)]\n    U = [[0.0]*n for _ in range(n)]\n    \n    for i in range(n):\n        # Upper Triangular\n        for k in range(i, n):\n            U[i][k] = A[i][k] - sum(L[i][j]*U[j][k] for j in range(i))\n        \n        # Lower Triangular\n        L[i][i] = 1.0\n        for k in range(i+1, n):\n            if U[i][i] == 0:\n                raise ZeroDivisionError(\"Zero pivot encountered.\")\n            L[k][i] = (A[k][i] - sum(L[k][j]*U[j][i] for j in range(i))) / U[i][i]\n    \n    return L, U\n\n\ndef print_matrix(M, name=\"Matrix\"):\n    print(f\"{name} =\")\n    for row in M:\n        print(\"  [\" + \"  \".join(f\"{val:8.3f}\" for val in row) + \"]\")\n    print()\n\n\nA = [\n    [2, 3, 1],\n    [4, 7, 7],\n    [6, 18, 22]\n]\n\nprint_matrix(A, name=\"A\")\n\n\nL, U = doolittle(A)\nprint_matrix(L, name=\"L\")\nprint_matrix(U, name=\"U\")\n\nA =\n  [   2.000     3.000     1.000]\n  [   4.000     7.000     7.000]\n  [   6.000    18.000    22.000]\n\nL =\n  [   1.000     0.000     0.000]\n  [   2.000     1.000     0.000]\n  [   3.000     9.000     1.000]\n\nU =\n  [   2.000     3.000     1.000]\n  [   0.000     1.000     5.000]\n  [   0.000     0.000   -26.000]\n\n\n\n\n\n110.4.2 Doolittle’s Algorithm with Partial Pivoting\nWhen performing LU decomposition, it is often necessary to use partial pivoting to ensure numerical stability and to handle cases where the matrix may be singular or nearly singular. Partial pivoting involves swapping rows of the matrix to place the largest absolute value in the pivot position.\nAdding partial pivoting is algebraically equivalent to multiplying the original matrix by a permutation matrix P, such that PA = LU, where P is a permutation matrix, L is a lower triangular matrix, and U is an upper triangular matrix.\n\ndef doolittle_partial_pivoting(A):\n    \"\"\"Performs LU decomposition with partial pivoting: PA = LU.\"\"\"\n    n = len(A)\n    # Deep copy of A\n    A = [row[:] for row in A]\n    P = list(range(n))\n    L = [[0.0]*n for _ in range(n)]\n    U = [[0.0]*n for _ in range(n)]\n\n    for k in range(n):\n        # Partial pivoting: find row with max abs value in column k\n        pivot_row = max(range(k, n), key=lambda i: abs(A[i][k]))\n        if A[pivot_row][k] == 0:\n            raise ZeroDivisionError(\"Matrix is singular.\")\n\n        # Swap rows in A and record permutation\n        A[k], A[pivot_row] = A[pivot_row], A[k]\n        P[k], P[pivot_row] = P[pivot_row], P[k]\n        for i in range(k):\n            L[k][i], L[pivot_row][i] = L[pivot_row][i], L[k][i]\n\n        # Compute U[k][k:] and L[k+1:][k]\n        L[k][k] = 1.0\n        for j in range(k, n):\n            U[k][j] = A[k][j] - sum(L[k][s]*U[s][j] for s in range(k))\n        for i in range(k+1, n):\n            L[i][k] = (A[i][k] - sum(L[i][s]*U[s][k] for s in range(k))) / U[k][k]\n\n    # Permutation matrix P as a 2D matrix\n    P_matrix = [[1 if j == P[i] else 0 for j in range(n)] for i in range(n)]\n    return P_matrix, L, U\n\nDemo for Doolittle’s algorithm with partial pivoting:\n\nA = [\n    [0, 3, 1],\n    [4, 7, 7],\n    [6, 18, 22]\n]\n\nprint_matrix(A, name=\"A\")\n\n\nP, L, U = doolittle_partial_pivoting(A)\nprint_matrix(P, name=\"P\")\nprint_matrix(L, name=\"L\")\nprint_matrix(U, name=\"U\")\n\nA =\n  [   0.000     3.000     1.000]\n  [   4.000     7.000     7.000]\n  [   6.000    18.000    22.000]\n\nP =\n  [   0.000     0.000     1.000]\n  [   0.000     1.000     0.000]\n  [   1.000     0.000     0.000]\n\nL =\n  [   1.000     0.000     0.000]\n  [   0.667     1.000     0.000]\n  [   0.000    -0.600     1.000]\n\nU =\n  [   6.000    18.000    22.000]\n  [   0.000    -5.000    -7.667]\n  [   0.000     0.000    -3.600]\n\n\n\n\nA = [\n    [2, 1, 1, 3, 2],\n    [1, 2, 2, 1, 1],\n    [3, 2, 3, 2, 1],\n    [2, 1, 2, 2, 1],\n    [1, 1, 1, 1, 1]\n]\n\nP, L, U = doolittle_partial_pivoting(A)\n\nprint_matrix(P, \"P\")\nprint_matrix(L, \"L\")\nprint_matrix(U, \"U\")\n\ndef matmul(A, B):\n    return [[sum(A[i][k] * B[k][j] for k in range(len(B)))\n             for j in range(len(B[0]))]\n            for i in range(len(A))]\n\ndef permute(A, P):\n    \"\"\"P is a permutation matrix; return PA.\"\"\"\n    return matmul(P, A)\n\nPA = permute(A, P)\nLU = matmul(L, U)\n\n# Print comparison\nprint_matrix(PA, \"PA\")\nprint_matrix(LU, \"LU\")\n\nP =\n  [   0.000     0.000     1.000     0.000     0.000]\n  [   0.000     1.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     1.000     0.000]\n  [   1.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     1.000]\n\nL =\n  [   1.000     0.000     0.000     0.000     0.000]\n  [   0.333     1.000     0.000     0.000     0.000]\n  [   0.667    -0.250     1.000     0.000     0.000]\n  [   0.667    -0.250    -3.000     1.000     0.000]\n  [   0.333     0.250    -1.000     0.250     1.000]\n\nU =\n  [   3.000     2.000     3.000     2.000     1.000]\n  [   0.000     1.333     1.000     0.333     0.667]\n  [   0.000     0.000     0.250     0.750     0.500]\n  [   0.000     0.000     0.000     4.000     3.000]\n  [   0.000     0.000     0.000     0.000     0.250]\n\nPA =\n  [   3.000     2.000     3.000     2.000     1.000]\n  [   1.000     2.000     2.000     1.000     1.000]\n  [   2.000     1.000     2.000     2.000     1.000]\n  [   2.000     1.000     1.000     3.000     2.000]\n  [   1.000     1.000     1.000     1.000     1.000]\n\nLU =\n  [   3.000     2.000     3.000     2.000     1.000]\n  [   1.000     2.000     2.000     1.000     1.000]\n  [   2.000     1.000     2.000     2.000     1.000]\n  [   2.000     1.000     1.000     3.000     2.000]\n  [   1.000     1.000     1.000     1.000     1.000]\n\n\n\n\n\n110.4.3 Vectorization of the doolittle algorithm\n\nimport numpy as np\n\ndef doolittle_numpy(A):\n    \"\"\"LU decomposition with partial pivoting using NumPy. Returns P, L, U such that PA = LU.\"\"\"\n    A = np.array(A, dtype=float)\n    n = A.shape[0]\n    P = np.eye(n)\n    L = np.zeros((n, n))\n    U = A.copy()\n\n    for k in range(n):\n        # Partial pivoting\n        pivot = np.argmax(abs(U[k:, k])) + k\n        if U[pivot, k] == 0:\n            raise ZeroDivisionError(\"Matrix is singular.\")\n        if pivot != k:\n            U[[k, pivot]] = U[[pivot, k]]\n            P[[k, pivot]] = P[[pivot, k]]\n            L[[k, pivot], :k] = L[[pivot, k], :k]\n\n        L[k, k] = 1.0\n        L[k+1:, k] = U[k+1:, k] / U[k, k]\n        U[k+1:] -= np.outer(L[k+1:, k], U[k])\n\n    return P, L, U\n\n\ndef random_sign_matrix(n, seed=None):\n    \"\"\"Generate an n×n matrix with random entries in {-1, 0, 1}.\"\"\"\n    rng = np.random.default_rng(seed)\n    return rng.choice([-1, 0, 1], size=(n, n))\n\nA = [\n    [2, 3, 1],\n    [4, 7, 7],\n    [6, 18, 22]\n]\nA = random_sign_matrix(16, seed=42)\n\nprint_matrix(A, name=\"A\")\n\nP, L, U = doolittle_numpy(A)\n\nprint_matrix(P, name=\"P\")\n\nprint_matrix(L, name=\"L\")\n\nprint_matrix(U, name=\"U\")\n\nA =\n  [  -1.000     1.000     0.000     0.000     0.000     1.000    -1.000     1.000    -1.000    -1.000     0.000     1.000     1.000     1.000     1.000     1.000]\n  [   0.000    -1.000     1.000     0.000     0.000     0.000    -1.000     1.000     1.000     0.000     0.000     1.000     0.000     0.000     0.000    -1.000]\n  [  -1.000     0.000     1.000    -1.000     1.000     1.000    -1.000     0.000    -1.000     1.000     1.000     0.000    -1.000     1.000     0.000     1.000]\n  [   1.000     1.000     1.000    -1.000     0.000     0.000     0.000    -1.000     0.000    -1.000     1.000     1.000     1.000     1.000     0.000     1.000]\n  [   0.000    -1.000     1.000     0.000    -1.000     0.000     1.000    -1.000     0.000    -1.000     1.000     0.000    -1.000    -1.000     0.000     1.000]\n  [   1.000     0.000    -1.000     1.000     0.000     1.000    -1.000    -1.000     1.000     1.000     0.000     1.000     1.000     0.000     1.000    -1.000]\n  [  -1.000     1.000     0.000    -1.000     1.000    -1.000     1.000    -1.000     1.000     1.000     1.000     0.000     0.000     1.000    -1.000     1.000]\n  [   0.000     0.000     0.000     0.000    -1.000    -1.000    -1.000    -1.000     0.000     1.000     0.000     0.000     1.000     0.000    -1.000     1.000]\n  [   0.000     0.000     0.000     0.000    -1.000     0.000     1.000    -1.000     0.000    -1.000     0.000     0.000     1.000    -1.000    -1.000     0.000]\n  [   1.000     1.000    -1.000    -1.000     1.000    -1.000     1.000    -1.000     1.000    -1.000     0.000     0.000    -1.000     0.000     0.000     1.000]\n  [   1.000     0.000     0.000     0.000     0.000     1.000    -1.000    -1.000     0.000    -1.000    -1.000    -1.000     1.000     1.000     1.000     0.000]\n  [   1.000    -1.000     1.000     0.000     1.000    -1.000     0.000     1.000     0.000     0.000    -1.000     0.000    -1.000    -1.000     1.000     0.000]\n  [   0.000     0.000     1.000    -1.000     0.000    -1.000     0.000     1.000     0.000     1.000     0.000     1.000     0.000    -1.000     1.000     1.000]\n  [  -1.000     1.000    -1.000     1.000     1.000     0.000     1.000    -1.000    -1.000    -1.000     0.000     1.000    -1.000     0.000     1.000    -1.000]\n  [   1.000    -1.000     1.000     0.000     0.000    -1.000     0.000     1.000    -1.000     1.000     0.000     1.000     0.000     0.000    -1.000     0.000]\n  [  -1.000     0.000    -1.000     0.000     0.000    -1.000     1.000     0.000     1.000    -1.000    -1.000     0.000    -1.000    -1.000     0.000    -1.000]\n\nP =\n  [   1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000]\n\nL =\n  [   1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     1.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.500     0.750     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.000    -0.500     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   1.000    -0.500    -0.750    -1.000     0.667     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -0.000     0.000    -0.000     0.000    -0.667    -0.571     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   1.000     0.000     0.500     0.667     0.667    -0.714    -0.938     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   1.000     0.000    -0.000    -0.667     0.333    -0.143    -0.187     0.537     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -0.000    -0.500    -0.750    -0.333     0.333     0.286     0.375    -0.232     0.670     1.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.500     0.250     0.333     0.000     0.429     0.125    -0.189    -0.393     0.619     1.000     0.000     0.000     0.000     0.000     0.000]\n  [  -0.000     0.000    -0.500    -0.667     0.000     0.000     0.438    -0.242     0.214    -0.636    -0.177     1.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.000    -0.500     0.000     0.333    -0.143     0.250    -0.211    -0.625    -0.996    -0.786     0.408     1.000     0.000     0.000     0.000]\n  [  -0.000     0.000    -0.000     0.000    -0.667    -0.143    -0.188     0.032    -0.161     0.360     0.480     0.771     0.733     1.000     0.000     0.000]\n  [  -0.000    -0.500    -0.750    -0.333    -0.333     0.143    -0.250     0.042    -0.045     0.933     0.070     0.108     0.143     0.431     1.000     0.000]\n  [   1.000    -0.500     0.250    -0.333    -0.333    -0.286     0.062     0.158     0.554     0.895     0.568     0.802     0.025     0.140    -0.221     1.000]\n\nU =\n  [  -1.000     1.000     0.000     0.000     0.000     1.000    -1.000     1.000    -1.000    -1.000     0.000     1.000     1.000     1.000     1.000     1.000]\n  [   0.000     2.000     1.000    -1.000     0.000     1.000    -1.000     0.000    -1.000    -2.000     1.000     2.000     2.000     2.000     1.000     2.000]\n  [   0.000     0.000    -2.000     0.000     1.000    -1.000     1.000     0.000     1.000     0.000    -1.000    -1.000    -2.000    -1.000     0.000     0.000]\n  [   0.000     0.000     0.000     1.500    -0.750     2.250    -2.250     0.000    -0.250     1.000     0.250     1.750     2.500     0.750     1.500    -1.000]\n  [   0.000     0.000     0.000     0.000     1.500    -0.500    -0.500     2.000    -0.500    -1.000    -1.500     0.500    -1.000    -0.500     2.000     1.000]\n  [   0.000     0.000     0.000     0.000     0.000     2.333    -1.667    -2.333     0.333     2.667     2.000     0.667     0.667     1.333    -0.333    -0.667]\n  [   0.000     0.000     0.000     0.000     0.000     0.000    -2.286    -1.000    -0.143     1.857     0.143     0.714     0.714     0.429     0.143     1.286]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000    -5.938     0.104     3.646     2.896     0.146    -0.854     0.688    -2.437    -1.271]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.965     1.772     0.425     0.151     1.688     0.568    -0.379    -0.172]\n  [   0.000     0.000     0.000     0.000     0.000    -0.000     0.000     0.000     0.000    -2.134     0.095     1.141    -1.120    -0.096     0.064    -1.137]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000    -1.551    -2.328     1.486     0.788     0.474     0.854]\n  [   0.000     0.000     0.000     0.000     0.000    -0.000     0.000     0.000     0.000     0.000     0.000     1.671    -0.663    -1.065     1.553    -1.072]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.449     2.208    -1.697    -0.149]\n  [   0.000     0.000     0.000     0.000     0.000    -0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000    -2.134     0.123     1.760]\n  [   0.000     0.000     0.000     0.000     0.000    -0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.765     2.843]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.802]\n\n\n\n\n\n110.4.4 Crout’s Algorithm\nCrout’s algorithm is another method for performing LU decomposition, similar to Doolittle’s algorithm. It constructs the lower triangular matrix L directly, while the upper triangular matrix U is obtained from the original matrix A.\nthe key parts are the formula for computing the entries of L:\n\nL[i][j] = A[i][j] - \\sum_{k=0}^{j-1} L[i][k] U[k][j]\n\nand using this to computing the entries of U:\n\nU[j][i] = \\frac{A[j][i] - \\sum_{k=0}^{j-1} L[j][k] U[k][i]}{L[j][j]}\n\n\nimport numpy as np\n\ndef crout_lu(A, *, pivot=True, rtol=1e-9, atol=None):\n    \"\"\"\n    Robust Crout LU with partial pivoting.\n\n    Improvements\n    ------------\n    1. Pivot is selected from the *updated* column, eliminating the\n       “false zero-pivot” issue.\n    2. Test `abs(pivot) &lt; max(atol, rtol*‖A‖∞)` so tolerance scales with data.\n\n    Returns L, U, P such that  P @ A = L @ U.\n    \"\"\"\n    A = np.asarray_chkfinite(A, dtype=float)\n    n  = A.shape[0]\n    if A.ndim != 2 or n != A.shape[1]:\n        raise ValueError(\"square matrix required\")\n\n    L = np.zeros_like(A)\n    U = np.eye(n, dtype=A.dtype)\n    P = np.eye(n, dtype=A.dtype)\n    rows = np.arange(n)\n\n    if atol is None:\n        atol = np.finfo(A.dtype).eps * np.linalg.norm(A, np.inf) * 10\n\n    for k in range(n):\n        # current residual column\n        col = A[rows[k:], k] - L[k:, :k] @ U[:k, k]\n\n        if pivot:\n            j = k + np.argmax(np.abs(col))        # best row\n            if np.abs(col[j - k]) &lt; max(atol, rtol*np.abs(col).max()):\n                raise np.linalg.LinAlgError(\"matrix is numerically singular\")\n            if j != k:                            # swap logical rows\n                rows[[k, j]] = rows[[j, k]]\n                L[[k, j], :k] = L[[j, k], :k]\n                P[[k, j]] = P[[j, k]]\n                col[[0, j - k]] = col[[j - k, 0]]\n\n        L[k:, k] = col\n        if np.abs(L[k, k]) &lt; max(atol, rtol*np.abs(col).max()):\n            raise np.linalg.LinAlgError(\"zero pivot encountered\")\n\n        # row k of U (unit diagonal)\n        U[k, k+1:] = (A[rows[k], k+1:] - L[k, :k] @ U[:k, k+1:]) / L[k, k]\n\n    return L, U, P\n\nTest the Crout LU decomposition with a random full rank sign matrix:\n\ndef random_full_rank_sign_matrix(n, seed=42):\n    rng = np.random.default_rng(seed)\n    while True:\n        A = rng.choice([-1,0, 1], size=(n, n))\n        if np.linalg.matrix_rank(A) == n:\n            return A\n\nA = random_full_rank_sign_matrix(12, seed=42)\nprint_matrix(A, name=\"A\")\nA_orig = A.copy()  # Keep original for verification\nL, U, P = crout_lu(A)\n\n# Check correctness\nassert np.allclose(P @ A_orig, L @ U, atol=1e-8)\n\nA =\n  [  -1.000     1.000     0.000     0.000     0.000     1.000    -1.000     1.000    -1.000    -1.000     0.000     1.000]\n  [   1.000     1.000     1.000     1.000     0.000    -1.000     1.000     0.000     0.000     0.000    -1.000     1.000]\n  [   1.000     0.000     0.000     1.000     0.000     0.000     0.000    -1.000    -1.000     0.000     1.000    -1.000]\n  [   1.000     1.000    -1.000     0.000    -1.000     1.000     1.000     0.000    -1.000     1.000     0.000     1.000]\n  [   1.000     1.000     1.000    -1.000     0.000     0.000     0.000    -1.000     0.000    -1.000     1.000     1.000]\n  [   1.000     1.000     0.000     1.000     0.000    -1.000     1.000     0.000    -1.000     0.000     1.000    -1.000]\n  [   0.000    -1.000     1.000     0.000    -1.000    -1.000     0.000     1.000     1.000     0.000    -1.000     1.000]\n  [   0.000     1.000    -1.000    -1.000     1.000     1.000     0.000     1.000     1.000     0.000     1.000    -1.000]\n  [  -1.000     1.000     0.000    -1.000     1.000    -1.000     1.000    -1.000     1.000     1.000     1.000     0.000]\n  [   0.000     1.000    -1.000     1.000     0.000     0.000     0.000     0.000    -1.000    -1.000    -1.000    -1.000]\n  [   0.000     1.000     0.000     0.000     1.000     0.000    -1.000     1.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.000     1.000    -1.000     0.000    -1.000     0.000     0.000     1.000    -1.000    -1.000     0.000]\n\n\n\nThe Cholesky decomposition has several important properties:\n\nIf A is positive definite, then L is unique and has positive diagonal entries.\nThe Cholesky decomposition can be used to solve linear systems of equations of the form Ax = b by first solving Ly = b for y, and then solving L^Tx = y.\nThe Cholesky decomposition can be used to generate samples from a multivariate normal distribution by transforming samples from a standard normal distribution using the Cholesky factor L.\n\nThe Cholesky decomposition is widely used in various applications, including numerical optimization, Bayesian inference, and machine learning. It is particularly useful for solving linear systems efficiently and for generating samples from multivariate normal distributions.",
    "crumbs": [
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A14.html#sec-kron-hadamard",
    "href": "A14.html#sec-kron-hadamard",
    "title": "110  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "110.5 Kronecker Product and Hadamard Product",
    "text": "110.5 Kronecker Product and Hadamard Product\nThe Kronecker product and Hadamard product are two important operations in linear algebra that are used to manipulate matrices in various ways. These operations are particularly useful in applications such as signal processing, image processing, and machine learning, where they can be used as short hand notation for certain matrix operations used in many algorithms.\nThe Kronecker product is a matrix operation that takes two matrices and produces a block matrix by multiplying each element of the first matrix by the entire second matrix.\nThe Hadamard product, on the other hand, is an element-wise multiplication of two matrices of the same dimensions.\n\nDefinition 110.4 (Definition of the Kronecker Product) The Kronecker product of two matrices A and B, denoted by A \\otimes B, is defined as the block matrix formed by multiplying each element of A by the entire matrix B. If A is an m \\times n matrix and B is a p \\times q matrix, then the Kronecker product A \\otimes B is an (mp) \\times (nq) matrix given by:\n\nA \\otimes B = \\begin{bmatrix}\na_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\\na_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1}B & a_{m2}B & \\cdots & a_{mn}B\n\\end{bmatrix}\n\\tag{110.12}\nwhere a_{ij} are the elements of matrix A.\n\nsome properties of the Kronecker product include:\n\nTheorem 110.4 (Properties of the Kronecker Product) Let A be an m \\times n matrix and B be a p \\times q matrix. Then:\n\na \\otimes A = A\\otimes a for any scalar a (scalar multiplication property)\n(aA) \\otimes (bB) = ab(A \\otimes B) for any scalars a and b (scalar multiplication property)\n(A \\otimes B) \\otimes C =  A \\otimes (B\\otimes C) (associative property)\n(A+B) \\otimes C = (A \\otimes C) + (B \\otimes C) when A,B,C are matrices of the same size (distributive property)\nA \\otimes (B+C) = (A \\otimes B) + (A \\otimes C) when A,B,C are matrices of the same size (distributive property)\n(A \\otimes B)^{\\prime} = A^{\\prime} \\otimes B^{\\prime} (transpose property)\n(\\mathbf{a} \\otimes \\mathbf{b})^{\\prime} = \\mathbf{a}^{\\prime} \\otimes \\mathbf{b}^{\\prime} (commutativity property for vectors)\n\\text{det}(A \\otimes B) = \\text{det}(A)^{p} \\cdot \\text{det}(B)^{m} (determinant property)\n\\text{rank}(A \\otimes B) = \\text{rank}(A) \\cdot \\text{rank}(B) (rank property)\n(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}, if both A and B are invertible (inverse property)\n(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD) (multiplication property) ;. \\text{tr}(A \\otimes B) = \\text{tr}(A) \\cdot \\text{tr}(B) (trace property)",
    "crumbs": [
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A15.html",
    "href": "A15.html",
    "title": "111  Appendix: Inequalities",
    "section": "",
    "text": "111.1 Markov’s inequality\nMarkov’s inequality is a fundamental result in probability theory that provides an upper bound on the probability that a non-negative random variable exceeds a certain threshold. It is particularly useful for establishing the existence of moments and for proving other inequalities.",
    "crumbs": [
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-markov-inequality",
    "href": "A15.html#sec-markov-inequality",
    "title": "111  Appendix: Inequalities",
    "section": "",
    "text": "Theorem 111.1 (Markov’s Inequality) Let X be a non-negative random variable and let a &gt; 0. Then,\n\n\\Pr(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}.",
    "crumbs": [
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-chebyshev-inequality",
    "href": "A15.html#sec-chebyshev-inequality",
    "title": "111  Appendix: Inequalities",
    "section": "111.2 Chebyshev’s inequality",
    "text": "111.2 Chebyshev’s inequality\nChebyshev’s inequality is a powerful tool in probability theory that provides an upper bound on the probability that a random variable deviates from its mean. It is particularly useful for establishing the concentration of measure and for proving other inequalities.\n\nTheorem 111.2 (Chebyshev’s Inequality) Let X be a random variable with mean \\mu = \\mathbb{E}[X] and variance \\sigma^2 = \\operatorname{Var}(X). Then for any k &gt; 0, \n\\Pr(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}.\n\n\n\nTheorem 111.3 (Measure-theoretic Chebyshev’s Inequality) Let (\\Omega, \\mathcal{F}, \\mathbb{P}) be a probability space and let X be a random variable measurable with respect to \\mathcal{F}. If \\mu = \\mathbb{E}[X] and \\sigma^2 = \\operatorname{Var}(X), then for any k &gt; 0, \n\\Pr(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}.",
    "crumbs": [
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-cantellis-inequality",
    "href": "A15.html#sec-cantellis-inequality",
    "title": "111  Appendix: Inequalities",
    "section": "111.3 Cantelli’s inequality",
    "text": "111.3 Cantelli’s inequality\nCantelli’s inequality is a refinement of Chebyshev’s inequality that provides a one-sided bound on the probability that a random variable deviates from its mean. It is particularly useful in statistical inference and hypothesis testing.\n\nTheorem 111.4 (Cantelli’s Inequality) Let X be a random variable with mean \\mu = \\mathbb{E}[X] and variance \\sigma^2 = \\operatorname{Var}(X). Then for any k &gt; 0, \n\\Pr(X - \\mu \\geq k\\sigma) \\leq \\frac{1}{1 + k^2}.",
    "crumbs": [
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-bhattacharyyas-inequality",
    "href": "A15.html#sec-bhattacharyyas-inequality",
    "title": "111  Appendix: Inequalities",
    "section": "111.4 Bhattacharyya’s inequality",
    "text": "111.4 Bhattacharyya’s inequality\nBhattacharyya’s inequality is a refinement of Cantelli’s inequality that provides a two-sided bound on the probability that a random variable deviates from its mean. It is particularly useful in statistical inference and hypothesis testing.\nThe neat idea is that it uses the third and fourth moments of the distribution to do this.\n\nTheorem 111.5 (Bhattacharyya’s Inequality) Let X be a random variable with mean \\mu = \\mathbb{E}[X] and variance \\sigma^2 = \\operatorname{Var}(X). Then for any k &gt; 0, \n\\Pr(|X - \\mu| \\geq k\\sigma) \\leq \\frac{2}{1 + k^2}.",
    "crumbs": [
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-kolmogorov-inequality",
    "href": "A15.html#sec-kolmogorov-inequality",
    "title": "111  Appendix: Inequalities",
    "section": "111.5 Kolmogorov’s inequality",
    "text": "111.5 Kolmogorov’s inequality\nKolmogorov’s inequality is a fundamental result in probability theory that provides an upper bound on the probability of the maximum absolute value of a sum of independent random variables exceeding a certain threshold. It is particularly useful in the context of stochastic processes and random walks.\n\nIt can be used to prove the weak law of large numbers.\nIt can be used like the empirical rule but for a broad class of distributions. Stating that at least 75% of the values lie within two standard deviations of the mean and at least 89% of the values lie within three standard deviations of the mean.\n\n\nTheorem 111.6 (Kolmogorov’s Inequality) Let X_1, X_2, \\ldots, X_n be a sequence of independent random variables with zero expectation and finite variances. Then for any \\lambda \\geq 0,\n\n\\Pr \\left(\\max _{1\\leq k\\leq n}|S_{k}|\\geq \\lambda \\right)\\leq {\\frac {1}{\\lambda ^{2}}}\\operatorname {Var} [S_{n}]\\equiv {\\frac {1}{\\lambda ^{2}}}\\sum _{k=1}^{n}\\operatorname {Var} [X_{k}]={\\frac {1}{\\lambda ^{2}}}\\sum _{k=1}^{n}{\\text{E}}[X_{k}^{2}],\n\\tag{111.1}\nwhere S_k = X_1 + X_2 + \\ldots + X_k is the partial sum of the first k random variables.",
    "crumbs": [
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "112  References",
    "section": "",
    "text": "112.1 Bibliography",
    "crumbs": [
      "<span class='chapter-number'>112</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "references.html#bibliography",
    "href": "references.html#bibliography",
    "title": "112  References",
    "section": "",
    "text": "Agresti, A. 2012. Categorical Data Analysis. Wiley Series in\nProbability and Statistics. Wiley. https://books.google.co.il/books?id=UOrr47-2oisC.\n\n\nAldrich, John. 2008. “R. A. Fisher on Bayes and Bayes’\nTheorem.” Bayesian Analysis 3 (March). https://doi.org/10.1214/08-BA306.\n\n\nAutolatry. 2015. “Why Square Brackets for Expectation.”\nMathematics Stack Exchange. https://math.stackexchange.com/q/1302543.\n\n\nBanerjee, S., A. E. Gelfand, and B. P. Carlin. 2026. Hierarchical\nModeling and Analysis for Spatial Data. CRC Press LLC. https://books.google.co.il/books?id=GRFT0QEACAAJ.\n\n\nBayesian Nonparametrics. 2010. Cambridge Series in Statistical\nand Probabilistic Mathematics. Cambridge University Press.\n\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. 1980. Regression\nDiagnostics. John Wiley & Sons, Inc. https://doi.org/10.1002/0471725153.\n\n\nBernoulli, J. 1713. Ars Conjectandi [the Art of Conjecturing].\nImpensis Thurnisiorum. https://books.google.co.il/books?id=Ba5DAAAAcAAJ.\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning.\nInformation Science and Statistics. Springer (India) Private Limited. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nBlundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan\nWierstra. 2015. “Weight Uncertainty in Neural Networks.” https://doi.org/10.48550/ARXIV.1505.05424.\n\n\nBroemeling, Lyle D. 2019. Bayesian Analysis of Time Series. CRC\nPress.\n\n\nCarlin, B. P., and T. A. Louis. 2008. Bayesian Methods for Data\nAnalysis. Chapman & Hall/CRC Texts in Statistical Science. CRC\nPress. https://books.google.co.il/books?id=GTJUt8fcFx8C.\n\n\nCasella, G., and R. L. Berger. 2002. Statistical Inference.\nDuxbury Advanced Series in Statistics and Decision Sciences. Thomson\nLearning. http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&Roger%20L.Berger--Statistical%20Inference.pdf.\n\n\nChen, J. 2023. Statistical Inference Under Mixture Models. ICSA\nBook Series in Statistics. Springer Nature Singapore. https://books.google.co.il/books?id=sBXlEAAAQBAJ.\n\n\nCook, R. Dennis. 1977. “Detection of Influential Observation in\nLinear Regression.” Technometrics 19 (1): 15. https://doi.org/10.2307/1268249.\n\n\nCowpertwait, P. S. P., and A. V. Metcalfe. 2009. Introductory Time\nSeries with r. Use r! Springer New York. https://books.google.co.il/books?id=QFiZGQmvRUQC.\n\n\nCressie, N., and C. K. Wikle. 2011. Statistics for Spatio-Temporal\nData. CourseSmart Series. Wiley. https://books.google.co.il/books?id=-kOC6D0DiNYC.\n\n\nDavidson-Pilon, C. 2015. Bayesian Methods for Hackers: Probabilistic\nProgramming and Bayesian Inference. Addison-Wesley Data &\nAnalytics Series. Pearson Education. https://books.google.co.il/books?id=rMKiCgAAQBAJ.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977.\n“Maximum Likelihood from Incomplete Data via the EM\nAlgorithm.” Journal of the Royal Statistical Society: Series\nB (Methodological) 39 (1): 1–22.\n\n\nDurbin, J. 1960. “The Fitting of Time-Series Models.”\nRevue de l’Institut International de Statistique / Review of the\nInternational Statistical Institute 28 (3): 233–44. http://www.jstor.org/stable/1401322.\n\n\nFinetti, Bruno de. 1937. “La Prévision: Ses Lois Logiques, Ses\nSources Subjectives.” Annales de l’Institut Henri\nPoincaré 7 (1): 1–68.\n\n\n———. 2017. “Theory of Probability.” Edited by Antonio Machí\nand Adrian Smith. Wiley Series in Probability and Statistics,\nJanuary. https://doi.org/10.1002/9781119286387.\n\n\nFisher, R. A. 1925. Statistical Methods for Research Workers.\n1st ed. Edinburgh Oliver & Boyd.\n\n\nFruhwirth-Schnatter, S., G. Celeux, and C. P. Robert. 2019. Handbook\nof Mixture Analysis. Chapman & Hall/CRC Handbooks of Modern\nStatistical Methods. CRC Press. https://books.google.co.il/books?id=N3yCDwAAQBAJ.\n\n\nFrühwirth-Schnatter, S. 2006. Finite Mixture and Markov Switching\nModels. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=f8KiI7eRjYoC.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third\nEdition. Chapman & Hall/CRC Texts in Statistical Science.\nTaylor & Francis.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Analytical Methods\nfor Social Research. Cambridge University Press. https://books.google.co.il/books?id=c9xLKzZWoZ4C.\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su.\n2008. “A Weakly Informative Default Prior Distribution for\nLogistic and Other Regression Models.” The Annals of Applied\nStatistics 2 (4). https://doi.org/10.1214/08-aoas191.\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation,\nGibbs Distributions, and the Bayesian Restoration of Images.”\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nPAMI-6 (6): 721–41. https://doi.org/10.1109/tpami.1984.4767596.\n\n\nGhosh, Joyee, Yingbo Li, and Robin Mitra. 2018. “On the Use of\nCauchy Prior Distributions for Bayesian Logistic Regression.”\nBayesian Analysis 13 (2). https://doi.org/10.1214/17-ba1051.\n\n\nHärdle, Wolfgang Karl, and Léopold Simar. 2019. Applied Multivariate\nStatistical Analysis. Springer International Publishing. https://doi.org/10.1007/978-3-030-26006-4.\n\n\nHobbs, N. Thompson, and Mevin B. Hooten. 2015. Bayesian Models: A\nStatistical Primer for Ecologists. STU - Student edition. Princeton\nUniversity Press. http://www.jstor.org/stable/j.ctt1dr36kz.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical\nMethods. Springer New York. https://doi.org/10.1007/978-0-387-92407-6.\n\n\nJackman, Simon. 2009. “Bayesian Analysis for the Social\nSciences.” Wiley Series in Probability and Statistics,\nOctober. https://doi.org/10.1002/9780470686621.\n\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An\nIntroduction to Statistical Learning: With Applications in r.\nSpringer Texts in Statistics. Springer New York. https://books.google.co.il/books?id=qcI_AAAAQBAJ.\n\n\nJeffreys, H. 1983. Theory of Probability. International Series\nof Monographs on Physics. Clarendon Press.\n\n\nJohnson, R. A., and D. W. Wichern. 2001. Applied Multivariate\nStatistical Analysis. Pearson Modern Classics for Advanced\nStatistics Series. Prentice Hall. https://books.google.co.il/books?id=QBqlswEACAAJ.\n\n\nKruschke, John K. 2011. Doing Bayesian Data Analysis: A Tutorial\nwith R and BUGS. Burlington, MA: Academic\nPress. http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855.\n\n\nLevinson, Norman. 1946. “The Wiener (Root Mean Square) Error\nCriterion in Filter Design and Prediction.” Journal of\nMathematics and Physics 25 (1-4): 261–78. https://doi.org/https://doi.org/10.1002/sapm1946251261.\n\n\nLunn, D., C. Jackson, N. Best, A. Thomas, and D. Spiegelhalter. 2012.\nThe BUGS Book: A Practical Introduction to Bayesian Analysis.\nChapman & Hall/CRC Texts in Statistical Science. Taylor &\nFrancis. https://books.google.co.il/books?id=Cthz3XMa_VQC.\n\n\nMartin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. Bayesian Modeling and Computation in Python.\nBoca Raton.\n\n\nMcElreath, Richard. 2015. Statistical Rethinking, a Course in r and\nStan.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and Jupyter. 3rd ed. O’Reilly Media.\n\n\nMcLachlan, G. J., and D. Peel. 2004. Finite Mixture Models.\nWiley Series in Probability and Statistics. Wiley. https://books.google.co.il/books?id=c2_fAox0DQoC.\n\n\nMoivre, Abraham De. 1718. The Doctrine of Chances. H. Woodfall.\nhttps://tellingstorieswithdata.com.\n\n\nMorita, Satoshi, Peter F Thall, and Peter Müller. 2008.\n“Determining the Effective Sample Size of a Parametric\nPrior.” Biometrics 64 (2): 595–602.\n\n\nNielsen, A. 2019. Practical Time Series Analysis: Prediction with\nStatistics and Machine Learning. O’Reilly Media. https://books.google.co.il/books?id=xNOwDwAAQBAJ.\n\n\nPearson, E. S., W. S. Gosset, R. L. Plackett, and G. A. Barnard. 1990.\nStudent: A Statistical Biography of William Sealy Gosset.\nClarendon Press. https://books.google.co.il/books?id=LBDvAAAAMAAJ.\n\n\nPfaff, B. 2008. Analysis of Integrated and Cointegrated Time Series\nwith r. Use r! Springer New York. https://books.google.co.il/books?id=ca5MkRbF3fYC.\n\n\nPoisson, S. -D. 2019. “English Translation of Poisson’s\n\"Recherches Sur La Probabilité Des Jugements En Matière Criminelle Et En\nMatière Civile\" / \"Researches into the Probabilities of Judgements in\nCriminal and Civil Cases\".” https://arxiv.org/abs/1902.02782.\n\n\nPolya, G. 1945. How to Solve It. Princeton University Press. https://doi.org/10.1515/9781400828678.\n\n\nPrado, Raquel, Gabriel Huerta, and Mike West. 2000. “Bayesian\nTime-Varying Autoregressions: Theory, Methods and Applications.”\nResenhas Do Instituto de Matemática e\nEstatı́stica Da Universidade de São Paulo\n4 (4): 405–22. https://www2.stat.duke.edu/~mw/MWextrapubs/Prado2001.pdf.\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series:\nModeling, Computation, and Inference. Chapman & Hall/CRC Texts\nin Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” In The\nFoundations of Mathematics and Other Logical Essays, edited by R.\nB. Braithwaite, 156–98. McMaster University Archive for the History of\nEconomic Thought. https://EconPapers.repec.org/RePEc:hay:hetcha:ramsey1926.\n\n\nRavishanker, N., B. Raman, and R. Soyer. 2022. Dynamic Time Series\nModels Using r-INLA: An Applied Perspective. CRC Press.\n\n\nRios Insua, David, Fabrizio Ruggeri, and Michael P Wiper. 2012.\nBayesian Analysis of Stochastic Process Models. John Wiley\n& Sons.\n\n\nSchott, James R. 2016. Matrix Analysis for Statistics. Wiley\nSeries in Probability and Statistics. Wiley. https://books.google.co.il/books?id=Y2PpCgAAQBAJ.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with r.\nSpringer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nSpanos, A. 2019. Probability Theory and Statistical Inference.\nCambridge University Press. https://books.google.co.il/books?id=9nCiDwAAQBAJ.\n\n\nSpiegelhalter, David J., Nicola G. Best, Bradley P. Carlin, and Angelika\nVan Der Linde. 2002. “Bayesian Measures of Model Complexity and\nFit.” Journal of the Royal Statistical Society Series B:\nStatistical Methodology 64 (4): 583–639. https://doi.org/10.1111/1467-9868.00353.\n\n\nStorch, H. von, and F. W. Zwiers. 2002. Statistical Analysis in\nClimate Research. Cambridge University Press.\n\n\nTheodoridis, S. 2015. Machine Learning: A Bayesian and Optimization\nPerspective. Elsevier Science.\n\n\nTrench, William F. 1964. “An Algorithm for the Inversion of Finite\nToeplitz Matrices.” Journal of the Society for Industrial and\nApplied Mathematics 12 (3): 515–22. http://ramanujan.math.trinity.edu/wtrench/research/papers/TRENCH_RP_6.PDF.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential\nTools for Working with Data. 1st ed. O’Reilly Media, Inc. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nVisser, I., and M. Speekenbrink. 2022. Mixture and Hidden Markov\nModels with r. Use r! Springer International Publishing. https://books.google.co.il/books?id=Eep3EAAAQBAJ.\n\n\nWalker, Gilbert Thomas. 1931. “On Periodicity in Series of Related\nTerms.” Proceedings of the Royal Society of London. Series A,\nContaining Papers of a Mathematical and Physical Character 131\n(818): 518–32. https://doi.org/10.1098/rspa.1931.0069.\n\n\nWeckerle, Melissa. 2022. “Statistics\nprofessor wins prestigious professional statistics society award\n Baskin School of Engineering.” https://engineering.ucsc.edu/news/statistics-professor-wins-zellner-medal.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic\nModels. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.\n\n\nWiesenfarth, Manuel, and Silvia Calderazzo. 2020. “Quantification\nof Prior Impact in Terms of Effective Current Sample Size.”\nBiometrics 76 (1): 326–36. https://doi.org/https://doi.org/10.1111/biom.13124.\n\n\nWikipedia contributors. 2023a. “68–95–99.7 Rule —\nWikipedia.” https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule.\n\n\n———. 2023b. “Functional (Mathematics) —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Functional_(mathematics)&oldid=1148699341.\n\n\n———. 2024a. “Autoregressive Model —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Autoregressive_model&oldid=1233171855#Estimation_of_AR_parameters.\n\n\n———. 2024b. “Levinson Recursion —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Levinson_recursion&oldid=1229942891.\n\n\nWikle, C. K., A. Zammit-Mangion, and N. Cressie. 2019.\nSpatio-Temporal Statistics with r. Chapman & Hall/CRC the r\nSeries. CRC Press. https://books.google.co.il/books?id=FD-IDwAAQBAJ.\n\n\nWoodward, W. A., B. P. Sadler, and S. Robertson. 2022. Time Series\nfor Data Science: Analysis and Forecasting. Chapman & Hall/CRC\nTexts in Statistical Science. CRC Press. https://books.google.co.il/books?id=_W16EAAAQBAJ.\n\n\nYao, W., and S. Xiang. 2024. Mixture Models: Parametric,\nSemiparametric, and New Directions. Chapman & Hall/CRC\nMonographs on Statistics and Applied Probability. CRC Press. https://books.google.co.il/books?id=scP9EAAAQBAJ.\n\n\nYule, George Udny. 1927. “VII. On a Method of Investigating\nPeriodicities Disturbed Series, with Special Reference to Wolfer’s\nSunspot Numbers.” Philosophical Transactions of the Royal\nSociety of London. Series A, Containing Papers of a Mathematical or\nPhysical Character 226 (636-646): 267–98. https://doi.org/10.1098/rsta.1927.0007.\n\n\nZohar, Shalhav. 1969. “Toeplitz Matrix Inversion: The Algorithm of\nw. F. Trench.” J. ACM 16: 592–601. https://api.semanticscholar.org/CorpusID:3115290.",
    "crumbs": [
      "<span class='chapter-number'>112</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "C3-L07.html#sec-mixture-models-naive-bayes",
    "href": "C3-L07.html#sec-mixture-models-naive-bayes",
    "title": "76  Classification - M4L7",
    "section": "",
    "text": "K-means clustering\n\n\n\n\nK-means clustering\n\n\n\n\nMixture Models for Clustering",
    "crumbs": [
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Classification - M4L7</span>"
    ]
  },
  {
    "objectID": "A14.html#sec-full-rank",
    "href": "A14.html#sec-full-rank",
    "title": "110  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "110.2 Full Rank",
    "text": "110.2 Full Rank\nA matrix is said to be of full row rank if its rows are linearly independent, and it is of full column rank if its columns are linearly independent. A matrix is said to be of full rank if it is either of full row rank or full column rank.",
    "crumbs": [
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "C3-L07.html#naive-bayes-classifiers-sec-naive-bayes-classifiers",
    "href": "C3-L07.html#naive-bayes-classifiers-sec-naive-bayes-classifiers",
    "title": "76  Classification - M4L7",
    "section": "76.2 Naive Bayes classifiers {sec-naive-bayes-classifiers}",
    "text": "76.2 Naive Bayes classifiers {sec-naive-bayes-classifiers}\n The idea of Naive Bayes classifiers is that we want to know what is the probability that observation i belongs to class k and we can obtain this using Bayes’ theorem by computing the prior probability that an observation is in that class. This is just the frequency of the class multiplied by the density of that class and divided by the sum over the classes of the same expression.\n\n\\mathbb{P}r(x_i \\in \\text{class}_k) = \\frac{w_k \\cdot g_k(x_i \\mid \\theta_k)}{\\sum_{j=1}^K w_j \\cdot g_j(x_i\\mid \\theta_j)}\n\\tag{76.1}\nwhere w_k is the prior probability of class k, g_k(x_i \\mid \\theta_k) is the density of class k, and \\theta_k is the parameter of class k.\nwith\n\n\\tilde{c}_i = \\arg \\max_k \\mathbb{P}r(x_i \\in \\text{class}_k)\\ for \\; i=n+1,\\ldots,n+m\n\\tag{76.2}\nThe naive Bayes classifier assumes that the features are conditionally independent given the class. This means that the density of class k can be written as the product of the densities of each feature given the class: \ng_k(x_i\\mid \\theta_k) = \\prod_{l=1}^p g_{kl}(x_{il}\\mid\\theta_{kl})\n\\tag{76.3}\nwhere g_{kl}(x_{il} \\mid \\theta_{kl}) is the density of feature l given class k and \\theta_{kl} is the parameter of feature l given class k. This means that we can estimate the density of each feature separately and then multiply them together to get the density of the class.\nThis is a very strong assumption and is not true in general. However, it works well in practice and is often used in text classification problems where the features are the words in the text.\nThe Naive Bayes classifier is a special case of the mixture model where the components are the classes and the densities are the product of the densities of each feature given the class. This means that we can use the EM algorithm to estimate the parameters of the model in the same way as we did for the mixture model. The only difference is that we need to estimate the densities of each feature separately and then multiply them together to get the density of the class.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\nThe last class of problems for which mixture models are very useful is classification problems. If you come from the machine learning literature, you will call this supervised classification to contrast, again, unsupervised classification that I called clustering before. The goal in supervised classification is to start with a training set and use the information in a training set to determine the classes or the labels of a second group of observations that you call the test set. So you start with a training set that contains known labels classes. You also have a test set that has unknown labels, and you want to use this information to make predictions about the test set labels. For example, you may want to decide whether a person suffers from a disease or not based on a set of medical tests, maybe P medical tests, and you have gone out and measured those tests in a number of individuals. So you know those individuals whether they are sick or they are not sick. Based on that training set that is labeled where you know what the real quality of the individuals is, then you go out and you are going to pick just a random person that comes into your medical appointment, and based on the results of the test, now you want to decide if that individual suffers from the disease or not. So the presence of the training set is really what distinguishes clustering problems from classification problems. In clustering problems, we don’t have a training set. We don’t have anything that gives us a hint about how the classes look like. We’re trying to do the process of dividing the observations into groups in some sense blindly. That’s why it’s sometimes called unsupervised classification because you can think that the training set provides supervision in how you do the classification. In typical supervised classification problems on the other hand, you do have that training set. You do have that group of labeled observations that can help you make decisions about how the new groups will look like. So in some sense, supervised classification is a simpler problem than unsupervised classification because of the presence of the training set. Now, there are a number of classification procedures out there. This is a fairly common problem in the literature. You may be familiar with things like support vector machines or logistic regression for classification. I want to discuss today the similarities between using mixture models for classification and some techniques such as linear discriminant analysis, and in particular with Naive Bayes classifiers. The idea of Naive Bayes classifiers is very simple. So if you want to know what is the probability that observation i belongs to class k, you can typically obtain that by just using Bayes’ theorem by computing the prior probability that an observation is in that class. That is typically just the frequency of the class multiplied by the density of that class and divided by the sum over the classes of the same expression. Now, again, this should be very familiar. This quantity here is essentially what we used both in the EM algorithm to compute the [inaudible] case and in the MCMC algorithm if you are fitting a mixture model from a Bayesian perspective to sample the class labels C sub x. So in other words, it’s clear just from writing the expression from Naive Bayes that there should be a very close relationship between doing Naive Bayes and doing mixture models. In fact, you can cast Naive Bayes classifiers as just as a special case of mixture models. Let’s discuss Naive Bayes classifiers where we use Gaussian kernels for the classification. Let’s enter this a little bit of notation. So remember that we have both a test set and a training set. So let’s call X_1 up to X_n my training set, and let’s call X_n plus 1 up to X_n plus m the test set. In other words, we have n observations in the training set, we have m observations in the test set and we just group the observations together so that the first n in the sample are the training and last m are the test. In addition to this, because the training set is labeled, we’re going to have C_1 up to C_n are known, but C_1 or C_n plus 1 up to C_m plus n are unknown and we want to protect them. Let’s write a Naive Bayes classifier that uses Gaussian kernels, and we’re going to use the more general Gaussian kernels that we can. So in that case, the probability that observation i belongs to class k, it’s going to be equal to Omega_k 1 over the square root 2 Pi to the p. Remember that we’re working with P variate normal. So we can have P features for each individual, determinant of Sigma_k to the minus one 1/2 X of minus one 1/2 X_i minus Mu k transpose sigma sub k inverse X_i minus Mu k, divided by the sum over the components of exactly the same expression. This has to be l, minus Mu sub l transpose sigma l inverse X_i minus Mu l. So this is just Bayes theorem as we have written multiple times in this course. So what you do is, you need this expression only for the training set because for the test set you already know what class you are in. So what you typically do is a two-step process in which you get Mu k hat and Sigma hat sub k are estimated from the training set. You could do different things, but it’s very fairly common to just fit a multivariate Gaussian to each one of the components. So your Cs, your labels divide your training set into groups. For each one of those groups, you fit one different normal and that gives you Sigma and Mu. Similarly, for Omega k, you want to get an estimate for Omega k, and the natural thing to do is to just use the frequency, the fraction of the observations in the training set that belong to each one of the classes. Once you have those, then you classify new observations as by letting C_i be equal to the org max of that probability. Where the probabilities are computed by plugging in these maximum likelihood estimators in this formula up here. As I said, this is done for n plus 1 all the way to n plus m. So you don’t need to do this for the training set, the training set you know the labels and you use those labels to compute the MLEs that get plugged into this. Now, with additional observations in those MLEs, you can decide what are the classes for them. So this is what a naive Bayes classifier based on Gaussian distributions for each one of the classes would look like. Now, this is exactly the same as the EM algorithm that we have discussed in the past for mixture models, if we make a couple of assumptions or if we incorporate a couple of assumptions into the algorithm. So let’s write down that next. We can recast the algorithms that we just saw for naive Bayes classifier based on Gaussian kernels in the context of the EM algorithm that we have been discussing for mixtures. That is very easy, we’re going to think, again, about an E-step and an M-step, and we’re going to add an additional post-processing step, if you will. In our E-step, if you remember, what we did in the past was to compute the indicators for the variables. So that is our variables V_i,k that corresponds to the weights that are associated with each one of the components. What we’re going to do in this case is we’re going to define the V_i,k in a very simple fashion rather than doing it using Bayes theorem. Because we actually know what observations or what components are generating each of the observations in the training set, we can call V_i,k just one or zero if C_i is equal to k and zero otherwise, for all the observations that go from one to n. In other words, this is for the training set. Once we have defined our E-step in this way, we’re going to have an M-step where we compute Mu sub k and Omega sub k. To put it in the same way that we did with the EM algorithm, this is going to have a very particular shape. It’s going to have the sum from one to n of V_i,k X_i divided by the sum from one to n of V_i,k. In a similar expression for my matrix Sigma, Sigma is going to be Sigma sub k, it’s going to be one over the sum of the V_i,k from one to n, sum from one to n of V_i,k X_i minus Mu k, X_i minus Mu k transpose. These are expressions that we have seen in the past when filling mixtures of multivariate Gaussians to data. This is just a fancy way, so casting it in terms of the E-step and the M-step, it’s just a fancy way to say, I know what my assignments are, for sure, because this is a training set. So this is just computing the average of the observations that are in category K because, in this case, these are either zeros or ones. Similarly, here, this is just the variance covariance matrix of the observations that are in component K, but it’s written in a fancy way using this V_i,k as indicators. Then, we have a post-processing. It’s in the post-processing step where the test set comes into play. So for now, we have only used the training set for our calculations. In the post-processing step, what we do is we allocate C_i based on the arc max over K of the posterior distribution of the class allocations. So that is probability that X_i belongs to class K. So this is just another way to write the algorithms as we had before, that is very simple in the context of [inaudible]. So why did I go through the trouble of expressing this in this complicated manner when I had a very simple description before? Well, because now you can try to generalize this from this supervised setting where you completely break apart the estimation of the parameters that only uses the training set and the classification that only uses the test set. You can actually try to combine information from both, and it should be clear that if you have training sets that are just very small compared to the test set, the estimates that you get for Mu and Sigma will be very bad because they will be based on very few observations, very few data points. So if you could somehow use some of the information that you are recovering by doing the classification to help you estimate what Mu and Sigma are, they’ll probably give you more robust, stronger algorithm. How to do that should be relatively straightforward once you think about it in this context. For the observations to the training set, we have the value of the V_i,k, but we could add an estimate of the value of the V_i,k for the observations in the test set to this calculation. We already know how to do that. So we’re going to turn the algorithm iterative now. So these guys are always going to be defined in this way because I know the C’s, but these guys are refined at every iteration of the algorithm. I’ll just make this essentially equal to the probability that X_i belongs to class K given the current parameters of the model, so given the current Omegas, the current Mus, and the current Sigmas. Then, I can extend my sums to m down here and down here. Now, what I’m doing is, for the observations that I know what class they are in, these weights are either zeros or ones. For the ones that I don’t know but I’m trying to classify, they will be some number between zero and one, and I’m just going to do a weighted average so you can think about this, again, as a weighted average of the information that I know for sure, and the information that I’m recovering about Mu and Sigma from the classification. So again, this now becomes an iterative algorithm, so I need to think about t plus 1, t plus 1, t plus 1, t plus 1, t plus 1, t plus 1, and t plus 1. So I have turned what was just a two-step algorithm that doesn’t require any iteration, I turned it into an iterative algorithm that uses the whole sample to estimate the parameters of the classes. This is sometimes called a semi-supervised; I don’t necessarily like the term very much. But this is sometimes called a semi-supervised algorithm, in the sense that it’s not completely supervised because of the addition of this information and the fact that now, the sums go up to m. But it’s also not fully unsupervised because I’m using the information, I’m using this piece up here that has information where I know their true labels. Once the algorithm converges, I’m still going to do the post-processing step that is to go from this V_i,k’s that I computed here for the test set to generate what are the labels for those observations.\n\n\n\n\n76.2.1 LDA and the EM algorithm 🎥\n\n\n\n\nLDA\n\n\n\n\nLDA\n\n\nIt is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis and also to quadratic discriminant analysis.\nTo illustrate that connection, we start with a very simple mixture model.\nSo let’s start with a mixture model of the form:\n\nf(x) = \\sum_{k=1}^2 \\omega_k \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{\\sqrt{\\text{det}(\\Sigma)}} e^{-\\frac{1}{2}(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k)}.\n\\tag{76.4}\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\nIt is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis. And also, by the way, quadratic discriminant analysis. But let’s start with linear discriminant analysis.\nAnd to illustrate that connection, let’s start with a very simple mixture model.\nSo let’s start with a mixture model of the form, f(x) = the sum from 1 to 2. So I’m going to be working only with two components of omega k, 1 over the square root 2pi to the p determinant of sigma to the -1 half, x- 1 half, x, mu sub k, transpose sigma inverse, x- mu sub k. So this is two-component mixture with locations bearing with a component, but the same variance-covariance matrix for the two components that I have in the mixture.\nAnd let’s think about how the procedure would look like if we were to do Naive Bayes classification using this mixture. If I follow the unsupervised example that I have discussed before, the probability that I put observation i in class, 1, say, I only have two classes.\nSo as you see again, consider one of them and the other one is just 1- the numbers that I get here. It’s going to be equal.\nAnd I’m going to expand this in all its glory. It’s going to be a little bit long. So it’s going to be omega 1, 1 over the square root 2pi to the p determinant of sigma to the- 1 half, x of- 1 half, x- mu k transpose sigma inverse x- mu k. And in the denominator, we’re going to have the same expression first. And then we’re going to have omega 2, that is just 1- omega 1 but 2pi to the p determinant of sigma to the- 1 half x- 1 half x- mu 2, sigma inverse x- mu 2. Okay, and we know that the probability that xi belongs to class 1 is exactly the same expression but replacing mu, 1 which is what should be up here, replacing mu1 with mu2.\nSo, in the post processing step, we are going to assign C sub i = 1 if and only if the probability that xi belongs to class 1 is greater than the probability that xi belongs to class 2. And because the two expressions are the same in the denominator, the only thing that changes is the numerator, then this happens if and only if omega 1, 1 over the square root 2pi to the p determinant sigma to the- 1 half x- 1 half, X- mu1 transpose sigma inverse x- mu1, Is greater than omega 2, 1 over the square root 2pi to the p determinant of sigma to the- 1 half x of- 1 half x- mu2, sigma inverse x- mu2. So probability of class 1 greater than probability of class 2 only if this quantity is greater than the same thing but evaluated for the second component in the mixture. So let’s do a little bit of algebra and let’s try to simplify this expression a little bit and we will see that that simplification leads to a very nice expression that matches exactly what you get out of linear discriminant analysis. So now we want to simplify this expression that corresponds to the situation where we’re going to label an observation coming from class 1, and we want to make it much more compact. So a few things that we can observe. So one of them is we have 1 over square root 2pi to the p on both sides, so we can cancel that. The other thing that we observe is that we have the determinant of the variance-covariance matrix on both sides. And because we’re assuming that the two components have the same variance- covariance matrix, we can again just simplify both terms on either side. And the next thing that I’m going to do is I’m going to move all the omegas to one side and bring all the terms with the exponentials to the other side. If I do that, I’m going to end up on the left hand side with the exponent of- 1 half, X- mu1 transpose sigma inverse x- mu1. And then this term came to the other side in the denominator, but that just means that when it goes into the exponential, I need to change all to reverse signs. So it’s going to be- x- mu2 transpose sigma inverse x- mu2. So that’s the expression once you move this to the denominator and combine the two exponentials. And this needs to be greater than omega 2 divided by omega 1. Now, some further simplifications. I can take the logarithm on both sides and I can multiply by -2 on both sides, and I end up with an expression that looks like x- mu 1 transpose sigma inverse x- mu1- x- mu 2 transpose sigma inverse x- mu 2 has to be less than, because I’m going to end up multiplying by a -2. So less than -2 log of omega 2 divided by omega 1. So now we have this difference of two quadratic forms needs to be less than a certain constant that depends on what are my prior weights for each one of the two components. Now, to finish simplifying this, we need to expand these two squares, which is pretty straightforward. So first we’re going to have x sigma inverse x transpose sigma inverse x. This is just a square. So it’s going to be 2 times x transpose sigma inverse mu1. And finally, \\mu_1 transpose sigma inverse \\mu_1. And then we need to subtract a similar expression but using mu2 for it turns. So it’s going to be x transpose sigma inverse x. It’s going to be +, in this case, 2x transpose sigma inverse mu2. And finally, again,- mu2 transpose sigma inverse mu2, and all of these needs to be less than -2 log of omega 2, Divided by omega 1. So you can see that the expressions are relatively straightforward.\nAnd one of the things that is very nice, and it’s a consequence of having the same variance-covariance matrix for each one of the components, is that now this quadratic term of the data is going to cancel out. And so, we can just basically learn together a couple of terms. So we can write, 2 times, X transpose sigma inverse multiplied by mu2- mu1. So I’m taking this term and combining it with this term. So, the term here and the term here.\nAnd then I’m going to say that this has to be less than -2 times log of omega 2 divided by omega 1, and I’m going to move this two terms to the right. So,+ mu2 transpose sigma inverse mu2- mu1 transpose sigma inverse mu1. So this is actually quite a bit of simplification and it’s a very interesting one. Because you can think about this, Thing on the right hand side, just call this T for threshold. So this is your sum threshold and that threshold is basically computed based on the training data. So if I know the classes of some observations, I can get what the means for each one of the classes are, I can estimate the common sigma, and I can estimate the relative frequencies. And with that, I can obtain a stress score from the training set. And I can think about this matrix product as sum vector a. The form of this simplified expression is very interesting. You can see that the right-hand side, all this expression in the box, it’s just a threshold that can be easily computed from the training set. We can estimate the weight and we can estimate the mean and the covariance of the two components. And then, this product of the variance-covariance or the inverse of the variance-covariance matrix times the difference of the means corresponds to a vector a that can also be computed from the training set. So essentially, the decision of whether we classify an observation in class 1 or class 2 is going to depend on whether a linear combination, and that’s what x transpose times a is, is just a linear combination of the values of x. So whether this linear combination of the values of x is greater than a given threshold or not. In other words, what we’re doing, In a setting where we only have two variables, for example, x1 and x2, the linear combination of the entries is just a line on the plane. So this product just corresponds to a line. And by deciding whether we are above the line or below the line, we’re just saying that one of the regions corresponds to class, 2, and the other region corresponds to class 1. So this is the reason why the procedure is called linear discriminant analysis because it uses a straight line to decide whether observations should be classified in class 1 and class 2. Now, there are some more interesting things that you can do.\nFor example, you don’t have to assume that the sigmas are the same, you could assume that the \\sigma_i are different. If you were to do that, then you’d be in a situation that is analogous to this one with the main difference being that now these terms here wouldn’t necessarily simplify.\nBut then, you can rearrange terms in such a way that now, you’re going to have a quadratic form of x being less than a certain threshold. And in that case, you’re separating hyperplane.\nInstead of being a hyperplane or line, it’s going to be a quadratic form. And that is the reason why when you’re doing Naive Bayes and you’re working with kernels that are Gaussian and have different variance-covariance matrices, you call the procedure quadratic discriminant analysis. Because it uses a quadratic form, a parabola or something like that to separate the two classes that you’re working with.\nThe nice thing about thinking about this classification procedures in the context of mixture models is again, thinking about ways in which you can generalize and address the shortcomings of the procedure. It’s clear that the main issue with classification procedures based on Gaussians is that data in the real world sometimes doesn’t look like multivariate Gaussian distributions.\nOne possible extension is to instead of considering the density, this ps here to be a single Gaussian, you can kind of use mixtures a second time and borrow some ideas from when we did density estimation. And say well, I’m going to have a mixture and each component of that mixture is in turn a second mixture that may have a few components. And that may allow for the shape of the clusters to be much more general, and that’s what we call mixture discriminant analysis.\n As before, if you instead of doing the Algorithm and the simple maximum likelihood estimation that I described before, you instead use Bayesian estimators for your process, then you will have Bayesian equivalent of linear discriminant analysis and quadratic discriminant analysis. So it is very useful to think about your statistical methods in the context of mixture models for the purpose of both generalizing and understanding the shortcomings of what you’re doing.",
    "crumbs": [
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Classification - M4L7</span>"
    ]
  },
  {
    "objectID": "C3-L07.html#sec-classification-linear-quadratic-discriminant-analysis",
    "href": "C3-L07.html#sec-classification-linear-quadratic-discriminant-analysis",
    "title": "76  Classification - M4L7",
    "section": "76.3 Linear and quadratic discriminant analysis in the context of Mixture Models 🎥",
    "text": "76.3 Linear and quadratic discriminant analysis in the context of Mixture Models 🎥\n\n76.3.1 Classification example\nThis video walks through the code in Section 76.4.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\nI’m going to illustrate now to use of mixture models for classification using the wind dataset. Unlike the previous datasets that we work with, this one is not included in R by default. So the two files that you need wind training and wind tests are available on the website, make sure that you download them and that you have them in the right directory for R to read them. And in this case, I made sure that I change the directory where I’m looking at before I start working with this, and that I put my files in there.\nOkay, so the wind dataset is an interesting one, it’s a series of measurements for different varieties of wine. They come from three different cultivars, and for each particular variety of wine. They did a chemical analysis and measure 13 different variables that have to do with different chemical components present in the world. So we have a label set where we know which samples from which of the three cultivars. And now we want to use the information that we clean out of that to classify to decide a series of new wines to assign them to the cultivar that we think they come from. We actually do know the truth for the test set, so we will actually first do the predictions we’ll act as if we don’t know what the cultivar of the test set is. And then we will compare the predictions that we’re making against the truth, as a way to tell how well the algorithm is to it, okay. So the first thing that we need to do is load our dataset as I said, you need to make sure that the two files are in the directory where you’re working. So make sure of that, remember that we called n the sample size of the training set and m the training size the size of the test set. So I’m just calling the variables that way, and I’m going to use mixture of normals mixture of multivariate normals by location and scale. So I’m going to use a method that is essentially equivalent to doing quadratic discriminant analysis. And, I want to run the Algorithm that I discussed on the board, but in a situation which we assume that we’re going to work with semi-supervised learning. In other words, I went around the Version of the algorithm in which we’re going to use all the observation both in the training and the test set, to learn the parameters of the classes. So it’s going to be an iterative algorithm. So we know in advance as we have three classes because we have three cultivars. B in this case is going to be 13 because there are 13 features that were measured on each wine. So if you come down here, you can see that B 13, we can try to do a graph of the data. In this case the graph is not going to be terribly readable because there are so many variables, but it may still provide a little bit of intuition. So the variables that are measured things like alcohol, the ash, the alkalinity, the level of magnesium, the hue that has to do with how dark the wine is, proline. So you can see here where the variables are there are measured, and even though the graph is not very readable at least you can see that the classes do not fully overlap. So we do have some hope that we may be able to do classification in the problem. That’s pretty much the main thing that you can say out of this graph here, okay. So, as I said before mixture of models with different components, different variances and different means for each component its normal component in the mixture. Same type of standard initialization that we have done before. And we’re going to do the E and the M step here, remember that for the observations in the training set. We know the class, so the value of B are either 0 or 1, and because we do the calculation first in the log scale, then we do either 0 or minus infinity. So 0 corresponds to probability of 1 and minus infinity corresponds to a probability of 0 in the log scale. And then for the observations in the test set, we have just a regular way in which we compute the probability that the observation comes from each class. And once we have done this then we subtract, we do as we have always done subtract maximums and then re-standardize. So this is how the ES step gets adapted in the case of semisupervised classification. And then the structure of the m-step is exactly the same structure of the regular Algorithm. So we compute means and variances for each one of the components as weighted averages of the different quantities. We check conversions in the standard way, in which we have been checking convergence. And finally once everything is done, we will get a classification, so let’s run it for this dataset. It runs actually quite quickly, we have only 12 iterations and we have converged. Now what the Algorithm gives gave us is just the B values, that is the probability that an observation comes from a given class. Now, we typically are going to want to convert those peas into Cs and as we saw on the board, that is done by just selecting the class that has the highest probability.\nSo if we do that for our training set in this case, and if you look at the indexes here, they run from n + 1 to n + m, which means that we’re looking at test set. If we just get what is the maximum we can see that the first block of observations is assigned to component two. Most of this block is assigned to component two except for this guy here, and then the the remaining block of observation is assigned to components three. So now how does that compare with the truth? So we can actually go into winder test, and the first column of that file contains the true labels, and we can say that it matches actually pretty well. So the ones all match, the twos match except for one guy, the one we had kind of identified before, and the threes all match together. And we can actually if you just want to have a summary of how many errors you make. You can do a little comparison like this, and you can find that there is only a single error in the classification that the algorithm does.\nNow let’s compare that with just using quadratic discriminant analysis and linear discriminant analysis. The way they are implemented in R, so QDA and LDA are the two functions that you will need, they are part of the mass package. So, We first feed the QDA model and then we that fitted model to predict the classes. And now if we see what the regular QDA does is it’s going to give me this long list of probabilities for the test set. And we can turn those into labels and in particular we can see how many errors we’re making in the prediction. And you can see that we make a single mistake, which is actually not the mistake that we had made before. So if we just look at this one here and we compare it against the, Classification that our algorithm did, and we compared it against the truth.\nWe see that our algorithm makes a mistake in this observation and QDA does not, and instead the error is somewhere else in this sample. It’s basically here, so you can see that the QDA classifies this as two, when the reality is that it’s a three. So our results are not identical to QDA even though our method is asymptotically going to be equivalent to QDA but they don’t give us exactly the same result, but they give us very similar accuracy. Interestingly if you run LDA and you try to look at how many errors you have in that case, you will see that LDA in this case has no errors, even though it’s a simpler more restrictive classification procedure. So this can happen, so it’s a relatively large sample, so a single a difference in a single error is not a very large difference. So, hopefully this illustrates how classification or how measurements can be used for classification in a real life setting.",
    "crumbs": [
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Classification - M4L7</span>"
    ]
  },
  {
    "objectID": "C3-L07.html#sec-em-algorithm-classification-sample",
    "href": "C3-L07.html#sec-em-algorithm-classification-sample",
    "title": "76  Classification - M4L7",
    "section": "76.4 Sample EM algorithm for classification problems 📖 ℛ",
    "text": "76.4 Sample EM algorithm for classification problems 📖 ℛ\nIn the following code sample we will illustrate how to use the EM algorithm for classification problems.\nUsing mixture models for classification in the wine dataset Compare linear and quadratic discriminant analysis and a (semi-supervised) location and scale mixture model with K normals Comparing only against the EM algorithm",
    "crumbs": [
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Classification - M4L7</span>"
    ]
  },
  {
    "objectID": "C3-L07.html#predicted-labels",
    "href": "C3-L07.html#predicted-labels",
    "title": "76  Classification - M4L7",
    "section": "77.1 Predicted labels",
    "text": "77.1 Predicted labels\napply(v, 1, which.max)[(n+1):(n+m)] ## True labels wine.test[,1] ## Comparison apply(v, 1, which.max)[(n+1):(n+m)] == wine.test[,1] sum(!(apply(v, 1, which.max)[(n+1):(n+m)] == wine.test[,1])) # One error",
    "crumbs": [
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>Classification - M4L7</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-multivariate-EM",
    "href": "C3-L03.html#sec-multivariate-EM",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.6 Sample code for multivariate normal EM 📖 ℛ",
    "text": "68.6 Sample code for multivariate normal EM 📖 ℛ\nThis variant differs from the code sample above in that it uses the mvtnorm package to generate multivariate normal distributions. It also uses the ellipse package to plot the ellipses around the means of the components.\n\n#### Example of an EM algorithm for fitting a mixtures of K p-variate Gaussian components\n#### The algorithm is tested using simulated data\n\n## Clear the environment and load required libraries\nrm(list=ls())\nlibrary(mvtnorm)    # Multivariate normals are not default in R\nlibrary(ellipse)    # Required for plotting\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\nset.seed(63252)     # For reproducibility\n\n## Generate data from a mixture with 3 components\nKK      = 3\np       = 2\nw.true = c(0.5,0.3,0.2)  # True weights associated with the components\nmu.true     = array(0, dim=c(KK,p))\nmu.true[1,] = c(0,0)   #True mean for the first component\nmu.true[2,] = c(5,5)   #True mean for the second component\nmu.true[3,] = c(-3,7)   #True mean for the third component\nSigma.true      = array(0, dim=c(KK,p,p))\nSigma.true[1,,] = matrix(c(1,0,0,1),p,p)   #True variance for the first component\nSigma.true[2,,] = matrix(c(2,0.9,0.9,1),p,p)   #True variance for the second component\nSigma.true[3,,] = matrix(c(1,-0.9,-0.9,4),p,p)   #True variance for the third component\nn  = 120\ncc = sample(1:3, n, replace=T, prob=w.true)\nx  = array(0, dim=c(n,p))\nfor(i in 1:n){\n  x[i,] = rmvnorm(1, mu.true[cc[i],], Sigma.true[cc[i],,])\n}\n\npar(mfrow=c(1,1))\nplot(x[,1], x[,2], col=cc, type=\"n\", xlab=expression(x[1]), ylab=expression(x[2]))\ntext(x[,1], x[,2], seq(1,n), col=cc, cex=0.6)\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma.true[k,,], centre=mu.true[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n}\n\n\n\n\n\n\n\n#title(main=\"Data + True Components\")\n\n\n### Run the EM algorithm\n## Initialize the parameters\nw   = rep(1,KK)/KK  #Assign equal weight to each component to start with\nmu  = rmvnorm(KK, apply(x,2,mean), var(x))   #RandomCluster centers randomly spread over the support of the data\nSigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\nSigma[1,,] = var(x)/KK  \nSigma[2,,] = var(x)/KK\nSigma[3,,] = var(x)/KK\n\npar(mfrow=c(1,1))\nplot(x[,1], x[,2], col=cc, xlab=expression(x[1]), ylab=expression(x[2]))\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n}\ntitle(main=\"Initial estimate + Observations\")\n\n\n\n\n\n\n\ns       = 0\nsw      = FALSE\nQQ      = -Inf\nQQ.out  = NULL\nepsilon = 10^(-5)\n\nwhile(!sw){\n  ## E step\n  v = array(0, dim=c(n,KK))\n  for(k in 1:KK){\n    v[,k] = log(w[k]) + dmvnorm(x, mu[k,], Sigma[k,,],log=TRUE)  #Compute the log of the weights\n  }\n  for(i in 1:n){\n    v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n  }\n  \n  ## M step\n  w     = apply(v,2,mean)\n  mu    = array(0, dim=c(KK, p))\n  for(k in 1:KK){\n    for(i in 1:n){\n      mu[k,]    = mu[k,] + v[i,k]*x[i,]\n    }\n    mu[k,] = mu[k,]/sum(v[,k])\n  }\n  Sigma = array(0, dim=c(KK, p, p))\n  for(k in 1:KK){\n    for(i in 1:n){\n      Sigma[k,,] = Sigma[k,,] + v[i,k]*(x[i,] - mu[k,])%*%t(x[i,] - mu[k,])\n    }\n    Sigma[k,,] = Sigma[k,,]/sum(v[,k])\n  }\n  \n  ##Check convergence\n  QQn = 0\n  for(i in 1:n){\n    for(k in 1:KK){\n      QQn = QQn + v[i,k]*(log(w[k]) + dmvnorm(x[i,],mu[k,],Sigma[k,,],log=TRUE))\n    }\n  }\n  if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n    sw=TRUE\n  }\n  QQ = QQn\n  QQ.out = c(QQ.out, QQ)\n  s = s + 1\n  print(paste(s, QQn))\n  \n  #Plot current components over data\n  layout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))\n  par(mar=c(3.1,4.1,0.5,0.5))\n  plot(QQ.out[1:s],type=\"l\", xlim=c(1,max(10,s)), las=1, ylab=\"Q\")\n  \n  par(mar=c(5,4,1,0.5))\n  plot(x[,1], x[,2], col=cc, main=paste(\"s =\",s,\"   Q =\", round(QQ.out[s],4)), \n       xlab=expression(x[1]), ylab=expression(x[2]), lwd=2)\n  for(k in 1:KK){\n    lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n    lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n    lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n  }\n}\n\n[1] \"1 -582.05125374123\"\n\n\n\n\n\n\n\n\n\n[1] \"2 -559.067366495985\"\n\n\n\n\n\n\n\n\n\n[1] \"3 -543.8803866857\"\n\n\n\n\n\n\n\n\n\n[1] \"4 -527.840823447868\"\n\n\n\n\n\n\n\n\n\n[1] \"5 -511.540892774085\"\n\n\n\n\n\n\n\n\n\n[1] \"6 -483.797796090743\"\n\n\n\n\n\n\n\n\n\n[1] \"7 -464.070439621255\"\n\n\n\n\n\n\n\n\n\n[1] \"8 -455.865736477295\"\n\n\n\n\n\n\n\n\n\n[1] \"9 -455.214732499627\"\n\n\n\n\n\n\n\n\n\n[1] \"10 -455.176042939796\"\n\n\n\n\n\n\n\n\n\n[1] \"11 -455.171446608628\"\n\n\n\n\n\n\n\n\n\n[1] \"12 -455.170550189128\"\n\n\n\n\n\n\n\n\n#Plot current components over data\nlayout(matrix(c(1,2),2,1), widths=c(1,1), heights=c(1.3,3))\npar(mar=c(3.1,4.1,0.5,0.5))\nplot(QQ.out[1:s],type=\"l\", xlim=c(1,max(10,s)), las=1, ylab=\"Q\", lwd=2)\n\npar(mar=c(5,4,1,0.5))\nplot(x[,1], x[,2], col=cc, main=paste(\"s =\",s,\"   Q =\", round(QQ.out[s],4)), xlab=expression(x[1]), ylab=expression(x[2]))\nfor(k in 1:KK){\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.50), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.82), col=\"grey\", lty=2, lwd=2)\n  lines(ellipse(x=Sigma[k,,], centre=mu[k,], level=0.95), col=\"grey\", lty=2, lwd=2)\n}",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-EM-algorithm-for-mixtures",
    "href": "C3-L03.html#sec-EM-algorithm-for-mixtures",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "",
    "text": "Figure 68.1: EM - Challenge\n\n\n\n\n\n\n\n\nFigure 68.2: EM - Steps\n\n\n\n\n\n\n\n\nFigure 68.3: EM - Deep Dive",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-EM-algorithm-for-general-mixtures",
    "href": "C3-L03.html#sec-EM-algorithm-for-general-mixtures",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "",
    "text": "Figure 68.1: EM - Challenge\n\n\n\n\n\n\n\n\nFigure 68.2: EM - Steps\n\n\n\n\n\n\n\n\nFigure 68.3: EM - Deep Dive\n\n\n\n\n\n\n\n\n\n\n\n\n\n68.1.1 E step:\nSet\n\nQ(\\omega,\\theta \\mid \\omega^{(t)}, \\theta^{(t)},x) = E_{c \\mid \\omega^{(t)},\\theta^{(t)}, x} \\left[ \\log \\mathbb{P}r(x,c \\mid \\omega,\\theta) \\right]\n\\tag{68.3}\nWhere c is the latent variable indicating the component from which each observation was generated, \\omega are the weights, and \\theta are the parameters of the Gaussian components (means and standard deviations).\n\n\n68.1.2 M step:\nSet\n\n\\hat{\\omega}^{(t+1)},\\hat{\\theta}^{(t+1)} = \\arg \\max_{\\omega,\\theta} Q(\\omega,\\theta \\mid \\hat{\\omega}^{(t)}, \\hat{\\theta}^{(t)},y)\n\\tag{68.4}\nwhere \\hat{\\omega}^{(t)} and \\hat{\\theta}^{(t)} are the current estimates of the parameters, and y is the observed data.\nThese two steps are repeated until convergence, which is typically defined as the change in the full-data log-likelihood Q function being below a certain threshold.\nA key point is that if we condition each component independently on the \\omega, \\theta, x we can write\n\n\\mathbb{P}r(c_i=k \\mid \\omega, \\theta, x_i) = \\frac{\\omega_k g_k(x_i \\mid \\theta_k)}{\\sum_{j=1}^{K} \\omega_j g_j(x_i \\mid \\theta_j)}= v_{ik}(\\omega, \\theta)\n\nwhere the value of v_{ik} is interpreted as the probability that the i-th observation comes from the k-th component of the mixture assuming the population parameters \\omega and \\theta.",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-em-for-location-mixtures-of-gaussians",
    "href": "C3-L03.html#sec-em-for-location-mixtures-of-gaussians",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.2 EM for location Mixture of Gaussians 🎥",
    "text": "68.2 EM for location Mixture of Gaussians 🎥\n\n\n\n\n\n\n\nFigure 68.4: the responsibility\n\n\n\n\n\n\n\n\nFigure 68.5: the derivative of Q wrt to w\n\n\n\n\n\n\n\n\nFigure 68.6: the derivative of Q wrt to mu\n\n\n\n\n\n\n\n\nFigure 68.7: the derivative of Q wrt to sigma",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L03.html#sec-em-example-2",
    "href": "C3-L03.html#sec-em-example-2",
    "title": "68  The EM algorithm for Mixture models - M2L3",
    "section": "68.5 EM example 2 🎥",
    "text": "68.5 EM example 2 🎥\nThis video covers the code sample given in Section 68.6 below. It is a more advanced implementation of the EM algorithm for fitting a mixture of multivariate Gaussian components to simulated data.",
    "crumbs": [
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>The EM algorithm for Mixture models - M2L3</span>"
    ]
  },
  {
    "objectID": "C3-L06.html#sec-mixture-clustering",
    "href": "C3-L06.html#sec-mixture-clustering",
    "title": "75  Clustering - M4L6",
    "section": "",
    "text": "Figure 75.1: Clustering Mixture Models 1\n\n\n\n\n\n\n\n\nFigure 75.2: Clustering Mixture Models 2\n\n\n\n\n\n\n\n\nFigure 75.3: Clustering Mixture Models 3\n\n\n\n\n\n\n\n\nFigure 75.4: Clustering Mixture Models 4\n\n\n\n\n\n\n\n\nFixes the number of clusters K\nAlternates between:\n\nAssignment step: Assigns each data point to its nearest cluster center.\nUpdate step: Recomputes centers as means of assigned points.\n\n\n\n\n\nEqual component weights \\omega_k = 1/K\nShared spherical covariance \\Sigma_k = \\sigma^2 I\nFitted via the EM algorithm, where:\n\nE-step: compute soft assignment probabilities V_{ik}\nM-step: update cluster means using weighted averages\n\n\n\n\n75.1.1 Limitations of K-means:\n\nAssumes equal-sized spherical clusters\nFails with:\n\nCorrelated features\nDifferent variances per dimension\nUnequal cluster sizes\n\n\n\n\n75.1.2 Advantages of Mixture Models:\n\nAllow flexible covariances \\Sigma_k\nEstimate weights \\omega_k\nCan use alternative kernels (e.g., t-distributions)\nEnable Bayesian clustering via MCMC\n\nThus, viewing K-means as a special case of Gaussian mixture models clarifies its assumptions and guides principled extensions.",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  },
  {
    "objectID": "C3-L06.html#clustering-example",
    "href": "C3-L06.html#clustering-example",
    "title": "75  Clustering - M4L6",
    "section": "75.2 Clustering example",
    "text": "75.2 Clustering example\n\n\n## Using mixture models for clustering in the iris dataset\n## Compare k-means clustering and a location and scale mixture model with K normals\n\n### Loading data and setting up global variables\nrm(list=ls())\nlibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nlibrary(mvtnorm)\n\n\nAttaching package: 'mvtnorm'\n\n\nThe following object is masked from 'package:mclust':\n\n    dmvnorm\n\n### Defining a custom function to create pair plots\n### This is an alternative to the R function pairs() that allows for \n### more flexibility. In particular, it allows us to use text to label \n### the points\npairs2 = function(x, col=\"black\", pch=16, labels=NULL, names = colnames(x)){\n  n = dim(x)[1]\n  p = dim(x)[2]\n  par(mfrow=c(p,p))\n  for(k in 1:p){\n    for(l in 1:p){\n      if(k!=l){\n        par(mar=c(3,3,1,1)+0.1)\n        plot(x[,k], x[,l], type=\"n\", xlab=\"\", ylab=\"\")\n        if(is.null(labels)){\n          points(x[,k], x[,l], pch=pch, col=col)\n        }else{\n          text(x[,k], x[,l], labels=labels, col=col)\n        }\n      }else{\n        plot(seq(0,5), seq(0,5), type=\"n\", xlab=\"\", ylab=\"\", axes=FALSE)\n        text(2.5,2.5,names[k], cex=1.2)\n      }\n    }\n  }\n}\n\n## Setup data\ndata(iris)\nx       = as.matrix(iris[,-5])\nn       = dim(x)[1]\np       = dim(x)[2]       # Number of features\nKK      = 3\nepsilon = 0.0000001\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1))\ncolscale = c(\"black\",\"blue\",\"red\")\nshortnam  = c(\"s\",\"c\",\"g\")\npairs2(x, col=colscale[iris[,5]], labels=shortnam[as.numeric(iris[,5])])\n\n\n# Initialize the parameters of the algorithm\nset.seed(63252)\nnumruns = 15\nv.sum   = array(0, dim=c(numruns, n, KK))\nQQ.sum  = rep(0, numruns)\n\nfor(ss in 1:numruns){\n  w   = rep(1,KK)/KK  #Assign equal weight to each component to start with\n  mu  = rmvnorm(KK, apply(x,2,mean), 3*var(x))   #Cluster centers randomly spread over the support of the data\n  Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\n  Sigma[1,,] = var(x)\n  Sigma[2,,] = var(x)\n  Sigma[3,,] = var(x)\n  \n  sw     = FALSE\n  QQ     = -Inf\n  QQ.out = NULL\n  s      = 0\n  \n  while(!sw){\n    ## E step\n    v = array(0, dim=c(n,KK))\n    for(k in 1:KK){  #Compute the log of the weights\n      v[,k] = log(w[k]) + mvtnorm::dmvnorm(x, mu[k,], Sigma[k,,], log=TRUE) \n    }\n    for(i in 1:n){\n      v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n    }\n    \n    ## M step\n    w = apply(v,2,mean)\n    mu = matrix(0, nrow=KK, ncol=p)\n    for(k in 1:KK){\n      for(i in 1:n){\n        mu[k,]    = mu[k,] + v[i,k]*x[i,]\n      }\n      mu[k,] = mu[k,]/sum(v[,k])\n    }\n    Sigma = array(0,dim=c(KK, p, p))\n    for(k in 1:KK){\n      for(i in 1:n){\n        Sigma[k,,] = Sigma[k,,] + v[i,k]*(x[i,] - mu[k,])%*%t(x[i,] - mu[k,])\n      }\n      Sigma[k,,] = Sigma[k,,]/sum(v[,k])\n    }\n    \n    ##Check convergence\n    QQn = 0\n    for(i in 1:n){\n      for(k in 1:KK){\n        QQn = QQn + v[i,k]*(log(w[k]) + mvtnorm::dmvnorm(x[i,],mu[k,],Sigma[k,,],log=TRUE))\n      }\n    }\n    if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n      sw=TRUE\n    }\n    QQ = QQn\n    QQ.out = c(QQ.out, QQ)\n    s = s + 1\n  }\n  \n  v.sum[ss,,] = v\n  QQ.sum[ss]  = QQ.out[s]\n  print(paste(\"ss =\", ss))\n}\n\n[1] \"ss = 1\"\n[1] \"ss = 2\"\n[1] \"ss = 3\"\n[1] \"ss = 4\"\n[1] \"ss = 5\"\n[1] \"ss = 6\"\n[1] \"ss = 7\"\n[1] \"ss = 8\"\n[1] \"ss = 9\"\n[1] \"ss = 10\"\n[1] \"ss = 11\"\n[1] \"ss = 12\"\n[1] \"ss = 13\"\n[1] \"ss = 14\"\n[1] \"ss = 15\"\n\n## Cluster reconstruction under the mixture model\ncc = apply(v.sum[which.max(QQ.sum),,], 1 ,which.max)\ncolscale = c(\"black\",\"blue\",\"red\")\npairs2(x, col=colscale[cc], labels=cc)\nARImle = adjustedRandIndex(cc, as.numeric(iris[,5]))  # Higher values indicate larger agreement\n\n## Cluster reconstruction under the K-means algorithm\nirisCluster &lt;- kmeans(x, 3, nstart = numruns)\ncolscale = c(\"black\",\"blue\",\"red\")\npairs2(x, col=colscale[irisCluster$cluster], labels=irisCluster$cluster)\nARIkmeans = adjustedRandIndex(irisCluster$cluster, as.numeric(iris[,5]))\n\n\n\n\n\n\n\nFigure 75.1: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.\n\n\n\n\n\n\n\n\n\n\n\nFigure 75.2: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.\n\n\n\n\n\n\n\n\n\n\n\nFigure 75.3: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#r-code-maximum-likelihood-estimation-arp-conditional-likelihood",
    "href": "C4-L05.html#r-code-maximum-likelihood-estimation-arp-conditional-likelihood",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.2 R code: Maximum likelihood estimation, AR(p), conditional likelihood 📖",
    "text": "92.2 R code: Maximum likelihood estimation, AR(p), conditional likelihood 📖\n\n\n  set.seed(2021)\n# Simulate 300 observations from an AR(2) with one pair of complex-valued reciprocal roots \nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Compute the MLE for phi and the unbiased estimator for v using the conditional likelihood\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"Estimate for v: \", s2, \"\\n\")\n\n\n MLE of conditional likelihood for phi:  1.65272 -0.9189823 \n Estimate for v:  0.9901292",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C3-L06.html#sec-clustering-example",
    "href": "C3-L06.html#sec-clustering-example",
    "title": "75  Clustering - M4L6",
    "section": "75.2 Clustering example",
    "text": "75.2 Clustering example\n\n\n## Using mixture models for clustering in the iris dataset\n## Compare k-means clustering and a location and scale mixture model with K normals\n\n### Loading data and setting up global variables\nrm(list=ls())\nlibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nlibrary(mvtnorm)\n\n\nAttaching package: 'mvtnorm'\n\n\nThe following object is masked from 'package:mclust':\n\n    dmvnorm\n\n### Defining a custom function to create pair plots\n### This is an alternative to the R function pairs() that allows for \n### more flexibility. In particular, it allows us to use text to label \n### the points\npairs2 = function(x, col=\"black\", pch=16, labels=NULL, names = colnames(x)){\n  n = dim(x)[1]\n  p = dim(x)[2]\n  par(mfrow=c(p,p))\n  for(k in 1:p){\n    for(l in 1:p){\n      if(k!=l){\n        par(mar=c(3,3,1,1)+0.1)\n        plot(x[,k], x[,l], type=\"n\", xlab=\"\", ylab=\"\")\n        if(is.null(labels)){\n          points(x[,k], x[,l], pch=pch, col=col)\n        }else{\n          text(x[,k], x[,l], labels=labels, col=col)\n        }\n      }else{\n        plot(seq(0,5), seq(0,5), type=\"n\", xlab=\"\", ylab=\"\", axes=FALSE)\n        text(2.5,2.5,names[k], cex=1.2)\n      }\n    }\n  }\n}\n\n## Setup data\ndata(iris)\nx       = as.matrix(iris[,-5])\nn       = dim(x)[1]\np       = dim(x)[2]       # Number of features\nKK      = 3\nepsilon = 0.0000001\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1))\ncolscale = c(\"black\",\"blue\",\"red\")\nshortnam  = c(\"s\",\"c\",\"g\")\npairs2(x, col=colscale[iris[,5]], labels=shortnam[as.numeric(iris[,5])])\n\n\n# Initialize the parameters of the algorithm\nset.seed(63252)\nnumruns = 15\nv.sum   = array(0, dim=c(numruns, n, KK))\nQQ.sum  = rep(0, numruns)\n\nfor(ss in 1:numruns){\n  w   = rep(1,KK)/KK  #Assign equal weight to each component to start with\n  mu  = rmvnorm(KK, apply(x,2,mean), 3*var(x))   #Cluster centers randomly spread over the support of the data\n  Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\n  Sigma[1,,] = var(x)\n  Sigma[2,,] = var(x)\n  Sigma[3,,] = var(x)\n  \n  sw     = FALSE\n  QQ     = -Inf\n  QQ.out = NULL\n  s      = 0\n  \n  while(!sw){\n    ## E step\n    v = array(0, dim=c(n,KK))\n    for(k in 1:KK){  #Compute the log of the weights\n      v[,k] = log(w[k]) + mvtnorm::dmvnorm(x, mu[k,], Sigma[k,,], log=TRUE) \n    }\n    for(i in 1:n){\n      v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n    }\n    \n    ## M step\n    w = apply(v,2,mean)\n    mu = matrix(0, nrow=KK, ncol=p)\n    for(k in 1:KK){\n      for(i in 1:n){\n        mu[k,]    = mu[k,] + v[i,k]*x[i,]\n      }\n      mu[k,] = mu[k,]/sum(v[,k])\n    }\n    Sigma = array(0,dim=c(KK, p, p))\n    for(k in 1:KK){\n      for(i in 1:n){\n        Sigma[k,,] = Sigma[k,,] + v[i,k]*(x[i,] - mu[k,])%*%t(x[i,] - mu[k,])\n      }\n      Sigma[k,,] = Sigma[k,,]/sum(v[,k])\n    }\n    \n    ##Check convergence\n    QQn = 0\n    for(i in 1:n){\n      for(k in 1:KK){\n        QQn = QQn + v[i,k]*(log(w[k]) + mvtnorm::dmvnorm(x[i,],mu[k,],Sigma[k,,],log=TRUE))\n      }\n    }\n    if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n      sw=TRUE\n    }\n    QQ = QQn\n    QQ.out = c(QQ.out, QQ)\n    s = s + 1\n  }\n  \n  v.sum[ss,,] = v\n  QQ.sum[ss]  = QQ.out[s]\n  print(paste(\"ss =\", ss))\n}\n\n[1] \"ss = 1\"\n[1] \"ss = 2\"\n[1] \"ss = 3\"\n[1] \"ss = 4\"\n[1] \"ss = 5\"\n[1] \"ss = 6\"\n[1] \"ss = 7\"\n[1] \"ss = 8\"\n[1] \"ss = 9\"\n[1] \"ss = 10\"\n[1] \"ss = 11\"\n[1] \"ss = 12\"\n[1] \"ss = 13\"\n[1] \"ss = 14\"\n[1] \"ss = 15\"\n\n## Cluster reconstruction under the mixture model\ncc = apply(v.sum[which.max(QQ.sum),,], 1 ,which.max)\ncolscale = c(\"black\",\"blue\",\"red\")\npairs2(x, col=colscale[cc], labels=cc)\nARImle = adjustedRandIndex(cc, as.numeric(iris[,5]))  # Higher values indicate larger agreement\n\n## Cluster reconstruction under the K-means algorithm\nirisCluster &lt;- kmeans(x, 3, nstart = numruns)\ncolscale = c(\"black\",\"blue\",\"red\")\npairs2(x, col=colscale[irisCluster$cluster], labels=irisCluster$cluster)\nARIkmeans = adjustedRandIndex(irisCluster$cluster, as.numeric(iris[,5]))\n\n\n\n\n\n\n\nFigure 75.5: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.\n\n\n\n\n\n\n\n\n\n\n\nFigure 75.6: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.\n\n\n\n\n\n\n\n\n\n\n\nFigure 75.7: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  }
]