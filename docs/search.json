[
  {
    "objectID": "C2-L02.html",
    "href": "C2-L02.html",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "",
    "text": "32.1 Components of a Bayesian Model 🎥\n\\begin{aligned}\ny_i \\mid \\mu,\\sigma &\\stackrel{iid}{\\sim} \\mathcal{N}(\\mu,\\sigma^2) \\\\\n\\mu &\\sim \\mathcal{N}(\\mu_0,\\sigma_0^2) \\\\\n\\sigma^2 &\\sim \\mathcal{IG}(\\alpha_0,\\beta_0)\n\\end{aligned}\n\\tag{32.1}\nIn lesson one, we defined a statistical model as a mathematical structure used to imitate or approximate the data generating process. It incorporates uncertainty and variability using the theory of probability. A model could be very simple, involving only one variable.\nA model can be as simple as the one right here or as complicated and sophisticated as we need to capture the behavior of the data. So far, this model is the same for Frequentists and Bayesians.\nAs you may recall from the previous course, the frequentist approach to fitting this model would be to consider \\mu and \\sigma to be fixed but unknown constants, and then we would estimate them. To calculate our uncertainty in those estimates a frequentist approach would consider how much the estimates of \\mu and \\sigma might change if we were to repeat the sampling process and obtain another sample of 15 men, over, and over.\nIn the rest of this segment, we’re going to review the three key components of Bayesian models, that were used extensively in the previous course. These three primary components of Bayesian models which we often work with are:\n\\mathbb{P}r(y\\mid \\theta)\\ \\text{(likelihood)}\n\\mathbb{P}r(\\theta)\\ \\text{(prior)}\n\\mathbb{P}r(y,\\theta) = \\underbrace{\\mathbb{P}r(\\theta)}_{prior} \\cdot\n                        \\underbrace{\\mathbb{P}r(y \\mid \\theta)}_{likelihood}\n                        \\qquad \\text{(joint probability)}\n\\mathbb{P}r(\\theta \\mid y)\\ \\text{(posterior)}\nThe posterior distribution is the distribution of \\mathbb{P}r(\\theta \\mid y), i.e. \\theta given y. We can obtain this expression using the laws of conditional probability and specifically using Bayes’ theorem.\n\\begin{aligned}\n\\mathbb{P}r(\\theta \\mid y) &= \\frac{\\mathbb{P}r(\\theta,y)}{\\mathbb{P}r(y)}\n\\\\ &= \\frac{\\mathbb{P}r(\\theta,y)}{\\int \\mathbb{P}r(\\theta,y)}\n\\\\ &= \\frac{\\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)}{\\int \\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)\\ d\\theta}\n\\end{aligned}\nWe start with the definition of conditional probability (1). The conditional distribution, \\mathbb{P}r(\\theta \\mid y) is the ratio of the joint distribution of \\theta and y, i.e. \\mathbb{P}r(\\theta,y); with the marginal distribution of y, \\mathbb{P}r(y).\nTo make this look like the Bayes theorem that we’re familiar with the joint distribution can be rewritten as the product of the prior and the likelihood. We start with the likelihood, because that’s how we usually write Bayes’ theorem. We have the same thing in the denominator here. But we’re going to integrate over the values of theta. These integrals are replaced by summations if we know that \\theta is a discrete random variable. The marginal distribution is another important piece which we may use when we more advanced Bayesian modeling.\nThe posterior distribution is our primary tool for achieving the statistical modeling objectives from lesson one.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#sec-c2l02-components",
    "href": "C2-L02.html#sec-c2l02-components",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "",
    "text": "Figure 32.1: a Bayesian Model\n\n\n\n\n\nExample 32.1 (heights of men)  Suppose our data consists of the heights of N=15 adult men. Clearly it would be very expensive or even impossible to collect the genetic information that fully explains the variability in these men’s heights. We only have the height measurements available to us. To account for the variability, we might assume that the men’s heights follow a normal distribution.heights of men\nSo we could write the model like this: \ny_i= \\mu + \\varepsilon_i\n\n\nwhere:\n\ny_i will represent the height for person i.\ni will be our index.\n\\mu is a constant that represents the mean for all men.\n\\varepsilon_i, the individual error term for individual i.\n\n\nWe are also going to assume that these uncertainties \\varepsilon_i are drawn independently from an the same normal distribution. We can make this more precise if we specify that the \\varepsilon_i a drawn from a normal distribution with mean zero and variance \\sigma^2.\n\n\\varepsilon_i \\stackrel{iid}\\sim \\mathcal{N}(0,\\sigma^2) \\quad  i\\in 1 \\dots N\n\n\nwhere:\n\ni equal to 1 up to N which will be 15 in our case.\n\n\nEquivalently1 we could rewrite this model expressing the variability for the y_i directly as:\n\ny_i \\stackrel{iid}\\sim \\mathcal{N}(\\mu,\\sigma^2) \\quad i \\in 1 \\dots N\n\nWhat we did is substitute Normal RV of \\varepsilon then push in the mean \\mu into the RV. Which is shown in the example below. So each y_i is drawn independently from identically distributed RV with a Normal distribution parameterized with mean \\mu and variance \\sigma^2. This specifies a probability distribution and a model for the data.\n\n\n\n\n\n\nNoteheights of men\n\n\n\n\n\\begin{aligned}\ny_i&= \\mu+\\varepsilon_i,\n\\\\ \\varepsilon_i &\\stackrel{iid}\\sim \\mathcal{N}(0,\\sigma^2)\n\\end{aligned}\n another way to write this if we know the values of \\mu and \\sigma:\n\n\\begin{aligned}\ny_i &\\stackrel{iid}\\sim \\mathcal{N}(\\mu,\\sigma^2)\n\\end{aligned}\n\n\n\nIt also suggests how we might generate more fake data that behaves similarly to our original data set.\n\n\n\n\n\n\n\n\n\n\nFigure 32.2: Components of a Bayesian Model\n\n\nThe Bayesian approach, which we will follow in this class, treats our uncertainty in \\mu and \\sigma^2 using probabilities directly. We will model them as  random variables with their own probability distributions. These are often called priors, and they complete a Bayesian model.uncertainty \\to random variables\n\n\n\\mathbb{P}r(y\\mid \\theta) the likelihood of the data,\n\\mathbb{P}r(\\theta) the prior distribution for the parameters,\n\\mathbb{P}r(\\theta \\mid y) the posterior distribution for the parameters given the data\n\n\n The likelihood is the probabilistic model for the data. It describes how, given the unknown parameters, the data might be generated. We’re going to call unknown parameter theta right here. Also, in this expression, you might recognize this from the previous class, as describing a probability distribution.The likelihood\n\n The prior, the next step, is the probability distribution that characterizes our uncertainty with the parameter theta. We’re going to write it as \\mathbb{P}r(\\theta). It’s not the same distribution as this one. We’re just using this notation \\mathbb{P}r to represent the probability distribution of \\theta.The prior\n By specifying a likelihood and a prior. We get have a joint probability model for both the knowns, i.e. the data, and the unknowns, i.e. the parameters \\theta.joint probability distribution\n\nWe can see this by using the chain rule of probability. If we wanted the joint distribution of both the data and the parameters theta. Using the chain rule of probability, we could start with the distribution of \\theta and multiply that by the probability of y \\mid \\theta. That gives us an expression for the joint distribution. However if we’re going to make inferences about data and we already know the values of y, we don’t need the joint distribution, what we need is the posterior distribution. posterior distribution\n\n\n\n\n We start with the joint distribution like we have on top, and we integrate out or marginalize over the values of theta (2)How do we get the marginal distribution of y?\n\n\n\n\n\n\n\n\nNoteAnatomy of a posterior probability\n\n\n\n\n  \\begin{aligned}\n  &\\mathbb{P}r(y\\mid \\theta) && (likelihood) \\\\\n&  \\mathbb{P}r(\\theta) && (prior) \\\\\n   \\mathbb{P}r(y,\\theta) &= \\mathbb{P}r(\\theta)\\mathbb{P}r(y|\\theta) &&(joint\\ distribution) \\\\\n   \\mathbb{P}r(\\theta \\mid y) &= \\frac{\\mathbb{P}r(\\theta,y)}{\\mathbb{P}r(y)} && (conditional\\ probability) \\\\\n&= \\frac{\\mathbb{P}r(\\theta,y)}{\\int \\mathbb{P}r(\\theta,y)} \\\\\n&= \\frac{\\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)}{\\int \\mathbb{P}r(y \\mid \\theta)\\ \\mathbb{P}r(\\theta)\\ d\\theta} \\\\\n\\end{aligned}\n\\tag{32.2}\n\n\n\nWhereas non-Bayesian approaches consider a probability model for the data only, the hallmark characteristic of Bayesian models is that they specify a joint probability distribution for both data and parameters. How does the Bayesian paradigm leverage this additional assumption?\n\n\n\nThis allows us to make probabilistic assessments about how likely our particular data outcome is under any parameter setting.\nThis allows us to select the most accurate prior distribution.\nThis allows us to make probabilistic assessments about hypothetical data outcomes given particular parameter values.\nThis allows us to use the laws of conditional probability to describe our updated information about parameters given the data.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#sec-c2l02-model-specification",
    "href": "C2-L02.html#sec-c2l02-model-specification",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "32.2 Model Specification 🎥",
    "text": "32.2 Model Specification 🎥\n\n\n\n\n\n\n\nFigure 32.3: Model specification\n\n\nBefore fitting any model we first need to specify all of its components.\n\n\n\n\n\n\n\n\nFigure 32.4: The graphical model specification for the height model\n\n\n\n\n\n\n32.2.1 Hierarchical representation\nOne convenient way to do this is to write down the hierarchical form of the model. By hierarchy, we mean that the model is specified in steps or in layers. We usually start with the model for the data directly, or the likelihood. Let’s write, again, the model from the previous lesson.\nWe had the height for person i, given our parameters \\mu and \\sigma^2, so conditional on those parameters, y_i came from a normal distribution that was independent and identically distributed, where the normal distribution has mean \\mu and variance \\sigma^2, and we’re doing this for individuals 1 up to N, which was 15 in this example.\ny_i \\mid \\mu,\\sigma^2 \\stackrel{iid}\\sim N(\\mu,\\sigma^2) \\qquad \\forall \\ i \\in 1,\\dots,15\n\nThe next level that we need is the prior distribution from \\mu and \\sigma^2. For now we’re going to say that they’re independent priors. So that our prior from \\mu and \\sigma^2 is going to be able to factor Into the product of two independent priors.\n\n\\mathbb{P}r(\\mu,\\sigma^2)~=~\\mathbb{P}r(\\mu)\\mathbb{P}r(\\sigma^2)\\qquad (independence)\n\nWe can assume independents in the prior and still get dependents in the posterior distribution.\nIn the previous course we learned that the conjugate prior for \\mu, if we know the value of \\sigma^2, is a normal distribution, and that the conjugate prior for \\sigma^2 when \\mu is known is the Inverse Gamma distribution.\nLet’s suppose that our prior distribution for \\mu is a normal distribution where mean will be \\mu_0.\n\n\\mu \\sim \\mathcal{N}(\\mu_0,\\sigma^2_0)\n\nThis is just some number that you’re going to fill in here when you decide what the prior should be. Mean \\mu_0, and less say \\sigma^2_0 would be the variance of that prior.\nThe prior for \\sigma^2 will be Inverse Gamma\n\n\\sigma^2 \\sim \\mathcal{IG}(\\nu_0,\\beta_0)\n\n\nwhich has two parameters:\n\na shape parameter, \\nu_0, and\na scale parameter, \\beta_0.\n\n\nWe need to choose values for these hyper-parameters here. But we do now have a complete Bayesian model.\nWe now introduce some new ideas that were not presented in the previous course.\n\n\n\n\n\n\nNoteHierarchical representation\n\n\n\nBy hierarchy, we mean that the model is specified in steps or in layers.\n\nstart with the model for the data, or the likelihood.\nwrite the priors\nadd hyper-priors for the parameters of the priors.\n\nMore details can be seen on this wikipedia article and on this one\n\n\n\n\n32.2.2 Graphical representation\nAnother useful way to write out this model Is using what’s called a graphical representation. To write a graphical representation, we’re going to do the reverse order, we’ll start with the priors and finish with the likelihood.\nIn the graphical representation we draw what are called nodes so this would be a node for mu. The circle means that the this is a random variable that has its own distribution. So \\mu with its prior will be represented with that. And then we also have \\sigma^2. The next part of a graphical model is showing the dependence on other variables. Once we have the parameters, we can generate the data.\nFor example we have y_1, \\dots y_n. These are also random variables, so we’ll create these as nodes. And I’m going to double up the circle here to indicate that these nodes are observed, you see them in the data. So we’ll do this for all of the y_i here. And to indicate the dependence of the distributions of the y_i on \\mu and \\sigma^2, we’re going to draw arrows. So \\mu influences the distribution of y for each one of these y_i. The same is true for sigma squared, the distribution of each y depends on the distribution of \\sigma^2. Again, these nodes right here, that are double-circled, mean that they’ve been observed. If they’re shaded, which is the usual case, that also means that they’re observed. The arrows indicate the dependence between the random variables and their distributions.\nNotice that in this hierarchical representation, I wrote the dependence of the distributions also. We can simplify the graphical model by writing exchangeable random variables and I’ll define exchangeable later.\nWe’re going to write this using a representative of the y_i here on what’s called the plate. So I’m going to re draw this hierarchical structure, we have \\mu and \\sigma^2. And we don’t want to have to write all of these notes again. So I’m going to indicate that there are n of them, and I’m just going to draw one representative, y_i. And they depend on \\mu and \\sigma^2. To write a model like this, we must assume that the y_i are exchangeable. That means that the distribution for the y_i does not change if we were to switch the index label like the i on the y there. So, if for some reason, we knew that one of the y_i was different from the other y_i in its distribution, and if we also know which one it is, then we would need to write a separate node for it and not use a plate like we have here.\n\n\n\n\n\n\nNoteGraphical representation\n\n\n\n\n\n\n\n\n\n\n\nFigure 32.5: pgm-posterior\n\n\n\n\n\nIn the graphical representation we start at the top by drawing:\n\ncircle nodes for the hyperparameters.\narrows indicating that they determine the\nnodes for the priors.\nnodes for the RVs (doubled circles)\nplates (rectangles) indicating RVs that are exchangeable. We add an index to the corner of the plate to indicate the amount of replicated RVs\n\nMore details can be seen on this wikipedia article\n\n\nBoth the hierarchical and graphical representations show how you could hypothetically simulate data from this model. You start with the variables that don’t have any dependence on any other variables. You would simulate those, and then given those draws, you would simulate from the distributions for these other variables further down the chain.\nThis is also how you might simulate from a prior predictive distribution.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#sec-c2l02-posterior-derivation",
    "href": "C2-L02.html#sec-c2l02-posterior-derivation",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "32.3 Posterior derivation 🎥",
    "text": "32.3 Posterior derivation 🎥\n\n\n\n\n\n\n\nFigure 32.6: Posterior derivation\n\n\n So far, we’ve only drawn the model with two levels. But in reality, there’s nothing that will stop us from diving deeper into the rabbit hole and adding more layers.\nFor example, instead of fixing the values for the hyperparameters in the previous segment recall those hyperparameters were the \\mu_0, the \\sigma_0, the \\nu_0 and the \\beta_0, we could either specify fixed numeric values for those, or we could try to infer them from the data and model them using additional prior distributions for those variables to make this a hierarchical model.\nA good reason to construct the model hierarchically is if the data generating process is organized in levels so that groups of observations generated at a certain level are more naturally grouped e.g there is greater similarity within groups than between groups together for each subsequent levels. We will examine these types of hierarchical models in depth later in the course. Another simple example of a hierarchical model is one you saw already in the previous course c.f. Section 24.2\nBack to our model:\n\nAt the top level are the observation: y_i \\mid \\mu,\\sigma^2. This is just like the model from the previous lesson, where the observations were from independent and identically distributed normal RV with a mean \\mu and a variance, \\sigma^2.\nAt the next level we diverge, instead of having independent priors for \\mu and \\sigma^2, we’re going to have the prior for \\mu depend on the value of \\sigma^2. That is given \\sigma^2, \\mu follows a normal distribution with mean \\mu_0, just some hyperparameter that you’re going to chose. And the variance of this prior will be \\sigma^2, this parameter, divided by \\omega_0. Another hyperparameter that will scale it.\n\n\n\\begin{aligned}\ny_i \\mid \\mu,\\sigma^2 &\\stackrel{iid}{\\sim} \\mathcal{N}(\\mu,\\sigma^2) \\\\\n\\mu \\mid \\sigma^2 &\\sim \\mathcal{N}(\\mu_0,\\frac{\\sigma^2}{\\omega_0}) \\\\\n\\sigma^2 \\mid  &\\sim \\mathcal{IG}(\\nu_0,\\beta_0)\n\\end{aligned}\n\\tag{32.3}\n\nwhere:\n\n\\mu_0 is the mean of the prior for \\mu,\n\\omega_0 is a scaling factor for the variance of the prior for \\mu,\n\\nu_0 and \\beta_0 are the shape and scale parameters of the inverse gamma prior for \\sigma^2.\n\n\nWe now have a joint distribution of y \\mid \\mu and \\mu \\mid \\sigma^2 To complete this model we need to provide a prior for \\sigma^2.\n\nWe’ll just use the standard Inverse-Gamma with the same hyperparameters as last time.\n\nThe graphical representation for this model looks like this:\n\n\n\n\n\n\nNoteGraphical representation\n\n\n\n\n\n\n\n\n\n\n\nFigure 32.7: pgm-posterior-2\n\n\n\n\n\n\n\n\nWe start with the variables that don’t depend on anything else.\n\nThat would be \\sigma^2 and move down the chain.\nThe next variable is \\mu, which depends on \\sigma^2.\nThen we have the y_i dependent on both, We use a double or filled circle because the y_i’s are observed, their data, and we’re going to assume that they’re exchangeable.\nWe place the y_i’s on a plate for i \\in 1 \\dots N.\nThe distribution of y_i depends on both \\mu and \\sigma^2, so we’ll draw curves connecting those pieces there.\n\n\n$$\n\\begin{aligned}\np(\\mu \\mid y_i \\dots y_n) &\\propto \\prod_{i=1}^{n} \\left[\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2 \\sigma^2}\\right)\\right] \\cdot \\frac{1}{\\pi(1 + \\mu^2)} \\\\\n&\\propto   \\exp \\left[\\left(-\\frac{(y_i - \\mu)^2}{2 \\sigma^2}\\right) \\right ] \\cdot \\frac{1}{(1 + \\mu^2)} \\\\\n&\\propto \\frac{ \\exp \\left[ n (\\bar{y} - \\frac{\\mu^2}2{}) \\right] }{1 + \\mu^2}\n\n\\end{aligned}\n$$\nTo simulate hypothetical data from this model, we would have to first draw from the distribution of the prior for \\sigma^2. Then the distribution for mu which depends on \\sigma^2. And once we’ve drawn both of these, then we can draw random draws from the y_i, which of course depends on both of those. With multiple levels, this is an example of a hierarchical model. Once we have a model specification, we can write out what the full posterior distribution for all the parameters given the data looks like. Remember that the numerator in Bayes’ theorem is the joint distribution of all random quantities, all the nodes in this graphical representation over here from all of the layers. So for this model that we have right here, we have a joint distribution that’ll look like this. We’re going to write the joint distribution of everything y_1:n, \\mu and \\sigma^2, using the chain rule of probability, we’re going to multiply all of the distributions in the hierarchy together. So let’s start with the likelihood piece. And we’ll multiply that by the next layer, the distribution of mu, given \\sigma^2. And finally, with the prior for sigma squared. So what do these expressions right here look like?\nThe likelihood in this level because they’re all independent will be a product of normal densities. So we’re going to multiply the normal density for each y_i, given those parameters. This, again, is shorthand right here for the density of a normal distribution. So that represents this piece right here. The conditional prior of \\mu given sigma squared is also a normal. So we’re going to multiply this by a normal distribution of \\mu, where its parameters are \\mu naught and sigma squared over omega naught. And finally, we have the prior for sigma squared. We’ll multiply by the density of an inverse gamma for \\sigma^2 given the hyper parameters \\mu naught, sorry, that is given, the hyper parameters \\mu naught and and beta naught. What we have right here is the joint distribution of everything. It is the numerator in Bayes theorem. Let’s remind ourselves really fast what Bayes theorem looks like again. We have that the posterior distribution of the parameter given the data is equal to the likelihood, Times the prior. Over the same thing again. So this gives us in the numerator the joint distribution of everything which is what we’ve written right here.\nIn Bayes theorem, the numerator and the denominator are the exact same expression accept that we integrate or marginalize over all of the parameters.\nBecause the denominator is a function of the y’s only, which are known values, the denominator is just a constant number. So we can actually write the posterior distribution as being proportional to, this symbol right here represents proportional to. The joint distribution of the data and parameters, or the likelihood times the prior. The poster distribution is proportional to the joint distribution, or everything we have right here. In other words, what we have already written for this particular model is proportional to the posterior distribution of \\mu and \\sigma^2, given all of the data. The only thing missing in this expression right here is just some constant number that causes the expression to integrate to 1. If we can recognize this expression as being proportional to a common distribution, then our work is done, and we know what our posterior distribution is. This was the case for all models in the previous course. However, if we do not use conjugate priors or if the models are more complicated, then the posterior distribution will not have a standard form that we can recognize. We’re going to explore a couple of examples of this issue in the next segment.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#sec-c2l02-non-conjugate-models",
    "href": "C2-L02.html#sec-c2l02-non-conjugate-models",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "32.4 Non-conjugate models 🎥",
    "text": "32.4 Non-conjugate models 🎥\n\n\n\n\n\n\n\nFigure 32.8: Non-conjugate models\n\n\nWe’ll first look at an example of a one parameter model that is not conjugate.\n\n\n32.4.0.1 Company Personnel\nSuppose we have values that represent the percentage change in total personnel from last year to this year for, we’ll say, ten companies. These companies come from a particular industry. We’re going to assume for now, that these are independent measurements from a normal distribution with a known variance equal to one, but an unknown mean.\nSo we’ll say the percentage change in the total personnel for company I, given the unknown mean \\mu will be distributed normally with mean \\mu, and we’re just going to use variance 1.\nIn this case, the unknown mean could represent growth for this particular industry.\nIt’s the average of the growth of all the different companies. The small variance between the companies and percentage growth might be appropriate if the industry is stable.\nWe know that the conjugate prior for \\mu in this location would be a normal distribution.\nBut suppose we decide that our prior believes about \\mu are better reflected using a standard t distribution with one degree of freedom. So we could write that as the prior for \\mu is a t distribution with a location parameter 0. That’s where the center of the distribution is. A scale parameter of 1 to make it the standard t-distribution similar to a standard normal, and 1 degree of freedom.\nThis particular prior distribution has heavier tails than the conjugate and normal distribution, which can more easily accommodate the possibility of extreme values for \\mu. It is centered on zero so, that apriori, there is a 50% chance that the growth is positive and a 50% chance that the growth is negative.\n\n\nRecall that the posterior distribution of \\mu is proportional to the likelihood times the prior. Let’s write the expression for that in this model. That is the posterior distribution for \\mu given the data y_1 \\dots y_n is going to be proportional to the likelihood.\nIt is a product from i equals 1 to n, in this case that’s 10.\nDensities from a normal distribution.\nLet’s write the density from this particular normal distribution.\nIs 1 \\over \\sqrt{2 \\pi}.\nE to the negative one-half.\ny_i - \\mu^2, this is the normal density for each individual y_i and we multiplied it for likelihood.\nThe density for this t prior looks like this.\nIt’s 1 over pi times 1 plus \\mu squared.\nThis is the likelihood times the prior.\nIf we do a little algebra here, first of all, we’re doing this up to proportionality.\nSo, constants being multiplied by this expression are not important.\nThe \\sqrt{2 \\pi}^n, is just a constant number, and \\pi creates a constant number. So we will drop them in our next step.\nSo this is now proportional too, we’re removing this piece and now we’re going to use properties of exponents.\nThe product of exponents is the sum of the exponentiated pieces.\nSo we have the exponent of negative one-half times the sum from i equals 1 to n, of Yi minus \\mu squared.\nAnd then we’re dropping the pie over here, so times 1 plus \\mu squared.\nWe’re going to do a few more steps of algebra here to get a nicer expression for this piece.\nBut we’re going to skip ahead to that.\nWe’ve now added these last two expressions.\nTo arrive at this expression here for the posterior, or what’s proportional to the posterior distribution.\nThis expression right here is almost proportional to a normal distribution except we have this 1 plus \\mu squared term in the denominator.\nWe know the posterior distribution up to a constant but we don’t recognize its form as a standard distribution.\nThat we can integrate or simulate from, so we’ll have to do something else.\nLet’s move on to our second example. For a two parameter example, we’re going to return to the case where we have a normal likelihood.\nAnd we’re now going to estimate \\mu and \\sigma^2, because they’re both unknown.\nRecall that if \\sigma^2 were known, the conjugate prior from \\mu would be a normal distribution.\nAnd if \\mu were known, the conjugate prior we could choose for \\sigma^2 would be an inverse gamma.\nWe saw earlier that if you include \\sigma^2 in the prior for \\mu, and use the hierarchical model that we presented earlier, that model would be conjugate and have a closed form solution. However, in the more general case that we have right here, the posterior distribution does not appear as a distribution that we can simulate or integrate.\nChallenging posterior distributions like these ones and most others that we’ll encounter in this course kept Bayesian methods from entering the main stream of statistics for many years. Since only the simplest problems were tractable. However, computational methods invented by physicists in the 1950’s, and implemented by statisticians decades later, revolutionized the field. We now do have the ability to simulate from the posterior distributions from this lesson, as well as for many other more complicated models.",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C2-L02.html#footnotes",
    "href": "C2-L02.html#footnotes",
    "title": "32  M1L2 - Bayesian Modeling",
    "section": "",
    "text": "via the linearity of the normal distribution in the mean↩︎",
    "crumbs": [
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>M1L2 - Bayesian Modeling</span>"
    ]
  },
  {
    "objectID": "C3-L09-Ex2.html",
    "href": "C3-L09-Ex2.html",
    "title": "84  Homework on Estimating the number of components in Bayesian settings - M5L09HW2",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>84</span>  <span class='chapter-title'>Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span>"
    ]
  },
  {
    "objectID": "C3-L09-Ex3.html",
    "href": "C3-L09-Ex3.html",
    "title": "85  Homework on Estimating the partition structure in Bayesian models - M5L09HW3",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>85</span>  <span class='chapter-title'>Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span>"
    ]
  },
  {
    "objectID": "C3-L09-Ex4.html",
    "href": "C3-L09-Ex4.html",
    "title": "86  Homework on BIC for zero-inflated mixtures - M5L09HW4",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>86</span>  <span class='chapter-title'>Homework on BIC for zero-inflated mixtures - M5L09HW4</span>"
    ]
  },
  {
    "objectID": "C4-L00.html",
    "href": "C4-L00.html",
    "title": "87  Week 0: Introductions to time series analysis and the AR(1) process",
    "section": "",
    "text": "87.1 Course Card\nI decided to migrate some material that is auxiliary to the course:",
    "crumbs": [
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>Week 0: Introductions to time series analysis and the AR(1) process</span>"
    ]
  },
  {
    "objectID": "C4-L00.html#course-card",
    "href": "C4-L00.html#course-card",
    "title": "87  Week 0: Introductions to time series analysis and the AR(1) process",
    "section": "",
    "text": "Course: Bayesian Statistics: Time Series\nOffered by: University of California, Santa Cruz\nInstructor: Raquel Prado\nCertificate: Yes\nLevel: Graduate\nCommitment: 4 weeks of study, 3-4 hours/week",
    "crumbs": [
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>Week 0: Introductions to time series analysis and the AR(1) process</span>"
    ]
  },
  {
    "objectID": "C4-L00.html#overview-of-the-course",
    "href": "C4-L00.html#overview-of-the-course",
    "title": "87  Week 0: Introductions to time series analysis and the AR(1) process",
    "section": "87.2 Overview of the course",
    "text": "87.2 Overview of the course\n This course seems very similar to classic basic time series course without the Bayesian part. (AR, MA, ARMA, ARIMA, SARIMA, DLM etc.)\nOne of the questions I had when I started this course was what is the difference between a Bayesian approach to time series analysis and a classical approach. The following is a summary of what I found:\n\n\n\n\n\n\nImportantAre we Being Bayesian ?\n\n\n\n The Bayesian approach presents primarily in:\n\nSections on Bayesian inference where we do inference on the parameters of the models.\nBayesian prediction unlike an MLE prediction is a distribution of predictions not just a point estimate, and therefore is useful for quantifying uncertainty.\nWe also cover some material on model selection - this again is where the Bayesian approach to optimization presents more powerful tools than the classical approach.\nWhen we want to quantify the uncertainty in our model we have four sources of uncertainty:\n\nUncertainty due to using the correct model (structure).\n\nI consider this is an epistemic uncertainty -\nOne could reduce it by collecting more data, then applying the Bayesian model selection to choose the best model.\n\nUncertainty due to the estimation of the model parameters. This is an epistemic uncertainty - we can reduce it by collecting more data reducing the plausible intervals for these parameters under the bayesian approach.\nUncertainty due to random shocks \\varepsilon_t. for the period being predicted. This is an aleatory uncertainty.\nUncertainty in the forecasted values X_{t+h} Items 2-3 can be quantified using a plausible interval in the Bayesian approach and as we predict further into the future the interval will grow.\n\nModel selection is a big part of the Bayesian approach. We can use the DIC, WAIC, and LOO to compare models.\n\n\n\n\nThe book by Professor Prado is very comprehensive and covers plenty of additional models and references lots of recent research. These including VAR, VARMA models, Kalman filters, SMC/Particle filters, etc. These are useful for the continuous control flavours of RL. But you will need to learn it on your own.\nIn the capstone project that is the next course in the specialization the teacher adds another layer of sophistication by introducing mixtures of TS models.\nHowever unlike some courses I took we dive deep enough and get sufficient examples to understand how to put all the bits together into more sophisticated time series models.\nOne issue is that no one has published notes on this course so far - and very few people have completed the course or the specialization compared to the number of people who finished the first course.\nI found this course poorly structured and often felt it was wasting my time on trying to sort things out, searching for motivations or and looking material up in the text books or external sources.\nTo a large extent much of the mathematics we learned in the last three courses isn’t relevant for this course. The AR(1) and AR(p) processes are autoregressive which imposes a specific Algebraic structure that we haven’t seen before. The NDLMs seem to be based on regression but in reality they are extending the AR(p) process while doing so and thus have to make room to incorporate the autoregressive structure of the AR(p) process. A lot of the equations are quite different and we need to use a number of techniques from numerical linear algebra and functional analysis. These are introduced in handouts or just reference by name during the videos. (Toeplitz matrix, Durbin-Levinson recursion, Yule-Walker equations, Wald’s theorem, fourier analysis, complex roots of characteristic polynomials, The generalized Inverses, and its role as a least square estimator and as a Maximum Likelihood estimator etc. Newton Raphson approximation. Random walks, Kalman Filters) In reality many of these were vaguely familiar but not necessarily in R or in Matrix form. I noticed that most of these were also missing even from the course text books and from the wider references recommended by the instructors of the previous courses in the specialization. The instructor did not state that this is a prerequisite nor that it is just a result she is using without explaining it.\nI often had to read the books to make sense of the material.\n\nPrado, Ferreira, and West (2023) is comprehensive but more often than not less than useful - requiring jumping back and forth through the material in a way that only makes sense to someone who know most of it already. It seems like the course either lacking a coherent structure or that this simply is too illusive for a student of this course to figure out. Also it is has lot and lots of material we don’t need. What I need is more like a tutorial and motivation for the main results.\nWest and Harrison (2013) I found the other reference more useful but also a big time waster in the sense that it can be very needlessly metaphysical, often feels like self-promotion and goes into endless detail about the more trivial cases. Also the authors frequently send the reader to other reference for details about non DLM models, but in this course we also cover AR(p). Much of the approach of the DLM is based on the super position principle which allows us to mix and match different models into an DLM, but the authors assume that the reader is already familiar with these concepts.\nSo covering the material for the course takes much too much time. There are some important results that Prado references that are explained in the final chapter but the notation is quite different and I wasn’t able to fill out the maths she goes over despite being convinced that the two key results about NDLM that she presented are indeed correct.\n\nOnly once most of my notes were written could I start to see how this material comes together. However I must have spent 5-10x time more than was required to complete the course and I still need to review it once or twice more.\n\n\n87.2.1 Mathematical Review\n\nThere is a issues with mathematics most of the results and techniques are so rarely useful that students will soon forget most but a few very useful results. Having a good memory is a great asset in mathematics but is rarely enough. I like to review some mathematical results from my undergraduate days every five years or so. This helps me keep many of the results fresh in my mind and also makes reading new mathematics easier. Fundamentals in mathematics can fo a very long way. This is material from topology, determinants and solving linear equations, numerical methods for decomposing matrices, and so on. Definitions of certain groups.\nOne reason this and other Bayesian courses and books can be challenging and even overwhelming is that they can use lots of mathematics. This can range from high school material like complex numbers and quadratics formulas to intermediate results like finding root of characteristic polynomials, eigenvalues, Topelitz matrices, jordan forms, and advanced topics like the Durbin-Levinson recursion and certain results from fourier analysis and even from functional analysis theory.\n\nNote that I have not even touched on probability and statistics in that list.\nRather than complain I see this as an opportunity to review/learn some mathematics and statistics that can be useful to a data scientist. During my last sting in Data science I often was able to write formulas but more often then not felt that I lacked sufficient mathematical tools to manipulate them to get the kind of results I wanted. Rather then learning lots of mathematics I wanted to find the most practical and useful results for wrangling maths. When I was a physics undergraduate these might be trigonometric identities, completing the square, being familiar with many integrals and Taylor or Maclaurin series approximations and a few useful inequalities occasionally we use l’Hopital’s rule. Familiarity with some ODEs was also greatly beneficial as these come up in many physical models. Later on hermitian and unitary matrices, fourier expansions, spectral theory, and some results from functional analysis were useful.\nFor statistics we have the variants of the law of large numbers and the central limit theorem, convergence theorems, manipulations of the normal distribution, linear properties of expectation can get you along way. But you have to remember lots of definitions and there are lots of results and theorems that seem to be stepping stones to other results rather than any practical use.\nOn the other hand conjugacy of certain distributions as demonstrated by Herbert Lee and other instructors in this specialization are often very challenging. Charts of Convergence of distributions to other distributions under certain conditions are neat but. There is Hoeffding’s inequality and the Markov’s inequality which can be useful but like most results in mathematics I never had a where they might be used. Then there are certain results - convergence of Markov chains, doubly stochastic matrices. De Finetti’s theorem in statistics.\nI have found that the more I learn the more I can understand and appreciate the material.\n\nThe autoregressive process gives rise to matrices that have diagonal bands and more specifically Toeplitz matrices which can be solved using the Durbin-Levinson recursion mentioned many times in the course.\nDurbin-Levinson recursion - is an advanced topic not covered in Numerical Analysis courses or Algebra courses I took.\nTo use it with time series we also need to understand the Yule-Walker equations.\nar(p) require some linear algebra concepts like eigenvalues and Eigenvectors, and characteristic polynomials over \\mathbb{C}\nThe infinite order moving average representation for AR(p) requires the Wold decomposition theorem and this is not a result I recall learning in my functional analysis course. We also use some complex numbers and Fourier analysis and spectral density functions.\n\nSummarize some of the extra curricular material I found useful in the course.\n\nComplex numbers\nEigenvalues, Eigenvectors and characteristic polynomials\nYule-Walker equations (for AR(p) processes)\nDurbin-Levinson recursion - for solving Yule-Walker equations efficiently\nWiener process (Random walk)\nBrownian motion (Continuous Random walk with drift)\nMarkov Chains, Markov Property, and the Stationary Distribution (for course 1-4)\nMartingales ()\nStopping theorem\nKalman filter - filtering and smoothing\nBayesian filter - filtering and smoothing\nWold’s theorem - Decomposition of stationary time series (ARMA)\nDe Finetti’s theorem (for course 1-3)\nCholesky decomposition (for big covariance matrices)\n\n\n\n87.2.2 Complex Numbers (Review)\nWhen we wish to find the roots of real valued polynomials we will often encounter complex numbers. In this course such polynomials arise naturally in the characteristic polynomials of AR(p) processes.\nWe will need the polar form of complex numbers to represent some variants of AR(p) process.\nThe numbers in the Complex field z \\in \\mathbb{C} numbers are numbers that can be expressed in the form z = a + bi, where a,b\\in\\mathbb{R} and i is the imaginary unit. The imaginary unit i is defined as the square root of -1. Complex numbers can be added, subtracted, multiplied, and divided just like real numbers.\nThe complex conjugate  of a complex number z = a + bi is denoted by \\bar{z} = a - bi. The magnitude of a complex number z = a + bi is denoted by |z| = \\sqrt{a^2 + b^2}. This is sometimes called the modulus of the complex number in this course. The argument of a complex number z = a + bi is denoted by \\text{arg}(z) = \\tan^{-1}(b/a). The polar form of a complex number is given by z = r e^{i \\theta}, where r = |z| and \\theta = \\text{arg}(z).complex conjugate\nThe polar form of a complex number is given by:\n\n\\begin{aligned}\nz &= \\mid z\\mid e^{i \\theta} \\\\\n  &= r (\\cos(\\theta) + i \\sin(\\theta))\n\\end{aligned}\n\\tag{87.1}\nwhere:\n\n|z| is the magnitude of the complex number, i.e. the distance from the origin to the point in the complex plane.\n\\theta is the angle of the complex number.\n\nI think we will also need the unit roots.\n\n\n87.2.3 Eigenvalues, Eigenvectors the characteristic polynomials and Unit roots\nThe Eigenvalues of a matrix are the roots of the characteristic polynomial of the matrix. The characteristic polynomial of a matrix A is defined as:\n\n\\begin{aligned}\n\\text{det}(A - \\lambda I) = 0\n\\end{aligned}\n\nwhere \\lambda is the Eigenvalue and I is the identity matrix. The eigenvectors of a matrix are the vectors that satisfy the equation:\n\n\\begin{aligned}\nA v = \\lambda v\n\\end{aligned}\n\nwhere v is the eigenvector and \\lambda is the eigenvalue. The eigenvalues and eigenvectors of a matrix are used in many applications in mathematics and physics, including the diagonalization of matrices, the solution of differential equations, and the analysis of dynamical systems.\n\n87.2.3.1 Unit Roots\nA unit root is a root of the characteristic polynomial of an autoregressive model that is equal to 1. The presence of a unit root in an autoregressive model indicates that the model is not stationary. The unit root test is a statistical test that is used to determine whether a time series is stationary or non-stationary. The unit root test is based on the null hypothesis that the time series has a unit root, and the alternative hypothesis that the time series is stationary. The unit root test is used to determine whether a time series is stationary or non-stationary, and is an important tool in time series analysis.\n\n\n\n87.2.4 Spectral analysis (1898)\nThe power spectrum of a signal is the squared absolute value of its Fourier transform. If it is estimated from the discrete Fourier transform it is also called periodogram. Usually estimated using the a fast Fourier transform (FFT) algorithm.",
    "crumbs": [
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>Week 0: Introductions to time series analysis and the AR(1) process</span>"
    ]
  },
  {
    "objectID": "C4-L00.html#kalman-filter-1960",
    "href": "C4-L00.html#kalman-filter-1960",
    "title": "87  Week 0: Introductions to time series analysis and the AR(1) process",
    "section": "87.3 Kalman Filter (1960)",
    "text": "87.3 Kalman Filter (1960)\n\n\\begin{aligned}\nx_{t} & = F_{t} x_{t-1} + G_{t} u_{t} + w_{t} && \\text{(transition equation)} \\\\\ny_{t} & = H_{t} x_{t} + v_{t} && \\text{(observation equation)}\n\\end{aligned}\n\\tag{87.2}\nwhere:\n\nx_{t} is the state vector at time t,\nF_{t} is the state transition matrix,\nG_{t} is the control input matrix,\nu_{t} is the control vector,\nw_{t} is the process noise vector,\ny_{t} is the observation vector at time t,\nH_{t} is the observation matrix,\nv_{t} is the observation noise vector.\n\nThe Kalman filter is a recursive algorithm that estimates the state of a linear dynamic system from a series of noisy observations. The Kalman filter is based on a linear dynamical system model that is defined by two equations: the state transition equation and the observation equation. The state transition equation describes how the state of the system evolves over time, while the observation equation describes how the observations are generated from the state of the system. The Kalman filter uses these two equations to estimate the state of the system at each time step, based on the observations received up to that time step. This could be implemented in real time in the 1960s and was used in the Apollo missions.\nThe Extended Kalman Filter (EKF) is an extension of the Kalman filter that can be used to estimate the state of a nonlinear dynamic system. The EKF linearizes the nonlinear system model at each time step and then applies the Kalman filter to the linearized system. The EKF is an approximation to the true nonlinear system, and its accuracy depends on how well the linearized system approximates the true system.\n\n87.3.1 Box Jenkins Method (1970)\nsee Box Jenkins Method\nA five step process for identifying, selecting and assessing ARMA (and similar) models.\n\nThere are three courses on Stochastic Processes on MIT OCW that I found useful:\n\nIntroduction to Stochastic Processes\nDiscrete Stochastic Processes\nhas lecture videos and notes\npoisson processes\nAdvanced Stochastic Processes\nmartingales\nito calculus\n\n\n\n\n\n\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>Week 0: Introductions to time series analysis and the AR(1) process</span>"
    ]
  },
  {
    "objectID": "C4-L01.html",
    "href": "C4-L01.html",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "",
    "text": "88.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-c4-introduction",
    "href": "C4-L01.html#sec-c4-introduction",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "",
    "text": "88.1.1 Welcome to Bayesian Statistics: Time Series\n\nObligatory introduction to the course and the instructors.\nRaquel Prado is a professor of statistics in the Baskin School of Engineering at the University of California, Santa Cruz. She was the recipient 2022 Zellner Medal, see Weckerle (2022).\n\n\n\n88.1.2 Introduction to R\n\nIntroduction to R",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-stationarity-acf-pacf",
    "href": "C4-L01.html#sec-stationarity-acf-pacf",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.2 Stationarity the ACF and the PACF 🎥",
    "text": "88.2 Stationarity the ACF and the PACF 🎥\nBefore diving into the material here is a brief overview of the notations for timer series.\n\n\n\n\n\n\nTip 88.1: Notation\n\n\n\n\n\\{y_t\\} - the time series process, where each y_t is a univariate random variable and t are the time points that are equally spaced.\ny_{1:T} or y_1, y_2, \\ldots, y_T - the observed data.\nYou will see the use of ’ to denote the transpose of a matrix,\nand the use of \\sim to denote a distribution.\nunder tildes \\utilde{y} are used to denote estimates of the true values y.\nE matrix of eigenvalues\n\\Lambda = diagonal(\\alpha_1, \\alpha_2, \\ldots , \\alpha_p) is a diagonal matrix with the eigenvalues of \\Sigma on the diagonal.\nJ_p(1) = a p by p Jordan form matrix with 1 on the super-diagonal\n\nalso see (Prado, Ferreira, and West 2023, 2–3)\n\n\n\n88.2.1 Stationarity 🎥\n\n\n\n\n\n\n\nFigure 88.1: strong and weak stationarity\n\n\n Stationarity c.f. (Prado, Ferreira, and West 2023, sec. 1.2) is a fundamental concept in time series analysis.\n\n\n\n\n\n\nImportantTL;DR – Stationarity\n\n\n\n\nStationarity\n\n\n\n\nA time series is said to be stationary if its statistical properties such as mean, variance, and auto-correlation do not change over time.\n\n\n\nWe make this definition more formal in the definitions of strong and weak stationarity below.\n\n\n\nStationarityStationarity is a key concept in time series analysis. A time series is said to be stationary if its statistical properties such as mean, variance, and auto-correlation do not change over time.\n\nDefinition 88.1 (Strong Stationarity)  Let y_t be a time series. We say that y_t is stationary if the following conditions hold:Strong Stationarity\nLet \\{y_t\\} \\quad \\forall n&gt;0 be a time series and h &gt; 0 be a lag. If for any subsequence the distribution of y_t, y_{t+1}, \\ldots, y_{t+n} is the same as the distribution of y_{t+h}, y_{t+h+1}, \\ldots, y_{t+h+n} we call the series strongly stationary.\n\nAs it’s difficult to verify strong stationarity in practice, we will often use the following weaker notion of stationarity.\n\nDefinition 88.2 (Weak Stationarity)   The mean, variance, and auto-covariance are constant over time.Weak StationaritySecond-order Stationarity\n\n\\begin{aligned}\n\\mathbb{E}[y_t] &= \\mu \\quad \\forall t \\\\\n\\mathbb{V}ar[y_t] &= \\nu =\\sigma^2 \\quad \\forall t \\\\\n\\mathbb{C}ov[y_t , y_s ] &= γ(t − s)\n\\end{aligned}\n\\tag{88.1}\n\n\nStrong stationarity \\implies Weak stationarity, but\nThe converse is not true.\nIn this course when we deal with a Gaussian process, our typical use case, they are equivalent!\n\n\n\n\n\n\n\nCautionCheck your understanding\n\n\n\nQ. Can you explain with an example when a time series is weakly stationary but not strongly stationary?\n\n\n\n\n88.2.2 The auto-correlation function ACF 🎥\n\n\n\n\n\n\n\nFigure 88.2: The auto correlation function ACF\n\n\n The auto correlation is simply how correlated a time series is with itself at different lags.\n\nCorrelation in general is defined in terms of covariance of two variables.\nThe covariance is a measure of the joint variability of two random variables.\n\n\n\n\n\n\n\nImportant\n\n\n\nRecall that the Covariance between two random variables y_t and y_s is defined as:\n\n\\begin{aligned}\n\\mathbb{C}ov[y_t, y_s] &= \\mathbb{E}[(y_t-\\mathbb{E}[y_t])(y_s-\\mathbb{E}[y_s])] \\\\\n              &= \\mathbb{E}[(y_t-\\mu_t)(y_s-\\mu_s)] \\\\\n              &= E[y_t y_s] - \\mu_t \\times \\mu_s\n\\end{aligned} \\qquad\n\\tag{88.2}\nWe get the second line by substituting \\mu_t = \\mathbb{E}(y_t) and \\mu_s = \\mathbb{E}(y_s) using the definition of the mean of a RV. the third line is by multiplying out and using the linearity of the expectation operator.\n\n\n\n\n\n\n\n\nTip 88.2: AFC notation\n\n\n\nWe will frequently use the notation \\gamma(h) to denote the autocovariance for a lag h i.e. between y_t and y_{t+h}\n\n\\gamma(h) = \\mathbb{C}ov[y_t, y_{t+h}] \\qquad\n\\tag{88.3}\n\n\nWhen the time series is stationary, then the covariance only depends on the lag h = \\|t-s\\| and we can write the covariance as \\gamma(h).\nLet \\{y_t\\} be a time series. Recall that the covariance between two random variables y_t and y_s is defined as:\n\n\\gamma(t,s)=\\mathbb{C}ov[y_t, y_s] = \\mathbb{E}[(y_t-\\mu_t)(y_s-\\mu_s)] \\qquad\n\\tag{88.4}\nwhere \\mu_t = \\mathbb{E}(y_t) and \\mu_s = \\mathbb{E}(y_s) are the means of y_t and y_s respectively.\n\n\\mu_t = \\mathbb{E}(y_t) \\qquad \\mu_s = \\mathbb{E}(y_s)\n\\tag{88.5}\n\n\\text{Stationarity} \\implies \\mathbb{E}[y_t] = \\mu \\quad \\forall t \\qquad \\therefore \\quad \\gamma(t,s)=\\gamma(|t-s|)\n\nIf h&gt;0 \\qquad \\gamma(h)=\\mathbb{C}ov[y_t,y_{t-h}]\n\n\n\n\n\n\nImportantAutocorrelation Function (AFC)\n\n\n\n\n\n\\rho(t,s) = \\frac{\\gamma(t,s)}{\\sqrt{\\gamma(t,t)\\gamma(s,s)}}\n\\tag{88.6}\n\n\nauto-correlation AFC\n\\text{Stationarity} \\implies \\rho(h)=\\frac{\\gamma(h)}{\\gamma(o)} \\qquad \\gamma(0)=Var(y_t)\n\n\n\n\n\n\n\n\nFigure 88.3: sample AFC\n\n\n\ny_{1:T}\n\\tag{88.7}\n\n\n\n\n\n\nImportantThe sample AFC\n\n\n\n\n\\hat\\gamma(h)= \\frac{1}{T} \\sum_{t=1}^{T-h}(y_{t+h}-\\bar y )(y_t-\\hat y)\n\\tag{88.8}\nwhere \\bar y is the sample mean of the time series y_{1:T}, and \\hat y is the sample mean of the time series y_{1:T-h}.\n\n\n\n\\bar y = \\frac{1}{T} \\sum_{t=1}^{T}y_t\n\\tag{88.9}\n\n\\hat \\rho = \\frac{\\hat\\gamma(h)}{\\hat\\gamma(o)}\n\\tag{88.10}\n\n\n88.2.3 The partial auto-correlation function PACF 📖\n\nDefinition 88.3 (Partial Auto-correlation Function (PACF)) Let {y_t} be a zero-mean stationary process, and let\n\n\\hat{y}_t^{h-1} = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\ldots + \\beta_{h-1} y_{t-(h-1)}\n\\tag{88.11}\nbe the best linear predictor of y_t based on the previous h − 1 values \\{y_{t−1}, \\ldots , y_{t−h+1}\\}. The best linear predictor of y_t based on the previous h − 1 values of the process is the linear predictor that minimizes\n\nE[(y_t − \\hat{y}_y^{h-1})^2]\n\\tag{88.12}\nThe partial autocorrelation of this process at lag h, denoted by \\phi(h, h) is defined as: partial auto-correlation PAFC\n\n\\phi(h, h) = Corr(y_{t+h} − \\hat{y}_{t+h}^{h-1}, y_t − \\hat{y}_t^{h-1})\n\\tag{88.13}\nfor h \\ge 2 and \\phi(1, 1) = Corr(y_{t+1}, y_{t}) = \\rho(1).\n\nThe partial autocorrelation function can also be computed via the Durbin-Levinson recursion for stationary processes as \\phi(0, 0) = 0,\n\n\\phi(n, n) = \\frac{\\rho(n) − \\sum_{h=1}^{n-1} \\phi(n − 1, h)\\rho(n − h)}{1- \\sum_{h=1}^{n-1}\\phi(n − 1, h)\\rho(h)}\n\\tag{88.14}\nfor n \\ge 1, and\n\n\\phi(n, h) = \\phi(n − 1, h) − \\phi(n, n)\\phi(n − 1, n − h),\n\\tag{88.15}\nfor n \\ge 2, and h = 1, \\ldots , (n − 1).\nNote that the sample PACF can be obtained by substituting the sample autocorrelations and the sample auto-covariances in the Durbin-Levinson recursion.",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-differencing-and-smoothing",
    "href": "C4-L01.html#sec-differencing-and-smoothing",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.3 Differencing and smoothing 📖",
    "text": "88.3 Differencing and smoothing 📖\n Differencing and smoothing are techniques used to remove trends and seasonality in time series data. They are covered in the (Prado, Ferreira, and West 2023, sec. 1.4).\nMany synthetic time series models are built under the assumption of stationarity. However, in the real world time series data often present non-stationary features such as trends or seasonality. These features render such a time series non-stationary, and therefore, not suitable for analysis using the tools and methods we have discussed so far. However practitioners can use techniques for detrending, deseasonalizing and smoothing that when applied to such observed data transforms it into a new time series that is consistent with the stationarity assumption.\nWe briefly discuss two methods that are commonly used in practice for detrending and smoothing.\n\n88.3.1 Differencing\nDifferencing, is a method which removes the trend from a time series data. The first difference of a time series is defined in terms of the difference operator, denoted as D, that produces the transformation differencing operator D\n\nDy_t \\doteqdot y_t - y_{t-1}\n\\tag{88.16}\nHigher order differences are obtained by successively applying the operator D. For example,\n\nD^2y_t = D(Dy_t) = D(y_t - y_{t-1}) = y_t - 2y_{t-1} + y_{t-2}\n\\tag{88.17}\nDifferencing can also be written in terms of the so called back-shift operator B, with back-shift operator B\n\nBy_t \\doteqdot y_{t-1},\n\\tag{88.18}\nso that\n\nDy_t \\doteqdot (1 - B) y_t\n\\tag{88.19}\nand\n\nD^dy_t \\doteqdot (1 - B)^d y_t.\n\\tag{88.20}\nthis notation lets us write the differences in by referencing items backwards in time, which is often more intuitive and also useful, for example, when we will want to write the differencing operator in terms of a polynomial.\n\n\n88.3.2 Smoothing\n Moving averages, which is commonly used to “smooth” a time series by removing certain features (e.g., seasonality) to highlight other features (e.g., trends).\nA moving average is a weighted average of the time series around a particular time t. In general, if we have data y_{1:T}, we could obtain a new time series such that moving average\n\nz_t = \\sum_{j=-q}^{p} w_j y_{t+j} \\qquad\n\\tag{88.21}\nfor t = (q + 1) : (T − p), with weights w_j \\ge 0 and \\sum^p_{j=−q} w_j = 1\nWe will frequently work with moving averages for which\n\np = q \\qquad \\text{(centered)}\n\nand\n\nw_j = w_{−j} \\forall j  \\text{(symmetric)}\n\nAssume we have periodic data with period d. Then, symmetric and centered moving averages can be used to remove such periodicity as follows:\n\nIf d = 2q :\n\n\nz_t =  \\frac{1}{d} \\left(\\frac{1}{2} y_{t−q} + y_{t−q+1} + \\ldots + y_{t+q−1} + \\frac{1}{2} y_{t+q}\\right )\n\\tag{88.22}\n\nif d = 2q + 1 :\n\n\nz_t = \\frac{1}{d} \\left( y_{t−q} + y_{t−q+1} + \\ldots + y_{t+q−1} + y_{t+q}\\right )\n\\tag{88.23}\n\nExample 88.1 (Seasonal Moving Average) To remove seasonality in monthly data (i.e., seasonality with a period of d = 12 months), we use a moving average with p = q = 6, a_6 = a_{−6} = 1/24, and a_j = a_{−j} = 1/12 for j = 0, \\ldots , 5 , resulting in:\n\nz_t = \\frac{1}{24} y_{t−6} + \\frac{1}{12}y_{t−5} + \\ldots + \\frac{1}{12}y_{t+5} + \\frac{1}{24}y_{t+6}\n\\tag{88.24}",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-differencing-and-smoothing-examples",
    "href": "C4-L01.html#sec-differencing-and-smoothing-examples",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.4 ACF PACF Differencing and Smoothing Examples 🎥",
    "text": "88.4 ACF PACF Differencing and Smoothing Examples 🎥\nThis video walks us through the code snippets in Figure 88.4 and Figure 88.5 below and provides examples of how to compute the ACF and PACF of a time series, how to use differencing to remove trends, and how to use moving averages to remove seasonality.\n\nWe begin by simulating data using the code in Section 88.6\nWe simulates white noise data using the rnorm(1:2000,mean=0,sd=1) function in R\nWe plot the white noise data which we can see lacks a temporal structure.\nWe plot the ACF using the acf function in R:\n\nwe specify the number of lags using the lag.max=20\nwe shows a confidence interval for the ACF values\n\nWe plot the PACF using the pacf function in R\nNext we define some time series objects in R using the ts function\n\nwe define and plot monthly data starting in January 1960\nwe define and plot yearly data with one observation per year starting in 1960\nwe define and plot yearly data with four observations per year starting in 1960\n\nWe move on to smoothing and differencing in Section 88.3\nWe load the CO2 dataset in R and plot it\nwe plot the ACF and PACF of the CO2 dataset\nwe use the filter function in R to remove the seasonal component of the CO2 dataset we plot the resulting time series highlighting the trend.\nTo remove the trend we use the diff function in R to take the first and second differences of the CO2 dataset\n\nthe diff function takes a parameter differences which specifies the number of differences to take\n\nwe plot the resulting time series after taking the first and second differences\nthe ACF and PACF of the resulting time series are plotted, they look different, in that they no longer have the slow decay characteristic of time series with a trend.",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-differencing-and-smoothing-reading",
    "href": "C4-L01.html#sec-differencing-and-smoothing-reading",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.5 code for Differencing and filtering via moving averages 📖 ℛ",
    "text": "88.5 code for Differencing and filtering via moving averages 📖 ℛ\n \n\n1data(co2)\n2co2_1stdiff = diff(co2,differences=1)\n3co2_ma = filter(co2,filter=c(1/24,rep(1/12,11),1/24),sides=2)\n\n#par(mfrow = c(3,1),  mar   = c(3, 4, 2, 1),  cex.lab=1.2, cex.main=1.2)\n\n4plot(co2)\n5plot(co2_1stdiff)\n6plot(co2_ma)\n\n\n1\n\nLoad the CO2 dataset in R\n\n2\n\nTake first differences to remove the trend\n\n3\n\nFilter via moving averages to remove the seasonality\n\n4\n\nplot the original data\n\n5\n\nplot the first differences (removes trend, highlights seasonality)\n\n6\n\nplot the filtered series via moving averages (removes the seasonality, highlights the trend)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) the original data\n\n\n\n\n\n\n\n\n\n\n\n(b) the first difference (TS - trend), highlightes the seasonality\n\n\n\n\n\n\n\n\n\n\n\n(c) the moving averages (TS - seasonality), highlights the trend\n\n\n\n\n\n\nFigure 88.4: Differencing and filtering via moving averages",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L01.html#sec-white-noise-simulation",
    "href": "C4-L01.html#sec-white-noise-simulation",
    "title": "88  Stationarity, The ACF and the PCF M1L1",
    "section": "88.6 Code: Simulate data from a white noise process 📖 ℛ",
    "text": "88.6 Code: Simulate data from a white noise process 📖 ℛ\n \n\nset.seed(2021)\nT=200\nt =1:T\ny_white_noise=rnorm(T, mean=0, sd=1)\n\n1yt=ts(y_white_noise, start=c(1960), frequency=1)\n\n\n#par(mfrow = c(3, 1), mar = c(3, 4, 2, 1),  cex.lab = 1.3, cex.main = 1.3) \n\n2plot(yt, type = 'l', col='red',\n     xlab = 'time (t)', \n     ylab = \"Y(t)\")\n\n3acf(yt,\n    lag.max = 20, \n    xlab = \"lag\",\n    ylab = \"Sample ACF\",\n    ylim=c(-1,1),main=\"\")\n\n4pacf(yt, lag.max = 20,\n     xlab = \"lag\",  ylab = \"Sample PACF\",\n     ylim=c(-1,1),main=\"\")\n\n\n1\n\nDefine a time series object in R - Assume the data correspond to annual observations starting in January 1960\n\n2\n\nPlot the simulated time series,\n\n3\n\nPlot the sample ACF\n\n4\n\nPlot the sample PACF\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Simulate data with no temporal structure (white noise)\n\n\n\n\n\n\n\n\n\n\n\n(b) Sample AFC\n\n\n\n\n\n\n\n\n\n\n\n(c) Sample PACF\n\n\n\n\n\n\nFigure 88.5: Simulate white noise data\n\n\n\n\n\n\n\n\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nWeckerle, Melissa. 2022. “Statistics professor wins prestigious professional statistics society award  Baskin School of Engineering.” https://engineering.ucsc.edu/news/statistics-professor-wins-zellner-medal.",
    "crumbs": [
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>Stationarity, The ACF and the PCF M1L1</span>"
    ]
  },
  {
    "objectID": "C4-L02.html",
    "href": "C4-L02.html",
    "title": "89  The AR(1) process: definitions and properties - M1L2",
    "section": "",
    "text": "89.1 The AR(1) process 🎥\nWe will next introduce the autoregressive process of order one, or AR(1) process, which is a fundamental model in time series analysis. We will discuss the definition of the AR(1) process, its properties, and how to simulate data from an AR(1) process.",
    "crumbs": [
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>The AR(1) process: definitions and properties - M1L2</span>"
    ]
  },
  {
    "objectID": "C4-L02.html#the-ar1-process",
    "href": "C4-L02.html#the-ar1-process",
    "title": "89  The AR(1) process: definitions and properties - M1L2",
    "section": "",
    "text": "Figure 89.1: AR(1) definition\n\n\n\n\n\n\n\n\nFigure 89.2: AR(1) properties\n\n\n\n\n89.1.1 AR(1) Definition\n The AR(1) process is defined as:\n\ny_t = \\phi y_{t-1} + \\varepsilon_t \\qquad \\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0, v)\n\\tag{89.1}\nwhere:\n\n\\phi is the AR(1) coefficient\n\\varepsilon_t are the innovations (or shocks) at time t, assumed to be independent and identically distributed (i.i.d.) with mean 0 and variance v.\n\n\n\n89.1.2 AR(1) Recursive Expansion\nRecursive substitution yields:\n\n\\begin{aligned}\ny_t &= \\phi(\\phi y_{t-1} )+ \\varepsilon_t \\\\\n    &= \\phi^2 y_{t-2} + \\phi \\varepsilon_{t-1} + \\varepsilon_t \\\\\n    &= \\phi^k y_{t-k} + \\sum_{j=0}^{k-1} \\phi^j \\varepsilon_{t-j}\n\\end{aligned}\n\\tag{89.2}\nFor \\|\\phi\\| &lt; 1, as k \\to \\infty, this becomes:\n\ny_t = \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\\tag{89.3}\nInterpreted as an infinite-order Moving Average \\operatorname{MA}(\\infty) process.\n\n\n89.1.3 AR(1) Mean\n Since \\mathbb{E}[\\varepsilon_t] = 0,\n\n\\mathbb{E}[y_t] = 0 \\text{ mean of the AR(1) process}\n\\tag{89.4}\n\n\n89.1.4 AR(1) Variance\n Using independence and identical distribution:\n\n\\mathbb{V}ar[y_t] = \\sum_{j=0}^{\\infty} \\phi^{2j} v = \\frac{v}{1 - \\phi^2}\n\\tag{89.5}\nRequires \\|\\phi\\| &lt; 1 for convergence (i.e., stationarity).\n\n\n89.1.5 AR(1) Autocovariance Function \\gamma(h)\n For lag h, the autocovariance:\n\n\\begin{aligned}\n\\gamma(h) &= \\mathbb{E}[y_t y_{t-h}] \\\\\n&= \\mathbb{E} \\left[ \\left( \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\\right )\\left (\\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-h-k}\\right) \\right] \\\\\n&= \\mathbb{E}[(\\varepsilon_{t} + \\phi \\varepsilon_{t-1} + \\phi^2 \\varepsilon_{t-2} + \\ldots ) \\times (\\varepsilon_{t-h} + \\phi \\varepsilon_{t-h-1} + \\phi^2 \\varepsilon_{t-h-2} + \\ldots ) ] \\\\\n&= \\mathbb{E}[\\phi ^h \\varepsilon_{t-h} \\varepsilon_{t} + \\phi^{h+1} \\varepsilon_{t-h-1} \\varepsilon_{t} + \\ldots] \\\\\n&= v \\sum_{j=0}^{\\infty} \\phi^{h+j}  \\phi^{j} \\\\\n&= v \\phi^h \\sum_{j=0}^{\\infty} \\phi^{2j}  \\\\\n&= \\frac{v \\phi^{\\|h\\|}}{1 - \\phi^2} \\qquad \\text { when } |\\phi| &lt; 1\n\\end{aligned}\n\\tag{89.6}\nWe used the definition and properties of the expectation, independence of the innovations \\varepsilon_t, and the fact that \\mathbb{E}[\\varepsilon_t^2] = v. In the cross product, only terms where lags are the same (j = k) contribute, as the others are independent, leading to the above result. In the final step, we used the formula for the sum of a geometric series.\n\n\n89.1.6 AR(1) Autocorrelation Function \\rho(h)\n Defined by:\n\n\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\phi^{\\|h\\|}\n\\tag{89.7}\n\n\n89.1.7 AR(1) other properties:\n\nfor any lag h:\n\n\n\\rho(h) = \\phi^{\\|h\\|}\n\\gamma(h) = \\frac{v \\phi^{\\|h\\|}}{1 - \\phi^2}\n\n\nExponential decay if \\|\\phi\\| &lt; 1\nIf \\phi &gt; 0: decay is monotonic\nIf \\phi &lt; 0: decay is oscillatory (alternates signs)\n\n\n\n89.1.8 Stationarity\n\n\nThe process is stationary when \\|\\phi\\| &lt; 1:\n\nMean and variance are constant over time\nAutocovariance depends only on lag h, not on t",
    "crumbs": [
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>The AR(1) process: definitions and properties - M1L2</span>"
    ]
  },
  {
    "objectID": "C4-L02.html#the-pacf-of-the-ar1-process",
    "href": "C4-L02.html#the-pacf-of-the-ar1-process",
    "title": "89  The AR(1) process: definitions and properties - M1L2",
    "section": "89.2 The PACF of the AR(1) process 📖",
    "text": "89.2 The PACF of the AR(1) process 📖\n It is possible to show that the PACF of an AR(1) process is zero after the first lag. We can use the Durbin-Levinson recursion to show this.\nFor lag n = 0 we have \\phi(0, 0) = 0\nFor lag n = 1 we have:\n\n\\phi(1, 1) =  \\rho(1) = \\phi\n\\tag{89.8}\nFor lag n = 2 we compute \\phi(2, 2) as:\n\n\\begin{aligned}\n\\phi(2, 2) &= \\frac{(\\rho(2) − \\phi(1, 1)\\rho(1))}{ (1 − \\phi(1, 1)\\rho(1))} \\\\\n&= \\frac{\\phi^2-\\phi^2}{1- \\phi^2}\\\\\n&=0\n\\end{aligned}\n\\tag{89.9}\nand we also obtain:\n\n\\phi(2, 1) = \\phi(1, 1) − \\phi(2, 2)\\phi(1, 1) = \\phi.\n\\tag{89.10}\nFor lag n = 3 we compute \\phi(3, 3) as\n\n\\begin{aligned}\n\\phi(3, 3) &= \\frac{(\\rho(3) − \\sum_{h=1}^2 \\phi(2, h)\\rho(3 − h))}{1 − \\sum_{h=1}^2 \\phi(2, h)\\rho(h)} \\newline\n&= \\frac{\\phi^3 - \\phi(2,1) \\rho(2) - \\phi(2,2) \\rho(1)}{1 - \\phi(2,1)\\rho(1) - \\phi(2,2)\\rho(2)} \\newline\n&= \\frac{\\phi^3 - \\phi^3 - 0}{1 - \\phi^2 } \\newline\n&= 0\n\\end{aligned}\n\\tag{89.11}\nand we also obtain\n\n\\phi(3, 1) = \\phi(2, 1) − \\phi(3, 3)\\phi(2, 2) = \\phi\n\\tag{89.12}\n\n\\phi(3, 2) = \\phi(2, 2) − \\phi(3, 3)\\phi(2, 1) = 0\n\\tag{89.13}\nWe can prove by induction that in the case of an AR(1), for any lag n,\n\\phi(n, h) = 0, \\phi(n, 1) = \\phi and \\phi(n, h) = 0 for h \\ge 2 and n \\ge 2.\nThen, the PACF of an AR(1) is zero for any lag above 1 and the PACF coefficient at lag 1 is equal to the AR coefficient \\phi\n\n89.2.1 Simulate data from an AR(1) process 🎥\nThis video walks through the code snippet below and provides examples of how to sample data from an AR(1) process and plot the ACF and PACF functions of the resulting time series.\nPrado demonstrates how to simulate AR(1) processes using arima.sim in R:\n\nSimulation Setup:\n\nset.seed() ensures reproducibility.\nSimulate 500 time points from an AR(1) with \\phi = 0.9 and variance = 1.\nThe process is stationary since |\\phi| &lt; 1.\n\narima.sim Function:\n\nCan simulate ARIMA(p,d,q) processes; here, only AR(1) is used.\nModel specified via a list: list(ar = phi), with sd as the standard deviation (√variance).\n\nComparative Simulation:\n\nSecond AR(1) simulated with \\phi = –0.9 to show the impact of negative \\phi.\nThe positive \\phi process shows persistent values (random walk-like).\nThe negative \\phi process shows oscillatory behavior.\n\nACF and PACF Analysis:\n\nTrue ACF: Exponential decay for both cases, oscillatory when \\phi &lt; 0.\nSample ACF: Matches theoretical ACF for each process.\nSample PACF: Only lag 1 is non-negligible, aligning with AR(1) properties:\n\nPositive at lag 1 for \\phi = 0.9.\nNegative at lag 1 for \\phi = –0.9.\nAll other lags ≈ 0.\n\n\n\nThe demonstration confirms our theoretical results regarding ACF/PACF behavior in AR(1) processes.\n\n\n89.2.2 R code: Sample data from AR(1) processes 📖\n \nSample data from 2 ar(1) processes: and plot their ACF and PACF functions\n\n1set.seed(2021)\n2T=500\n\n3v=1.0\n4sd=sqrt(v)\n5phi1=0.9\n6yt1=arima.sim(n = T, model = list(ar = phi1), sd = sd)\n\n7phi2=-0.9\n8yt2=arima.sim(n = T, model = list(ar = phi2), sd = sd)\n\n\n1\n\nset seed for reproducibility\n\n2\n\nnumber of time points\n\n3\n\ninnovation variance\n\n4\n\ninnovation standard deviation\n\n5\n\nAR coefficient for the first process\n\n6\n\nSample data from an ar(1) with ar coefficient phi = 0.9 and variance = 1\n\n7\n\nAR coefficient for the second process\n\n8\n\nSample data from an ar(1) with ar coefficient phi = -0.9 and variance = 1\n\n\n\n\n\n\n89.2.3 Plot the time series of both processes\n\npar(mfrow = c(2, 1),mar = c(3, 4, 2, 1), cex.lab = 1.3)\nplot(yt1,main=expression(phi==0.9))\nplot(yt2,main=expression(phi==-0.9))\n\npar(mfrow = c(3, 2),mar = c(3, 4, 2, 1), cex.lab = 1.3)\nlag.max=50 # max lag\n\n\n\n\n\n\n\nFigure 89.3\n\n\n\n\n\n\n\n89.2.4 Plot true ACFs for both processes\n\n1cov_0=sd^2/(1-phi1^2)\n2cov_h=phi1^(0:lag.max)*cov_0\nplot(0:lag.max, cov_h/cov_0, pch = 1, \n     type = 'h', col = 'red',\n     ylab = \"true ACF\", \n     xlab = \"Lag\",\n     ylim=c(-1,1), \n3     main=expression(phi==0.9))\n\n\n1\n\ncompute auto-covariance at h=0\n\n2\n\ncompute auto-covariance at lag h\n\n3\n\nPlot autocorrelation function (ACF) for the first process\n\n\n\n\n\n\n\n\n\n\nFigure 89.4: True ACF for the first AR(1) process\n\n\n\n\n\n\n4cov_0=sd^2/(1-phi2^2)\n5cov_h=phi2^(0:lag.max)*cov_0\n# Plot autocorrelation function (ACF)\nplot(0:lag.max, cov_h/cov_0, pch = 1, \n     type = 'h', col = 'red',\n     ylab = \"true ACF\", \n     xlab = \"Lag\",\n     ylim=c(-1,1),\n6     main=expression(phi==-0.9))\n\n\n4\n\ncompute auto-covariance at h=0 for the second process\n\n5\n\ncompute auto-covariance at lag h for the second process\n\n6\n\nPlot autocorrelation function (ACF) for the second process\n\n\n\n\n\n\n\n\n\n\nFigure 89.5: True ACF for the second AR(1) process\n\n\n\n\n\n\n\n89.2.5 plot sample ACFs for both processes\n\nacf(yt1, lag.max = lag.max, type = \"correlation\", ylab = \"sample ACF\",\n    lty = 1, ylim = c(-1, 1), main = \" \")\nacf(yt2, lag.max = lag.max, type = \"correlation\", ylab = \"sample ACF\",\n    lty = 1, ylim = c(-1, 1), main = \" \")\n## plot sample PACFs for both processes\n\npacf(yt1, lag.ma = lag.max, ylab = \"sample PACF\", ylim=c(-1,1),main=\"\")\npacf(yt2, lag.ma = lag.max, ylab = \"sample PACF\", ylim=c(-1,1),main=\"\")\n\n\n\n\n\n\n\nFigure 89.6: Sample ACF for the first AR(1) process\n\n\n\n\n\n\n\n\n\n\n\nFigure 89.7: Sample ACF for the first AR(1) process\n\n\n\n\n\n\n\n\n\n\n\nFigure 89.8: Sample ACF for the first AR(1) process\n\n\n\n\n\n\n\n\n\n\n\nFigure 89.9: Sample ACF for the first AR(1) process",
    "crumbs": [
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>The AR(1) process: definitions and properties - M1L2</span>"
    ]
  },
  {
    "objectID": "C4-L03.html",
    "href": "C4-L03.html",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "",
    "text": "90.1 Maximum likelihood and Bayesian inference in regression 📖\nThis section is based on the handout provided in the course materials. The gist of this handout is that if we can write the regression as the equation of a straight line, we should be able to make use of basic algebra to invert the equation and obtain an expression for the regression coefficients. We do this by writing the equation in matrix form and then using the Moore-Penrose pseudoinverse to obtain the maximum likelihood estimator for the regression coefficients.",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#maximum-likelihood-and-bayesian-inference-in-regression",
    "href": "C4-L03.html#maximum-likelihood-and-bayesian-inference-in-regression",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "",
    "text": "CautionMoore-Penrose pseudoinverse\n\n\n\nPrado snuck the Moore-Penrose pseudoinverse in this handout, then she used it in the class and notes. I thought this isn’t the first time this has come up in the specialization as we used MLE to some degree in all the courses, and in depth in the previous course on mixtures. However a cursory review did not uncover previous use of this mathematical tool. So I thought it might be useful to provide a brief overview of the Moore-Penrose pseudoinverse, its properties and how it is used in regression models.\nMoore-Penrose pseudoinverse is a generalization of the inverse matrix that can be applied to non-square matrices.\n\nIt is not covered in typical linear algebra courses. It might come up in numerical methods courses or a second linear algebra course. But unless they cover application in regression models, it is quite likely that they skip an important point, that the Moore-Penrose pseudoinverse is used to obtain the maximum likelihood estimator for the regression coefficients in linear regression models.\n\nI found a couple of reference in (Schott 2016 ch. 5) but they it didn’t cover they MLE aspect of the pseudoinverse.\nI cover some of this material in an Section 116.3 to the course notes, however I am still missing a good source for Equation 90.1.\n\nIt is denoted as X^+ and has the following properties:\nThis pseudoinverse is a neat bit of mathematics which allows us to not only invert the equation from the left but it somehow minimizes the sum of squared errors in the regression - I.e. we get the parameters while minimizing the sum of squared errors due to the variance term. One strong assumption we must make is that the design matrix X is full rank, which means that the columns of the matrix are linearly independent. The columns of the design matrix correspond to the explanatory variables in the regression model.\nIn reality as we are dealing with hierarchical models, so we will not always have a full rank design matrix, but we can still use the Moore-Penrose pseudoinverse to obtain the maximum likelihood estimator for the regression coefficients.\n\n\n\n90.1.1 Regression Models: Maximum Likelihood Estimation\n \nAssume a regression model with the following structure: \ny_i = \\beta_1x_{i,1} + \\ldots + \\beta_kx_{i,k} + \\varepsilon_i,\n\nfor i = 1, \\ldots, n and \\varepsilon_i independent random variables with \\varepsilon_i \\sim \\mathcal{N}(0, v) \\quad \\forall i. This model can be written in matrix form as:\n\ny = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol\\varepsilon \\qquad \\boldsymbol\\varepsilon \\sim \\mathcal{N} (0, v\\mathbf{I})\n\nwhere:\n\ny = (y_1, \\ldots, y_n)′ is an n-dimensional vector of responses,\n\\mathbf{X} is an n × k matrix containing the explanatory variables,\n\\boldsymbol \\beta = (\\beta_1, \\ldots, \\beta_k)' is the k-dimensional vector of regression coefficients,\n\\boldsymbol \\varepsilon = (\\varepsilon_1, \\ldots, \\varepsilon_n)' is the n-dimensional vector of errors,\n\\mathbf{I} is an n \\times n identity matrix.\n\nIf \\mathbf{X} is a full rank matrix with rank k , the maximum likelihood estimator for \\boldsymbol\\beta, denoted as \\hat{\\boldsymbol\\beta}_{MLE} is given by:\n\n\\hat{\\boldsymbol{\\beta}}_{MLE} = (\\mathbf{X}'\\mathbf{X})^{−1}\\mathbf{X}'\\mathbf{y},\n\\tag{90.1}\n where (X^+ = \\mathbf{X}'\\mathbf{X})^{−1}\\mathbf{X}' is the Moore-Penrose pseudoinverse of the matrix \\mathbf{X}. This Moore-Penrose pseudoinverse of the matrix \\mathbf{X} is the most widely known generalization of the inverse matrix and is used to obtain the least squares solution to the linear regression problem.\nand the MLE for v is given by:\n\n\\hat{v}_{MLE} = \\frac{1}{n} (y − \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{MLE})′(y − \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{MLE})\n\\tag{90.2}\n\\hat{v}_{MLE} is not an unbiased estimator of v, therefore, the following unbiased estimator of v is typically used:\n\ns^2 = \\frac{1}{n-k}(y − \\mathbf{X} \\hat{\\boldsymbol\\beta}_{MLE} )′(y − \\mathbf{X} \\hat{\\boldsymbol\\beta}_{MLE} )\n\\tag{90.3}\n\n\n90.1.2 Regression Models: Bayesian Inference\nAssume once again we have a model with the structure in (1), which results in a likelihood of the form\n\n\\mathbb{P}r(y \\mid \\boldsymbol{\\beta} , v) = \\frac{1}{(2\\pi v)^{n/2}}\\exp \\left\\{ -\\frac{1}{2} (y − \\mathbf{X} \\boldsymbol{\\beta})′(y − \\mathbf{X} \\boldsymbol{\\beta}) \\right\\}\n\nIf a prior of the form :\n\n\\mathbb{P}r(\\boldsymbol{\\beta}, v) \\propto \\frac{1}{v}\n\\tag{90.4}\nis used, we obtain that the posterior distribution is given by:\n\n\\mathbb{P}r(\\boldsymbol{\\beta},v \\mid \\mathbf{y}) \\propto \\frac{1}{v^{n/2+1}}\\exp \\left\\{ -\\frac{1}{2v} (\\mathbf{y} − \\mathbf{X} \\boldsymbol{\\beta})′(\\mathbf{y} − \\mathbf{X} \\boldsymbol{\\beta}) \\right\\}\n\\tag{90.5}\nIn addition it can be shown that\n\n(\\boldsymbol{\\beta}\\mid v, \\mathbf{y}) \\sim \\mathcal{N} (\\hat{\\boldsymbol{\\beta}}_{MLE} , v(\\mathbf{X}'\\mathbf{X})^{-1})\n(v \\mid \\mathbf{y}) \\sim \\mathcal{IG}\\left(\\frac{(n − k)}{2}, \\frac{1}{2}d\\right) with\n\n\nd = (\\mathbf{y} − \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{MLE} )′(\\mathbf{y} − \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{MLE} )\n\\tag{90.6}\nwhere \\mathcal{IG}(a, b) denotes the inverse-gamma distribution with shape parameter a and scale parameter b.\nwith k = dim(\\boldsymbol\\beta).\nGiven that \\mathbb{P}r(\\boldsymbol\\beta, v \\mid \\mathbf{y}) = \\mathbb{P}r(\\boldsymbol\\beta \\mid v, \\mathbf{y})p(v \\mid \\mathbf{y}) the equations above provide a way to directly sample from the posterior distribution of \\boldsymbol \\beta and v by first sampling v from the inverse-gamma distribution above and then conditioning on this sampled value of v, sampling \\boldsymbol \\beta from the normal distribution above.",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#sec-mle-ar1",
    "href": "C4-L03.html#sec-mle-ar1",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.2 Maximum likelihood estimation in the AR(1) 🎥",
    "text": "90.2 Maximum likelihood estimation in the AR(1) 🎥\n\n\n\n\n\n\n\nFigure 90.1: MLE 1\n\n\n\n\n\n\n\n\nFigure 90.2: Full Likelihood MLE\n\n\n\n\n\n\n\n\nFigure 90.3: Conditional Likelihood MLE\n\n\n\n\nThere are two main strategies for performing MLE for an AR(1) model:\n\nFull Likelihood: Considers the joint distribution of all observations y_1, \\dots, y_T.\nConditional Likelihood: Conditions on the first observation (y_1) and works with the likelihood of the remaining observations (y_2, \\dots, y_T).\n\n\n90.2.1 Model Setup\nThe focus is on the zero-mean AR(1) model:\n\nY_t = \\phi Y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0, v), \\quad \\phi \\in (-1,1)\n\nThis condition ensures stationarity of the process.\n\n\n90.2.2 Distributional Assumptions\n\nY_1 \\sim \\mathcal{N}\\left(0, \\frac{v}{1 - \\phi^2}\\right)\nY_t \\mid Y_{t-1} \\sim \\mathcal{N}(\\phi Y_{t-1}, v) for t \\geq 2\n\n\n\n90.2.3 Likelihood Approaches\nTwo approaches are considered:\n\n90.2.3.1 1. Full Likelihood\n\n\\begin{aligned}\np(y_{1:T} \\mid \\phi, v) &= p(y_1 \\mid \\phi, v) \\cdot \\prod_{t=2}^T p(y_t \\mid y_{t-1}, \\phi, v) \\\\\n&= \\frac{1}{\\sqrt{2\\pi \\frac{v}{1 - \\phi^2}}} \\exp\\left( -\\frac{y_1^2 (1 - \\phi^2)}{2v} \\right) \\cdot  \\prod_{t=2}^T \\frac{1}{\\sqrt{2\\pi v}} \\exp\\left( -\\frac{(y_t - \\phi y_{t-1})^2}{2v} \\right) \\\\\n  &= \\frac{(1 - \\phi^2)^{1/2}}{(2\\pi v)^{T/2}} \\cdot\n\\exp\\left( -\\frac{1}{2v} \\left[ \\underbrace{ y_1^2(1 - \\phi^2) + \\sum_{t=2}^T (y_t - \\phi y_{t-1})^2 }_{\\text{Quadratic Loss } Q^*(\\phi)} \\right] \\right) \\\\\n&= \\frac{(1 - \\phi^2)^{1/2}}{(2\\pi v)^{T/2}} \\exp\\left( -\\frac{Q^*(\\phi)}{2v} \\right)\n\\end{aligned}\n\\tag{90.7}\nwhere Q^*(\\phi) is defined as:\n\nQ^*(\\phi) =\n\\underbrace{y_1^2(1 - \\phi^2)\\vphantom{\\sum_{t=2}^T (y_t - \\phi y_{t-1})^2}}_{\\text{Initial Loss}}\n+ \\underbrace{\\sum_{t=2}^T (y_t - \\phi y_{t-1})^2}_{\\text{Remaining Loss } Q(\\phi)}\n\\tag{90.8}\n\n\\begin{aligned}\np(y_{1:T} \\mid \\phi) &= \\prod_{t=2}^T \\frac{1}{\\sqrt{2\\pi v}} \\exp\\left( -\\frac{(y_t - \\phi y_{t-1})^2}{2v} \\right) \\\\\n&= \\frac{1}{(2\\pi v)^{T/2}} \\exp\\left( -\\sum_{t=2}^T \\frac{1}{2v} \\left( y_t - \\phi y_{t-1} \\right)^2 \\right) \\\\\n&= \\frac{1}{(2\\pi v)^{T/2}} \\exp\\left( -\\frac{Q(\\phi)}{2v} \\right)\n\\end{aligned}\n\\tag{90.9}\nwhere Q(\\phi) is the quadratic loss function defined as:\n\n\\underbrace{ \\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} }_{\\utilde{y}} =\n\\underbrace{ \\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} }_{\\mathbb{X}}\n\\underbrace{ \\phi \\vphantom{\\begin{pmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T \\end{pmatrix} }}_{\\beta} +\n\\underbrace{ \\begin{pmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_T\\end{pmatrix}}_{\\utilde {\\varepsilon}}\n\\tag{90.10}\nwhere \\utilde{y} is the vector of observations, \\mathbb{X} is the design matrix with y_1 as the first column and y_2, \\ldots, y_{T-1} as the second column, \\beta = \\phi is the AR coefficient, and \\utilde{\\varepsilon} \\sim \\mathcal{N}(0, vI) is the error term.\n\n\\utilde{y} = \\mathbb{X} \\beta + \\utilde{\\varepsilon} \\qquad \\utilde{\\varepsilon} \\sim \\mathcal{N}(0, vI)\n\nwhere \\mathbb{X} is the design matrix\nwith y_1 as the first column and y_2, \\ldots, y_{T-1} as the second column.\nIf the matrix \\mathbb{X} is full rank, the MLE for \\phi can be obtained as:\n\n\\hat{\\beta} = (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'y\n and the MLE for v is given by:\n\n\\hat{v} = S^2 = \\frac{(y - \\mathbb{X}\\hat{\\beta})'(y - \\mathbb{X}\\hat{\\beta})}{\\dim(y)-\\dim(\\beta)}\n\nand the MLE for \\phi. \n\\hat{\\phi}_{MLE} = \\frac{\\sum_{t=2}^T y_t y_{t-1}}{\\sum_{t=2}^T y_{t-1}^2}\n\\tag{90.11}\nand the unbiased estimator for v is given by:\n\nS^2 = \\sum_{t=2}^T (y_t - \\hat{\\phi}_{MLE} y_{t-1})^2 / (T - 2)\n\\tag{90.12}\nwhere T is the number of time points and S^2 is the unbiased estimator for the variance v. And we usually use this unbiased estimator for v in practice, as the MLE for v is biased.\n\n\n90.2.3.2 2. Conditional Likelihood\nMaximizing the full likelihood requires numerical optimization methods (e.g., Newton-Raphson), as there’s no closed-form solution.\nThis setup is equivalent to a linear regression:\n\n\\mathbf{y} = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, vI)\n\nWhere:\n\n\\mathbf{y} = [y_2, \\dots, y_T]^T\nX = [y_1, \\dots, y_{T-1}]^T\n\\beta = \\phi\n\nCondition on y_1:\n\n\\begin{aligned}\np(y_1 \\mid \\phi) &\\sim \\mathcal{N}(0, 1/(1 - \\phi^2)) \\\\\np(y_t \\mid y_{t-1}, \\phi) &\\sim \\mathcal{N}(\\phi y_{t-1}, 1) \\\\\n\\end{aligned}\n\n\n\\begin{aligned}\np(y_{1:T} \\mid y_1, \\phi, v) &= p(y_1 \\mid \\phi)  \\cdot \\prod_{t=2}^T p(y_t \\mid y_{t-1}, \\phi)\\\\\n&= \\frac{(1 - \\phi^2)^{1/2}}{(2\\pi)^{T/2}} \\exp\\left(-\\frac{y_1^2(1 - \\phi^2)}{2}\\right) \\cdot \\prod_{t=2}^T \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y_t - \\phi y_{t-1})^2}{2}\\right) \\\\\n&= \\frac{(1 - \\phi^2)^{1/2}}{(2\\pi)^{T/2}} \\exp\\left(-\\frac{1}{2} \\left[y_1^2(1 - \\phi^2) + \\sum_{t=2}^T (y_t - \\phi y_{t-1})^2\\right]\\right) \\\\\n\\end{aligned}\n\nSo the log-likelihood becomes:\n\n\\log p(y_{1:T} \\mid y_1, \\phi) = \\frac{1}{2} \\log(1 - \\phi^2) - \\frac{1}{2} Q(\\phi) + K\n\\tag{90.13}\nwhere K is a constant that does not depend on \\phi.\nSo if I were to look at maximizing this function, we can think about taking first derivatives with respect to phi. And then we will see that again the expression that you obtain doesn’t allow you to obtain a close form expression for \\hat{\\phi}_{MLE}. Instead, we will need to use a numerical optimization method such as Newton Raphson to obtain the maximum likelihood estimator for phi.\n\n\n\n90.2.4 Conclusion\n\nFull likelihood is more general but computationally intensive.\nConditional likelihood simplifies estimation by leveraging regression theory.\nWhen variance v is known (e.g., v = 1), the optimization reduces to maximizing a univariate function of \\phi.\nFor full likelihood, optimization of Q^*(\\phi) is required.\n\n\n\n\n\n\n\n\n\nFeature\nFull Likelihood\nConditional Likelihood (Regression)\n\n\n\n\nAccounts for Y\\_1\n✅ Yes\n❌ No\n\n\nMLE for \\phi\n❌ No closed form\n✅ Closed form\n\n\nMLE for v\n❌ Biased unless adjusted\n✅ Unbiased estimator available\n\n\nOptimization Needed\n✅ Yes (numerical methods)\n❌ No (closed-form MLE for \\phi)\n\n\nUseful when\nModeling full joint process\nEstimating \\phi efficiently in practice",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#mle-for-the-ar1-ℛ",
    "href": "C4-L03.html#mle-for-the-ar1-ℛ",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.3 MLE for the AR(1) 📖 ℛ",
    "text": "90.3 MLE for the AR(1) 📖 ℛ\n The following code allows you to compute the MLE of the AR coefficient \\psi, the unbiased estimator of v, s^2 , and the MLE of v based on a dataset simulated from an AR(1) process and using the conditional likelihood.\n\nset.seed(2021)\n1phi=0.9\nv=1\n2sd=sqrt(v)\n3T=500\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Case 1: Conditional likelihood\n4y=as.matrix(yt[2:T])\n5X=as.matrix(yt[1:(T-1)])\n6phi_MLE=as.numeric((t(X)%*%y)/sum(X^2))\n7s2=sum((y - phi_MLE*X)^2)/(length(y) - 1)\n8v_MLE=s2*(length(y)-1)/(length(y))\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"MLE for the variance v: \", v_MLE, \"\\n\", \n    \"Estimate s2 for the variance v: \", s2, \"\\n\")\n\n\n1\n\nar coefficient\n\n2\n\ninnovation standard deviation\n\n3\n\nnumber of time points\n\n4\n\nresponse variable\n\n5\n\ndesign matrix\n\n6\n\nMLE for phi\n\n7\n\nUnbiased estimate for v\n\n8\n\nMLE for v\n\n\n\n\n\n MLE of conditional likelihood for phi:  0.9261423 \n MLE for the variance v:  1.048 \n Estimate s2 for the variance v:  1.050104 \n\n\nThis code allows you to compute estimates of the AR(1) coefficient and the variance using the arima function in R. The first case uses the conditional sum of squares, the second and third cases use the full likelihood with different starting points for the numerical optimization required to compute the MLE with the full likelihood.\n\n# Obtaining parameter estimates using the arima function in R\nset.seed(2021)\nphi=0.9 # ar coefficient\nv=1\nsd=sqrt(v) # innovation standard deviation\nT=500 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n# Using conditional sum of squares, equivalent to conditional likelihood \narima_CSS=arima(yt,order=c(1,0,0),method=\"CSS\",n.cond=1,include.mean=FALSE)\ncat(\"AR estimates with conditional sum of squares (CSS) for phi and v:\", arima_CSS$coef,arima_CSS$sigma2,\n\"\\n\")\n\nAR estimates with conditional sum of squares (CSS) for phi and v: 0.9261423 1.048 \n\n#Uses ML with full likelihood \narima_ML=arima(yt,order=c(1,0,0),method=\"ML\",include.mean=FALSE)\ncat(\"AR estimates with full likelihood for phi and v:\", arima_ML$coef,arima_ML$sigma2,\n\"\\n\")\n\nAR estimates with full likelihood for phi and v: 0.9265251 1.048434 \n\n#Default: uses conditional sum of squares to find the starting point for ML and \n#         then uses ML \narima_CSS_ML=arima(yt,order=c(1,0,0),method=\"CSS-ML\",n.cond=1,include.mean=FALSE)\ncat(\"AR estimates with CSS to find starting point for ML for phi and v:\", \narima_CSS_ML$coef,arima_CSS_ML$sigma2,\"\\n\")\n\nAR estimates with CSS to find starting point for ML for phi and v: 0.9265252 1.048434 \n\n\nThis code shows you how to compute the MLE for \\psi using the full likelihood and the function optimize in R.\n\nset.seed(2021)\n1phi=0.9\nv=1\n2sd=sqrt(v)\n3T=500\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## MLE, full likelihood AR(1) with v=1 assumed known \n\n4log_p &lt;- function(phi, yt){\n  0.5*(log(1-phi^2) - sum((yt[2:T] - phi*yt[1:(T-1)])^2) - yt[1]^2*(1-phi^2))\n}\n\nresult = optimize(log_p, c(-1, 1), \n                  tol = 0.0001, \n                  maximum = TRUE, \n5                  yt = yt)\ncat(\"\\n MLE of full likelihood for phi: \", result$maximum)\n\n\n1\n\nar coefficient\n\n2\n\ninnovation standard deviation\n\n3\n\nnumber of time points\n\n4\n\nlog likelihood function\n\n5\n\nusing a built-in optimization method to obtain MLE\n\n\n\n\n\n MLE of full likelihood for phi:  0.9265928",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#bayesian-inference-in-the-ar1",
    "href": "C4-L03.html#bayesian-inference-in-the-ar1",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.4 Bayesian inference in the AR(1)",
    "text": "90.4 Bayesian inference in the AR(1)\n\n\n\n\n\n\n\nFigure 90.4: inference\n\n\n\ny_t= \\phi y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0, v), \\quad \\phi \\in (-1,1)\n\\tag{90.14}\n\n\\utilde{y} = \\mathbb{X}\\utilde{\\beta} + \\utilde{\\varepsilon}\n\\tag{90.15}\nwhere \\mathbb{X} is the design matrix with y_1 as the first column and y_2, \\ldots, y_{T-1} as the second column, \\utilde{\\beta} = \\phi is the AR coefficient, and \\utilde{\\varepsilon} \\sim \\mathcal{N}(0, v\\mathbf{I}) is the error term.\n\n\\utilde{y} = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_{T-1}\n\\end{pmatrix}\n\\tag{90.16}\n\n\\mathbb{X} = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_{T-1}\n\\end{pmatrix}\n\\tag{90.17}\n\n\\utilde{\\beta} = \\phi\n\\tag{90.18}\n\n\\utilde{\\varepsilon} \\sim \\mathcal{N}(0, v\\mathbf{I})\n\\tag{90.19}\n\np(y_{2:T} \\mid y_1, \\phi, v) =  \\frac{1}{(2\\pi v)^{\\frac{T-1}{2}}} \\exp\\left(-\\frac{(\\utilde{y} - \\mathbb{X}\\utilde{\\beta})'(\\utilde{y} - \\mathbb{X}\\utilde{\\beta})}{2v}\\right)\n\\tag{90.20}\n\np(\\phi, v \\mid y_{1:T}) \\propto p(\\phi, v) p(y_{2:T} \\mid y_1, \\phi, v)\n\\tag{90.21}\n\n\\begin{aligned}\np(\\phi, v) \\propto \\frac{1}{v} &\\cdot (\\utilde{\\beta} \\mid v, \\mathbb{X}, \\utilde{y}) &\\sim & \\mathcal{N}(\\hat{\\utilde{\\beta}}, v(\\mathbb{X}'\\mathbb{X})^{-1}) \\\\\n  & \\cdot (v \\mid \\mathbb{X}, \\utilde{y}) &\\sim & \\mathcal{IG}\\left(\\frac{T-2}{2}, \\frac{1}{2}Q(\\hat{\\utilde{\\beta}}_{MLE}) \\right)\n\\end{aligned}\n\\tag{90.22}\n\n\\begin{aligned}\n\\hat{\\utilde{\\beta}_{MLE}} &= (\\mathbb{X}'\\mathbb{X})^{-1}\\mathbb{X}'\\utilde{y} \\\\\n&= \\hat{\\phi}_{MLE} = \\frac{\\sum_{t=2}^T y_t y_{t-1}}{\\sum_{t=2}^T y_{t-1}^2}\n\\end{aligned}\n\\tag{90.23}\n\nQ(\\hat{\\phi}_{MLE}) = \\sum_{t=2}^T (y_t - \\hat{\\phi}_{MLE}\\ y_{t-1})^2\n\\tag{90.24}",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#bayesian-inference-in-the-ar1-conditional-likelihood-example",
    "href": "C4-L03.html#bayesian-inference-in-the-ar1-conditional-likelihood-example",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.5 Bayesian inference in the AR(1): Conditional likelihood example 🎥",
    "text": "90.5 Bayesian inference in the AR(1): Conditional likelihood example 🎥\nThis video walks through the code snippet below and provides examples of how to sample from the posterior distribution of the AR coefficient \\psi and the variance v using the conditional likelihood and a reference prior.",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#r-code-ar1-bayesian-inference-conditional-likelihood-example",
    "href": "C4-L03.html#r-code-ar1-bayesian-inference-conditional-likelihood-example",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.6 R Code: AR(1) Bayesian inference, conditional likelihood example 📖",
    "text": "90.6 R Code: AR(1) Bayesian inference, conditional likelihood example 📖\n\n####################################################\n#####             MLE for AR(1)               ######\n####################################################\nset.seed(2021)\nphi=0.9 # ar coefficient\nsd=1 # innovation standard deviation\nT=200 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) # sample stationary AR(1) process\n\ny=as.matrix(yt[2:T]) # response\nX=as.matrix(yt[1:(T-1)]) # design matrix\nphi_MLE=as.numeric((t(X)%*%y)/sum(X^2)) # MLE for phi\ns2=sum((y - phi_MLE*X)^2)/(length(y) - 1) # Unbiased estimate for v\nv_MLE=s2*(length(y)-1)/(length(y)) # MLE for v \n\nprint(c(phi_MLE,s2))\n\n[1] 0.9178472 1.0491054\n\n#######################################################\n######     Posterior inference, AR(1)               ###\n######     Conditional Likelihood + Reference Prior ###\n######     Direct sampling                          ###\n#######################################################\n\nn_sample=3000   # posterior sample size\n\n## step 1: sample posterior distribution of v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2)/2, sum((yt[2:T] - phi_MLE*yt[1:(T-1)])^2)/2)\n\n## step 2: sample posterior distribution of phi from normal distribution\nphi_sample=rep(0,n_sample)\nfor (i in 1:n_sample){\nphi_sample[i]=rnorm(1, mean = phi_MLE, sd=sqrt(v_sample[i]/sum(yt[1:(T-1)]^2)))}\n\n## plot histogram of posterior samples of phi and v\npar(mfrow = c(1, 2),mar = c(3, 4, 2, 1), cex.lab = 1.3)\nhist(phi_sample, xlab = bquote(phi), \n     main = bquote(\"Posterior for \"~phi),xlim=c(0.75,1.05), col='lightblue')\nabline(v = phi, col = 'red')\nhist(v_sample, xlab = bquote(v), col='lightblue', main = bquote(\"Posterior for \"~v))\nabline(v = sd, col = 'red')",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#quiz---mle-and-bayesian-inference-in-the-ar1",
    "href": "C4-L03.html#quiz---mle-and-bayesian-inference-in-the-ar1",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.7 Quiz - MLE and Bayesian inference in the AR(1)",
    "text": "90.7 Quiz - MLE and Bayesian inference in the AR(1)\nOmitted per Coursera honor code",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L03.html#practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1",
    "href": "C4-L03.html#practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1",
    "title": "90  The AR(1): MLE and Bayesian inference - M1L3",
    "section": "90.8 Practice Graded Assignment: MLE and Bayesian inference in the AR(1)",
    "text": "90.8 Practice Graded Assignment: MLE and Bayesian inference in the AR(1)\nThis peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you’ve learned in R and prepare you for your data analysis project in week 5.\n\nConsider the R code below: MLE for the AR(1)\n\n\n\n\n\nListing 90.1: R Code: MLE for the AR(1) process, conditional likelihood example\n\n\n####################################################\n#####             MLE for AR(1)               ######\n####################################################\nphi=0.9 # ar coefficient\nv=1\nsd=sqrt(v) # innovation standard deviation\nT=500 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Case 1: Conditional likelihood\ny=as.matrix(yt[2:T]) # response\nX=as.matrix(yt[1:(T-1)]) # design matrix\nphi_MLE=as.numeric((t(X)%*%y)/sum(X^2)) # MLE for phi\ns2=sum((y - phi_MLE*X)^2)/(length(y) - 1) # Unbiased estimate for v \nv_MLE=s2*(length(y)-1)/(length(y)) # MLE for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"MLE for the variance v: \", v_MLE, \"\\n\", \n    \"Estimate s2 for the variance v: \", s2, \"\\n\")\n\n\n\n\n\n MLE of conditional likelihood for phi:  0.9048951 \n MLE for the variance v:  1.084559 \n Estimate s2 for the variance v:  1.086737 \n\n\nModify the code above to sample 800 observations from an AR(1) with AR coefficient \\psi = -0.8 and variance v = 2. Plot your simulated data. Obtain the MLE for \\psi based on the conditional likelihood and the unbiased estimate s^2 for the variance v.\n\nConsider the R code below: AR(1) Bayesian inference, conditional likelihood\n\n\n\n\n\nListing 90.2: R Code: AR(1) Bayesian inference, conditional likelihood example\n\n\n#######################################################\n######     Posterior inference, AR(1)               ###\n######     Conditional Likelihood + Reference Prior ###\n######     Direct sampling                          ###\n#######################################################\n\nn_sample=3000   # posterior sample size\n\n## step 1: sample posterior distribution of v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2)/2, sum((yt[2:T] - phi_MLE*yt[1:(T-1)])^2)/2)\n\n## step 2: sample posterior distribution of phi from normal distribution\nphi_sample=rep(0,n_sample)\nfor (i in 1:n_sample){\nphi_sample[i]=rnorm(1, mean = phi_MLE, sd=sqrt(v_sample[i]/sum(yt[1:(T-1)]^2)))}\n\n## plot histogram of posterior samples of phi and v\npar(mfrow = c(1, 2), mar = c(3, 4, 2, 1), cex.lab = 1.3)\nhist(phi_sample, xlab = bquote(phi), \n     main = bquote(\"Posterior for \"~phi),xlim=c(0.75,1.05), col='lightblue')\nabline(v = phi, col = 'red')\nhist(v_sample, xlab = bquote(v), col='lightblue', main = bquote(\"Posterior for \"~v))\nabline(v = sd, col = 'red')\n\n\n\n\n\n\n\n\n\n\n\nUsing your simulated data from part 1 modify the code above to summarize your posterior inference for \\psi and v based on 5000 samples from the joint posterior distribution of \\psi and v.\n\n\n\n\n\n\nTipGrading Criteria\n\n\n\nThe responses should follow the same template as the sample code provided above but you will submit your code lines in plain text. Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect that :\n\nyou generate 800 time points from the AR(1) rather than 500 and plot your simulated data.\nyour simulated data is from an AR(1) with AR coefficients \\psi = -0.8 and variance v = 2 rather than AR(1) with AR coefficient \\psi = 0.9 and variance v = 1 and\nyou obtain 5000 rather than 3000 samples from the posterior distribution from the new simulated process.\n\n\n\n\n90.8.1 Bayesian Inference in the AR(1), : full likelihood example 📖\nWe consider a prior distribution that assumes that \\phi and v are independent:\n\n\\mathbb{P}r(v) \\propto \\frac{1}{v},\n\n\n\\mathbb{P}r(\\phi) = \\frac{1}{2}, \\quad \\text{for } \\phi \\in (-1, 1),\n\ni.e., we assume a Uniform prior for \\phi \\in (-1, 1). Combining this prior with the full likelihood in the AR(1) case, we obtain the following posterior density:\n\n\\mathbb{P}r(\\phi, v \\mid y_{1:T}) \\propto \\frac{(1 - \\phi^2)^{1/2} }{v^{T/2 + 1}} \\exp\\left(-\\frac{Q^*(\\phi)}{2v}\\right), \\quad -1 &lt; \\phi &lt; 1,\n\nwith\n\nQ^*(\\phi) = y_1^2(1 - \\phi^2) + \\sum_{t=2}^{T} (y_t - \\phi y_{t-1})^2.\n\nIt is not possible to get a closed-form expression for this posterior or to perform direct simulation. Therefore, we use simulation-based Markov Chain Monte Carlo (MCMC) methods to obtain samples from the posterior distribution.\n\n\n90.8.2 Transformation of \\phi\nWe first consider the following transformation on \\phi:\n\n\\eta = \\log\\left(\\frac{1 - \\phi}{\\phi + 1}\\right),\n\nso that \\eta \\in (-\\infty, \\infty). The inverse transformation on \\eta is:\n\n\\phi = \\frac{1 - \\exp(\\eta)}{1 + \\exp(\\eta)}.\n\nWriting down the posterior density for \\eta and v, we obtain\n\n\\mathbb{P}r(\\eta, v \\mid y_{1:T}) \\propto\\frac{ (1 - \\phi^2)^{1/2} }{v^{T/2 + 1}} \\exp\\left(-\\frac{Q^*(\\phi)}{2v}\\right) \\cdot \\frac{2 \\exp(\\eta)}{(1 + \\exp(\\eta))^2},\n\nwith \\phi written as a function of \\eta. We proceed to obtain samples from this posterior distribution using the MCMC algorithm outlined below. Once we have obtained M samples from \\eta and v after convergence, we can use the inverse transformation above to obtain posterior samples for \\phi.\n\n\n90.8.3 MCMC Algorithm: Bayesian Inference for AR(1), Full Likelihood\nAlgorithm:\n\nInitialize \\eta^{(0)} and \\beta^{(0)}.\nFor m in 1:M do:\n\nSample v^{(m)} \\sim \\text{IG}\\left(\\frac{T}{2}, \\frac{Q^*(\\phi^{(m-1)})}{2}\\right).\nSample \\eta^{(m)} using Metropolis-Hastings:\n\nSample \\eta^* \\sim N(\\eta^{(m-1)}, c), where c is a tuning parameter.\nCompute the importance ratio:\n\n\n\n\n        r = \\frac{p(\\eta^*, v^{(m)} \\mid y_{1:T})}{p(\\eta^{(m-1)}, v^{(m)} \\mid y_{1:T})}.\n\n\nSet:\n\n\n        \\eta^{(m)} =\n        \\begin{cases}\n        \\eta^* & \\text{with probability } \\min(r, 1), \\\\\n        \\eta^{(m-1)} & \\text{otherwise}.\n        \\end{cases}\n\n\n\n\n\n\n\nSchott, James R. 2016. Matrix Analysis for Statistics. Wiley Series in Probability and Statistics. Wiley. https://books.google.co.il/books?id=Y2PpCgAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>The AR(1): MLE and Bayesian inference - M1L3</span>"
    ]
  },
  {
    "objectID": "C4-L04.html",
    "href": "C4-L04.html",
    "title": "91  The AR(p) process - M2L4",
    "section": "",
    "text": "91.1 AR(p) Definition and State-space Representation 🎥\nThe under tildes used in the slides denote a vector or a matrix rather than a statistical property. They are usually denoted via bold fonts and not by under tildes which have other meanings too so for the sake of clarity I have replaced them with bold font in the outline. I provide detailed derivation in this section but there is a shorter outline in Section 91.7 which may be easier to review once you know the derivations.\nIn this segment we will see two important representations of the AR(p) process. You can follow along in (Prado, Huerta, and West 2000 ch. 2)",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#sec-arp-defn-state-space-rep",
    "href": "C4-L04.html#sec-arp-defn-state-space-rep",
    "title": "91  The AR(p) process - M2L4",
    "section": "",
    "text": "Figure 91.1: AR(p) process, characteristic polynomial, stability, stationarity and MA representation\n\n\n\n\n\n91.1.1 AR(p) definition\nAR(p), shorthand, for Auto Regressive Process of order p which generalizes the AR(1) process by defining the current time step in terms of the previous p time steps. We denote the number of parameter required to characterize the current value as p, and call it the order of the autoregressive process. The order tells us how many lags we will be considering..  Therefore the AR(1) process as a special case of the more general AR(p) process with p=1.order p\n We will assume AR(P) has the following structure:AR(P)\n\n\\textcolor{red}{y_t} = \\textcolor{blue}{\\phi_1} \\textcolor{red}{y_{t-1}} + \\textcolor{blue}{\\phi_2} \\textcolor{red}{y_{t-2}} + \\ldots + \\textcolor{blue}{\\phi_p} \\textcolor{red}{y_{t-p}} + \\textcolor{grey}{\\varepsilon_t}, \\qquad \\textcolor{grey}{\\varepsilon_t} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0,v) \\quad \\forall t\n\\tag{91.1}\nwhere:\n\n\\textcolor{red}{y_t} is the value of the time series at time t\n\\textcolor{blue}{\\phi_{1:p}} are the AR coefficients\n\\textcolor{grey}{\\varepsilon_t} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0,v) \\quad \\forall t is a white noise process with zero mean and constant variance v.\n\n\n\n91.1.2 AR(p) Characteristic Polynomial\nA central outcome of the autoregressive nature of the AR(p) is due to the properties the AR characteristic polynomial \\Phi.  This is defined as :\\Phi AR characteristic polynomial\nrecall the backshift operator B is defined as:\n\n\\operatorname{B} y_t = y_{t-1}\n\nso that\n\n\\operatorname{B}^j y_t = y_{t-j}\n\nWe now use the backshift operator to rewrite the AR(p) as a inhomogeneous linear difference equation: \n\\begin{aligned}\n       y_t &= \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\varepsilon_t  && \\text{(Ar(p) defn.)} \\newline\n       y_t &= \\phi_1 \\operatorname{B} y_t + \\phi_2 \\operatorname{B}^2 y_t + \\ldots + \\phi_p \\operatorname{B}^p y_t + \\varepsilon_t && \\text{(B defn.)} \\newline\n\\varepsilon_t &= y_t - \\phi_1 \\operatorname{B} y_t - \\phi_2 \\operatorname{B}^2 y_t - \\ldots - \\phi_p \\operatorname{B}^p y_t    && \\text{(rearranging)} \\newline\n\\varepsilon_t  &= (1- \\phi_1 \\operatorname{B} - \\phi_2 \\operatorname{B}^2 - \\ldots - \\phi_p \\operatorname{B}^p) y_t            && \\text{(factoring out $y_t$)}\n\\end{aligned}\n\\tag{91.2}\n\n\\Phi(u) = 1 - \\phi_1 u - \\phi_2 u^2 - \\ldots - \\phi_p u^p \\qquad \\text{(Characteristic polynomial)}\n\\tag{91.3}\nwhere:\n\nu \\in \\mathbb{C} i.e. complex-valued roots\n\\phi_j are the AR coefficients.\n\n\n\n\n\n\n\nCautionReplacing the Backshift operator by Z\n\n\n\n\n\nAs far as the mathematics goes it isn’t clear how we get from Equation 91.2 to the characteristic polynomial \\Phi(z) in Equation 91.3.\nAs far as I can tell we can justify it by saying we have done three things:\n\nWe set \\varepsilon_t = 0 to study the homogeneous equation. This is not about minimizing residuals but about analyzing the underlying linear recurrence, which governs the dynamics of the AR process without noise.\nWe assume y_t \\neq 0. Since \\Phi(B)y_t = 0, we can treat this as a linear operator equation. By analogy to polynomial equations, if ab = 0 and b \\neq 0, then a = 0. Thus, we analyze the operator polynomial \\Phi(B) = 0 as the condition for nontrivial solutions.\nWe replace the backshift operator B with a complex number z, leading to the characteristic polynomial \\Phi(z):\n\n\nThis is justified by assuming exponential solutions of the form y_t = z^t, a standard method in solving linear recurrences. Substituting into \\Phi(B)y_t = 0 yields \\Phi(z^{-1}) = 0, which we often rewrite as \\Phi(z) = 0 for convenience.\nAlternatively, we view B as a placeholder in a polynomial ring of operators, and replacing B by z evaluates that polynomial. This is a standard algebraic move in analyzing recurrence relations or z-transforms.\nAnother point is that the backshift operator B is just a linear operator on the space of time series. I.e it can be viewed as a matrix that shifts the time series back by one step.\n\n\n\n\nDiving a little deeper \\operatorname{B} is represented by a nilpotent matrix, which is a matrix that shifts the time series back by one step. This is a standard algebraic move in analyzing recurrence relations or z-transforms.\n\nB = \\begin{pmatrix}\n0 & 0 & 0 & \\cdots & 0 \\\\\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{pmatrix}\n\\tag{91.4}\nThis matrix is nilpotent because if you multiply it by itself enough times, it will eventually become the zero matrix. This is a key property that allows us to define the characteristic polynomial in terms of the backshift operator.\nNote: that the above derivation wasn’t presented in the slides but is my own attempt to clarify the steps involved I hope it is helpful not 100% sure it is correct.\n\nThis polynomial and its roots tells us a lot about the process and its properties. One of the main characteristics is it allows us to think about things like quasi-periodic behavior, whether it’s present or not in a particular AR(p) process.\nIt allows us to think about whether a process is stationary or not, depending on some properties related to this polynomial.\nIn particular, we are going to say that the process is stable if all the roots of the characteristic polynomial have a modulus greater than one. \n\nstability condition\nWhy are we interested in this autoregressive lag polynomial?\n\n\n\\Phi(z) = 0 \\iff |z| &gt; 1  \\qquad \\text{(stability condition)}\n\\tag{91.5}\n\nFor any of the roots, it has to be the case that the modulus of that root, they have to be all outside the unit circle.\nIf a process is stable, it will also be stationary.\n\nWe can show this as follows:\nIf the AR(p) has all the roots of its characteristic polynomial outside the unit circle, it is stable and stationary and can be written in terms of an infinite order moving average process:\n\ny_t = \\Psi(\\operatorname{B}) \\varepsilon_t = \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j} \\quad \\text {with} \\ \\psi_0 = 1 \\quad \\text{ and } \\sum_{j=0}^{\\infty} |\\psi_j| &lt; \\infty\n\\tag{91.6}\nwhere:\n\n\\varepsilon_t is a white noise process with zero mean and constant variance v.\n\\operatorname{B} is the lag operator AKA the backshift operator defined by \\operatorname{B} \\varepsilon_t = \\varepsilon_{t-1}. This need to be applied to a time series \\varepsilon_t to get the lagged values.\n\\Psi(\\operatorname{B}) is the infinite order polynomial in \\operatorname{B} that represents a linear filter applied to the noise process.​\n\\psi_t = 1 is the weight for the white noise at time t.\nthe constraint \\psi_0 = 1 ensures that the current shock contributes directly to y_t\nthe constraint on the weights \\sum_{j=0}^{\\infty} |\\psi_j| &lt; \\infty ensures that the weights decay sufficiently fast, so that the process does not explode making it is stable and therefore stationary.\n\n\n\n\n\n\n\nCautionNotation Confusion\n\n\n\n\n\nthe notation with \\psi a functional of operator B and \\psi_i as constants is confusing in both the reuse if the symbol and the complexity.\n\n\n\nWe can also rewrite the characteristic polynomial in terms of the reciprocal roots of the polynomial.\nThe zeros of the characteristic polynomial are the roots of the AR(p) process.\n\n\\Phi(u) = \\prod_{j=1}^{p} (1 - \\alpha_j u) = 0  \\implies u = \\frac{1}{ \\alpha_j} \\qquad \\text{(reciprocal roots)}\n\\tag{91.7}\nwhere:\n\n\\alpha_j are the reciprocal roots of the characteristic polynomial.\n\nHere, u is any complex valued number.\n\n\n\n91.1.3 State Space Representation of AR(p)\n\n\n\n\n\n\n\nFigure 91.2: A state space representation of Ar(p)\n\n\nThis material is covered in (Prado, Huerta, and West 2000, sec. 2.1.2)\nAnother important representation of the AR(P) process, is based on a state-space representation of the process. This representation is useful because it allows us to study some important properties of the process. We will make some connections with these representations later when we talk about dynamic linear models, is given as follows for an AR(P).\n\ny_t = \\operatorname{F}^\\top \\mathbf{x}_t \\qquad \\text{(observational equation)}\n\\tag{91.8}\nwhere:\n\n\\operatorname{F} is a linear mapping from the state space into the state space, so it is just vector of coefficients, specifically F = (1, 0, \\ldots, 0)^\\top for the AR(P) process. The rank for this operator is p since it has to match the dimension of the state vector \\mathbf{x}_t.\n\\mathbf{x}_t is a vector with the state of the process at time t.\n\nTo demystify \\operatorname{F}, it is just picking the current state from vector \\mathbf{x}_t with states for the p previous time steps.\n\n\\mathbf{x}_t = G \\mathbf{x}_{t-1} + \\mathbf{w}_t \\qquad \\text{(state equation)}\n\\tag{91.9}\nwhere:\n\n\\mathbf{x}_t is a vector of the current state of the process.\nG is a state transition matrix that describes the relationship between the current state and the previous state.\n\\mathbf{x}_{t-1} is a vector of the previous state of the process.\n\\mathbf{w}_t is a vector of innovations or noise at time t, which is assumed to be normally distributed with zero mean and constant variance. The first component is going to be the \\varepsilon_t and the rest of the components are going to be zero and the dimension of this vector is going to be p.\n\n and state transition matrix G\nThe G matrix in this representation is going to be a very important matrix, the first row is going to contain the AR parameters, the AR coefficients, and we have p of those. In the block below this is an identity matrix, and a zero column on it’s right.\n\nG = \\begin{pmatrix}\n\\phi_1 & \\phi_2 & \\phi_3 & \\dots & \\phi_{p-1} & \\phi_p \\\\\n1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & 0 & \\dots & 0 & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1 & 0\n\\end{pmatrix}.\n\\tag{91.10}\nThis state transition matrix \\mathbf{G} is important because it is related to the characteristic polynomial, in particular, is related to the reciprocal roots representation of the characteristic polynomial that we discussed before. The structure of this \\mathbf{G} matrix is such that it captures the Markovian dynamics of the autoregressive process, wherein each \\mathbf{x}_t is a function of \\mathbf{x}_{t-1}.\nThe eigenvalues of this matrix correspond precisely to the reciprocal roots of the characteristic polynomial.\nPrado points out that if you perform the matrix operations described in Equation 91.8 and Equation 91.9, you will recover the form of your autoregressive process from the definition in Equation 91.1 .\nSo we have given the state-space representation of an AR(p). One advantage of working with this representation is that we can use with it some definitions that apply to dynamic linear models or state-space models. One such definition is the so-called forecast function.\n\n\n91.1.4 The forecast Function of AR(p)\n The forecast function, which we denote as f_t(h) is a function f that depends on the time t that you’re considering, and then you’re looking at forecasting h steps ahead in your time series. If you have observations up to today and you want to look at what is the forecast function five days later, we will set h=5 there. The forecast function is just the expected value and we can just think of this as the expected value of y_{t+h}. conditional on all the observations or all the information you have received up to time t.\n\n\\begin{aligned}\nf_t(h) &= \\mathbb{E}[y_{t+h} \\mid y_{1:t}] &\\text{(defn)}\\\\\n       &= \\mathbf{F}^\\top \\mathbb{E}[\\mathbf{x}_{t+h} \\mid y_{1:t}] &\\text{(observation eq.)} \\\\\n       &= \\mathbf{F}^\\top \\mathbf{G} \\mathbb{E}[\\mathbf{x}_{t+h-1} \\mid y_{1:t}] &\\text{(state eq.)} \\\\\n       &= \\mathbf{F}^\\top \\mathbf{G}^h \\mathbb{E}[\\mathbf{x}_t\\mid y_{1:t}], & \\text{(repeat)} \\\\\n       &= \\mathbf{F}^\\top \\mathbf{G}^h \\mathbf{x}_t & h &gt; 0, \\forall t \\ge p\n\\end{aligned}\n\\tag{91.11}\nwhere:\n\ny_{1:t} is the vector of all observations up to time t,\n\\mathbf{x}_t is the state vector at time t.\n\\mathbf{G}^h is the state transition matrix raised to the power h, which captures the dynamics of the AR process over h steps ahead. The eigenvalues of this matrix are the reciprocal roots of the characteristic polynomial of the AR(p) process.\nIn this derivation:\n\nWe start with the expectation and rewrite it expectation using the state-space representation equations:\nWe start using the observation equation, replacing y_{t+h} with F^T(\\mathbf x_{t+h}) in that case.\nNext we apply the second (state) equation.\n\nThis introduces \\mathbf{G}\nand updates our expected value to the next time step, so we have \\mathbf{X}_{t+h-1}.\nNote: that since the w_{t+h-1} terms are independent of the past observations, their expected value is zero, so we can leave them out.\n\nWe repeat applying the state equation for all the lags until we get to time t, ending up with a product of h \\mathbf{G} matrices here, so we end up with \\mathbf{G}^h and the each time we drop a lag in the expectation of \\mathbf{X}_{t+h}.\nWe end up with the expected value of \\mathbb{E}[\\mathbf{x}_t] which is just the vector \\mathbf{x}_t the current state of the process.\n\n\nThis result is significant because it now allows us to make the connection of \\mathbf{G} and its eigenstructure. One of the features of \\mathbf{G} is that the eigenstructure is related to the reciprocal roots of the characteristic polynomial. So when we working with the case in which we have exactly p distinct roots. We can further simplify using by rewriting \\mathbf{G} in terms of its eigendecomposition. We can rewrite \\mathbf{G} as \\mathbf{E}, a matrix \\mathbf{\\Lambda} here, \\mathbf{E}^{-1}.\n\n\\mathbf{G}= \\mathbf{E} \\mathbf{\\Lambda} \\mathbf{E}^{-1} \\qquad \\text{(eigendecomposition)}\n\\tag{91.12}\nwhere:\n\n\\mathbf{\\Lambda} = \\operatorname{diag}(\\alpha_1, \\ldots, \\alpha_p) is a diagonal matrix consisting of the reciprocal roots \\alpha_i, from the reciprocal formulation of the characteristic polynomial in Equation 91.7.\n\nWhile the order of the roots doesn’t matter but there is a tradition of order eigenvalues them in decreasing value and this can help us to identify our model!\n\n\\mathbf{E} is a eigenvectors decomposition for the matrix \\mathbf{G} E = [\\mathbf{e}_1 ; \\cdots ; \\mathbf{e}_p], where \\mathbf{e}_i is the eigenvector corresponding to \\alpha_i.\n\nSince each root is unique the eigenvectors are all different and linearly independent.\nNote that the eigendecomposition, i.e. the eigenvectors, has to follow the order set in \\Lambda\n\n\nWe can now rewrite \\mathbf{G}^h as:\n\n\\mathbf{G}^h= \\mathbf{E} \\mathbf{\\Lambda}^h \\mathbf{E}^{-1} \\qquad \\text{(eigendecomposition of G)}\n\\tag{91.13}\n\n\n\n\n\n\nCautionWhy arn’t the eigendecomposition powered up\n\n\n\n\n\nSo here is an easy answer\nif \\mathbf{G}= \\mathbf{E} \\mathbf{\\Lambda} \\mathbf{E}^{-1}\nif we multiply out the all the E and E^{-1} terms cancel out except the last and first.\n\n\n\nWhatever elements have in the matrix of eigenvectors \\mathbf{E}, they are now going to be functions of the reciprocal roots u_i=\\frac{1}{\\phi_j}. The power that appears here, which is the number of steps ahead that you want to forecast in your time series for prediction,\n\nf_t(h) = E(y_{t+h} \\mid y_{1:t}) = F^\\top G^h x_t, \\quad h &gt; 0, \\quad \\forall t \\ge p\n\\tag{91.14}\nwhere:\n\nc_t are constants that depend on the E matrix.\n\\alpha_i^h are the reciprocal roots raised to the power h.\n\nWe can see from the form of Equation 91.22 that if the process is stable, i.e. all the all the moduli of my reciprocal roots are going to be below one. So it is going to decay exponentially as a function of h. And this AR(p) process is going to be stationary.\n\n\n\n\n\n\nImportantInterpreting the forecast function\n\n\n\nI recall that in physics we often view the eigenvectors as resonances of the system’s dynamics, and the eigenvalues as the corresponding resonant frequencies. This is a good analogy to think about the reciprocal roots of the AR(p) process. The contribution of each of the roots \\alpha_i to f(t) depends on how close that modulus of that reciprocal root is to 1 or -1. For roots that have relatively large values of the modulus, then they are going to have more contribution in terms of what’s going to happen in the future.\nDepending on whether those reciprocal roots \\alpha_i are real-valued or complex-valued, you’re going to have behavior here that may be quasiperiodic for complex-valued roots or just non-quasiperiodic for the real valued roots.\n\n\nIn the text book AR(p) forecasting is covered in (Prado, Huerta, and West 2000, sec. 2.2) and the mean square errors can be estimated using an algorithm by (Brockwell and Davis 1991).\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\nWe will now discuss the general autoregressive process of order p.  As in the AR1, we’re going to think about expressing current values of the time series in terms of past values of the time series. In this case, we’re going to think about expressing the current time in terms of the past p-values of the time series process. That’s why it’s called an autoregressive model of order p. The order tells you how many lags you are going to be considering. We’re going to then assume a process that is going to have this structure. Again, as I said, we are regressing on the past p-values. As before, in this case, we’re going to assume that the epsilon t’s are independent identically distributed random variables with a normal distribution centered at zero and variance v. This is the assumption that we are going to use here. As you see now, the number of parameters has increased. We had one coefficient before, now we’re going to have p coefficients, and we’re going to have also the variance of the process. One thing that is very important used to characterize and understand the properties of autoregressive processes is the so-called characteristic polynomial. The AR characteristic polynomial is a polynomial, I’m going to denote it like this, and it’s going to be a function of the phi coefficients here. It’s going to look like a polynomial here, and is a polynomial of order p. Here, U is any complex valued number. Why do we study this polynomial? This polynomial tells us a lot about the process and a lot about the properties of this process. One of the main characteristics is it allows us to think about things like quasi-periodic behavior, whether it’s present or not in a particular AR p process. It allows us to think about whether a process is stationary or not, depending on some properties related to this polynomial. In particular, we are going to say that the process is stable here. This is the stability condition. If all the roots of this polynomial have a modulus, that is that they all have modulus that are greater than one, if, I’m going to write it like this, Phi of U, this polynomial is zero for a root, so for any value of U such that this happens, then we say that the process is stable. For any of the roots, it has to be the case that the modulus of that root, they have to be all outside the unit circle. When a process is stable, it’s also going to be stationary. In this case, if the process is stable, then we have a stationary process. This is going to characterize also the stationarity of the process in terms of the roots of the characteristic polynomial. Once the process is stationary, and if all the roots of the characteristic polynomial are outside the unit circle, then we will be able to write this process in terms of an infinite order moving average process. In this case, if the process is stable, then we are going to be able to write it like this. I’m sorry, this should be epsilon t. I am going to have an infinite order polynomial here on B, the backshift operator that I can write down just as the sum, j goes from zero to infinity. Here Psi_0 is one. Then there is another condition on the Psi’s for this to happen. We have to have finite sum of these on these coefficients. Once again, if the process is stable, then it would be stationary and we will be able to write down the AR as an infinite order moving average process here. If you recall, B is the backshift operator. Again, if I apply this to y_t, I’m just going to get y_t minus j. I can write down Psi of B, as 1 plus Psi_1 B, B squared, and so on. It’s an infinite order process. The AR characteristic polynomial can also be written in terms of the reciprocal roots of the polynomial. So instead of considering the roots, we can consider the reciprocal roots. In that case, let’s say the Phi of u for Alpha 1, Alpha 2, and so on. The reciprocal roots. Why do we care about all these roots? Why do we care about this structure? Again, we will be able to understand some properties of the process based on these roots as we will see. We will now discuss another important representation of the AR(P) process, one that is based on a state-space representation of the process. Again, we care about this type of representations because they allow us to study some important properties of the process. In this case, our state-space or dynamic linear model representation, we will make some connections with these representations later when we talk about dynamic linear models, is given as follows for an AR(P). I have my y_t. I can write it as F transpose and then another vector x_t here. Then we’re going to have x_t is going to be a function of x_t minus 1. That vector there is going to be an F and a G. I will describe what those are in a second. Then I’m going to have another vector here with some distribution. In our case, we are going to have a normal distribution also for that one. In the case of the AR(P), we’re going to have x_t to be y_t, y_t minus 1. It’s a vector that has all these values of the y_t process. Then F is going to be a vector. It has to match the dimension of this vector. The first entry is going to be a one, and then I’m going to have zeros everywhere else. The w here is going to be a vector as well. The first component is going to be the Epsilon t. That we defined for the ARP process. Then every other entry is going to be a zero here. Again, the dimensions are going to match so that I get the right equations here. Then finally, my G matrix in this representation is going to be a very important matrix, the first row is going to contain the AR parameters, the AR coefficients. We have p of those. That’s my first row. In this block, I’m going to have an identity matrix. It’s going to have ones in the diagonal and zeros everywhere else. I’m going to have a one here, and then I want to have zeros everywhere else. In this portion, I’m going to have column vector here of zeros. This is my G matrix. Why is this G matrix important? This G matrix is going to be related to the characteristic polynomial, in particular, is going to be related to the reciprocal roots of the characteristic polynomial that we discussed before. The eigenvalues of this matrix correspond precisely to the reciprocal roots of the characteristic polynomial. We will think about that and write down another representation related to this process. But before we go there, I just want you to look at this equation and see that if you do the matrix operations that are described these two equations, you get back the form of your autoregressive process. The other thing is, again, this is called a state-space representation because you have two equations here. One, you can call it the observational level equation where you are relating your observed y’s with some other model information here. Then there is another equation that has a Markovian structure here, where x_t is a function of x_t minus 1. This is why this is a state-space representation. One of the nice things about working with this representation is we can use some definitions that apply to dynamic linear models or state-space models, and one of those definitions is the so-called forecast function. The forecast function, we can define it in terms of, I’m going to use here the notation f_t h to denote that is a function f that depends on the time t that you’re considering, and then you’re looking at forecasting h steps ahead in your time series. If you have observations up to today and you want to look at what is the forecast function five days later, you will have h equals 5 there. It’s just the expected value. We are going to think of this as the expected value of y_t plus h. Conditional on all the observations or all the information you have received up to time t. I’m going to write it just like this. Using the state-space representation, you can see that if I use the first equation and I think about the expected value of y_t plus h is going to be F transpose, and then I have the expected value of the vector x_t plus h in that case. I can think of just applying this, then I would have expected value of x_t plus h given y_1 up to t. But now when I look at the structure of x_t plus h, if I go to my second equation here, I can see that x_t plus h is going to be dependent on x_t plus h minus 1, and there is a G matrix here. I can write this in terms of the expected value of x_t plus h, which is just G, expected value of x_t plus h minus 1, and then I also have plus expected value of the w_t’s. But because of the structure of the AR process that we defined, we said that all the Epsilon T’s are independent normally distributed random variables center at zero. In this case, those are going to be all zero. I can write down this as F transpose G, and then I have the expected value of x_t plus h minus 1 given y_1 up to t. If I continue with this process all the way until I get to time t, I’m going to get a product of all these G matrices here, and because we are starting with this lag h, I’m going to have the product of that G matrix h times. I can write this down as F transpose G to the power of h, and then I’m going to have the expected value of, finally, I get up to here. This is simply is going to be just my x_t vector. I can write this down as F transpose G^h, and then I have just my x_t. Again, why do we care? Now we are going to make that connection with this matrix and the eigenstructure of this matrix. I said before, one of the features of this matrix is that the eigenstructure is related to the reciprocal roots of the characteristic polynomial. In particular, the eigenvalues of this matrix correspond to the reciprocal roots of the characteristic polynomial. If we are working with the case in which we have exactly p different roots. We have as many different roots as the order of the AR process. Let’s say, p distinct. We can write down then G in terms of its eigendecomposition. I can write this down as E, a matrix Lambda here, E inverse. Here, Lambda is going to be a diagonal matrix, you just put the reciprocal roots, I’m going to call those Alpha 1 up to Alpha p. They are all different. You just put them in the diagonal and you can use any order you want. But the eigendecomposition, the eigenvectors, have to follow the order that you choose for the eigenvalues. Then what happens is, regardless of that, you’re going to have a unique G. But here, the E is a matrix of eigenvectors. Again, why do we care? Well, if you look at what we have here, we have the power G to the power of h. Using that eigendecomposition, we can get to write this in this form. Whatever elements you have in the matrix of eigenvectors, they are now going to be functions of the reciprocal roots. The power that appears here, which is the number of steps ahead that you want to forecast in your time series for prediction, I’m just going to have the Alphas to the power of h. When I do this calculation, I can end up writing the forecast function just by doing that calculation as a sum from j equals 1 up to p of some constants. Those constants are going to be related to those E matrices but the important point is that what appears here is my Alpha to the power of h. What this means is I’m breaking this expected value of what I’m going to see in the future in terms of a function of the reciprocal roots of the characteristic polynomial. You can see that if the process is stable, is going to be stationary, all the moduli of my reciprocal roots are going to be below one. This is going to decay exponentially as a function of h. You’re going to have something that decays exponentially. Depending on whether those reciprocal roots are real-valued or complex-valued, you’re going to have behavior here that may be quasiperiodic for complex-valued roots or just non-quasiperiodic for the real valued roots. The other thing that matters is, if you’re working with a stable process, are going to have moduli smaller than one. The contribution of each of the roots to these forecasts function is going to be dependent on how close that modulus of that reciprocal root is to one or minus one. For roots that have relatively large values of the modulus, then they are going to have more contribution in terms of what’s going to happen in the future. This provides a way to interpret the AR process.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#examples",
    "href": "C4-L04.html#examples",
    "title": "91  The AR(p) process - M2L4",
    "section": "91.2 Examples 🎥",
    "text": "91.2 Examples 🎥\n\n\n\n\n\n\n\nFigure 91.3: AR(1)\n\n\n\n\n\n\n\n\nFigure 91.4: AR(2) two positive roots\n\n\n\n\n\n\n\n\nFigure 91.5: AR(2) complex roots\n\n\n\n\n\n91.2.1 AR(1) Process\n\nState-space form: X_t = \\phi X_{t-1} + \\omega_t\nForecast function: \\mathbb{E}[y_{t+h} \\mid \\mathcal{F}_t] = c \\cdot \\phi^h\nBehavior: Exponential decay (oscillatory if \\phi &lt; 0), mimicking the autocorrelation function.\nStability: |\\phi| &lt; 1 (reciprocal root 1/\\phi has modulus &gt; 1).\n\n\n\n91.2.2 AR(2) Process\n\nCharacteristic polynomial: 1 - \\phi_1 z - \\phi_2 z^2\nThree root types:\n\nTwo real distinct reciprocal roots: Forecast function:\n\n\\mathbb{E}[y_{t+h} \\mid \\mathcal{F}_t] = c_{t1} \\alpha_1^h + c_{t2} \\alpha_2^h\n\nExponential decay, dominated by root with larger modulus.\nTwo complex conjugate reciprocal roots: Let roots be $r e^{i}$. Forecast function:\n\n\\mathbb{E}[y_{t+h} \\mid \\mathcal{F}_t] = A_t r^h \\cos(\\omega h + \\delta_t)\n\nBehavior: Quasiperiodic with exponential envelope.\nRepeated reciprocal root ($$ with multiplicity 2): Forecast function:\n\n\\mathbb{E}[y_{t+h} \\mid \\mathcal{F}_t] = (\\alpha^h)(a_t + b_t h)\n\nPolynomial-exponential form due to root multiplicity.\n\n\n\n\n91.2.3 Key Concepts\n\nForecast structure mirrors the roots of the characteristic polynomial.\nStability depends on reciprocal roots (modulus &lt; 1).\nComplex roots → sinusoidal terms;\nRepeated roots → polynomial multipliers.\n\nThis analysis connects forecast behavior to the algebraic properties of AR model roots.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#acf-of-the-arp",
    "href": "C4-L04.html#acf-of-the-arp",
    "title": "91  The AR(p) process - M2L4",
    "section": "91.3 ACF of the AR(p) 🎥",
    "text": "91.3 ACF of the AR(p) 🎥\n\n\n\n\n\n\n\nFigure 91.6: ACF of the AR(p)\n\n\nFor a stationary AR(p) process, the autocorrelation function (ACF) satisfies a homogeneous linear difference equation whose solution is a sum of terms involving the reciprocal roots of the characteristic polynomial. Key points:\n\nIf there are r distinct reciprocal roots \\alpha_1, \\ldots, \\alpha_r with multiplicities m_1, \\ldots, m_r such that \\sum m_j = p, the ACF has the general form:\n\n\\rho(h) = \\sum_{j=1}^r P_j(h)\\alpha_j^h,\n\nwhere each P_j(h) is a polynomial of degree m_j - 1.\nFor distinct reciprocal roots (common case), all m_j = 1, so \\rho(h) is a linear combination of powers of the roots.\nAR(1): ACF is \\rho(h) = \\phi^h, where \\phi is the AR coefficient.\nAR(2): Three cases arise:\n\nTwo distinct real roots → exponential decay.\nComplex conjugate roots → damped sinusoidal behavior r^h \\cos(\\omega h + \\delta).\nOne real root with multiplicity 2 → decay with polynomial factor.\n\nACF decays exponentially if all reciprocal roots lie inside the unit circle.\nThe Partial ACF (PACF) of AR(p) is zero for all lags &gt; p.\nPACF values can be computed recursively via the Durbin–Levinson algorithm, using sample autocorrelations.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#simulating-data-from-an-arp",
    "href": "C4-L04.html#simulating-data-from-an-arp",
    "title": "91  The AR(p) process - M2L4",
    "section": "91.4 Simulating data from an AR(p) 🎥",
    "text": "91.4 Simulating data from an AR(p) 🎥\nThis video goes through the code in the following sections, which simulates data from an AR(p) process and plots the sample ACF and PACF.\n\nCharacteristic Roots from AR Coefficients:\n\nGiven AR coefficients (e.g. for AR(8)), compute characteristic roots using polyroot() on the reversed sign polynomial (first term 1, followed by negative AR coefficients).\nReciprocal roots are obtained as 1/\\text{root}.\nUse Mod() for modulus and 2π / Arg() for approximate periods of the reciprocal roots.\n\nExample AR(8):\n\nYields 4 complex-conjugate pairs.\nMost persistent: modulus ≈ 0.97, period ≈ 12.7.\nOthers show lower modulus and shorter periods, contributing less to persistence.\n\nSimulating AR(2) with Complex Roots:\n\nReciprocal root modulus 0.95, period 12 → converted to AR coefficients (≈ 1.65, -0.902).\nSimulated data shows quasi-periodic behavior.\nACF: decaying sinusoidal pattern.\nPACF: significant at lags 1 and 2, then drops, consistent with AR(2).\n\nSimulating AR(2) with Real Roots:\n\nRoots: 0.95 and 0.5.\nAR coefficients derived from these.\nNo quasi-periodic pattern in data; resembles damped random walk.\nACF: smooth decay.\nPACF: only first two lags significant.\n\nSimulating AR(3) with Complex + Real Root:\n\nComplex root pair: modulus 0.95, period 12; real root: modulus 0.8.\nThree AR coefficients derived.\nSimulated series shows quasi-periodic behavior plus extra persistence.\nACF: still shows decaying periodicity.\nPACF: more than two significant lags, consistent with AR(3).\n\n\nKey Insight:\nThe modulus and type (real vs. complex) of reciprocal roots determine persistence and periodicity. The ACF reflects these traits, while the PACF helps identify AR order.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#computing-the-roots-of-the-ar-polynomial-ℛ",
    "href": "C4-L04.html#computing-the-roots-of-the-ar-polynomial-ℛ",
    "title": "91  The AR(p) process - M2L4",
    "section": "91.5 Computing the roots of the AR polynomial 📖 ℛ",
    "text": "91.5 Computing the roots of the AR polynomial 📖 ℛ\nCompute AR reciprocal roots given the AR coefficients\n\n# Assume the folloing AR coefficients for an AR(8)\nphi=c(0.27, 0.07, -0.13, -0.15, -0.11, -0.15, -0.23, -0.14)\nroots=1/polyroot(c(1, -phi)) # compute reciprocal characteristic roots\nr=Mod(roots) # compute moduli of reciprocal roots\nlambda=2*pi/Arg(roots) # compute periods of reciprocal roots\n\n# print results modulus and frequency by decreasing order\nprint(cbind(r, abs(lambda))[order(r, decreasing=TRUE), ][c(2,4,6,8),]) \n\n             r          \n[1,] 0.9722428 12.731401\n[2,] 0.8094950  5.103178\n[3,] 0.7196221  2.987712\n[4,] 0.6606487  2.232193",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#simulating-data-from-an-arp-ℛ",
    "href": "C4-L04.html#simulating-data-from-an-arp-ℛ",
    "title": "91  The AR(p) process - M2L4",
    "section": "91.6 Simulating data from an AR(p) 📖 ℛ",
    "text": "91.6 Simulating data from an AR(p) 📖 ℛ\n \n\nR code to simulate data from an AR(2) with one pair of complex-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n## simulate data from an AR(2)\nset.seed(2021)\n## AR(2) with a pair of complex-valued roots with modulus 0.95 and period 12 \nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]&lt;- 2*r*cos(2*pi/lambda) \nphi[2] &lt;- -r^2\nphi\n\n[1]  1.645448 -0.902500\n\nT=300 # number of time points\nsd=1 # innovation standard deviation\nyt=arima.sim(n=T, model = list(ar = phi), sd=sd)\n\npar(mfrow = c(3, 1), mar = c(3, 4, 2, 1), cex.lab = 1.5)\n## plot simulated data\nts.plot(yt)\n## draw sample autocorrelation function\nacf(yt, lag.max = 50,\n    type = \"correlation\", ylab = \"sample ACF\", \n    lty = 1, ylim = c(-1, 1), main = \" \")\n\n## draw sample partial autocorrelation function\npacf(yt, lag.ma = 50, main = \"sample PACF\")\n\n\n\n\n\n\n\n\n\nR code to simulate data from an AR(2) with two different real-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n### Simulate from AR(2) with two real reciprocal roots (e.g., 0.95 and 0.5)\nset.seed(2021)\nrecip_roots=c(0.95, 0.5) ## two different real reciprocal roots\nphi=c(sum(recip_roots), -prod(recip_roots)) ## compute ar coefficients\nphi\n\n[1]  1.450 -0.475\n\nT=300 ## set up number of time points\nsd=1 ## set up standard deviation\nyt=arima.sim(n=T,model = list(ar=phi),sd=sd) # generate ar(2)\n\npar(mfrow = c(3, 1), mar = c(3, 4, 2, 1),  cex.lab = 1.5, cex.main = 1.5)\n### plot simulated data \nts.plot(yt)\n### plot sample ACF\nacf(yt, lag.max = 50, type = \"correlation\",  main = \"sample ACF\")\n### plot sample PACF\npacf(yt, lag.max = 50, main = \"sample PACF\")\n\n\n\n\n\n\n\n\n\nR code to simulate data from an AR(3) with one real reciprocal root and a pair of complex-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n### Simulate from AR(3) with one real root \n### and a pair of complex roots (e.g., r=0.95 and lambda = 12 and real root with\n### 0.8 modulus)\nset.seed(2021)\nr= c(0.95, 0.95, 0.8) ## modulus\nlambda=c(-12, 12) ## lambda\nrecip_roots=c(r[1:2]*exp(2*pi/lambda*1i), r[3]) ## reciprocal roots\nphi &lt;- numeric(3) # placeholder for phi\nphi[1]=Re(sum(recip_roots)) # ar coefficients at lag 1\nphi[2]=-Re(recip_roots[1]*recip_roots[2] + recip_roots[1]*recip_roots[3] + recip_roots[2]*recip_roots[3]) # ar coefficients at lag 2\nphi[3]=Re(prod(recip_roots))\nphi\n\n[1]  2.445448 -2.218859  0.722000\n\nT=300 # number of time points\nsd=1 # standard deviation\nyt=arima.sim(n=T,model = list(ar=phi), sd = sd) # generate ar(3)\n\npar(mfrow = c(3,1),  mar = c(3, 4, 2, 1), cex.lab = 1.5, cex.main = 1.5)\n### plot simulated data \nts.plot(yt)\n### plot sample ACF\nacf(yt, lag.max = 50, type = \"correlation\",  main = \"sample ACF\")\n### plot sample PACF\npacf(yt, lag.max = 50, main = \"sample PACF\")",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L04.html#sec-arp-review",
    "href": "C4-L04.html#sec-arp-review",
    "title": "91  The AR(p) process - M2L4",
    "section": "91.7 The AR(p): Review 📖",
    "text": "91.7 The AR(p): Review 📖\nThis section is based on material from the handout but we also covered it in greater detail at the beginning of the lecture.\n\n91.7.1 AR(p): Definition, stability, and stationarity\n\n\nAR(p)\nA time series follows a zero-mean autoregressive process of order p, of AR(p), if:\n\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\varepsilon_t \\qquad\n\\tag{91.15}\nwhere \\phi_1, \\ldots, \\phi_p are the AR coefficients and \\varepsilon_t is a white noise process\nwith \\varepsilon_t \\sim \\text{i.i.d. } N(0, v), for all t.\n\nThe AR characteristic polynomial is given by\n\n\\Phi(u) = 1 - \\phi_1 u - \\phi_2 u^2 - \\ldots - \\phi_p u^p,\n\nwith u complex-valued.\nThe AR(p) process is stable if \\phi(u) = 0 only when \\|u\\| &gt; 1. In this case, the process is also stationary and can be written as\n\ny_t = \\psi(B) \\varepsilon_t = \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j},\n\nwith \\psi_0 = 1 and \\sum_{j=0}^{\\infty} |\\psi_j| &lt; \\infty. Here B denotes the backshift operator, so B^j \\varepsilon_t = \\varepsilon_{t-j} and\n\n\\psi(B) = 1 + \\psi_1 B + \\psi_2 B^2 + \\ldots + \\psi_j B^j + \\ldots\n\nThe AR polynomial can also be written as\n\n\\Phi(u) = \\prod_{j=1}^{p} (1 - \\alpha_j u),\n\nwith \\alpha_j being the reciprocal roots of the characteristic polynomial. For the process to be stable (and consequently stationary), |\\alpha_j| &lt; 1 for all j = 1, \\ldots, p.\n\n91.7.2 AR(p): State-space representation\nAn AR(p) can also be represented using the following state-space or dynamic linear (DLM) model representation:\n\ny_t = F^{\\top} x_t,\n\\tag{91.16}\n\nx_t = G x_{t-1} + \\omega_t,\n\\tag{91.17}\nwith\n\nx_t = (y_t, y_{t-1}, \\dots, y_{t-p+1})^{\\top}\n\\tag{91.18}\nwhere F is a mapping from the state vector to the observed variable:\n\nF = (1, 0, \\dots, 0)^{\\top}\n\\tag{91.19}\n\n\\omega_t = (\\varepsilon_t, 0, \\dots, 0)^{\\top}\n\\tag{91.20}\n and state transition matrix G\n\nG = \\begin{pmatrix}\n\\phi_1 & \\phi_2 & \\phi_3 & \\dots & \\phi_{p-1} & \\phi_p \\\\\n1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & 0 & \\dots & 0 & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1 & 0\n\\end{pmatrix}.\n\\tag{91.21}\n Using this representation, the expected behavior of the process in the future can be exhibited via the forecast function:\n\nf_t(h) = E(y_{t+h} \\mid y_{1:t}) = F^\\top G^h x_t, \\quad h &gt; 0, \\quad \\forall t \\ge p\n\\tag{91.22}\nWhere G^h is the h-th power of the matrix G. The eigenvalues of the matrix G are the reciprocal roots of the characteristic polynomial.\n\n\n\n\n\n\nNoteEigenvalues\n\n\n\n\nThe eigenvalues can be real-valued or complex-valued.\nIf they are Complex-valued the eigenvalues/reciprocal roots appear in conjugate pairs.\n\n\n\nAssuming the matrix G has p distinct eigenvalues, we can decompose G into G = E \\Lambda E^{-1}, with\n\n\\Lambda = \\text{diag}(\\alpha_1, \\dots, \\alpha_p),\n\nfor a matrix of corresponding eigenvectors E. Then, G^h = E \\Lambda^h E^{-1} and we have:\n\nf_t(h) = \\sum_{j=1}^{p} c_{tj} \\alpha_j^h.\n\n\n\n91.7.3 ACF of AR(p)\nFor a general AR(p), the ACF is given in terms of the homogeneous difference equation:\n\n\\rho(h) - \\phi_1 \\rho(h-1) - \\ldots - \\phi_p \\rho(h-p) = 0, \\quad h &gt; 0.\n\nAssuming that \\alpha_1, \\dots, \\alpha_r denotes the characteristic reciprocal roots each with multiplicity m_1, \\ldots, m_r, respectively, with \\sum_{i=1}^{r} m_i = p. Then, the general solution is\n\n\\rho(h) = \\alpha_1^h p_1(h) + \\ldots + \\alpha_r^h p_r(h),\n\nwith p_j(h) being a polynomial of degree m_j - 1.\n\n\n91.7.4 Example: AR(1)\nWe already know that for h \\ge 0, \\rho(h) = \\phi^h. Using the result above, we have\n\n\\rho(h) = a \\phi^h,\n\nand so to find a, we take \\rho(0) = 1 = a \\phi^0, hence a = 1.\n\n\n91.7.5 Example: AR(2)\nSimilarly, using the result above in the case of two complex-valued reciprocal roots, we have\n\n\\rho(h) = a \\alpha_1^h + b \\alpha_2^h = c r^h \\cos(\\omega h + d).\n\n\n\n91.7.6 PACF of AR(p)\n We can use the Durbin-Levinson recursion to obtain the PACF of an AR(p). c.f. Section 115.1.\nUsing the same representation but substituting the true autocovariances and autocorrelations with their sampled versions, we can also obtain the sample PACF.\nIt is possible to show that the PACF of an AR(p) is equal to zero for h &gt; p.\n\n\n\n\n\n\nBrockwell, Peter J, and Richard A Davis. 1991. Time Series: Theory and Methods. Springer science & business media.\n\n\nPrado, Raquel, Gabriel Huerta, and Mike West. 2000. “Bayesian Time-Varying Autoregressions: Theory, Methods and Applications.” Resenhas Do Instituto de Matemática e Estatı́stica Da Universidade de São Paulo 4 (4): 405–22. https://www2.stat.duke.edu/~mw/MWextrapubs/Prado2001.pdf.",
    "crumbs": [
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>The AR(p) process - M2L4</span>"
    ]
  },
  {
    "objectID": "C4-L05.html",
    "href": "C4-L05.html",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "",
    "text": "92.1 Bayesian inference in the AR(p): Reference prior, conditional likelihood 🎥",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#bayesian-inference-in-the-arp-reference-prior-conditional-likelihood",
    "href": "C4-L05.html#bayesian-inference-in-the-arp-reference-prior-conditional-likelihood",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "",
    "text": "Figure 92.1: inference for AR(p)\n\n\n\n92.1.1 Model Setup\nwe start with an AR(p) model as we define in the previous lesson: \ny_t = \\phi_1 y_{t-1} + \\ldots + \\phi_p y_{t-p} + \\varepsilon_t \\qquad \\varepsilon_t \\stackrel{iid}{\\sim} \\mathcal{N}(0, \\nu)\n\\tag{92.1}\nbut now we wish to infer:\n\n\\phi_i the AR(p) coefficients and\n\\nu the innovation variance.\n\n\n\n92.1.2 Conditional likelihood\nWe make use of the autoregressive structure and rewrite y_t conditionally on the previous p values of the process and the parameters:\n\n(y_t \\mid y_{t-1}, \\ldots, y_{t-p}, \\phi_1, \\ldots, \\phi_p) \\sim \\mathcal{N}\\left (\\sum_{j=1}^{p} \\phi_j y_{t-j}, v\\right )\n\\tag{92.2}\nWe condition on the first p values of the process. The conditional distribution of y_t given the previous p values and parameters is normal with mean given by the weighted sum \\sum \\phi_j y_{t-j} and variance v.\nthe density for the first p observations is given by: \n\\mathbb{P}r(y_{(p+1):T}\\mid y_{1:p}, \\phi_1, \\ldots, \\phi_p, v) = \\prod_{t=p+1}^{T} \\mathbb{P}r(y_t \\mid y_{t-1}, \\ldots, y_{t-p}, \\phi_1, \\ldots, \\phi_p, v)\n\\tag{92.3}\nThis product of conditionals yields the full conditional likelihood. Each term is Gaussian and independent, given the past values and parameters.\n\n\n92.1.3 Regression Formulation\nThis is recast as a linear regression: response vector \\mathbf{y} starting from y_{p+1} to y_T, design matrix \\mathbb{X} built from lagged values, and \\boldsymbol\\beta as the AR coefficients \\phi_j.\n\n\\mathbf{y}= \\mathbb{X}\\boldsymbol \\beta + \\boldsymbol \\varepsilon \\qquad \\varepsilon \\sim \\mathcal{N}(0, vI)\n\\tag{92.4}\n\n\\mathbf{y} = \\begin{pmatrix} y_{p+1} \\\\ y_{p+2} \\\\ \\vdots \\\\ y_T \\end{pmatrix}, \\quad\n\\boldsymbol \\beta = \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\vdots \\\\ \\phi_p \\end{pmatrix}, \\quad\n\\mathbb{X} = \\begin{pmatrix} y_{p} & y_{p-1} & \\cdots & y_{1} \\\\ y_{p+1} & y_{p} & \\cdots & y_{2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_{T-1} & y_{T-2} & \\cdots & y_{T-p} \\end{pmatrix}\n\nIn the design matrix \\mathbb{X} each row corresponds to lagged observations used to predict the next value. This setup enables applying linear regression machinery.\nAssuming full-rank \\mathbb{X}, we may now infer the parameters by using the left generalized Moore-Penrose inverse of \\mathbb{X} as the maximum likelihood estimator (MLE) of the AR coefficients \\boldsymbol \\beta.\n\n\\boldsymbol \\beta_{MLE} = (\\mathbb{X}^\\top \\mathbb{X})^{-1} \\mathbb{X}^\\top \\mathbf{y}\n\\tag{92.5}\nwhere:\n\n\\mathbb{X} is the design matrix,\n\\boldsymbol \\beta is the vector of AR coefficients, and\n\\mathbf{y} is the response vector.\n\nIt matches the usual OLS solution in linear regression.\n\n\n92.1.4 Reference prior and posterior distribution\nThe reference prior reflects non-informative beliefs. This yields a conjugate posterior due to the Gaussian likelihood.\n\np(\\beta,v) \\propto 1/v\n\\tag{92.6}\n\n(\\boldsymbol \\beta | \\mathbf{y}_{:T}, v) \\sim \\mathcal{N}(\\boldsymbol \\beta_{MLE}, v (\\mathbb{X}^\\top \\mathbb{X})^{-1})\n\\tag{92.7}\nThe posterior is Gaussian with mean \\boldsymbol \\beta_{MLE} and scaled covariance v(\\mathbb{X}^\\top \\mathbb{X})^{-1}, analogous to the OLS variance.\n\n(v | \\mathbf{y}_{:T}) \\sim \\text{Inverse-Gamma}\\left(\\frac{T - 2p}{2}, \\frac{S^2}{2}\\right)\n\\tag{92.8}\nwhere:\n\nS^2 = \\sum_{t=p+1}^{T} (y_t - \\mathbb{X} \\boldsymbol \\beta_{MLE})^2 is the unbiased estimator of the variance v.\nT is the total number of observations.\n\nPosterior for v is inverse gamma. The shape parameter is (T - 2p)/2 because the number of residual degrees of freedom is T - 2p (residual = T-p obs minus p params). Scale is half the sum of squared residuals.\n\n\n92.1.5 Simulation-based Inference\nPosterior sampling:\n\nDraw v from the inverse gamma posterior.\nPlug v into the posterior of \\boldsymbol\\beta and sample from the Gaussian.\n\nThis gives full Bayesian posterior samples of (\\boldsymbol\\beta, v).\n\n\n92.1.6 Summary\n\nIn this lesson, we discussed the Bayesian inference for the AR(p) process using the conditional likelihood and reference prior.\nWe have established a connection between the AR(p) process and a regression model, allowing us to use standard regression techniques to estimate the parameters.\nThe posterior distribution can be derived, and we can perform simulation-based inference to obtain samples from the posterior distribution of the AR coefficients and variance.\n\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\nWe can now think about Bayesian in the general case of an AR(P) process. In the Bayesian in case, we have two choices to make first what kind of likelihood we are going to be using and then what kind of prior distribution we’re going to be using for the model parameters. I’m going to discuss the case in which we use the conditional likelihood. We have to condition this time on the first p values of the process. And then I’m also going to discuss the case in which we use the reference prior for the model parameters. As in the AR(1) case, there is a correspondence between that conditional likelihood setting and the regression model. So recall we can write down the process, Like this. So, And the assumption is that the epsilon t are iid normally distributed random variables with zero mean and variance v. So that’s the variance of the process. The parameters of the process are going to be all the phi’s, And the variance of the process. So we want to make inference on those parameters. What we know given the conditional dependency structure and past values of the series, is that if I think about the distribution of yt, given yt -1 all the way to y t-p. And then also given the phi’s, And v, this is a normally distributed, Random variable with the mean is going to be the sum, Of these. Past values weighted by the phi’s, And then I have variance v here. So now that I use this conditionally independent structure, I can write a likelihood function as the product of these terms. So if I think about my density here for y p+1 all the way to capital T. So let’s assume that we have capital T, we have observed capital T. Data points and then we are conditioning on y1. And p, all the first p observations and all the phi’s again and v. I can write this as the product of all these terms. All these are are going to have a normal density given by this expression. So as we said before, there is a correspondence between this likelihood function, the conditional likelihood and a regression model of the form y=Xbeta +epsilon. Where y here is a vector, X matrix, beta is a vector and epsilon is another vector with this vector being multivariate normal. And then we’re going to have a v times the identity. So I can write down this expression and make a a connection with this model by simply setting y. Again, because I’m conditioning on the first p values. I’m going to start with yp+1, yp+2 and so on all the way to yT. Then here my beta is the vector that goes with the coefficients. In this case, the linear component has to do with the phi’s. So it’s going to be my beta with all the phi’s. And those are the AR parameters that we want to estimate. And then I’m going to have an X matrix. The design matrix here for the linear regression in the case of an AR(P), again, if I think about the first row is going to be related to the yp+1. So we are regressing on the past p values. So it’s going to go from yp, All the way, so that’s the first row, to y1. Then for y p+2, I’m going to have something similar and then I go all the way down to yT. And so I’m going to have yT -1, -2 all the way to yT-p. So this is my X matrix. So as you know, I can find my maximum likelihood estimator for this process. Which I’m just simply going to call beta hat, its going to be assuming that the design matrix is full rank. We can write it down like this. So we can simply just plug in this X and this y vector to obtain the maximum likelihood estimator of the model parameters for the AR coefficients. And then using these results, we can also write the posterior distribution using a reference prior. So again, this gives me the likelihood. The reference prior assumes that we are going to use a prior of this form. And in this case, the Bayesian inference can be done by writing down the density of the beta given all the y’s and v. This is going to be normally distributed. The mean it’s going to be beta hat, which is the maximum likelihood estimator. And then I have my v times X transpose X inverse. And then I have my marginal distribution for v given all the observations here, it’s going to be an inverse gamma. And then the parameters for the inverse gamma distribution are going to be related to, again, you think about what is the dimension of the vector here y. In the case of the AR(p) I have something that is dimension T- p. So that’s the first dimension that we are going to consider. But then we also have to subtract the dimension of the beta vector which is p. So I’m going to have T -p-p, which gives me my 2p here. And then I have the other component is my s square over 2. So once again, if I want to do simulation based posterior inference, I can simply get a sample of v from this inverse gamma distribution. And then I plug in that sample here and I get a sample for the AR coefficients. So I can get full posterior inference in this way.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#r-code-maximum-likelihood-estimation-arp-conditional-likelihood",
    "href": "C4-L05.html#r-code-maximum-likelihood-estimation-arp-conditional-likelihood",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.2 R code: Maximum likelihood estimation, AR(p), conditional likelihood 📖",
    "text": "92.2 R code: Maximum likelihood estimation, AR(p), conditional likelihood 📖\n\n\n  set.seed(2021)\n# Simulate 300 observations from an AR(2) with one pair of complex-valued reciprocal roots \nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Compute the MLE for phi and the unbiased estimator for v using the conditional likelihood\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"Estimate for v: \", s2, \"\\n\")\n\n\n MLE of conditional likelihood for phi:  1.65272 -0.9189823 \n Estimate for v:  0.9901292",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#sec-arp-order-selection",
    "href": "C4-L05.html#sec-arp-order-selection",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.3 Model order selection 🎥",
    "text": "92.3 Model order selection 🎥\n\n\n\n\n\n\n\nFigure 92.2: model order selection\n\n\n\n92.3.1 Goal\nDetermine the best model order p for an AR(p) process by comparing candidate models.\n\n\n92.3.2 Step 1: Define a candidate set of model orders\n\np^* \\qquad p=1:p^*\n\nYou choose a maximum order p^* (e.g., 20), then evaluate AR models of each order p \\in {1, \\dots, p^*}.\n\n\n92.3.3 Step 2: Estimate residual variance for each model\n\nS^2_p \\qquad y_{(p^*+1):T}\n\nFor each model of order p, compute S_p^2 as the residual sum of squares (RSS) using a fixed evaluation window: t = p^*+1 to T, to ensure all models are evaluated on the same data subset.\n\n\n92.3.4 Step 3: Compute model selection criteria\n \nAIC balances fit (log-likelihood) with complexity (number of parameters). The first term measures model fit using \\log(S_p^2), and the second term penalizes complexity with 2p.\n\nAIC_p = (T-p^*) \\log(S_p^2) + 2p\n\nBIC uses the same fit term but a heavier penalty: p \\log(T - p^*), which increases with sample size. This often leads BIC to favor smaller models than AIC.\n\nBIC_p = (T-p^*) \\log(S_p^2) + p \\log(T-p^*)\n\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\nOne of the big questions when you’re working with autoregressive processes is what model order you should use for your particular data. One thing that you can do is you can look at the sample ACF and PACF of the data that you have and try to begin with a model order that reflects, the behavior that you see in those sample ACF’s and PACF’s. But you can also have a more formal approach in which you consider using model order selection criteria. Model selection criteria to pick the model order. You could even also be more sophisticated and assume that the model order is also uncertain and consider that as a random variable. Put a prior on that random variable, and then perform Bayesian inference. I’m going to just discuss here how we can use two model selection criteria. One is the AIC, Akaike’s Information Criterion, and the other one is BIC, to choose the model order. These are usually implemented in software packages in R and you can use them in practice for your given data. The first thing is if you’re working with the conditional likelihood, you can also work with the full likelihood. But if you’re working with the conditional likelihood, you have to evaluate these model selection criteria using the same data. Let’s say that you have a set candidate of models. You have to pick a maximum model order, I’m going to call that p-star. For example, you could have p-star to be 20. Then you’re going to consider all the possible orders from 0-20. So your model orders that you will be considering go from one or from zero all the way to p-star. Here for each of those model orders that you consider, if you’re working with a conditional likelihood, you’re going to have estimates of the model parameters. You’re going to have your phi hats using maximum likelihood estimation. For each of those model orders, you’re going to have Sp, I’m going to call it Sp square. It’s your S square that you get from each of those model orders. Now p here, I’m putting that subscript there just to indicate that this is related to the model with model order p. Then you can evaluate everything using your data and your data for evaluation here is going to go from p star plus 1 all the way to T. It’s important that you just use this data when you’re computing your regressions so that you evaluate everything on the same set of data points. You can write down the Akaike’s Information Criterion in this case, is a function that is going to have two components. One has to do with how good is the fit of the model. The other one is penalizing the number of parameters you have in the model. Usually in these model selection criteria, we have those two components. You can write this down as the number of observations you have, which is, again, we’re conditioning on the first p star observations. We’re only going to evaluate this on the Data starting from p star plus 1 up to T. Then you’re going to have the log of that Sp squared. That’s the part of the AIC that has to do with the goodness of fit. Then there is a penalty for the number of parameters. In this case, AIC uses a penalty of two times p. What you do is you fit all these model orders. It can go from one to p star, from zero to p star if you want to incorporate the white noise component as well. Then you just get an AIC for each of these, and then you compare all of those. There is going to be essentially a value for each of these p. Then you look at the optimal value is the one that minimizes that AIC expression. You look at those values, you pick the model order that minimizes the AIC. You can also use BIC. In the BIC, you’re going to have the same component here related to the goodness of fit. You’re going to have the same piece. But now BIC is going to penalize the number of parameters in the model in a different way. You’re going to have something that looks like p log of the T minus p star. Here we had a penalty. We had two times the number of parameters. Here we have the number of parameters times something that depends on the number of observations you have. It’s a different penalty. You may get different results again here you evaluate your BIC for each of the model orders you are considering, and then you pick the model order that minimizes this expression. As you can see, what happens is there is a balance usually between the more parameters you consider, the better is going to be your model fit. But you have that penalty that you are overfitting, you maybe overfitting, you have too many parameters in your model. These model selection criteria try to balance those components and give a penalty for the number of parameters. When you run things in practice, you may get the same model order, the same optimal model order for AIC or BIC, or you may get different numbers. You can also look at other quantities. You can look at posterior predictive distributions and see how well the model does in terms of those posterior predictive densities. You can, as I said before, think about considering p as another variable in your model. We will stay simple here, and we will consider just these model selection criteria.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#example-bayesian-inference-in-the-arp-conditional-likelihood",
    "href": "C4-L05.html#example-bayesian-inference-in-the-arp-conditional-likelihood",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.4 Example: Bayesian inference in the AR(p), conditional likelihood 🎥",
    "text": "92.4 Example: Bayesian inference in the AR(p), conditional likelihood 🎥\nThis video walks through the code in the next section which demonstrates Bayesian inference for an AR(2) process with complex-valued roots.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\nWe now talk about how to obtain maximum likelihood estimation and Bayesian inference using the conditional likelihood function in this case of the AR2 process. In this particular example is the a AR2. Here I’m first again, just sampling from a specific AR2 process so that we have the data here. I’m using an AR2 with one pair of complex reciprocal roots. The modulus of the reciprocal root is 0.95 and the period is 12, so we’re going to observe this quasi-periodicity in the sampled time series. Then these are 300 observations, the standard deviation of the innovation is one, and we are using the arima.sim function as before. As you recall, in the case of the conditional likelihood we have a correspondence between the conditional likelihood and the equations of the linear regression model. Here I’m just applying the equations we discussed before and we can write everything in terms of a regression model in vector form. So we have a response y and X design matrix and then we obtain, just reading in, so I’m looking at as a conditional likelihood so I’m going to condition on the first P observations. P in this particular case is two and I have my X matrix and my maximum likelihood estimator for Phi is just obtained using the equation X transpose X_inverse times X transpose y. This is my MLE for the AR coefficients, and then I have the unbiased estimator for v which we called S2 here using again, the equations that we discussed before. If you look at the estimates, again the AR coefficients, the true values of the AR coefficients, we can compare with the estimated AR coefficients so the true values are here in the Phi and the true value is for the AR coefficient in the first lag. The Phi 1 is about 1.65 which is what we obtain also in the maximum likelihood estimate using the conditional likelihood, and then the second coefficient is around negative 0.9 which is again close to what we get in terms of the estimate of the maximum likelihood estimator. s square is an estimate for v and here the true variance is one. Again this is based estimator using all these is based on these data that we simulated. Now we can just run in the case of the AR2 we can obtain posterior inference using these conditional likelihood and the reference prior, and in that case we can do everything using direct sampling. I’m going to use the library MASS here just to use the functions to sample random variables from that library. If you recall the form of the posterior distribution is such that the marginal for the variance given the data is just an inverse Gamma distribution so we can sample from this inverse Gamma using the function rgamma and then taking the inverse of that with the corresponding parameters that we have discussed again, the equations before. In the step 2, conditional on v, we can sample Phi from a normal distribution in this case is a multivariate normal bivariate in this case since we have an AR2, and here I’m just simply obtaining samples from that distribution. I set the number of samples to be 1,000. You can choose as many or as few posterior samples as you want. Then based on these we can look at posterior estimates in terms of graphical representations of those like we can look at these histograms of the samples for Phi and the variance. Here we can see if we zoom in here I have three histograms, the first one is the histogram that corresponds to the samples of the first AR coefficient Phi_1, the red line corresponds to the true value and this is just the posterior distribution for Phi_1. Similarly this histogram gives me representation of the posterior distribution for Phi_2 based on those 1,000 samples from the posterior distribution. Then I have also the samples for the v and then the true value which is one. We can see that we’re getting those estimates and we can then look also at graphical representations for functions of those parameters. For example, we could also summarize the posterior distribution for the modulus and the period of the reciprocal roots of the characteristic polynomial. So in this case, the good thing with simulation-based posterior sampling is that if we have the samples of the AR coefficients and the variance, we can simply just do any transformations for any functions of those and obtain the posterior summaries of those transform values. In this case, the modulus, we can obtain it using a transformation of the Phi_2 coefficients so if we have samples of those we can just look at those, and then for the period we can also do the same so this is a function now of the modulus and this first coefficient so we just look at that and then can obtain just the summaries of those histograms for the posterior distribution of the modulus. We can see here the true value was 0.95 and this is a histogram with the posterior samples for that modulus based on those 1,000 samples from the posterior, and this is what I get in terms of the histogram for the period. The true value for the period was 12 in this particular example. This gives you also uncertainty quantification for those parameters of the distribution. Another thing we discussed is we can fix the model order and then obtain posterior inference but in practice we usually also have uncertainty on the model order and we said, well, we can use different procedures to try to look at that problem. One of the methods that we can use is just pick the model order using some model selection criteria. This code just looks at how you can use AIC and BIC for finding the optimal model order and then conditional on that optimal model order then you can proceed with your posterior inference. In this case, again, this is for the simulated dataset, I’m going to assume that I don’t know the model order and we’re going to set the maximum model order to be 10. I’m going to be searching for the optimal model order in AR models that have a maximum model order of 10 and then we will see what happens here. For each of those model orders I can just keep all the matrices, the X matrix and the Y matrix just to proceed with the maximum likelihood inference, and the maximum likelihood inference is just using I just created this function here to compute the MLE and it returns essentially all the quantities that we need to have here, the degrees of freedom, the residuals, the MLE, and so on for a given model order with a given structure in terms of the X which depends on the data that you have and the Y that also depends on the data that you have. There is this AIC BIC function that simply uses calls this MLE function and then computes the AIC and the BIC with different penalties. As we said, if you look a little bit inside this function you will see that the AIC has a penalty in terms of the number of parameters in the model which is 2 times P and in this case we have a log N times P, so this is just how to compute this AIC. So it’s just again computing those. Based on this information that we have we can plot the AIC and the BIC for different values of the model orders and highlight in this case we’re looking at the minimizes, so is the difference between the value and the minimum value of the AIC. In this case, we want to look at the value that minimizes this quantity and we see that the model order for both the AIC and the BIC, the optimal model order happens to be two. Once we have that optimal model order we can then say, well, I can choose the model order that minimizes one of the two criteria and then in this case I’m choosing BIC. For this particular example, both model selection criteria are giving me the same answer in terms of the model order, this doesn’t have to be the case for all the datasets so you will have different answers in terms of what AIC is telling you is the optimal model order and what BIC is telling you that is the optimal model order. In this example, they gives you the same answer. Then you can proceed with the posterior mean, the maximum likelihood estimation, the reference and the posterior mean, and the posterior mean of the standard deviation, and then you can look at estimates also based on those posterior means of the AR coefficients, you can obtain the estimate for the modulus and the period of the reciprocal roots, so I’m just here using that optimal model order looking at estimates of those based on the posterior distribution. Here again, you can just pick one of the two model selection criteria to obtain your optimal model order and then do your inference conditional on that.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#sec-arp-bayesian-inference",
    "href": "C4-L05.html#sec-arp-bayesian-inference",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.5 code: Bayesian inference, AR(p), conditional likelihood 📖 ℛ",
    "text": "92.5 code: Bayesian inference, AR(p), conditional likelihood 📖 ℛ\n\n92.5.1 Simulate 300 observations from an AR(2) with one pair of complex-valued roots\n\nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\npar(mfrow=c(1,1), mar = c(3, 4, 2, 1) )\nplot(yt)\n\n\n\n\n\n\n\nFigure 92.3: Simulated AR(2) process with complex-valued roots\n\n\n\n\n\n\n\n92.5.2 Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood\n\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\nphi_MLE\n\n           [,1]\n[1,]  1.6527203\n[2,] -0.9189823\n\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\ns2\n\n[1] 0.9901292\n\n\n\n\n92.5.3 Posterior inference, conditional likelihood + reference prior via direct sampling\n\nn_sample=1000 # posterior sample size\nlibrary(MASS)\n\n## step 1: sample v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)\n\n## step 2: sample phi conditional on v from normal distribution\nphi_sample=matrix(0, nrow = n_sample, ncol = p)\nfor(i in 1:n_sample){\n  phi_sample[i, ]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)\n}\n\npar(mfrow = c(2, 3), mar = c(3, 4, 2, 1),  cex.lab = 1.3)\n## plot histogram of posterior samples of phi and v\n\nfor(i in 1:2){\n  hist(phi_sample[, i], xlab = bquote(phi), \n       main = bquote(\"Histogram of \"~phi[.(i)]),col='lightblue')\n  abline(v = phi[i], col = 'red')\n}\n\nhist(v_sample, xlab = bquote(nu), main = bquote(\"Histogram of \"~v),col='lightblue')\nabline(v = sd, col = 'red')\n\n\n\n\n\n\n\nFigure 92.4: Posterior distributions of AR(2) parameters\n\n\n\n\n\n\n\n92.5.4 Graph posterior for modulus and period\nr_sample=sqrt(-phi_sample[,2])\nlambda_sample=2*pi/acos(phi_sample[,1]/(2*r_sample))\n\nhist(r_sample,xlab=\"modulus\",main=\"\",col='lightblue')\nabline(v=0.95,col='red')\n\nhist(lambda_sample,xlab=\"period\",main=\"\",col='lightblue')\nabline(v=12,col='red')\n\n\n\n\n\n\n\n\n\n\n\n(a) modulus\n\n\n\n\n\n\n\n\n\n\n\n(b) period\n\n\n\n\n\n\n\nFigure 92.5: Posterior distributions of AR(2) roots",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#sec-ar-2-model-order-selection",
    "href": "C4-L05.html#sec-ar-2-model-order-selection",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.6 code: Model order selection 📖 ℛ",
    "text": "92.6 code: Model order selection 📖 ℛ\n\n92.6.1 Simulate data from an AR(2)\n\nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \npar(mfrow=c(1,1), mar = c(3, 4, 2, 1) )\nplot(yt)\n\n\n\n\n\n\n\nFigure 92.6: Simulated AR(2) process with complex-valued roots\n\n\n\n\n\n\n\n92.6.2 compute MLE for different AR(p)s\n\npmax=10 # the maximum of model order\nXall=t(matrix(yt[rev(rep((1:pmax),T-pmax)+rep((0:(T-pmax-1)),\n              rep(pmax,T-pmax)))], pmax, T-pmax));\ny=rev(yt[(pmax+1):T])\nn_cond=length(y) # (number of total time points - the maximum of model order)\n\n## compute MLE\nmy_MLE &lt;- function(y, Xall, p){\n  n=length(y)\n  x=Xall[,1:p]\n  a=solve(t(x) %*%x)\n  a=(a + t(a))/2 # for numerical stability \n  b=a%*%t(x)%*%y # mle for ar coefficients\n  r=y - x%*%b # residuals \n  nu=n - p # degrees freedom\n  R=sum(r*r) # SSE\n  s=R/nu #MSE\n  return(list(b = b, s = s, R = R, nu = nu))\n}\n\n\n\n92.6.3 Compute AIC and BIC for different AR(p)s based on simulated data\n\n## function for AIC and BIC computation \nAIC_BIC &lt;- function(y, Xall, p){\n  ## number of time points\n  n &lt;- length(y)\n  \n  ## compute MLE\n  tmp=my_MLE(y, Xall, p)\n  \n  ## retrieve results\n  R=tmp$R\n  \n  ## compute likelihood\n  likl= n*log(R)\n  \n  ## compute AIC and BIC\n  aic =likl + 2*(p)\n  bic =likl + log(n)*(p)\n  return(list(aic = aic, bic = bic))\n}\n# Compute AIC, BIC \naic =numeric(pmax)\nbic =numeric(pmax)\n\nfor(p in 1:pmax){\n  tmp =AIC_BIC(y,Xall, p)\n  aic[p] =tmp$aic\n  bic[p] =tmp$bic\n  print(c(p, aic[p], bic[p])) # print AIC and BIC by model order\n}\n\n[1]    1.000 2166.793 2170.463\n[1]    2.000 1635.816 1643.156\n[1]    3.000 1637.527 1648.536\n[1]    4.000 1639.059 1653.738\n[1]    5.000 1640.743 1659.093\n[1]    6.000 1641.472 1663.491\n[1]    7.000 1643.457 1669.147\n[1]    8.000 1645.370 1674.729\n[1]    9.000 1646.261 1679.290\n[1]   10.000 1647.915 1684.614\n\n## compute difference between the value and its minimum\naic =aic-min(aic) \naic\n\n [1] 530.977092   0.000000   1.710562   3.242352   4.927067   5.655454\n [7]   7.641270   9.553690  10.444845  12.099207\n\nbic =bic-min(bic) \nbic\n\n [1] 527.307211   0.000000   5.380443  10.582114  15.936709  20.334978\n [7]  25.990674  31.572975  36.134011  41.458254\n\n\n\n\n92.6.4 Plot AIC, BIC, and the marginal likelihood\n\npar(mfrow = c(1, 1), mar = c(3, 4, 2, 1) )\nmatplot(1:pmax,matrix(c(aic,bic),pmax,2),ylab='value',\n        xlab='AR order p',pch=\"ab\", col = 'black', main = \"AIC and BIC for Model Selection\")\n# highlight the model order selected by AIC\ntext(which.min(aic), aic[which.min(aic)], \"a\", col = 'red') \n# highlight the model order selected by BIC\ntext(which.min(bic), bic[which.min(bic)], \"b\", col = 'red') \n\nlegend(\"topright\", legend = c(\"AIC\", \"BIC\"), col = c(\"black\", \"black\"), pch = c(\"a\", \"b\"))\n\np &lt;- which.min(bic) # We set up the moder order\nprint(paste0(\"The chosen model order by BIC: \", p))\n\n[1] \"The chosen model order by BIC: 2\"\n\n\n\n\n\n\n\n\nFigure 92.7: AIC, BIC for different AR(p) model orders",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#spectral-representation-of-the-arp",
    "href": "C4-L05.html#spectral-representation-of-the-arp",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.7 Spectral representation of the AR(p) 🎥",
    "text": "92.7 Spectral representation of the AR(p) 🎥\n\n\n\n\n\n\n\nFigure 92.8: Spectral representation of the AR(p)\n\n\nThe spectral representation of the AR(p) process is a powerful tool for understanding the frequency domain properties of the process. It allows us to analyze how the process behaves at different frequencies and provides insights into its periodicity and persistence.\nSupose we have an AR(p) process with parameters:\n\n\\phi_1, \\ldots, \\phi_2, v\n\nProperties related to the autocorrelation function of the process, and the forecast function as well as we have seen, are related to the reciprocal roots of the characteristic polynomial.\nUsing this information, we can also obtain a spectral representation of the process, meaning a representation in the frequency domain. We can think of this as a density in the frequency domain where you’re going to have a power associated to different frequencies. In the case of an AR(p) process, I can write down the spectral density of the process, as:\n\nf(\\omega) = \\frac{v}{|1 - \\phi_1 e^{-i\\omega} - \\ldots - \\phi_p e^{-i \\omega p})|^2 2\\pi}\n\nwith:\n\n\\omega \\in [0,\\pi] the frequency\n\nFor instance, if you have an AR(2), an autoregressive process of order 2, and you happen to have a pair of complex roots. Let’s assume that the reciprocal roots are \\alpha_1, \\alpha_2 with modulus r and period \\lambda, then the spectral density of the AR(2) process is given by:\n\n\\alpha_1 \\qquad r=0.7\n\n\n\\alpha_2 \\qquad \\lambda=12\n\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\nIt is good to talk a little bit about what is the relationship between, the AR(p) in the time domain and what spectral representation in the frequency domain we can get. This is important for interpretability and it provides a way to just see what’s happening or what kind of properties we have in the process as well. In the case of an AR(p), you’re going to have your coefficients and your variance v. If you have an AR(p), those are the AR coefficients that we have talked about. Properties related to the autocorrelation function of the process, and the forecast function as well. We’ve seen that they are related to the reciprocal roots of the characteristic polynomial.\nUsing this information, we can also obtain a spectral representation of the process, meaning a representation in the frequency domain.\nThink of this as a density in the frequency domain where you’re going to have a power associated to different frequencies. In the case of an AR(p), I can write down the spectral density of the process, calling that f(\\omega) here is a frequency between 0 and \\pi. You can think of this frequency. Then I can write this down as v, which is the variance of the process. Then I’m going to have here, let me write it in terms of the 1 - \\phi_1 e^{-i\\omega} - \\ldots - \\phi_p e^{-i \\omega p}. I have my component here, and then I have a times 2\\pi over here. If I think about a frequency between zero and \\pi, for example, then I can plug in a frequency here. You can see that this is essentially the AR characteristic polynomial evaluated on e^{-i\\omega}, which is that representation. Once again, this frequency domain representation is going to be related to the characteristic roots of the AR polynomial.\nFor instance, if you have an AR(2), an autoregressive process of order 2, and you happen to have a pair of complex roots. Let’s assume that the reciprocal roots are \\alpha_1 and \\alpha_2, and then that I have a modulus which is 0.7.\nThen a frequency that I’m going to write it in terms of the period. The periodicity here, let’s say that is the quasi periodicity in the AR is given by 12.\nThere is again, I can go back and forth between the complex valued representation and the polar coordinates and so on.\nLet’s say that I have that pair of roots with modulus 0.7 and period 12. What I’m going to do is I plug in the representation in here.\nFor each of these \\alpha_1 and \\alpha_2, that correspond to values of \\phi_1 and \\phi_2. I can plug those in here, use the variance, and then plot that as a function of the \\omega.\nWhat happens there is you are going to have something that looks like this.\nThe peak of that density is going to be at the frequency that corresponds to this period of 12. The height that you have, the power in that spectral density is going to be related to the modulus of that pair of complex roots. If you increase the modulus, if you now have a different process with the same quasi-periodicity, but a modulus that is higher. What’s going to happen is that’s going to result in a higher peak, in a higher power for your spectral density. If you have another process in which you get a larger period or a smaller period that’s going to result in shifts in the mode of that density.\nThis is a way to represent the process in the spectral domain.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#spectral-representation-of-the-arp-example",
    "href": "C4-L05.html#spectral-representation-of-the-arp-example",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.8 Spectral representation of the AR(p): Example 🎥",
    "text": "92.8 Spectral representation of the AR(p): Example 🎥\nThis video walks through the code in the next section.\n\n92.8.1 Summary: Posterior Spectral Density Estimation for AR(2)\n\nGoal: Estimate and visualize the spectral density of an AR(2) process and its posterior distribution based on observed data.\nSimulation: 300 observations are drawn from an AR(2) process with complex conjugate roots of modulus 0.95 and period 12, leading to a quasi-periodic process.\nEstimation:\n\nAR coefficients estimated via MLE.\nInnovation variance v estimated as s^2 (unbiased).\nPosterior samples:\n\nv \\sim \\text{Inverse-Gamma}\n\\boldsymbol\\Phi \\mid v \\sim \\text{Bivariate Normal}\n\n\nSpectral density estimation:\n\nspec.ar(): Uses MLEs to estimate the spectral density.\narma.spec() from astsa: Allows using arbitrary (MLE or posterior) estimates of AR coefficients and variance.\n\nAccepts posterior samples, producing a distribution of spectral density curves.\n\n\nResults:\n\nPlots show a spectral peak at period 12, as expected from the simulation.\nPosterior spectral densities are overlaid.\nSolid line = MLE-based spectrum; dotted line = true spectrum peak.\n\n\n This process illustrates Bayesian uncertainty in frequency-domain features of AR processes.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\nWe will now see how to obtain the spectral density of an autoregressive process. Also how to look at posterior distributions of the spectral densities of this AR process just based on data that we have.\nThe first thing I’m going to do here is again, I’m going to simulate some data from an autoregressive process of order 2. This process has one pair of complex valued reciprocal roots with modulus 0.95 and period 12. In this case, I’m just going to do what we’ve done before, which is just simulate 300 observations from this AR(2) process.\nBased on this model order of two, we can obtain the maximum likelihood estimators for the AR coefficients and also the unbiased estimator for v, which we call s^2. Based on these, we can obtain 200 samples from the posterior distribution using direct simulation. We’ve done this before.\nAgain, I’m going to be sampling v from the inverse Gamma distribution, and then conditional on v, I can obtain samples for the \\Phi using the bivariate normal.\nBased on these estimates or these samples, I can just look at what is the spectral density of this process.\nI’m going to use the spec.ar function. You just pass the data, you specify the model order, and then it just uses estimates.\nThe maximum likelihood estimator of this AR process for the coefficients and the variance and then it does the transformation to obtain this estimate of the spectrum density. You can use the function spec.ar, and then R automatically produces this estimate of the spectral density. As you can see for this process, the process is going to be quasiperiodic. There is going to be a peak at the periodicity. In this case the periodicity is 12 because that’s how we simulated the process.\nYou can also use another function that is called arma.spec from the astsa package, to draw the spectral density. I’m going to also illustrate this here. If you want to use this, you call this library. The good thing with this function is that you can pass your estimates, whatever they are, for the AR coefficients and for the variance. In particular here, passing the maximum likelihood estimator based on the conditional likelihood for the AR coefficients. Then for the variance of the error terms, I’m just passing the s^2, the s squared. But you can pass any other estimator. It could be a posterior sample. It could be anything else.\nThen we obtain this again. This is a figure that is very similar to the one we had before, just plotting the spectral density. Again, we see that the power spectrum has this peak at the quasi periodic component with periodicity 12.\nWe can also pass, as I said, this function, the nice thing is that it allows you to pass any estimates or any values for the Phis and the variance. In this case, I’m just passing samples from the posterior distribution. For each of the samples of the posterior distribution that I have, I can just compute this arma.spec, this estimate of the spectral density. This takes a little bit of time to compute because we have a few samples here and it has to do the calculations for each of those samples. Then based on the results, you can then plot. For each of those samples, you obtain a spectral density and for each of those spectral densities, you can plot essentially the corresponding spectral density.\nFor each of those samples that you have for the AR coefficients and the variance, you’re going to get one of this samples for the spectral density.\nI’m just plotting here these as a distribution. Then the solid line here corresponds to the maximum likelihood estimator. The dotted line corresponds to the true value in the sense of where the peak of the log spectral density should be and it corresponds to a period of 12, which is consistent with what we obtain in terms of the posterior distribution.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#code-spectral-density-of-arp-ℛ",
    "href": "C4-L05.html#code-spectral-density-of-arp-ℛ",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.9 code: Spectral density of AR(p) 📖 ℛ",
    "text": "92.9 code: Spectral density of AR(p) 📖 ℛ\n We will now obtain the spectral density of an autoregressive process, as well as visualize posterior distributions of the spectral densities of this AR process just based on data that we have\nSimulate 300 observations from an AR(2) process with a pair of complex-valued roots\n\nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]&lt;- 2*r*cos(2*pi/lambda) \nphi[2] &lt;- -r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# sample from the AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\nCompute the MLE of \\phi and the unbiased estimator of v using the conditional likelihood\n\np=2\ny=rev(yt[(p+1):T])\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\nObtain 200 samples from the posterior distribution under the conditional likelihood and the reference prior and the direct sampling method\n\nn_sample=200 # posterior sample size\nlibrary(MASS)\n\n## step 1: sample v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)\n\n## step 2: sample phi conditional on v from normal distribution\nphi_sample=matrix(0, nrow = n_sample, ncol = p)\nfor(i in 1:n_sample){\n  phi_sample[i,]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)\n}\n\nusing spec.ar to draw spectral density based on the data assuming an AR(2)\n\nspec.ar(yt, order = 2, main = \"yt\")\n\n\n\n\n\n\n\n\nusing arma.spec from astsa package to draw spectral density\nplot spectral density of simulated data with posterior sampled ar coefficients and innvovation variance\n\nlibrary(\"astsa\")\n\npar(mfrow = c(1, 1), mar = c(3, 4, 2, 1) )\n#result_MLE=arma.spec(ar=phi_MLE, var.noise = s2, log='yes',main = '')\nresult_MLE=arma.spec(ar=phi_MLE, var.noise = s2, main = '')\nfreq=result_MLE$freq\n  \nspec=matrix(0,nrow=n_sample,ncol=length(freq))\n\nfor (i in 1:n_sample){\nresult=arma.spec(ar=phi_sample[i,], var.noise = v_sample[i],# log='yes',\n                 main = '',plot=FALSE)\nspec[i,]=result$spec\n}\n\n\n\n\n\n\n\nFigure 92.9: Spectral density of AR(2) process using arma.spec\n\n\n\n\n\n\nplot(2*pi*freq,log(spec[1,]),type='l',ylim=c(-3,12),ylab=\"log spectra\",\n     xlab=\"frequency\",col=0)\n#for (i in 1:n_sample){\nfor (i in 1:2){\nlines(2*pi*freq,log(spec[i,]),col='darkgray')\n}\nlines(2*pi*freq,log(result_MLE$spec))\nabline(v=2*pi/12,lty=2,col='red')\n\n\n\n\n\n\n\nFigure 92.10: Spectral density of AR(2) process using arma.spec",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05.html#arima-processes",
    "href": "C4-L05.html#arima-processes",
    "title": "92  Bayesian Inference in the AR(p) - M2L5",
    "section": "92.10 ARIMA processes 📖",
    "text": "92.10 ARIMA processes 📖\nThis section introduces the ARMA and ARIMA processes, their definitions, stability, invertibility, and spectral density. It is based on a handout from the course. Ideally it should be expanded with more details and examples and code for simulating ARMA and ARIMA processes. I’m not sure why Prado didn’t go any deeper in the course, but the NDLMs generalize the ARMA and ARIMA processes, so perhaps we will cover them as special cases of the NDLMs in the next lessons.\n\n\n\n\n\n\n\nImportantARMA Model Definition\n\n\n\nA time series process is a zero-mean autoregressive moving average process if it is given by\n\n\\operatorname{ARMA}(p,q)= \\textcolor{red}\n                {\\underbrace{\\sum_{i=1}^{p} \\phi_i y_{t-i}}_{AR(P)}}\n      +\n      \\textcolor{blue}\n                {\\underbrace{\\sum_{j=1}^{q} \\theta_j \\varepsilon_{t-j}}_{MA(Q)}}\n      + \\varepsilon_t\n\\tag{92.9}\nwith \\varepsilon_t \\sim N(0, v).\n\nFor q = 0, we get an AR(p) process.\nFor p = 0, we get a MA(q) i.e. moving average process of order q.\n\n\n\nNext we will define the notions of stability and invertibility of an ARMA process. \n\n\n\n\n\n\nImportantStability Definition\n\n\n\nAn ARMA process is stable if the roots of the AR characteristic polynomial \n\n\\Phi(u) = 1 - \\phi_1 u - \\phi_2 u^2 - \\ldots - \\phi_p u^p\n\nlie outside the unit circle, i.e., for all u such that \\Phi(u) = 0, |u| &gt; 1.\nEquivalently, this happens when the reciprocal roots of the AR polynomial have moduli smaller than 1.\nThis condition implies stationarity.\n\n\nstable\n\n\n\n\n\n\nImportantInvertible ARMA Definition\n\n\n\nAn ARMA process is invertible if the roots of the MA characteristic polynomial given by \n\n\\Theta(u) = 1 + \\theta_1 u + \\ldots + \\theta_q u^q,\n\\tag{92.10}\nlie outside the unit circle.\n\n\ninvertibleNote that \\Phi(B) y_t = \\Theta(B) \\varepsilon_t.\n\nWhen an ARMA process is stable, it can be written as an infinite order moving average process.\nWhen an ARMA process is invertible, it can be written as an infinite order autoregressive process.\n\n\n\n\n\n\n\n\nImportantARIMA Processes\n\n\n\nAn autoregressive integrated moving average process with orders p, d, and q is a process that can be written as\n\n(1 - B)^d y_t = \\sum_{i=1}^{p} \\phi_i y_{t-i} + \\sum_{j=1}^{q} \\theta_j \\varepsilon_{t-j} + \\varepsilon_t,\n\\tag{92.11}\nwhere B is the backshift operator, d is the order of integration, and \\varepsilon_t \\sim N(0, v).\nin other words, y_t follows an ARIMA(p, d, q) if the d difference of y_t follows an ARMA(p, q).\n\n\nEstimation in ARIMA processes can be done via least squares, maximum likelihood, and also in a Bayesian way. We will not discuss Bayesian estimation of ARIMA processes in this course.\n\n92.10.1 Spectral Density of ARMA Processes\n For a given AR(p) process with AR coefficients \\phi_1, \\dots, \\phi_p and variance v, we can obtain its spectral density as\n\nf(\\omega) = \\frac{v}{2\\pi |\\Phi(e^{-i\\omega})|^2} = \\frac{v}{2\\pi |1 - \\phi_1 e^{-i\\omega} - \\ldots - \\phi_p e^{-ip\\omega}|^2},\n\\tag{92.12}\nwith \\omega a frequency in (0, \\pi).\nThe spectral density provides a frequency-domain representation of the process that is appealing because of its interpretability.\nFor instance, an AR(2) process that has one pair of complex-valued reciprocal roots with modulus 0.7 and a period of \\lambda = 12, will show a mode in the spectral density located at a frequency of 2\\pi/12. If we keep the period of the process at the same value of 12 but increase its modulus to 0.95, the spectral density will continue to show a mode at 2\\pi/12, but the value of f(2\\pi/12) will be higher, indicating a more persistent quasi-periodic behavior.\nSimilarly, we can obtain the spectral density of an ARMA process with AR characteristic polynomial \\Phi(u) = 1 - \\phi_1 u - \\ldots - \\phi_p u^p and MA characteristic polynomial \\Theta(u) = 1 + \\theta_1 u + \\ldots + \\theta_q u^q, and variance v as\n\nf(\\omega) = \\frac{v}{2\\pi} \\frac{|\\Theta(e^{-i\\omega})|^2}{|\\Phi(e^{-i\\omega})|^2}.\n\\tag{92.13}\nNote that if we have posterior estimates or posterior samples of the AR/ARMA coefficients and the variance v, we can obtain samples from the spectral density of AR/ARMA processes using the equations above.",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Bayesian Inference in the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05-Ex1.html",
    "href": "C4-L05-Ex1.html",
    "title": "93  Quiz: Spectral representation of the AR(p) - M2L5",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>Quiz: Spectral representation of the AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L05-Ex2.html",
    "href": "C4-L05-Ex2.html",
    "title": "94  Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p) - M2L5",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p) -  M2L5</span>"
    ]
  },
  {
    "objectID": "C4-L06.html",
    "href": "C4-L06.html",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "",
    "text": "96 The Normal Dynamic Linear Model: Definition, Model classes & The Superposition Principle\nNormal Dynamic Linear Models (NDLMs) are defined and illustrated in this module using several examples Model building based on the forecast function via the superposition principle is explained. Methods for Bayesian filtering, smoothing and forecasting for NDLMs in the case of known observational variances and known system covariance matrices are discussed and illustrated..\nThe Normal Dynamic Linear Model (DLM) is covered (R. Prado, Ferreira, and West 2023, 117–44)\nDynamic Linear Models (DLMs) extend classical linear regression to time-indexed data, introducing dependencies between observations through latent evolving parameters. A Normal DLM (NDLM) assumes Gaussian noise at both observation and system levels, enabling tractable Bayesian inference through the Kalman filter.\nWhile superficially complex, NDLMs are conceptually close to linear regression. Instead of I.I.D. observations indexed by i, we index data by time t and allow parameters to evolve with time, resulting in a two-level hierarchical model. At the top level is the observation equation. Below this there is the evolution equation(s) that can be understood as a latent state transition model that can capture trends, periodicity, and regression. The evolution equations can have more than one level however we will see that with some work these are summarized into a matrix form.\nTo make things simpler this is demonstrated using a white noise process and then a random walk model. What makes the NDLM somewhat different is that that there are two variance elements at two levels, necessitating learning more parameters. Once we cover these to models the instructor walks us though all the bits and pieces of the notation. Later we will see that we can add trends, periodicity, regression components in a more or less systematic way. However we need to pick and choose these components to get a suitable forecast function. This approach require an intimate familiarity with the data generating process to model.\nThis approach is Bayesian in that we draw our parameters from a multivariate normal and use updating to improve this initial estimate by incorporating the data and we end up with a posterior i.e. we have distributional view of the time series incorporating uncertainties. Additionally we have a number of Bayesian quantities that can be derived from the model, such as\nHowever the DLM framework is quite flexible and once you understand it it can ve adapted to support features like seasonality using the superposition principle. NDLMs don’t need to be non-stationary time series.\nAs far as I cen tell NDLMs are just DLM with their errors distributed normally at the different levels.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#ndlm-definition",
    "href": "C4-L06.html#ndlm-definition",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "96.1 NDLM Definition 🎥",
    "text": "96.1 NDLM Definition 🎥\n\n\n\n\nNDLM Motivation\n\n\n\n\nNDLM general form\n\n\n\n\nthe forecast function\n\n\n\n\nIn this module, we will motivate and develop a class of models suitable for for analyzing and forecasting non-stationary time series called normal dynamic linear models . We will talk about Bayesian inference and forecasting within this class of models and describe model building as well.\n\n96.1.1 White Noise - A motivating example\nLet’s begin with a very simple model that has no temporal structure, just a mean value with some variation that is:\n\ny_t = \\mu + v_t \\qquad v_t \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, \\nu) \\qquad  \\text{(white noise model)}\n\\tag{96.1}\nwhere:\n\ny_t is the observed time series at time t,\n\\mu is the expected value of y_t this is characteristic we are interested in,\n\\nu_t is a white noise process as usual iid standard normal N(0,1).\n\nIf we plot this model we might see the following graph:\n\nset.seed(123)\nn &lt;- 100\nV &lt;- 1\nmu &lt;- 0\ny &lt;- mu + rnorm(n, 0, V)\nplot(y, type = \"l\", col = \"blue\", lwd = 2, xlab = \"Time\", ylab = \"y\", main = \"Model with no temporal structure\")\n\n\n\n\n\n\n\nFigure 96.1\n\n\n\n\n\nFor this model the mean of the time series is \\mu will be the the expected value of y_t, which is \\mu. And the variance of y_t is \\nu.\n\n\\mathbb{E}[y_t] = \\mu \\qquad \\text{and} \\qquad \\mathbb{V}ar[y_t] = \\nu \\qquad\n\\tag{96.2}\n\n\n96.1.2 A Random walk model with a slowly changing mean\nNext we incorporate some temporal structure, we allow the expected value of the time series, to change over time. To can achieve this, by update the model definition with a \\mu_t where the index indicates that it can change at every time step. And let us keep the noise unchanged. i.e. we set it to \\mu_t \\in N(0,\\nu).\nWe get the following model:\n\ny_t = \\mu_t + \\nu_t \\quad \\nu_t \\overset{\\text{iid}}{\\sim} N(0, V) \\qquad \\text{(radom walk model)}\n\\tag{96.3}\nTo complete this we need to also decide how to incorporate the the changes over time in the parameter \\mu_t. We might consider different options but we should pick the simplest possible to start with. One option is to assume that the expected value of \\mu_t is just the expected value of \\mu_{t-1} plus some noise.\nWe now have that random walk type of structure where \\mu_t can be written in terms of \\mu(t-1). The expected value of \\mu_t, we can think of it as \\mu_{t-1} + \\text{some noise}. This error is once again, assumed to be normally distributed random variable centered at zero and with variance W. Another assumption that we have made here is that the \\nu_t and \\omega_t, are also independent of each other.\nputting this together we get:\n\n\\begin{aligned}\ny_t &= \\mu_t + \\nu_t & \\nu_t & \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, V)  & \\text{(Observation eq.)} \\\\\n\\mu_t &= \\mu_{t-1} + \\omega_t  & \\omega_t & \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, W) & \\text{(System/evolution eq.)}\n\\end{aligned}\n\\tag{96.4}\nWith this model, what we are assuming is that the mean level of the series is changing over time. Note that this is an example of a Gaussian or Normal dynamic linear model.\nNDLMs are a two level hierarchical models where :\n\nAt the top is an observation level equation relating observations y at time t to some time dependent, (hidden) state parameters and some observation level iid distributed error.\nThe system evolution level equation describes the dynamics of parameters over time and incorporates some system iid distributed error.\nThese equations have a linear structure, in the sense that the expected value of y at time t is a linear function of the parameters.\nWe have the assumption of normality for the noise terms in both these equations as well as independence within and between levels.\n\nThis is our first example. Next we will be discuss the general class of models. Later we will consider how to incorporate different structures into the model, and how to perform Bayesian inference for filtering smoothing and forecasting.\n\n\n96.1.3 General form of the NDLM\nThe general class of dynamic linear models can be written as follows:\nWe are going to have two equations. One is the so-called observation equation that relates the observations to the parameters in the model, and the notation we are going to use is as follows.\n\\begin{aligned}\ny_t &= \\vec{F}_t' \\vec{\\theta}_t   + \\nu_t && \\nu_t \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, V_t) && \\text{(obs)} \\\\\n\\vec{\\theta}_t &= G_t \\vec{\\theta}_{t-1} + \\vec{\\omega}_t && \\vec{\\omega}_t \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, W_t) && \\text{(system)}\n\\end{aligned}\n\\tag{96.5}\nWhere:\n\ny_t a univariate observation at time t.\n\\vec{\\theta}_t the state vector is a k-dimensional vector of unknown parameters at time t.\n\\vec{F_t} the observation operator a k*1-dimensional vector at time t that transforms the state parameters into observations.\n\\nu_t is the observation noise at time t from a Normal distribution with variance V_t.\nG_t the state evolution operator is a k \\times k matrix (known)\n\\omega_t the innovation or state evolution noise at time t distributed as N(0,W_t)(known)\nthe noise at the observation level and the system level are each iid and mutually iid.\n\nWe also have the prior distribution for the state vector at time 0:\n\n\\vec{\\theta}_0 \\sim N(\\vec{m}_0,c_0) a prior k-dimensional Normal distribution.\n\nm_0 the mean in the prior is a k-dimensional vector of means. (known)\nc_0 is the covariance matrix k by k. (known)\n\n\n\n\n\n\n\n\nNoteSome Thoughts on NDLM the definition\n\n\n\n\n\nQ. Why are F_t and G_t a vector and a matrix respectively?\n\nIt may helps to think about F and G as follows:\nF_t' acts as a linear transformation that maps the latent state \\vec{\\theta}_t into the observation space, of y.\nG_t is a linear transformation that describes how the state vector evolves over time. I like to think about it as a Hidden Markov state transition matrix.\nIn other words, F_t takes the current hidden state \\theta_t and produces an observation y_t, while G_t takes the current state and produces the next state.\n\nQ. Why is this called a linear model?\n\nThis is because both the observation equation is a linear equation that relates the observations to the parameters in the model and the system equation is a linear equation that tells us how the time-varying parameter is going to be changing over time. This is why we call this a linear model.\n\nQ. Why are the noise terms \\nu_t and \\omega_t assumed to be normally distributed?\n\nThis is a common assumption in time series analysis. It is a convenient assumption that allows us to perform Bayesian inference and forecasting in a very simple way. And this is why we call this a normal dynamic linear model.\n\nQ. Isn’t this just a hierarchical model?\n\nIndeed, this is a hierarchical model. We have a model for the observations and a model for the system level. The system level is changing over time and the observations are related to the system level through the observation equation. And so it is possible to extend this model to more complex structures if we wish to do so by adding another level, etc… However adding more levels leads to extra dynamics that are captured in G without changing the overall framework!\n\n\n\n\n\n\n96.1.4 Inference in the NDLM\nIn terms of the inference, there are a few different kinds of densities and quantities that we are interested in:\n One of the distributions that we are interested in finding is the so-called filtering distribution. We may be interested here in finding what is the density of \\theta_t given all the observations that we have up to time t.Filtering distribution\n\n\\mathcal{D}_t= \\{\\mathcal{D}_0, y_{1:T}\\}\n\\tag{96.6}\nWe will denote information as \\mathcal{D}_t. Usually, it is all the information we have at time zero (i.e. our prior), coupled with all the data points I have up to time t.\nHere we conditioning on all the observed quantities and the prior information up to time t, and I may be interested in just finding what is the distribution for \\theta_t. This is called filtering.\n\n\\mathbb{P}r(\\theta_t \\mid \\mathcal{D}_t) \\qquad \\text{filtering distribution}\n\\tag{96.7}\nforecasting distribution\nAnother distribution that is very important in time series analysis is the forecasting distribution. We may be interested in the distribution of y{t+h}? where we consider h lags into the future and we have all the information \\mathcal{D}_t, up to time t. We want to do a predictions here\n\n\\mathbb{P}r(y_{t+h} \\mid \\mathcal{D}_t) \\qquad \\text{forecasting distribution}\n\\tag{96.8}\n Another important quantity or an important set of distributions is what we call the smoothing distribution. Usually, you have a time series, when you get your data, you observe, I don’t know, 300 data points. As you go with the filtering, you are going to start from zero all the way to 300 and you’re going to update these filtering distributions as you go and move forward. We may want instead to revisit the parameter at time 10, for example, given that you now have observed all these 300 observations. In that case, you’re interested in densities that are of the form. Let’s say that you observe capital T in your process and now you are going to revisit that density for \\theta_t. This is now in the past. Here we assume that t&lt;T. This is called smoothing.Smoothing Distribution\nSo you have more observation once you have seen the data. We will talk about how to perform Bayesian inference to obtain all these distributions under this model setting.\n\n\\mathbb{P}r(\\theta_t \\mid \\mathcal{D}_T)  \\qquad t &lt; T \\qquad \\text{smoothing distribution}\n\\tag{96.9}\n\n\n96.1.5 The forecast function for the NDLM\nIn addition to all the structure that we described before and all the densities that we are interested in finding, we also have as usual, the so-called forecast function, which instead of being the density is just \\mathbb{E}[y(t+h)\\mid \\mathcal{D}_t] i.e. expected value of y at time t given all the information we have before time t.\n\n\\mathbb{E}[y(t+h)\\mid \\mathcal(D_t)] = F'_{t+h} G_{t+h} \\ldots G_{t+1} \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t]\n\\tag{96.10}\nThis is the form of the forecast function.\nThere are particular cases and particular models that we will be discussing in which the F_t=F, i.e. constant and also G_t = G is also constant for all t. In these cases, the forecast function can be simplified and written as:\n\nf_t(h) = \\mathbb{E}(y_{t+h} \\mid D_t) = F'G^h \\mathbb{E}(\\theta_t \\mid \\mathcal{D}_t)\n\\tag{96.11}\nOne thing that we will learn is that the eigen-structure of this matrix is very important to define the form of the forecast function, and it’s very important for model building and for adding components into your model.\n\n\n96.1.6 NDLM short form notation\nFinally, just in terms of short notation, we can always write down when we’re working with normal dynamic linear models, we may be referring to the model instead of writing the two equations, the system and the observation equation. I can just write all the components that define my model. \n\\{F_t, G_t, v_t, W_t\\}\n\\tag{96.12}\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\nIn this part of the course, I will discuss the class of normal dynamic linear models for analyzing and forecasting non-stationary time series. We will talk about Bayesian inference and forecasting within this class of models and describe model building as well.\n\n96.1.7 Motivating example\nI want to begin first with a motivating example. Suppose you have a model that is very simple and has no temporal structure here, just a model that looks like this. You have your time series y_t. Then you’re interested in just thinking about what is the mean level of that time series. That mean level, I’m going to call it \\mu and then I have some noise and the noise is normally distributed. They are all independent, identically distributed normal random variables \\mathcal{N}(0,v). Again, I can think of my time series. Suppose that I have my time series here, and then I’m plotting y_t. Then I have something that looks like this. In this model that \\mu is going to try to get the mean of that time series, this expected value of y_t, which is \\mu. The variance here of y_t is v under this model. What may happen in practice again, this model has no temporal structure, I may want to incorporate some temporal structure that says, well, I think that the level of this, the expected value of this time series, should be changing over time. If you were to do that, you will write down a model where the \\mu changes over time, so it’s indexed in time. Then you have still your same noise here. Let’s again assume \\mathcal{N}(0,v). I have now to make a decision on how I’m going to incorporate temporal structure by modeling the changes over time in this parameter \\mu_t. You could consider different options.\nThe simplest possible, probably that you can consider is something that looks like this. You have that random walk type of structure where \\mu_t is now going to be written as \\mu_{t-1}. The expected value of \\mu_t, you’ll think of it as \\mu_{t-1} plus some noise. That error here is going to be again, assume normally distributed random variable centered at zero and with variance w. There is another assumption that we can make here and is that the nu t and omega t here, are also independent of each other. When I have this model, what am assuming here is that the mean level of the series is changing over time.\nThese type of models have a few characteristics. This is an example of a normal dynamic linear model, as we will see later. In this models, we usually have a few things.\nThe first thing is we have two equations. One is the so-called observation equation that is relating your y_t, your observed process to some parameters in the model that are changing over time. The next equation is the so-called system level equation or evolution equation that tells me how that time varying parameter is going to be changing over time. The other thing you may notice is that we have a linear structure both in the observational level and in the system level. The linear structure, in the sense of the expected value of y_t is just a linear function of that \\mu_t. It happens to be \\mu_t in this particular case. In the second level, I can think of the expected value of \\mu_t as a linear function given \\mu_{t-1}, so it’s a function that is linear on \\mu_{t-1}. There is that linear structure. The other thing that we have here is at both levels, we have the assumption of normality for the noise terms in those equations. This is an example of a Gaussian or normal dynamic. These are time-varying parameters linear model. We will be discussing the general class of models. This is just an example. We will also discuss how to build different structures into the model, as well as how to perform Bayesian inference and forecasting.\n\n\n96.1.8 General form of the model\nThe general class of dynamic linear models can be written as follows. Again, we are going to have two equations. One is the so-called observation equation that relates the observations to the parameters in the model, and the notation we are going to use is as follows. Here, my observations are univariate. We are discussing models for univariate time series. I have that related to a vector of parameters, \\theta_t plus some noise here. This is the noise. The noise are assumed to be independent, identically distributed normal random variables, 0, V_t. Then I have another equation which is a system equation that has this form. There is a general G_t matrix. This is going to be depending on \\theta_{t-1}. This is a vector, and then I have again, these are iid multivariate \\mathcal{N}(0, W_t). This is the observation equation. This is the system equation or evolution equation. This defines a normal dynamic linear model. Here, we are going to say that F_t is a vector. The dimension of the vector is going to be the same as the number of parameters in the model. Let’s say we have k. This is a vector of known values. For each t, we are going to assume that we know what that vector is. Then we have the vector of parameters here is also of dimension k of parameters. The G is the next thing we need to define is a known matrix. That one is also assumed to be known, and then I have V_t is variance at the observational level. The W_t we are going to assume at the beginning that these two quantities are also known for all the values t. This is the variance-covariance matrix at the system level. Again, if we think about these two equations, we have the model defined in this way.\nThere is a next piece that we need to consider if we are going to perform based in inference for the model parameters. The next piece that we need to consider to just fully specify the model is what is the prior distribution. In a normal dynamic linear model, the prior distribution is assumed to be conjugate here. In the case again in which V_t and W_t are known, we are going to be assuming that, say that zero, the parameter vector before observing any data is going to be normally distributed Multivariate normal with M_0 and C_0. The mean is a vector, again of the same dimension as \\theta_0. Then I have k by k covariance matrix there as well. These are assumed to be also given to move forward with the model.\n\n\n96.1.9 Inference, forecasting, smoothing, and filtering.\nIn terms of the inference, there are different kinds of densities and quantities that we are interested in. One distribution that we are interested in finding is the so-called filtering distribution. We may be interested here in finding what is the density of \\theta_{t} given all the observations that we have up to time t. I’m going to call and all the information that I have up to time t. I’m going to call that D_t . It can also be, in some cases, I will just write down. So D_t, you can view with all the info up to time t. Usually, it is all the information I have at time zero. Then coupled, if there is no additional information that’s going to be coupled with all the data points I have up to that time. Here I’m conditioning on all the observed quantities and the prior information up to time t, and I may be interested in just finding what is the distribution for \\theta_{t}.\nThis is called filtering. Another quantity that is very important in time series analysis is forecasting.\nI may be interested in just what is the density, the distribution of y_{t+h} ? Again, the number of steps ahead here, here I’m thinking of h, given that I have all this information up to time t. I’m interested in predictions here. We will be talking about forecasting. Then another important quantity or an important set of distributions is what we call the smoothing distribution. Usually, you have a time series, when you get your data, you observe, I don’t know, 300 data points. As you go with the filtering, you are going to start from zero all the way to 300 and you’re going to update these filtering distributions as you go and move forward. But then you may want to revisit your parameter at time 10, for example, given that you now have observed all these 300 observations. In that case, you’re interested in densities that are of the form. Let’s say that you observe capital T in your process and now you are going to revisit that density for \\theta_t. This is now in the past. Here we assume that t is smaller than capital T. This is called smoothing. So you have more observation once you have seen the data. We will talk about how to perform Bayesian inference to obtain all these distributions under this model setting.\n\n\n96.1.10 The forecast function\nIn addition to all the structure that we described before and all the densities that we are interested in finding, we also have as usual, the so-called forecast function, which is just instead of being the density is just expected value of y(t+h) given all the information I have up to time t. In the case of a general normal dynamic linear model, we have the structure for these just using the equations, the observation and the system of equations.\nWe’re going to have here G_{t+h}. We multiply all these all the way to G_(t+1), and then we have the \\mathbb{E}[\\theta_{t}\\mid D_t]. This is the form of the forecast function. There are particular cases and particular models that we will be discussing in which the F_t is equal to F, so is constant for all t and G_t is also constant for all t. In those cases, the forecast function can be simplified and written as F'G^h expected value. One thing that we will learn is that the eigen-structure of this matrix is very important to define the form of the forecast function, and it’s very important for model building and for adding components into your model.\n\n\n96.1.11 Short-form notation\nFinally, just in terms of short notation, we can always write down when we’re working with normal dynamic linear models, we may be referring to the model instead of writing the two equations, the system and the observation equation. I can just write all the components that define my model. This fully specifies the model in terms of the two equations. If I know what Ft is, what Gt is, what Vt is, and the covariance at the system level. I sometimes will be just talking about a short notation like this for defining the model.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#polynomial-trend-models",
    "href": "C4-L06.html#polynomial-trend-models",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "96.2 Polynomial Trend Models 🎥",
    "text": "96.2 Polynomial Trend Models 🎥\n\n\n\n\n\n\n\nFigure 96.2: first and second order polynomial model\n\n\n\n\n\n\n\n\nFigure 96.3: p-order polynomial model\n\n\n\nWhile we haven’t talked about the superposition principle yet we start at looking at adding different components to the DLM.\nWe might :\n\nsetting a baseline mean and variance\nadding a random walk with its variance\nadd a trend\nadd a regression\nadd seasonality\n\nNext we want to extend the random walk model to include different types of trends and this will be covered by the polynomial trend models. These are models that are useful to model linear trends or polynomial trends in your time series. So if you have a data set, where you have an increasing trend, or a decreasing trend, you would use one of those components in your model. Also\n\n96.2.1 First order polynomial model\nThe first order model is developed at great detail in chapter In (West and Harrison 2013 ch. 2). I don’t know what to make of it, isn’t this a trivial white noise model?\nThe math for Bayesian updating is fairly straight forward and must be much more complex with more sophisticated dynamics. So this is used by the authors to introduce their DLM and an 30 pages of the book is dedicated to in depth analysis and Bayesian development of this specific model and different distribution of interests as well as including comparison to other models and a look at the signal to noise ratio in the model.\nIt is worthwhile pointing out that these models get their name from their forecast function which will takes the general form Equation 96.22\nThe first order polynomial model is a model that is useful to describe linear trends in your time series. If you have a data set where you have an increasing trend or a decreasing trend, you would use one of those components in your model.\nSo the all those can be incorporated using the general p-order polynomial model, so I will just describe the form of this model.\nA first order polynomial is of the form Ax+B where A is the slope and B is the intercept. This is the same random walk model we saw above.\n\n\\begin{aligned}\ny_t &= \\theta_t + \\nu_t, \\qquad & \\nu_t & \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, V_t) \\\\\n\\theta_t &= \\theta_{t-1} + \\omega_t, \\qquad & \\omega_t & \\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, W_t) \\\\\n&\\{1,1,v_t,W_t\\} && \\text{(short form)} \\\\\nf_t(h) &= \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t] && \\text{(forecast fn)} \\\\\n\\end{aligned}\n\\tag{96.13}\nIn the observation equation, \\theta_{t} is the level of the series at time t and \\nu_t is the observation error. In the evolution equation we see the mean for this parameter changing over time as a random walk or a local constant mean with evolution noise \\omega_t.\n(West and Harrison 2013, sec. 2.1) gives the following representation of the model:\nIt is useful to think of \\theta_t as a smooth function of time \\theta(t) with an associated Taylor series representation\n\n\\theta(t + \\delta t) = \\theta(t) + \\text{higher-order terms}\n\\tag{96.14}\nwhere the higher-order terms are assumed to be zero-mean noise. This is a very important point, because it means that we are not trying to model the higher-order terms explicitly, but rather we are assuming that they are just noise.\nwith the model simply describing the higher-order terms as zero-mean noise.\nThis is the genesis of the first-order polynomial DLM: the level model is a locally constant (first-order polynomial) proxy for the underlying evolution.·\nWe can write it down in short form with the following quadruple/\n\n\\{1, 1, V_t, W_t\\} \\qquad f_t(h) = \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t] = k_t \\ \\forall   h&gt;0\n\\tag{96.15}\nNext we can write the forecast function f_t(h) of this model using the representation we gave in Equation 96.15.\nAgain, we’re going to have something of the form F transposed G to the power of h and then the expected value of that \\theta_t given \\mathcal{D}_t. F is 1, G is 1, therefore I’m going to end up having just expected value of \\theta_t given \\mathcal{D}_t.\nWhich depending on the data that you have is you’re just going to have something that is a value that depends on t and it doesn’t depend on h. What this model is telling you is that the forecast function, how you expect to see future values of the series h steps ahead is something that looks like the level that you estimated at time t.\n\n\n96.2.2 Second order Polynomial model AKA Linear Growth model\n(West and Harrison 2013, secs. 7.1–7.2) gives a detailed analysis of this model.\nNow we want to create a model in which captures things that has a linear trend either increasing or decreasing. To do thus we need to have two components in our parameter vector of the state vector. For this we will need two components in our parameter vector of the state vector1.\nSo we have again something that looks like in my observation equation. I’m going to have, I’m going to call it say \\theta_{t,1} \\sim  \\mathcal{N}(v_t), and then I’m going to have say \\theta_{t,1} is going to be of the form to \\theta_{t-1,1} and there is another component here. The other component enters this equation plus let’s call this \\theta_{t-1,2}. And then I have finally also I need an evolution for the second component of the process which is going to be again having a random walk type of behavior.\n\\begin{aligned}\n  y_t &= \\theta_{t,1} + \\nu_t \\quad &\\nu_t &\\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, v_t) \\\\\n  \\theta_{t,1} &= \\theta_{t-1,1} + \\theta_{t-1,2} + \\omega_{t,1} \\qquad &\\omega_{t,1} &\\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, w_{t,11}) \\\\\n  \\theta_{t,2} &= \\theta_{t-1,2} + \\omega_{t,2} \\qquad &\\omega_{t,2} &\\overset{\\text{iid}}{\\sim}  \\mathcal{N}(0, w_{t,22})\n\\end{aligned}\n\\tag{96.16}\nSo there are different ways in which you can interpret this two parameters but essentially:\n\n\\theta_{t-1,1} is related to the baseline level of the series\n\\theta_{t-1,2} is related to the rate of change of the of the series.\n\n\n\n\n\n\n\nTipShort form DLM notation\n\n\n\n\n\n\nHaving the short form notation makes the model easier to understand in relation to other DLM models.\nIt will soon be instrumental in communicating the model structure with different software packages.\n\n\n\n\nNext we should summarize this model using the familiar short form DLM representation, which requires a bit of creative algebra.\n\n\\mathbf{\\theta}_t = (\\theta_{t,1}, \\theta_{t,2}) \\qquad \\{\\mathbf{F}, \\mathbf{G}, V_t, \\mathbf{W}_t\\}\n\nFirst we collect the two variances for the evolution two components into the vector \\utilde{w}_t and then assume that this w_t is Normal. Now this is a bi-variate normal.\n\n\\utilde{\\omega}_t = (\\omega_{t,1},\\omega_{t,2})' \\qquad \\utilde{\\omega}_t \\sim  \\mathcal{N}(0,W_t)\n\nSo what would be my F and my G in this model? So again my theta vector has two components, thus my G, so my F is going to be a two dimensional. We can write down F transposed as the only component that appears at this level is the first component of the vector. I’m going to have 1 and then a zero for F transposed. c.f. Equation 96.17 And then my G here if you think about writing down \\theta_t times G say the t-1 + \\omega_t. Then you have that you’re G is going to have this form.\n\n\\begin{aligned}\n\\mathbf{F} &= (1,0)' & V_t &= v_t \\\\\n\\mathbf{G} &=\n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix}\n& \\mathbf{W}_t &=\n\\begin{pmatrix}\nw_{t,11} & 0 \\\\\n0 & w_{t,22}\n\\end{pmatrix}\n\\end{aligned}\n\\tag{96.17}\nthis is the form from the video \n\\begin{aligned}\n\\mathbf{F} &= (1,0)' & V_t &= v_t \\\\\n\\mathbf{G} &=\n\\begin{pmatrix} 1 & h \\\\\n0 & 1 \\end{pmatrix}\n& \\mathbf{W}_t &=\n\\begin{pmatrix}\nw_{t,11} & w_{t,12} \\\\\nw_{t,21} & w_{t,22}\n\\end{pmatrix}\n\\end{aligned}\n\\tag{96.18}\nthis is the more general form from the handout. Note that in this case we have w_{t,12}=w_{t,21} so there is just one extra parameter.\nThe lesson videos and the handouts differ in the form \\mathbf{W}_t. In the lecture we assumed zero covariance but in the handout the covariance was snuck in. This gives us a slightly more general model. The covariance though is symmetric so we get an extra parameter we need to infer and include in the prior. Anyhow I kept the more general form, though in most cases we will keep the off diagonal terms at zero.\nSo for the first component, I have past values of both components. That’s why I have a 1 and 1 here for the second component I only have the past value of the second component. So there is a zero and a 1. So this tells me what is the structure of this second order polynomial. If I think about how to obtain the forecast function for this second order polynomial is going to be very similar to what we did before. So you can write it down as F transposed G to the power of h, expected value of theta t given Dt. Now the expected value is going to be vector also with two components because theta_t is a two dimensional vector. The structure here if you look at what G is G to the power of h going to be a matrix, that is going to look like 1, h, 0 1. When you multiply that matrix time this times this F what you’re going to end up having is something that looks like 1 h times this expected value of theta t given Dt. So I can think of two components here, so this gives you a constant on h, this part is not going to depend on h. So I can write this down as k t 11 component multiplied by 1 and then I have another constant, multiplied by h. So you can see what happens now is that your forecast function has the form of a linear polynomial. So it’s just a linear function on the number of steps ahead. The slope and the intercept related to that linear function are going to depend on the expected value of, theta_t given the all the information I have up to time t. But essentially is a way to model linear trends. So this is what happens with the second order polynomial model.\nAs we included linear trends and constant values in the forecast function, we may want to also incorporate other kinds of trends, polynomial trends in the model. So you may want to have a quadratic form, the forecast function or a cubic forecast function as a function of h.\n\n\\theta_t = (\\theta_{t,1}, \\theta_{t,2})' \\qquad \\mathbf{G} = \\mathbf{J}_2(1) \\qquad \\mathbf{E}_2 = (1, 0)'\n\n\n\\mathbf{G^h} =\n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix}\n\n\n\\begin{aligned}\nf_t(h) &= F' G^h \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t] \\\\\n&= (1,h) \\mathbb{E}[\\theta_{t}\\mid D_t] \\\\\n&= (1,h)(K_{t,0}, K_{t,1})' \\\\\n&= (K_{t,0} + K_{t,1} h)\n\\end{aligned}\n\\tag{96.19}\n\n\\begin{aligned}\n\\mathbf{G^h} &=\n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix}\n\\end{aligned}\n\\tag{96.20}\n\n\n96.2.3 General p-th order polynomial model\nWe can consider a so called p-th order polynomial model. This model will have a state-space vector of dimension p and a polynomial of order p − 1 forecast function on h. The model can be written as\n\n\\{E_p, J_p(1), v_t, W_t\\}\n\nwith F_t = E_p = (1, 0, \\ldots, 0)′ and G_t = J_p(1), with\n\nJ_p(1) = \\begin{pmatrix}\n1 & 1 & 0 & \\cdots & 0 & 0  \\\\\n0 & 1 & 1 & \\cdots & 0 & 0  \\\\\n0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 1\n\\end{pmatrix}\n\\tag{96.21}\nThe forecast function is given by \nf_t(k) = a_{t_0} +  a_{t_1}k + \\ldots + a_{t_{n-1}} k^{n-1} \\qquad k \\in \\mathbb{N}\n\\tag{96.22}\nwhere a_{t_i} are the coefficients of the polynomial and k is the number of steps ahead we need in our forecast. There is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function given by \\{Ep, Lp, vt, W t\\}, with\n\nL_p = \\begin{pmatrix}\n1 & 1 & 1 & \\cdots & 1 & 1  \\\\\n0 & 1 & 1 & \\cdots & 1 & 1   \\\\\n0 & 0 & 1 & \\cdots & 1 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 1\n\\end{pmatrix}\n\\tag{96.23}\nAnd in this type of model, the forecast function is going to have order p-1. So the parameter vector is going to have dimension p. So you’re going to have \\theta_t =  \\theta_{t1:p}.\nThe observation operator F is just a constant and if we write it as a row vector we get F' as a p-dimensional vector with the one in the first entry and zeros everywhere else.\nThe dynamics matrix G may be written using either a J Jordan form Equation 96.21 or as a triangular form Equation 96.23. These result in different parameterization of this model and we will talk a little bit about this.\nIn the Equation 96.21 we have a matrix with ones on the diagonal and the super diagonal, the matrix is needs to be p \\times p i.e. with dimension p to be compatible with the dimension of the hidden state vector \\theta. So this matrix G is what we call a Jordan block of dimension p of 1. So here 1 is the number that appears in the diagonal. And then I have a p I_p matrix, I have ones in the upper diagonal part. So this is the form of the model, so once again I have the F the G, and the W_t. I have my model.\nThe forecast function in this case again can be written as F' G^h \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_t]. And when you simplify times expected value of \\theta_t, given D_t. Once you simplify those functions you get something that is a polynomial of order p-1 in h. So I just can write this down as k_t + k_{t,1} h + k_{t, p-1} h^{p-1}, so that’s my forecast function.\nThere is an alternative parameterization of this model that has the same F and the same algebraic form of the forecast function, the same form of the forecast function. But instead of using Equation 96.21 form of the G matrix, it has a Equation 96.23 form that has ones in the diagonal and ones everywhere above the diagonal. So it’s an upper triangular matrix with ones in the diagonal and above the diagonal. That’s a different parameterization of the same model but it leads to the same general form of the forecast function just with a different parameterization.\nSo again, we can consider the way you think about these models?\n\nWhat is you think what kind of forecast function makes sense here ?\nWhat is the type of predictions that I expect to have in my model?\nIf they look like a linear trend, I use a second order polynomial.\nIf it looks like a quadratic trend in the forecast then I would use 3rd order polynomial model representation.\n\nNote that the third order polynomial model is covered in\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\n96.2.4 First order Polynomial Models\nI will begin describing the structure of a particular class of models now, the polynomial trend models. These are models that are useful to describe linear trends or polynomial trends in your time series. So if you have a data set, where you have an increasing trend, or a decreasing trend, you would use one of those components in your model.\nWe will begin with the first order polynomial model, which we have already described. It’s the one that has y_t is a single parameter, I’m going to call it just \\theta_t + \\nu_t. And then a random walk evolution for that single parameter, so that’s the mean level of the series. And then we assume that it changes as a random walk, so this is the first order polynomial model.\nSo in general, I’m going to begin with the first order polynomial model, which we have already described. It’s the one that has y_t is a single parameter, I’m going to call it just \\theta_t + \\nu_t. And then a random walk evolution for that single parameters, so that’s the mean level of the series. And then we assume that it changes As a random walk, so this is the first order polynomial model. In this model if I want to write it down in short form I would have a quadruple that looks like this. So the F here that goes F transposed times the parameter vector in this case we have a scalar vector, scalar parameter. It’s going to be 1 my G that goes next to the state of t-1 is going to also be 1. And then I have vt and Wt here. So this fully defines my model if I think about the forecast function of this model using the representation we had before. Again, we’re going to have something of the form F'G^h and then the expected value of that \\theta_t | \\mathcal{D}_t. F is 1, G is 1, therefore I’m going to end up having just expected value of \\theta_t | \\mathcal{D}_t. Which depending on the data that you have is you’re just going to have something that is a value that depends on t and it doesn’t depend on h.\nWhat this model is telling you is that the forecast function, how you expect to see future values of the series h steps ahead is something that looks like the level that you estimated at time t.\nSo that’s the forecast function, you is a first order is a zero order polynomial is a constant on h and it’s called the first order polynomial model.\n\n\n96.2.5 Second order Polynomial Models\nIn the case of a second order polynomial We are going to now think about about a model in which we want to capture things that are not a constant over time but may have an increasing or decreasing linear trend. In this case we’re going to need two components in your parameter vector in the state vector.\nSo we have again something that looks like in my observation equation. I’m going to have, I’m going to call it say theta{t,1} Normal vt, and then I’m going to have say theta_{t,1} is going to be of the form to theta_{t-1,1} and there is another component here. The other component enters this equation plus let’s call this And then I have finally also I need an evolution for the second component of the process which is going to be again having a random walk type of behavior. So there is different ways in which you can interpret this two parameters but essentially one of them is related to the baseline level of the series the other one is related to the rate of change of the of the series. So if you think about the dlm representation again, these two components, I can collect into the vector wt. and then assume that this wt Is normal. Now this is a bivariate normal. So what would be my F and my G in this model? So again my theta vector has two components My G, so my F is going to be a two dimensional vectors. So I can write down F transposed as the only component that appears at this level is the first component of the vector. I’m going to have 1 and then a zero for F transposed. And then my G here if you think about writing down theta t times G say the t -1 +wt. Then you have that you’re G is going to have this form. So for the first component, I have past values of both components. That’s why I have a 1 and 1 here for the second component I only have the past value of the second component. So there is a zero and a 1. So this tells me what is the structure of this second order polynomial. If I think about how to obtain the forecast function for this second order polynomial is going to be very similar to what we did before. So you can write it down as F transposed G to the power of h, expected value of theta t given Dt. Now the expected value is going to be vector also with two components because theta_t is a two dimensional vector. The structure here if you look at what G is G to the power of h going to be a matrix, that is going to look like 1, h, 0 1. When you multiply that matrix time this times this F what you’re going to end up having is something that looks like 1 h times this expected value of theta t given Dt. So I can think of two components here, so this gives you a constant on h, this part is not going to depend on h. So I can write this down as k t 11 component multiplied by 1 and then I have another constant, multiplied by h. So you can see what happens now is that your forecast function has the form of a linear polynomial. So it’s just a linear function on the number of steps ahead. The slope and the intercept related to that linear function are going to depend on the expected value of, theta_t given the all the information I have up to time t. But essentially is a way to model linear trends. So this is what happens with the second order polynomial model.\n\n\n96.2.6 P-th Order Polynomial Models\nAs we included linear trends and constant values in the forecast function, we may want to also incorporate other kinds of trends, polynomial trends in the model. So you may want to have a quadratic form, the forecast function or a cubic forecast function as a function of h. So the all those can be incorporated using the general p-order polynomial model, so I will just describe the form of this model. And in this type of model, the forecast function is going to have order p-1. So your parameter vector is going to have dimension p. So you’re going to have theta_t theta t1 to tp. Your F matrix is going to be constant if I write it as a row vector. F transpose is going to be a p dimensional vector with the one in the first entry and zeros everywhere else. My G matrix is going to have this form and there is different parameterizations of this model and I will talk a little bit about this. But one way to parameterize the model is something that looks like this. So you have ones in the diagonal of the matrix, the matrix is going to be a p by p has to be the dimension of the p compatible with the dimension of the state vector. And then you have zeros’s below the diagonal above that set of ones that are also ones above the diagonal. So this matrix G is what we call a Jordan block of dimension p of 1. So here 1 is the number that appears in the diagonal. And then I have a p Ip matrix, I have ones in the upper diagonal part. So this is the form of the model, so once again I have the F the G, and the wt. I have my model. The forecast function in this case again can be written as F transposed G to the power of h. And when you simplify times expected value of theta_t, given Dt. Once you simplify those functions you get something that is a polynomial of order p-1 in h. So I just can write this down as kt constant.Plus kt1 h + kt p- 1, h to the p -1, so that’s my forecast function. There is an alternative parameterization of this model that has the same F and the same algebraic form of the forecast function, the same form of the forecast function. But instead of having this G matrix, it has a matrix that has ones in the diagonal and ones everywhere above the diagonal. So it’s an upper triangular matrix with ones in the diagonal and above the diagonal. That’s a different parameterization of the same model is going to have the same general form of the forecast function is a different parameterization. So again, you can consider the way you think about these models is you think what kind of forecast function I want to have for my future? What is the type of predictions that I expect to have in my model? And if they look like a linear trend, I use a second order polynomial. If it looks like a quadratic trend in the forecast then I would use 3rd order polynomial model representation.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#summary-of-polynomial-trend-models",
    "href": "C4-L06.html#summary-of-polynomial-trend-models",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "96.3 Summary of polynomial trend models 📖",
    "text": "96.3 Summary of polynomial trend models 📖\n\n96.3.1 Polynomial Trend Models\n\n96.3.1.1 First-Order Polynomial\n\n\\begin{aligned}\ny_t &= \\mu_t + \\nu_t, \\qquad & \\nu_t &\\sim  \\mathcal{N}(0, v_t) \\\\\n\\mu_t &= \\mu_{t-1} + \\omega_t, \\qquad & \\omega_t &\\sim  \\mathcal{N}(0, w_t)\n\\end{aligned}\n\nIn this case, we have:\n\\theta_t = \\mu_t \\quad \\forall t\n\nF_t = 1 \\quad \\forall t \\qquad G_t = 1 \\quad \\forall t\n\nresulting in:\n\n\\{1, 1, v_t, w_t\\} \\qquad \\text{(short notation)}\n\nThe forecast function is:\n\nf_t(h) = E(\\mu_t \\mid \\mathcal{D}_t) = k_t, \\quad \\forall h &gt; 0.\n\n\n\n96.3.1.2 Second-Order Polynomial\n\\begin{aligned}\n  y_t &= \\theta_{t,1} + \\nu_t, \\quad &\\nu_t &\\sim  \\mathcal{N}(0, v_t) \\\\\n  \\theta_{t,1} &= \\theta_{t-1,1} + \\theta_{t-1,2} + \\omega_{t,1}, \\qquad &\\omega_{t,1} &\\sim  \\mathcal{N}(0, w_{t,11}) \\\\\n  \\theta_{t,2} &= \\theta_{t-1,2} + \\omega_{t,2}, \\qquad &\\omega_{t,2} &\\sim  \\mathcal{N}(0, w_{t,22}),\n\\end{aligned}\n\nwhere we can also have:\n\n\\mathbb{C}ov(\\theta_{t,1}, \\theta_{t,2} ) = w_{t,12} = w_{t,21}\n\nThis can be written as a DLM with the state-space vector \\theta_t = (\\theta_{t,1}, \\theta_{t,2})', and\n\n\\{\\mathbf{F}, \\mathbf{G}, v_t, \\mathbf{W}_t\\}  \\qquad \\text{(short notation)}\n\nwith \\mathbf{F} = (1, 0)' and\n\n\\mathbf{G} =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\quad \\mathbf{W}_t =\n\\begin{pmatrix}\nw_{t,11} & w_{t,12} \\\\\nw_{t,21} & w_{t,22}\n\\end{pmatrix}.\n\nNote that\n\n\\mathbf{G}^2 =\n\\begin{pmatrix}\n1 & 2 \\\\\n0 & 1\n\\end{pmatrix}, \\quad \\mathbf{G}^h =\n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix},\n\nand so:\n\nf_t(h) = (1, h) E(\\mathbf{\\theta}_t \\mid \\mathcal{D}_t) = (1, h) (k_{t,0}, k_{t,1})' = (k_{t,0} + h k_{t,1}).\n\nHere \\mathbf{G} = \\mathbf{J}_2(1) (see below).\nAlso, we denote \\mathbf{E}_2 = (1, 0)', and so the short notation for this model is\n\n\\{E_2, J_2(1), \\cdot, \\cdot\\}\n\n\n\n96.3.1.3 General p-th Order Polynomial Model\nWe can consider a p-th order polynomial model. This model will have a state-space vector of dimension p and a polynomial of order p-1 forecast function on h. The model can be written as\n\\{E_p, J_p(1), v_t, W_t\\}  \\qquad \\text{(short notation)}\n\nwith \\mathbf{F}_t = \\mathbf{E}_p = (1, 0, \\dots, 0)' and \\mathbf{G}_t = \\mathbf{J}_p(1), with\n\n\\mathbf{J}_p(1) =\n\\begin{pmatrix}\n1 & 1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n0 & 1 & 1 & \\cdots & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0 & 1\n\\end{pmatrix}.\n\nThe forecast function is given by\n\nf_t(h) = k_{t,0} + k_{t,1} h + \\dots + k_{t,p-1} h^{p-1}.\n\nThere is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function, given by \\{E_p, L_p, v_t, W_t\\}, with\n\nL_p =\n\\begin{pmatrix}\n1 & 1 & 1 & \\cdots & 1 \\\\\n0 & 1 & 1 & \\cdots & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{pmatrix}.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#regression-models",
    "href": "C4-L06.html#regression-models",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "96.4 Regression models 🎥",
    "text": "96.4 Regression models 🎥\n\n\n\n\nRegression models\n\n\n96.4.1 Simple dynamic regression\n\n\\begin{aligned}\ny_t &= \\beta_{t,0} + \\beta_{t,1}x_t + ν_t \\\\\n\\beta_{t,0} &= \\beta_{t−1,0} + \\omega_{t,0} \\\\\n\\beta_{t,1} &= \\beta_{t−1,1} + \\omega_{t,1}\n\\end{aligned}\n\\tag{96.24}\nand so \\theta_t = (\\beta_t,0, \\beta_{t,1})′, F_t = (1, x_t)′ and G = I_2.\nThis results in a forecast function of the form\n\nf_t(h) = k_{t,0} + k_{t,1}x_{t+h}\n\\tag{96.25}\nwhere k_{t,0} = \\mathbb{E}[\\beta_{t,0} \\mid \\mathcal{D}_t] and k_{t,1} = \\mathbb{E}[\\beta_{t,1} \\mid \\mathcal{D}_t].\n\n\n96.4.2 General dynamic regression\n\n\\begin{aligned}\ny_t &= \\beta_{t,0} + \\beta_{t,1}x_{t,1} + \\ldots \\beta_{t,M} x_{t,M} + ν_t \\\\\n\\beta_{t,m} &= \\beta_{t−1,m} + \\omega_{t,m,} & m = 0 : M.\n\\end{aligned}\n\\tag{96.26}\nThen, \\theta = (\\beta_t,0, \\ldots , \\beta_{t,M} )′, F_t = (1, x_{t,1}, \\ldots , x_{t,M} )′ and G = I_M . The forecast function is given by\n\nf_t(h) = k_{t,0} + k_{t,1}x_{t+h,1} + \\ldots + k_{t+h,M}x_{t+h,M}\n\\tag{96.27}\nA particular case is of dynamic regressions is the case of time-varying auto-regressions (TVAR) with\n\n\\begin{aligned}\ny_t &= \\varphi_{t,1}y_{t−1} + \\varphi_{t,2}y_{t−2} + \\ldots + \\varphi_{t,p} y_{t−p} + ν_t,\\\\\n\\varphi_{t,m} &= \\varphi_{t−1,m} + \\omega_{t,m,} & m = 1 : p\n\\end{aligned}\n\\tag{96.28}\nThere is a paper (Raquel Prado, Huerta, and West 2000) on TVAR models that is a good reference for this model.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\n96.4.3 Regression Models\nIn regression models, we may also have additional covariates that are also measured sequentially over time. We may want to regress the y_t time series and see what relationships they have with other covariates that are also measured over time. The simplest possible case is the dynamic simple regression model. In this case, I can write down. I have a single covariate, that covariate is X_t that is observed here, and then I have the usual. In this case, I have an intercept and a slope, and this is representing my simple linear regression. It’s just the regression where both the intercept and the slope are time-varying. I can define the variation. I need to specify what’s the evolution of the two components, and we are going to use this random walk. We could use other structures, but again, in the normal linear case, we are going to be using these evolution equations. Then I collect here my W’s as a single vector. The \\omega_t is going to have the two components in here. These are normally distributed zero and variance covariance matrix W_t, that is a two-by-two matrix. This is the case of the simple regression model. In the case of this model, we have F now is time-varying. This is going to change depending on the value of X_t. I can write Ft transpose as one and X_t. My Theta vector. Again, if I think about what it is, is just Beta t, 0 Beta t, 1. I have those two components.\nThe G matrix is going to be the identity, and you can see that essentially the first component is related to the first component in t minus one, and the second component at time t is related to the second component at time t minus 1. So the identity matrix will be the G. Therefore, if I think about my forecast function in the simple linear regression case, this is going to be my F transpose, which is 1 xt times the G, the G is the identity, times the expected value of Theta t, given Dt. For the expected value of Theta t given Dt, This is a two-dimensional vector, so I’m going to have components in there. I can write this down as K_t0 plus K_t1 Xt. We can see that the forecast function is again has that form that depends on that covariate at the time. This should be t plus h because we are evaluating this at t plus h. You need to have the covariate evaluated at t plus h here.\n\n\n96.4.4 General Dynamic Regression Model\n In the case of general dynamic regression model, we’re going to have a set of covariates. We can have, let’s say k of those covariates or p of those covariates, X_t1. This is my observation equation. Instead of having a single covariate, now I’m going to have p of them. I’m going to have coefficients that go with each of those and I may have the Beta t0 coefficient. My G matrix now, if I think about my parameter vector is just p plus 1 dimensional, p plus 1. Yeah, so that I have the 0 and then the p values, so is a p plus 1 vector. Then my G is the identity. My F_t is going to be a vector, is also p plus 1 dimension. The first entry is one, the second is X_t1 X_tp. My forecast function is going to be similar to this, but now we are going to have more than one covariate, so we end up with a forecast function that has this form, p. This is the case for the dynamic regression.\n\n\n96.4.5 TVAR\n In the case of dynamic regression, we can also have a time-varying autoregressive process. This is a particular case of dynamic regression where the covariates are just past values of the time series itself. In this case, we can think about the observation equation as being a linear combination of past values of the time series. One particular example of dynamic regression model is the case of a time-varying autoregressive process. This brings us back to those autoregressive processes that we were discussing earlier in the course. When you you’re regressing each of the X’s correspond to pass values, you have a regression model that we call a time-varying ARP. In this case, your observation equation is going to have the AR coefficients, but the AR coefficients are going to be varying over time. If we assume that we put all the coefficients together and have a random walk evolution equation for those. If I said, I call Phi_t the vector that contains all the components with all the coefficients from one to p, then I can now define this evolution equation. Then my Omega_t here is a p-dimensional vector, and I have Omega t, normal zero, WT, and my epsilon t normal 0 vt.\nThis defines a time-varying AR. It’s the same structure that we had before. The only difference is my covariates are just past values of the time series. Therefore my forecast function for the time-varying AR is going to have this form where every_thing is going to depend on past values of the time series. We will study this model in particular and make connections with the AR that we studied earlier in the class.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#summary-of-regression-models",
    "href": "C4-L06.html#summary-of-regression-models",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "96.5 Summary of Regression Models 📖",
    "text": "96.5 Summary of Regression Models 📖\n\n96.5.1 Dynamic Regression Models\n\n96.5.1.1 Simple Dynamic Regression\n\n\\begin{aligned}\n  y_t &= \\beta_{t,0} + \\beta_{t,1} x_t + \\nu_t \\\\\n  \\beta_{t,0} &= \\beta_{t-1,0} + \\omega_{t,0} \\\\\n  \\beta_{t,1} &= \\beta_{t-1,1} + \\omega_{t,1}\n\\end{aligned}\n\nThus:\n\n\\theta_t = (\\beta_{t,0}, \\beta_{t,1})'\n\n\nF_t = (1, x_t)'\n\nand\n\nG = I_2\n\nThis results in a forecast function of the form\n\nf_t(h) = k_{t,0} + k_{t,1} x_{t+h}.\n\n\n\n96.5.1.2 General Dynamic Regression\n\n\\begin{aligned}\ny_t &= \\beta_{t,0} + \\beta_{t,1} x_{t,1} + \\dots + \\beta_{t,M} x_{t,M} + \\nu_t \\\\\n\\beta_{t,m} &= \\beta_{t-1,m} + \\omega_{t,m}, \\quad &m = 0:M.\n\\end{aligned}\n\\tag{96.29}\nThen,\n\\theta_t = (\\beta_{t,0}, \\dots, \\beta_{t,M})',\n\\mathbf{F}_t = (1, x_{t,1}, \\dots, x_{t,M})' and\n\\mathbf{G} = \\mathbf{I}_M.\nThe forecast function is given by:\n\nf_t(h) = k_{t,0} + k_{t,1} x_{t+h,1} + \\dots + k_{t,M} x_{t+h,M}.\n\\tag{96.30}\nA particular case of dynamic regressions is the case of time-varying autoregressive (TVAR) with time-varying autoregressive (TVAR)\n\n\\begin{aligned}\n  y_t &= \\phi_{t,1} y_{t-1} + \\phi_{t,2} y_{t-2} + \\dots + \\phi_{t,p} y_{t-p} + \\nu_t \\\\\n  \\phi_{t,m} &= \\phi_{t-1,m} + \\omega_{t,m}, \\quad m = 1:p.\n\\end{aligned}",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#the-superposition-principle",
    "href": "C4-L06.html#the-superposition-principle",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "96.6 The superposition principle 🎥",
    "text": "96.6 The superposition principle 🎥\n\n\n\n\n\n\n\nFigure 96.4: The superposition principle\n\n\nWe can use the superposition principle to build models that have different kinds of components. The main idea is to think about what is the general structure we want for the forecast function and then isolate the different components of the forecast function and think about the classes of dynamic linear models that are represented in each of those components. Each of those components has a class and then we can build the general dynamic linear model with all those pieces together using this principle.\nTwo references for the Superposition principle are\n\n(West and Harrison 2013, sec. 3.1 p. 98)\n(R. Prado, Ferreira, and West 2023, sec. 4.2.1 p. 136)\n\n\n\n\n\n\n\nImportant 96.1: Superposition Principle\n\n\n\nIn the first the author state:\n\nConditional independence also features strongly in initial model building and in choosing an appropriate parametrization. For example, the linear superposition principle states that any linear combination of deterministic linear models is a linear model. This extends to a normal linear superposition principle:\nAny linear combination of independent normal DLMs is a normal DLM. - &gt; – (West and Harrison 2013, sec. 3.1 p. 98)\n\n\n\nWe will illustrate how to do that with an example:\nLet’s say that we want to create a model here with a forecast function that has a linear trend component. Let’s say we have a linear function as a function of the number of steps ahead that you want to consider. Then suppose you also have a covariate here that you want to include in your model as a regression component. \nf_t(h) = \\underbrace{(k_{t,0} + k_{t,1}\\; h)}_{\\text{linear trend component}} + \\underbrace{(k_{t,2}\\; x_{t+h})}_{\\text{regression component}}\n\\tag{96.31}\nwhere:\n\nf_t(h) is our forecast function.\nk_{t,0}, k_{t,1} and k_{t,2} are just constants (that we index using time t and a second subscript).\nx_{t+h} is a time dependent regression covariate.\n\nWhen we look at the forecast function, we can isolate a linear trend and a regression components as indicated. Each of these can be set in terms of two forecast functions]{.mark}. I’m going to call the forecast function f_{1,t}(h), this is just the first piece.\n\n\\begin{aligned}\nf_t(h) &= f_{1,t}(h) + f_{2,t}(h) \\\\\nf_{1,t}(h) &= k_{t,0} + k_{t,1} & \\text{(linear trend component)} \\\\\nf_{2,t}(h) &= k_{t,2}x_{t+h} & \\text{(regression component)}\n\\end{aligned}\n\\tag{96.32}\nWe know how to represent forecast function f_{1,t} and f_{2,t} in terms of dynamic linear models.\nFor the linear trend component, f_{1,t}(h) , we have a 2-dimensional state vector, \\theta_t = (\\theta_{t,1}, \\theta_{t,2})', which yields the following DLM shortform:\n\n\\{F_1, G_1, \\cdot, \\cdot\\}  \\qquad \\text{(short notation)}\n\\tag{96.33}\n\nWhere we don’t explicitly specify the observational and system variances, V and W\nThe important bit are F and G. The forecast function is given by:\n\n\nF_{1} = E_2 = (1, 0)'\n\\tag{96.34}\n\nG_{1} =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n\\tag{96.35}\nfor the regression component f_{2,t}(h) we have the following DLM representation:\n\n\\{F_2,t, G_2, \\cdot, \\cdot\\}  \\qquad \\text{(short notation)}\n\\tag{96.36}\nwhere we have F_{2t} is X_t and my G is simply going to be 1. This is a one-dimensional vector in terms of the state parameter vector.\n\nF_{2,t} = x_{t+h}\n\\tag{96.37}\n\nG_{2} = 1\n\\tag{96.38}\nOnce we have these, we can assemble them into our final model. \\{F_t, G, \\cdot, \\cdot\\}\nWe care more about F, G, and less about the observational variance and some covariance also for the system where the\nF is going to be an F that has, you just concatenate the two Fs. You’re going to get 1, 0 and then you’re going to put the next component here. Again, this one is dependent on time because this component is time dependent and\nThe model with forecast function f_t(h) above is a model with a 3-dimensional state vector with\n\nF_t = (F_1', F_{2,t})' = (1, 0, x_t)'\n\nThen the G, you can create it just taking a block diagonal structure by concatenating G_1 and G_2. though formally there must be a better term for this operation.\n\nG = \\text{blockdiag}[G_1, G_2] =\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n\nThis gives us the full G dynamics matrix for the model. A model with this F_t and this G that is constant over time will give us this particular forecast function Equation 96.31 we started with.\nWe used the superposition principle to build this model. If we need additional components, we will learn how to incorporate seasonal components, regression components, trend components. One can build a fairly sophisticated model with different structures into this particular model using the superposition principle.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\n96.6.1 The superposition principle\nWe can use the superposition principle to build models that have different kinds of components. The main idea is to think about what is the general structure we want for the forecast function and then isolate the different components of the forecast function and think about the classes of dynamic linear models that are represented in each of those components. Each of those components has a class and then we can build the general dynamic linear model with all those pieces together using this principle. I will illustrate how to do that with an example. Let’s say that you want to create a model here with a forecast function that has a linear trend component. Let’s say we have a linear function as a function of the number of steps ahead that you want to consider. Then suppose you also have a covariate here that you want to include in your model as a regression component. Let’s say we have a K_t2 and then we have X_t plus h, this is my covariate. Again, the k’s here are just constants, as of constants in terms of h, they are dependent on time here. This is the general structure we want to have for the forecast function. Now you can see that when I look at the forecast function, I can isolate here and separate these two components. I have a component that looks like a linear trend and then I have a component that is a regression component. Each of this can be set in terms of two forecast functions. I’m going to call the forecast function F_1t h, this is just the first piece. Then I have my second piece here. I’m going to call it F_2t, is just this piece here with the regression component. We know how to represent this forecast function in terms of a dynamic linear model. I can write down a model that has an F, G, and some V, and some W that I’m going to just leave here and not specify them explicitly because the important components for the structure of the model are the F and the G. If you’ll recall the F in the case of a forecast function with a linear trend like this, is just my E_2 vector, which is a two-dimensional vector. The first entry is one, and the second one is a zero. Then the G in this case is just this upper triangular matrix that has 1, 1 in the first row and 0, 1 in the second one. Remember, in this case we have a two-dimensional state vector where one of the components in the vector is telling me information about the level of the time series, the other component is telling me about the rate of change in that level. This is a representation that corresponds to this forecast function. For this other forecast function, we have a single covariate, it’s just a regression and I can represent these in terms of an F_2, G_2, and then some observational variance and some system variance here in the case of a single covariate and this one depends on t. We have F_2t is X_t and my G here is simply going to be one. This is a one-dimensional vector in terms of the state parameter vector. We have a single state vector and it’s just going to tell me about the changes, the coefficient that goes with the X_t covariate. Once I have these, I can create my final model and I’m going to just say that my final model is F, G, and then I have some observational variance and some covariance also for the system where the F is going to be an F that has, you just concatenate the two Fs. You’re going to get 1, 0 and then you’re going to put the next component here. Again, this one is dependent on time because this component is time dependent and then the G, you can create it just taking a block diagonal structure, G_1 and G_2. You just put together, the first one is 1, 1, 0, 1 and then I concatenate this one as a block diagonal. This should be one. This gives me the full G function for the model. Now a model with this F_t and this G that is constant over time will give me this particular forecast function. I’m using the superposition principle to build this model. If you want additional components, we will learn how to incorporate seasonal components, regression components, trend components. You can build a fairly sophisticated model with different structures into this particular model using the superposition principle.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#superposition-principle-general-case",
    "href": "C4-L06.html#superposition-principle-general-case",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "96.7 Superposition principle: General case 📖",
    "text": "96.7 Superposition principle: General case 📖\nYou can build dynamic models with different components, for example, a trend component plus a regression component, by using the principle of superposition. The idea is to think about the general form of the forecast function you want to have for prediction. You then write that forecast function as a sum of different components where each component corresponds to a class of DLM with its own state-space representation. The final DLM can then be written by combining the pieces of the different components.\nFor example, suppose you are interested in a model with a forecast function that includes a linear polynomial trend and a single covariate x_t, i.e.,\n\nf_t(h) = k_{t,0} + k_{t,1}h + k_{t,3}x_{t+h}.\n\nThis forecast function can be written as f_t(h) = f_{1,t}(h) + f_{2,t}(h), with\n\nf_{1,t}(h) = (k_{t,0} + k_{t,1}h), \\quad f_{2,t}(h) = k_{t,3}x_{t+h}.\n\nThe first component in the forecast function corresponds to a model with a 2-dimensional state vector, F_{1,t} = F_1 = (1, 0)',\n\nG_{1,t} = G_1 =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n\nThe second component corresponds to a model with a 1-dimensional state vector, F_{2,t} = x_t, G_{2,t} = G_2 = 1.\nThe model with forecast function f_t(h) above is a model with a 3-dimensional state vector with F_t = (F_1', F_{2,t})' = (1, 0, x_t)' and\n\nG_t = \\text{blockdiag}[G_1, G_2] =\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n\n\n96.7.1 General Case\nThe general case wasn’t covered in the video and we didn’t have a proper statement of the superposition principle. However, in Important 96.1 I extracted the statement of the principle above. This statement clarifies that the principle arises via conditional independence, a tool we also used extensively in the previous course on mixture models. Now let us consider the general case from the handout.\nAssume that you have a time series process y_t with a forecast function\n\nf_t(h) = \\sum_{i=1}^{m} f_{i,t}(h),\n\nwhere each f_{i,t}(h) is the forecast function of a DLM with representation \\{F_{i,t}, G_{i,t}, v_{i,t}, W_{i,t}\\}.\nThen, f_t(h) has a DLM representation \\{F_t, G_t, v_t, W_t\\} with\n\nF_t = (F_{1,t}', F_{2,t}', \\dots, F_{m,t}')',\n\n\nG_t = \\text{blockdiag}[G_{1,t}, \\dots, G_{m,t}],\n\n\nv_t = \\sum_{i=1}^{m} v_{i,t},\n\nand\n\nW_t = \\text{blockdiag}[W_{1,t}, \\dots, W_{m,t}].\n\n\n\n\n\n\n\nPrado, Raquel, Gabriel Huerta, and Mike West. 2000. “Bayesian Time-Varying Autoregressions: Theory, Methods and Applications.” Resenhas Do Instituto de Matemática e Estatı́stica Da Universidade de São Paulo 4 (4): 405–22. https://www2.stat.duke.edu/~mw/MWextrapubs/Prado2001.pdf.\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L06.html#footnotes",
    "href": "C4-L06.html#footnotes",
    "title": "95  Normal Dynamic Linear Models, Part 1 - M3L6",
    "section": "",
    "text": "the state makes it’s appearance↩︎",
    "crumbs": [
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>Normal Dynamic Linear Models, Part 1 - M3L6</span>"
    ]
  },
  {
    "objectID": "C4-L07.html",
    "href": "C4-L07.html",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "",
    "text": "97\nWe will now delve into Bayesian inference in the case of the normal dynamic linear model where both the observational variance and the system variance are known. We will talk about filtering equations, smoothing equations and also forecasting in this setting using Bayesian approach",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#filtering",
    "href": "C4-L07.html#filtering",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.1 Filtering 🎥",
    "text": "97.1 Filtering 🎥\n\n\n\n\n\n\n\nFigure 97.1: Derivation for the Prior and Forecast at Time t\n\n\n\n\n\n\n\n\nFigure 97.2: Derivation of the Posterior at Time t\n\n\n\nRecall we are working in a Bayesian setting where a NDLM model with a normal prior would like this:\n\n\\begin{aligned}\n  y_t &= F_t' \\theta_t + \\nu_t & \\nu_t &\\sim \\mathcal{N}(0, v_t) & \\text{(observation)} \\\\\n  \\theta_t &= G_t \\theta_{t-1} + \\omega_t & \\omega_t &\\sim  \\mathcal{N}(0, W_t) & \\text{(evolution)} \\\\\n  & &(\\theta_0 \\mid \\mathcal{D}_0) & \\sim  \\mathcal{N}(m_0, C_0) & \\text{(prior)}\n\\end{aligned}\n\\tag{97.1}\n\nIn the prior \\mathcal{D}_0 stands for the information that we have before collecting any data and\nWe are assuming \\theta_0 follows a normal distribution with\nm_0 mean\nC_0 variance covariance matrix.\n\nSince we are doing filtering which is a retrospective analysis, of past states we assume that we know m_0, C_0, \\nu_t, \\omega_t, F_t, G_t \\qquad \\forall t.\nHowever, there is often great interest in looking back in time in order to get a clearer picture of what happened.\nWe are interested in performing Bayesian inference in this setting and we talked about different kinds of distributions.\n\nOne is the filtering distribution that allows us to update the distribution of \\theta_t as we receive observations and information over time.\nThe other one is smoothing equations that allows us to just revisit the past once we have observed a chunk of data.\n\nIn a Bayesian setting, you have to set a prior distribution. We will work with the prior distribution that is conjugate.\nIn this case we have to begin with a distribution at time zero for \\theta_0. So before we have seen any data at all, I have this prior distribution.\nWe also assume a prior distribution of the form:\n\n(\\theta_{t} \\mid \\mathcal{D}_{t-1}) \\sim \\mathcal{N}(m_{t-1}, C_{t-1}).\n\\tag{97.2}\nWe assume that this the filtering distribution follows this normal distribution based on\n\nthe prior in Equation 97.9 being conjugate of the normal and\n\nthe linearity of the model in Equation 97.9.\n\nThese result in updates to the model parameters and uncertainty, at each time step, preserving the normal structure from the prior.\nThen, we can obtain the following distributions:\n\nPrior at Time t\n\n\n  (\\theta_t \\mid \\mathcal{D}_{t-1}) \\sim  \\mathcal{N}(a_t, R_t) \\qquad \\text{(prior at time t)} \\qquad\n   \\tag{97.3}\nwith\n \\begin{aligned}\n  a_t \\doteq& \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_{t-1}] =& G_t  \\mathbb{E}[G_t \\theta_{t-1} \\mid \\mathcal{D}_{t-1} ] =& G_t m_{t-1} \\\\\n  R_t \\doteq& \\mathbb{V}ar[\\theta_t \\mid \\mathcal{D}_{t-1}] =& G_t \\mathbb{V}ar[\\theta_t \\mid \\mathcal{D}_{t-1}] =& G_t C_{t-1} G_t' + W_t.\n  \\end{aligned}\n   \\tag{97.4}\nWhere we simply took the first and second moments of the system equation from Equation 97.9 conditioned on our information set \\mathcal{D}_{t-1}\n\nOne-Step Forecast\n\n\n  (y_t \\mid \\mathcal{D}_{t-1}) \\sim  \\mathcal{N}(f_t, q_t) \\qquad \\text{(one step forecast fn)} \\qquad\n   \\tag{97.5}\nwith\n\\begin{aligned}\n  f_t\n     & \\doteq \\mathbb{E}[ y_t \\mid \\mathcal{D}_{t-1} ]\n     & = F_t' \\mathbb{E}[ y_t \\mid \\mathcal{D}_{t-1} ]\n     & = F_t' a_t \\\\\n  q_t\n     & \\doteq \\mathbb{V}ar[y_t \\mid \\mathcal{D}_{t-1}]\n     & = F_t' \\mathbb{V}ar[y_t \\mid \\mathcal{D}_{t-1}]  \n     & = F_t' R_t F_t + v_t\n  \\end{aligned}\n   \\tag{97.6}\nWhere we took the first moments on the observation equation conditioned on the information set \\mathcal{D}_t and substituted Equation 97.5\n\nPosterior at Time t\n\n\n  (\\theta_t \\mid \\mathcal{D}_t) \\sim  \\mathcal{N}(m_t, C_t)\n  \nwith\n\\begin{aligned}\n  m_t &= a_t + R_t F_t q_t^{-1} (y_t - f_t), \\\\\n  C_t &= R_t - R_t F_t q_t^{-1} F_t' R_t.\n  \\end{aligned}\n   \\tag{97.7}\nThese can be derived via Normal theory or via the Multivariate Bayes’ theorem. The background for both seems to be provided in (West and Harrison 2013, secs. 17.2.3 p.639)\nNow, denoting e_t = (y_t - f_t) and A_t = R_t F_t q_t^{-1}, we can rewrite the equations above as:\nIt follows that\n\n\\begin{pmatrix}Y \\\\ \\theta\\end{pmatrix} \\sim \\mathcal{N}\n\\left(\n  \\begin{pmatrix}F'a \\\\ a \\end{pmatrix},\n  \\begin{pmatrix} F'RF + V & F'R \\\\ RF & R \\end{pmatrix}\n\\right)\n\nTherefore, identifying Y with X_1 and \\theta with X_2 in the partition of X in 17.2.2, we have RF R )]\nTherefore, identifying Y with X1 and θ with X2 in the partition of X in 17.2.2, we have \nY \\sim \\mathcal{N}[F'a, F'RF + V]\n\n\n(\\theta \\mid Y) \\sim \\mathcal{N}[m, C],\n\nwhere\n\nm = a + RF[F′RF + V]−1[Y − F′a]\n\nand \nC = R − RF[F′RF + V]−1F′R.\n\n\n  \\begin{aligned}\n  \\theta \\mid \\mathcal{D}_t &\\sim \\mathcal{N}(m_t,C_t)\\\\\n  m_t &\\doteq a_t + A_t e_t, \\\\\n  C_t &\\doteq R_t - A_t q_t A_t'\n  \\end{aligned}\n\\tag{97.8}\nEquation 97.4 , Equation 97.6 and Equation 101.12 are often referred to as the Kalman filtering equations.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\nI will now discuss Bayesian inference in the case of the normal dynamic linear model where both the observational variance and the system variance are known. We will talk about filtering equations, smoothing equations and also forecasting in this setting using Bayesian approach.\nSo recall we are working with a model that looks like this: … And then this is my first equation, the observation equation and I have a system equation that looks like this.\nWe are going to assume that V_t and W_t are known for every t. And we also know what the F_t’s and the G_t’s are here. So the response is a uni-dimensional y_t and then I have, say, \\theta_t is a vector of a given dimension, depending on the structure of the model.\nWe are interested in performing Bayesian inference in this setting and we talked about different kinds of distributions\n\nOne is the filtering distribution that allows us to update the distribution of \\theta_t as we receive observations and information over time.\nThe other one is smoothing equations that allows us to just revisit the past once we have observed a chunk of data.\n\nSo I will be talking about those and also smoothing.\nIn a Bayesian setting, you have to set a prior distribution. We will work with the prior distribution that is conjugate.\nIn this case I have to begin with distribution at time zero. So before I know, I have seen any data at all, I have this prior distribution. D_0 stands for the information that I have before collecting any data. And we are going to assume, That this \\theta_0 follows a normal distribution with m_0 mean and variance covariance matrix C_0. So these are also specified when you’re working with this model.\nSo we assume that this m_0 and C_0 is known.\nOnce we have this setting using these equations, we can obtain the filtering equations.\nSo the first assumption is going to be that we have, a structure.\nSo for \\theta_{t -1} \\mid \\mathcal{D}_{t-1} is going to have this normal structure which is going to happen basically because we’re using this conjugate prior. And because we have normal structure in the model, is going to lead to the following distribution. So the first one is the prior at time t.\nSo if I want to think about why my distribution for the t is given the information I have up to t-1, I can look at the equations of the model and use this second equation. And by looking at this equation, if I condition on the information I have up to t-1, I can see that, say, \\theta_t is written as a linear function of, \\theta_{t -1} and I have the assumption of normality here.\nTherefore, say, \\theta_t going to follow a normal distribution with some mean and some variance. So now we’re going to compute this mean and this variance using this equation. So if you think about the expected value of \\theta_t, given D_{t -1}, that’s just going to be G_t is a constant here. So I have my G_t and then I have expected value of \\theta_{t -1} given G_{t -1} plus expect the value of this \\omega_t.\nBut \\omega_t is a zero mean, normally distributed quantity, so it’s just going to be zero. Using the assumption that I have this structure, then I have that the \\mathbb{E}[\\theta_t \\mid \\mathcal{D}_{t -1}] = G_t \\times m_{t-1}. We’re going to call this quantity a_t, so we have here a_t. For the variance covariance matrix, then we just have to compute, do the same type of operation. And again, we can use this equation and see that we obtain this G_t variance of \\theta_{t-1} \\mid \\mathcal{D}_{t -1} G_t'. And then we have now the variance of the omega, the variance of the omega is just W_t. So we have G_t = C_{t -1} G_t' + W_t. So we can call this quantity R_t and just have the form of this prior distribution at time t.\nI can now think about another distribution which is the distribution of y_t \\mid \\mathcal{D}_{t-1}. So this is the so called one-step ahead, Forecast, And in the one-step ahead forecast again is a similar type of structure. So now we’re going to use the first equation rather than the second equation and we see that y_t is written in terms of a linear function of \\theta_t. And we have also the Gaussian in assumption here. So again the y_t is going to be normally distributed, And we just have to compute the mean and the variance for this y_t. So using the first equation, we have the expected value of y_t given D_{t -1} is just F_t' \\mathbb{E}[\\theta_t \\mid D_{t -1}]. And we computed this before, so this is, again, the expected value of \\theta_t given D_{t -1} is what we computed here. So this is to be F_t' a_t. And we are going to call this little f_t. Then, for the variance, Again, we use this equation, we have this component, so we are going to get F_t' R_t F_t + D_t. And I’m going to call this q_t. So my final distribution, the one-step ahead forecast distribution, tells me that this follows a normal f_t q_t. The next equations we are going to discuss are the equations that tell me about what is the distribution of \\theta_t once we incorporate the information provided by y_t. The next distribution is the posterior of \\theta_t given D_t. So that’s, \\theta_t given D_t. And we can write D_t as whatever information we have at time t- 1. And the new data point with this just y_t. So we just want to update the distribution of \\theta_t given that we have received this additional data point at time t. There are two ways of computing this distribution. One uses normal theory, the other one uses Bayes’ theorem. And you obtain that the distribution of \\theta_t given D_t is going to be a normal, with mean we call it m_t and variance C_t. We will see how to obtain this distribution or the moments of this distribution using normal theory.\n\n\nSo, again, we can write down, if we think about just combining the vector \\theta_t with the observation\n\n\nY_t given D_{t -1}, right? We have information about \\theta_t \\mid t-1. That’s the prior for \\theta_{ta,t}, based on the information at t -1. And then we also computed before the one step ahead forecast distribution for y_t| \\mathcal{D}_{t -1}. So we know that when we combine these two in a single vector, we’re going to have a multivariate normal distribution and the first component is going to be a_t. The second component is what we have called F_t, so that’s the mean. And then for the covariance matrix. We’re going to have now, what goes here is just the variance of \\theta_t given D_{t -1}, which we have called R_t. What goes here is the variance of y_t \\mid \\mathcal{D}_{t -1} and we have called this q_t. And now we have to compute the covariance between \\theta_t and y_t, and that goes here. And the covariance between y_t and \\theta_t, which is just the transpose of that, is going to go here. So if I think about computing the covariance of \\theta_t and y_t \\mid \\mathcal{D}_{t -1}, I can write y_t using the first equation here as a function of \\theta_t. That’s going to give us, F_t' \\theta_t + v_t given D_{t -1}. And in this one we can see that this is going to give us basically the variance of \\theta_t given D_{t -1} and then multiplied by F_t' F_t which gives me the F_t. So this is going to be variance of \\theta_t given D_{t -1} times F_t. And then there is a term that combines the \\theta_t with the noise but they are independent, so the covariance is going to be zero. So this one is simply going to be my R_t F_t, so this goes here, And what goes here is just the covariance of y_t with \\theta_t or the transpose of this. So this is going to give me F_t' R_t', but R_t is a covariance matrix, so R_t' = R_t. So now I have my full multivariate distribution and I can use properties of the multivariate distribution to compute the distribution of, \\theta_t, given y_t and D_{t -1}. So that’s going to be a conditional distribution, I’m going to condition on the y_t. And when I combine y_t and D_{t -1} that gives me just the information up to time t. So we are interested in just finding, say, \\theta_t given y_t and D_{t -1} which is the same as \\theta_t given D_t. We partition the normal distribution in this way, so I can just think about this is the first component and then I have these different pieces in my covariance matrix. And we know from normal theory that if we have a distribution, if we have a vector that is partitioned into vectors here where they are normally distributed. And I have my mean partition here and let’s say I have one component here, Then we know that if I wanted to compute the distribution of X_1 conditional on X_2, that’s going to give me normal, let’s say \\alpha^*. And let’s call this one the \\sigma^*, where \\alpha^* is going to be my \\alpha_1 + \\sigma_{12}^{-1}. And then I have _1 - \\alpha_2 and then I have my \\sigma^*. And this one gives me my \\sigma_{11} - \\sigma_{21}. So this is a result from normal theory. So if I want my conditional distribution of X_1 given X_2 I can apply these equations. So we notice we have the same type of structure here. If I partition my vector and in \\theta_t and y_t. And now I condition on, I take the distribution of \\theta_t conditioning on y_t. I’m going to have that same structure where this is normal, m_t C_t. And my m_t using normal theory, again, is going to be a_t + \\sigma_{22}^{-1}. And then I have y_t - f_t. So that’s my mean and my covariance matrix. It’s going to be R_t - q_t^{-1} and then I have this transpose again. So if we simplify things a bit here and we call e_t, it’s just the error that we make when we compare y_t, which is the observation with the prediction, right? And then I also use the notation I call a_t, let’s call here A_t R_t F_t q_t^{-1}. Then we can write this down, to mean, we can write as a_t + A_t. And the covariance matrix. We can write it as R_t, A_t q_t A_t'. So this gives me the posterior mean after receiving this y_t observation. And you can see that you can write down the posterior mean, has this usual form of the prior plus something that relates to the error that I make with the prediction.\nSo the y_t appears there and then is weighted by this quantity that we just call a_t.\nAnd for the covariance structure, we are also incorporating information about the prior and what the y_t observation provides. So this gives us our filtering equation for \\theta_t given D_t. And now we can apply all these equations as we receive observations from t = 1 all the way to T. If we happen to have T observations in the time series, we can do this filtering process and obtain these distributions as we receive information.",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#summary-of-filtering-distributions",
    "href": "C4-L07.html#summary-of-filtering-distributions",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.2 Summary of filtering distributions 📖",
    "text": "97.2 Summary of filtering distributions 📖\n\n97.2.1 Bayesian Inference in NDLM: Known Variances\nConsider an NDLM given by:\n\n\\begin{aligned}\ny_t &= F_t' \\theta_t + \\nu_t, \\quad \\nu_t \\sim  \\mathcal{N}(0, v_t), \\\\\n\\theta_t &= G_t \\theta_{t-1} + \\omega_t, \\quad \\omega_t \\sim  \\mathcal{N}(0, W_t),\n\\end{aligned}\n\\tag{97.9}\nwith F_t, G_t, v_t, and W_t known. We also assume a prior distribution of the form (\\theta_0 \\mid \\mathcal{D}_0) \\sim  \\mathcal{N}(m_0, C_0), with m_0, C_0 known.\n\n97.2.1.1 Filtering\nWe are interested in finding \\mathbb{P}r(\\theta_t \\mid \\mathcal{D}_t) for all t. Assume that the posterior at t-1 is such that:\n\n(\\theta_{t-1} \\mid \\mathcal{D}_{t-1}) \\sim  \\mathcal{N}(m_{t-1}, C_{t-1}).\n\\tag{97.10}\nThen, we can obtain the following:\n\nPrior at Time t\n\n\n(\\theta_t \\mid \\mathcal{D}_{t-1}) \\sim  \\mathcal{N}(a_t, R_t),\n\nwith\n\na_t = G_t m_{t-1} \\qquad R_t = G_t C_{t-1} G_t' + W_t.\n\n\nOne-Step Forecast\n\n\n(y_t \\mid D_{t-1}) \\sim  \\mathcal{N}(f_t, q_t),\n\nwith\n\nf_t = F_t' a_t, \\quad q_t = F_t' R_t F_t + v_t.\n\n\nPosterior at Time t: (\\theta_t \\mid \\mathcal{D}_t) \\sim  \\mathcal{N}(m_t, C_t) with\n\n\\begin{aligned}\nm_t &= a_t + R_t F_t q_t^{-1} (y_t - f_t), \\\\\nC_t &= R_t - R_t F_t q_t^{-1} F_t' R_t.\n\\end{aligned}\n\nNow, denoting e_t = (y_t - f_t) and A_t = R_t F_t q_t^{-1}, we can rewrite the equations above as:\n\\begin{aligned}\nm_t &= a_t + A_t e_t, \\\\\nC_t &= R_t - A_t q_t A_t'\n\\end{aligned}",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#code-filtering-in-the-ndlm-example-ℛ",
    "href": "C4-L07.html#code-filtering-in-the-ndlm-example-ℛ",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.3 code Filtering in the NDLM: Example 📖 ℛ",
    "text": "97.3 code Filtering in the NDLM: Example 📖 ℛ\n\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices &lt;- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states &lt;- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter &lt;- function(data, matrices, initial_states){\n  ## retrieve dataset\n  y_t &lt;- data$y_t\n  T &lt;- length(y_t)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF &lt;- matrices$FF  \n  GG &lt;- matrices$GG\n  VV &lt;- matrices$VV\n  WW &lt;- matrices$WW\n  \n  ## retrieve initial states\n  m0 &lt;- initial_states$m0\n  C0 &lt;- initial_states$C0\n  \n  ## create placeholder for results\n  d &lt;- dim(GG)[1]\n  at &lt;- matrix(NA, nrow=T, ncol=d)\n  Rt &lt;- array(NA, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(NA, nrow=T, ncol=d)\n  Ct &lt;- array(NA, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(m0)\n      Rt[, , i] &lt;- GG %*% C0 %*% t(GG) + WW\n      Rt[, , i] &lt;- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }else{\n      at[i, ] &lt;- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[, , i] &lt;- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(FF) %*% (at[i, ]) \n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% FF / Qt[i]\n    et[i] &lt;- y_t[i] - ft[i]\n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[, , i] &lt;- 0.5*Ct[, , i]+ 0.5*t(Ct[, , i])\n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = \n                Rt, ft = ft, Qt = Qt))\n}\n\nforecast_function &lt;- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  WW &lt;- matrices$WW\n  VV &lt;- matrices$VV\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state-space parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[, , i] &lt;- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }else{\n      at[i, ] &lt;- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[, , i] &lt;- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }\n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval &lt;- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile &lt;- qnorm(quantile)\n  bound &lt;- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] &lt;- mu + \n    z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] &lt;- mu + \n    z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron,main=\"Lake Huron Data\",\n     ylab=\"level in feet\") # Total of 98 observations \n\n\n\n\n\n\n\nk=4\nT=length(LakeHuron)-k # We take the first 94 observations \n                      # only as our data\nts_data=LakeHuron[1:T]\nts_validation_data &lt;- LakeHuron[(T+1):98]\n\ndata &lt;- list(y_t = ts_data)\n\n# First order polynomial model \n\n## set up the DLM matrices \nFF &lt;- as.matrix(1)\nGG &lt;- as.matrix(1)\nVV &lt;- as.matrix(1)\nWW &lt;- as.matrix(1)\nm0 &lt;- as.matrix(570)\nC0 &lt;- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices &lt;- set_up_dlm_matrices(FF, GG, VV, WW)\ninitial_states &lt;- set_up_initial_states(m0, C0)\n\n## filtering\nresults_filtered &lt;- forward_filter(data, matrices, \n                                   initial_states)\n\nForward filtering is completed!\n\nnames(results_filtered)\n\n[1] \"mt\" \"Ct\" \"at\" \"Rt\" \"ft\" \"Qt\"\n\nci_filtered &lt;- get_credible_interval(results_filtered$mt, \n                                     results_filtered$Ct)\n\n## forecasting \nresults_forecast &lt;- forecast_function(results_filtered,k, \n                                      matrices)\n\nForecasting is completed!\n\nci_forecast &lt;- get_credible_interval(results_forecast$ft, \n                                     results_forecast$Qt)\n\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\nindex_forecast=index[(T+1):98]\n\nplot(index, LakeHuron, ylab = \"level\", \n     main = \"Lake Huron Level\",type='l',\n     xlab=\"time\",lty=3,ylim=c(574,584))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered$mt, type='l',\n      col='red',lwd=2)\nlines(index_filt, ci_filtered[, 1], type='l', \n      col='red', lty=2)\nlines(index_filt, ci_filtered[, 2], type='l', col='red', lty=2)\n\n\nlines(index_forecast, results_forecast$ft, type='l',\n      col='green',lwd=2)\nlines(index_forecast, ci_forecast[, 1], type='l',\n      col='green', lty=2)\nlines(index_forecast, ci_forecast[, 2], type='l',\n      col='green', lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"forecast\"),\n       col = c(\"red\", \"green\"), lty=c(1, 1))\n\n\n\n\n\n\n\n#Now consider a 100 times smaller signal to noise ratio \nVV &lt;- as.matrix(1)\nWW &lt;- as.matrix(0.01)\nmatrices_2 &lt;- set_up_dlm_matrices(FF,GG, VV, WW)\n\n## filtering\nresults_filtered_2 &lt;- forward_filter(data, matrices_2, \n                                     initial_states)\n\nForward filtering is completed!\n\nci_filtered_2 &lt;- get_credible_interval(results_filtered_2$mt, \n                                       results_filtered_2$Ct)\n\nresults_forecast_2 &lt;- forecast_function(results_filtered_2, \n                             length(ts_validation_data), \n                             matrices_2)\n\nForecasting is completed!\n\nci_forecast_2 &lt;- get_credible_interval(results_forecast_2$ft, \n                                       results_forecast_2$Qt)\n\n\nplot(index, LakeHuron, ylab = \"level\", \n     main = \"Lake Huron Level\",type='l',\n     xlab=\"time\",lty=3,ylim=c(574,584))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered_2$mt, type='l', \n      col='magenta',lwd=2)\nlines(index_filt, ci_filtered_2[, 1], type='l', \n      col='magenta', lty=2)\nlines(index_filt, ci_filtered_2[, 2], type='l', \n      col='magenta', lty=2)\n\nlines(index_forecast, results_forecast_2$ft, type='l', \n      col='green',lwd=2)\nlines(index_forecast, ci_forecast_2[, 1], type='l', \n      col='green', lty=2)\nlines(index_forecast, ci_forecast_2[, 2], type='l', \n      col='green', lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"forecast\"),\n       col = c(\"magenta\", \"green\"), lty=c(1, 1))\n\n\n\n\n\n\n\nplot(index_filt,results_filtered$mt,type='l',col='red',lwd=2,\n     ylim=c(574,584),ylab=\"level\")\nlines(index_filt,results_filtered_2$mt,col='magenta',lwd=2)\npoints(index,LakeHuron,pch=20)\nlines(index,LakeHuron,lty=2)",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#smoothing-and-forecasting",
    "href": "C4-L07.html#smoothing-and-forecasting",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.4 Smoothing and forecasting 🎥",
    "text": "97.4 Smoothing and forecasting 🎥\n\n\n\n\n\n\n\nFigure 97.3: Smoothing\n\n\n\n\n\n\n\n\nFigure 97.4: Forecasting\n\n\n\nWe now discuss the smoothing equations for the case of the NDLM, where we are assuming that the variance at the observation level \\nu_t and the covariance matrix at the system level \\mathbf{W}_t are both known.\n \\begin{aligned}\ny_t &= \\mathbf{F}_t' \\mathbf{\\theta}_t + \\nu_t, &\\nu_t &\\sim \\mathcal{N} (0, v_t), & \\text{(observation)} \\\\\n\\mathbf{\\theta}_t & = \\mathbf{G}_t \\mathbf{\\theta}_{t-1} + \\mathbf{\\omega}_t, &\\mathbf{\\omega}_t & \\sim \\mathcal{N} (0, \\mathbf{W}_t), & \\text{(evolution)} \\\\\n&\\{ \\mathbf{F}_t, \\mathbf{G}_t, v_t, \\mathbf{W}_t \\}  &(\\mathbf{\\omega}_0 \\mid \\mathcal{D}_0) & \\sim \\mathcal{N}(\\mathbf{m}_0, \\mathbf{C}_0) & \\text{(prior)}\n\\end{aligned}\n\\tag{97.11}\nwith F_t, G_t, v_t, W_t, m_0 and C_0 known.\nWe have discussed the filtering equations, i.e. the process for obtaining the distributions of \\theta_t \\mid \\mathcal{D}_t, as we collect observations over time, called filtering.\nWe do this by updating the distribution of \\theta_t given the data we have collected step by step, as we move forward in time - updating the from the prior distribution.\nNow we will discuss what happens when we do smoothing, meaning when we revisit the distributions of \\theta_t, given now that we have received a set of observations.\n\n97.4.0.1 Smoothing\nFor t &lt; T, we have that:\n\n(\\theta_t \\mid D_T) \\sim  \\mathcal{N}(a_T(t - T), R_T(t - T)),\n\nwhere\n\na_T(t - T) = m_t - B_t [a_{t+1} - a_T(t - T + 1)],\n\n\nR_T(t - T) = C_t - B_t [R_{t+1} - R_T(t - T + 1)] B_t',\n\nfor t = (T - 1), (T - 2), \\dots, 0, with B_t = C_t G_t' R_{t+1}^{-1}, and a_T(0) = m_T, R_T(0) = C_T. Here a_t, m_t, R_t, and C_t are obtained using the filtering equations as explained before.\n\n\n97.4.0.2 Forecasting\nFor h \\geq 0, it is possible to show that:\n\n(\\theta_{t+h} \\mid D_t) \\sim  \\mathcal{N}(a_t(h), R_t(h)),\n\n\n(y_{t+h} \\mid D_t) \\sim  \\mathcal{N}(f_t(h), q_t(h)),\n\nwith\n\na_t(h) = G_{t+h} a_t(h - 1),\n\n\nR_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},\n\n\nf_t(h) = F_{t+h}' a_t(h),\n\n\nq_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},\n\nand\n\na_t(0) = m_t, \\quad R_t(0) = C_t.\n\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\n\nSmoothing\n\nWe know, we now discuss the smoothing equations for the case of the normal dynamic linear model. When we are assuming that both the variance at the observation level is known and the covariance matrix at the system level is also known.\n\n\n\nthe NDLM we will be inferring\n\nRecall we have two equations here, we have the observation equation, where y_t is modeled as F_t'\\theta_t + \\text{noise} the noise is \\mathcal{N}(0,\\nu_t). And we’re assuming that the vt is given. We are also assuming that we know Ft for all t. And then in the evolution equation we have \\theta_t= G_t \\theta(t-1) + noise. And then again, the assumption for the w_t is here is that they are normally distributed with mean zero, and these variants co variance matrix, capital W_t. So we can summarize the model in terms of F_t, G_t, Vt and W_t, that are given for all t. We have discussed the filtering equations.\n\n\n\nRecall what is Filtering?\n\nSo the process for obtaining the distributions of \\theta_t \\mid \\mathcal{D}_t, as we collect observations over time is called filtering.\n\n\n\nRecall what is Smoothing?\n\nNow we will discuss what happens when we do smoothing, meaning when we revisit the distributions of \\theta_t, given now that we have received a set of observations.\n\n\n\nFiltering illustrated\n\nSo Just to illustrate the process, we have here, \\theta_0,\\theta_1 all way up to \\theta_4. And we can assume just for the sake of the example, that we are going to receive three observations. So we are going to proceed with the filtering, and then once we receive the last observation at time three, we’re going to go backwards and we’re going to revisit the distributions for the state parameters.\nSo just to remind you how the filtering works, we move forward, before we receive any observations. In the NDLM, when we have all the variances known. The conjugate prior distribution is a \\mathcal{N}(m_0,C_0), and this is specified by the user, before collecting any observations.\nWe can then use the structure of the model, meaning the system equation and the observation equation to obtain the distribution of \\theta_t \\mid \\mathcal{D}_0. Before observing the first y. This gives us first the distribution of \\theta_t, \\theta_1 \\mid \\mathcal{D}_0, which is \\mathcal{N}(a_1, R_1). And then we can also get the one step ahead forecast distribution for y_1 \\mid  \\mathcal{D}_0, which is a \\mathcal{N}(f_1, q_1). And we have discussed how to obtain these moments using the filtering equations.\nThen we received the first observation, and the first observation can allows us to update the distribution of . So we obtain now the distribution of \\theta1 \\mid y_1, and whatever information we have at \\mathcal{D}_0. So this gives us \\mathcal{N}(m_1, C_1). And using again the structure of the model, we can get the prior distribution for \\theta_2 given the one and that’s a \\mathcal{N}(a_2, R_2). And then the one step ahead forecast distribution now for y_2 \\mid \\mathcal{D}_1 and that’s a \\mathcal{N}(f_2, q_2). So we can receive y_2 update the distribution of and we can continue this process, now get the priors at T=3. And then once we get the observation at T=3, we update the distribution. And we can continue like this with the prior for \\theta_4 and so on. Let’s say that we stop here, at T=3.\n\n\nAnd now we are interested in answering the question. Well, what is the distribution for example of \\theta_2 given that, now, I obtain not only y_1 and y_2, but also y_3. I want to revisit that distribution using all that information. Same thing for say, the distribution of \\theta_0 \\mid D_0, y_1, y_2, y_3. So that’s what it’s called smoothing.\nSo the smoothing equations, allow us to obtain those distributions. So just to talk a little bit about the notation again, in the normal dynamic linear model where v_t and w_t are known for all t’s. We have that this is a normal, so the notation here, the T &gt;t, here. So we’re looking at the distribution of \\theta_t, now in the past and that one follows a normal distribution with mean aT(t-T). So the notation here for the subscript T means that I’m conditioning on all the information I have to T. And then the variance covariance matrix is given by this, RT(t-T). So this is just going to indicate how many steps I’m going to go backwards as you will see in the example.\n\n\nSo we have some recursions in the same way that we have the filtering equations. Now we have the smoothing equations. And for these smoothing equations we have that the mean. You can see here, that whenever you’re computing a particular step t- T, you’re going to need a quantity that you computed in the previous step, t-T+1. So you’re going to need that, is a recursion, but you’re also going to need mt and and at+1. So those are quantities that you computed using the filtering equations. So in order to get the smoothing equations, you first have to proceed with the filtering. Similarly for RT(t-T), you have also that depends on something you previously obtained. And then you also have the Ct, the Rt+1 and so on. So those quantities you computed when you were updating the filtering equations. The recursion begins with aT(0) meaning that you are not going to go backwards any points in time. So that is precisely the mean is going to be whatever you computed with the filtering equations of up to T, that’s mT. And then RT(0) is going to be CT. So just to again illustrate how this would work in the example, if we start here right? If we condition, so the first step would be to compute again to initialize using the distribution of given D3. And that is a normal with mean a3(0) and variance covariance matrix R3(0), But those are precisely m3 and C3 respectively. Then we go backwards one step. And if we want to look at what is the distribution of \\theta^2, now conditional on D3. That’s a normal with mean a3(-1) and variance covariance matrix R3(-1). So if you look at the equations down here, you will see that, in order to compute a3 (-1), and R3(-1). You’re going to need m2,C2, a3,R3 and then what you computed here these moments in the previous step, a3(0) and R3(0). Then you obtain that distribution and you can now look at the distribution of given D3, that’s the normal a3(-2), R3(-2). And once again, to compute these moments, you’re going to need m1,C1,a2,R2 and then you’re going to need a3(-1),R3(-1). And you can continue all the way down to given D3 using these recursions. So the smoothing equations allow us to, just compute all these distributions. And the important equations work basically because of the linear and Gaussian structure in the normal dynamic linear model.\n\n\n\n97.4.1 Forecasting\n\nIn a similar way, we can compute the forecasting distributions. Now we are going to be looking forward, and in the case of forecasting, we are interested in the distribution of \\theta(t+h) given D_t. And now h is a positive lag. So here we assume that is h≥0. So we are going to have the recursion is a N(a_t(h), R_t(h)). The mean is a_t(h) and we are going to use the structure of the model to obtain these recursions, again. So here we are using the system equation, and the moment at(h) depends on what you computed at a_t(h-1) the previous lag, times G_{t+h}. And then, would you initialize the recursion with a_t(0)=m_t.\n\n\nSimilarly, for the covariance matrix h steps ahead, you’re going to have a recursion that depends on Rt(h-1). And then you’re going to need to input also G_{t+h} and W_{t+h}. To initialize, the recursion with Rt(0)= Ct. So you can see that in order to compute these moments, you’re going to need mt and Ct to start with. And then you’re also going to have to input all the G’s and the W’s for the number of steps ahead that you require.\n\n\nSimilarly, you can compute the distribution, the h steps ahead distribution of y_t+h given Dt. And that one also follows a normal, with mean f_t(h), q_t(h). And now we also have a recursion here, ft(h) depends on at(h) and as we said, a_t(h) depends on a_t(h-1) and so on. And q_t(h) is just given by these equations. So once again, you have to have access to F_{t+h} for all the h, a number of steps ahead that you are trying to compute this distribution. And then you also have to provide the observational variance for every h value. So that you get v_{t+h}. So this is specified in the modeling framework as well. If you want proceed with the forecasting distributions.",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#summary-of-the-smoothing-and-forecasting-distributions",
    "href": "C4-L07.html#summary-of-the-smoothing-and-forecasting-distributions",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.5 Summary of the smoothing and forecasting distributions 📖",
    "text": "97.5 Summary of the smoothing and forecasting distributions 📖\n\n\n97.5.1 Bayesian Inference in NDLM: Known Variances\nConsider the NDLM given by:\n \\begin{aligned}\ny_t &= \\mathbf{F}_t' \\mathbf{\\theta}_t + \\nu_t, &\\nu_t &\\sim \\mathcal{N} (0, v_t), \\\\\n\\mathbf{\\theta}_t &= \\mathbf{G}_t \\mathbf{\\theta}_{t-1} + \\mathbf{\\omega}_t, &\\mathbf{\\omega}_t &\\sim \\mathcal{N} (0, \\mathbf{W}_t), \\\\\n&\\{ \\mathbf{F}_t, \\mathbf{G}_t, v_t, \\mathbf{W}_t \\}  &(\\mathbf{\\omega}_0 \\mid \\mathcal{D}_0) &\\sim  \\mathcal{N}(\\mathbf{m}_0, \\mathbf{C}_0)\n\\end{aligned}\n\\tag{97.12}\nwith F_t, G_t, v_t, and W_t known.\nWe also assume a prior distribution of the form (\\theta_0 \\mid D_0) \\sim  \\mathcal{N}(m_0, C_0), with m_0 and C_0 known.\n\n97.5.1.1 Smoothing\nFor t &lt; T, we have that:\n\n(\\theta_t \\mid D_T) \\sim  \\mathcal{N}(a_T(t - T), R_T(t - T)),\n\nwhere\n\na_T(t - T) = m_t - B_t [a_{t+1} - a_T(t - T + 1)],\n\n\nR_T(t - T) = C_t - B_t [R_{t+1} - R_T(t - T + 1)] B_t',\n\nfor t = (T - 1), (T - 2), \\dots, 0, with B_t = C_t G_t' R_{t+1}^{-1}, and a_T(0) = m_T, R_T(0) = C_T. Here a_t, m_t, R_t, and C_t are obtained using the filtering equations as explained before.\n\n\n97.5.1.2 Forecasting\nFor h \\geq 0, it is possible to show that:\n\n(\\theta_{t+h} \\mid D_t) \\sim  \\mathcal{N}(a_t(h), R_t(h)),\n\n\n(y_{t+h} \\mid D_t) \\sim  \\mathcal{N}(f_t(h), q_t(h)),\n\nwith\n\na_t(h) = G_{t+h} a_t(h - 1),\n\n\nR_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},\n\n\nf_t(h) = F_{t+h}' a_t(h),\n\n\nq_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},\n\nand\n\na_t(0) = m_t, \\quad R_t(0) = C_t.",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#smoothing-in-the-ndlm-example",
    "href": "C4-L07.html#smoothing-in-the-ndlm-example",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.6 Smoothing in the NDLM, Example 🎥",
    "text": "97.6 Smoothing in the NDLM, Example 🎥\nThis segment covers the code in the following section",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#sec-smoothing-in-the-NDLM",
    "href": "C4-L07.html#sec-smoothing-in-the-NDLM",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.7 Code: Smoothing in the NDLM, Example 📖 ℛ",
    "text": "97.7 Code: Smoothing in the NDLM, Example 📖 ℛ\n\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices &lt;- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states &lt;- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter &lt;- function(data, matrices, initial_states){\n  ## retrieve dataset\n  y_t &lt;- data$y_t\n  T &lt;- length(y_t)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF &lt;- matrices$FF  \n  GG &lt;- matrices$GG\n  VV &lt;- matrices$VV\n  WW &lt;- matrices$WW\n  \n  ## retrieve initial states\n  m0 &lt;- initial_states$m0\n  C0 &lt;- initial_states$C0\n  \n  ## create placeholder for results\n  d &lt;- dim(GG)[1]\n  at &lt;- matrix(NA, nrow=T, ncol=d)\n  Rt &lt;- array(NA, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(NA, nrow=T, ncol=d)\n  Ct &lt;- array(NA, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  \n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(m0)\n      Rt[, , i] &lt;- GG %*% C0 %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] &lt;- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(FF) %*% (at[i, ]) \n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% FF / Qt[i]\n    et[i] &lt;- y_t[i] - ft[i]\n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[,,i] &lt;- 0.5*Ct[,,i] + 0.5*t(Ct[,,i]) \n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = Rt, \n              ft = ft, Qt = Qt))\n}\n\n\nforecast_function &lt;- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  WW &lt;- matrices$WW\n  VV &lt;- matrices$VV\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] &lt;- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval &lt;- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile &lt;- qnorm(quantile)\n  bound &lt;- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] &lt;- mu + z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] &lt;- mu + z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n### smoothing equations ###\nbackward_smoothing &lt;- function(data, matrices, \n                               posterior_states){\n  ## retrieve data \n  y_t &lt;- data$y_t\n  T &lt;- length(y_t) \n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  \n  ## retrieve matrices\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  at &lt;- posterior_states$at\n  Rt &lt;- posterior_states$Rt\n  \n  ## create placeholder for posterior moments \n  mnt &lt;- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt &lt;- array(NA, dim = dim(Ct))\n  fnt &lt;- numeric(T)\n  Qnt &lt;- numeric(T)\n  for(i in T:1){\n    # moments for the distributions of the state vector given D_T\n    if(i == T){\n      mnt[i, ] &lt;- mt[i, ]\n      Cnt[, , i] &lt;- Ct[, , i]\n      Cnt[, , i] &lt;- 0.5*Cnt[, , i] + 0.5*t(Cnt[, , i]) \n    }else{\n      inv_Rtp1&lt;-solve(Rt[,,i+1])\n      Bt &lt;- Ct[, , i] %*% t(GG) %*% inv_Rtp1\n      mnt[i, ] &lt;- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n      Cnt[, , i] &lt;- Ct[, , i] + Bt %*% (Cnt[, , i + 1] - Rt[, , i+1]) %*% t(Bt)\n      Cnt[,,i] &lt;- 0.5*Cnt[,,i] + 0.5*t(Cnt[,,i]) \n    }\n    # moments for the smoothed distribution of the mean response of the series\n    fnt[i] &lt;- t(FF) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] &lt;- t(FF) %*% t(Cnt[, , i]) %*% FF\n  }\n  cat(\"Backward smoothing is completed!\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron,main=\"Lake Huron Data\",ylab=\"level in feet\") \n\n\n\n\n\n\n\n# 98 observations total \nk=4\nT=length(LakeHuron)-k # We take the first 94 observations \n                     #  as our data\nts_data=LakeHuron[1:T]\nts_validation_data &lt;- LakeHuron[(T+1):98]\n\ndata &lt;- list(y_t = ts_data)\n\n## set up matrices\nFF &lt;- as.matrix(1)\nGG &lt;- as.matrix(1)\nVV &lt;- as.matrix(1)\nWW &lt;- as.matrix(1)\nm0 &lt;- as.matrix(570)\nC0 &lt;- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices &lt;- set_up_dlm_matrices(FF,GG,VV,WW)\ninitial_states &lt;- set_up_initial_states(m0, C0)\n\n## filtering\nresults_filtered &lt;- forward_filter(data, matrices, \n                                   initial_states)\n\nForward filtering is completed!\n\nci_filtered&lt;-get_credible_interval(results_filtered$mt,\n                                   results_filtered$Ct)\n## smoothing\nresults_smoothed &lt;- backward_smoothing(data, matrices, \n                                       results_filtered)\n\nBackward smoothing is completed!\n\nci_smoothed &lt;- get_credible_interval(results_smoothed$mnt, \n                                     results_smoothed$Cnt)\n\n\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\n\nplot(index, LakeHuron, main = \"Lake Huron Level \",type='l',\n     xlab=\"time\",ylab=\"level in feet\",lty=3,ylim=c(575,583))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered$mt, type='l', \n      col='red',lwd=2)\nlines(index_filt, ci_filtered[,1], type='l', col='red',lty=2)\nlines(index_filt, ci_filtered[,2], type='l', col='red',lty=2)\n\nlines(index_filt, results_smoothed$mnt, type='l', \n      col='blue',lwd=2)\nlines(index_filt, ci_smoothed[,1], type='l', col='blue',lty=2)\nlines(index_filt, ci_smoothed[,2], type='l', col='blue',lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"smoothed\"),\n       col = c(\"red\", \"blue\"), lty=c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#sec-second-order-polynomial",
    "href": "C4-L07.html#sec-second-order-polynomial",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.8 Second order polynomial: Filtering and smoothing example 🎥",
    "text": "97.8 Second order polynomial: Filtering and smoothing example 🎥\nIn this video walk through the code provided in the section below the comment\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\nWe now consider another example where instead of fitting a first order polynomial we’re fitting a second order polynomial DLM. So I just want to show you how to set up the structure of the model in a case in which you have a state parameter vector. That is of dimension larger than one in this particular case we have a bivariate state parameter vector. So once again we are going to source this file that has all the DLM functions for the case in which the F, G, V and W are known. So we’re just assuming that this is the case and then we’re assuming that F, G, V and W are constant over time in these examples. So we just I’m going to use a new data set which is also data set available in R this data set corresponds to the atmospheric CO2 concentrations in parts per million in the location of Mauna Loa. And this is monthly data so I’m just plotting the data here. If you look at the data you can see that it has two important features. One of them is an increasing trend as the time increases the concentration increases. And then the other very specific feature that you can see in this data set is this seasonal behavior. So right now what I’m going to do with this example is we are going to ignore the seasonal behavior, and we are going to try to fit the model that captures the linear increasing trend using a second order polynomial model.\nSo I’m going to just specify everything here. We are going to use the entire data set here. We’re going to analyze the entire data. We are going to read in this into a list and then we’re going to set up the DLM in matrices. So here because the model it’s a second order polynomial we are going to have a state vector. That is of dimension two the F matrix is going to be, so it’s a vector that has 1 in the first entry and 0 in the second one. And then G is this upper triangular matrix that has 1s in the diagonal and 1 above the diagonal as well. So the two parameters that we’re fitting here one of them you can view the two components in the state of theta_t parameter vector. The first component corresponds to the baseline of the level and then the second component corresponds to the rate of growth in that level that we are fitting. So just defining the F and G like that. And then V the observational variance I’m just going to set it at 10. You can play with different numbers here, and the W is a diagonal matrix with .0001 in each of the elements in the diagonal. So these models are not as flexible as the ones that we are going to consider later. So in particular we are using an assumption that the two components in the state sector are independent over time which is usually not very realistic. And we can consider more flexible models later but just to show you here how to fit these models, for the prior distribution I have again two components. So I’m going to say that a priori my baseline is 315 parts per million. And then for the second, the rate of growth is going to be 0 a priori. And then I have C0 which is this 10 times the diagonal of dimension 2 so this is an identity matrix. So is we have a diagonal with the elements in the diagonal equal to 10. So we wrap up all the DLM matrices with the functions that we defined before. And then we proceed with the filtering equations just using the forward filter function. We can obtain credible intervals for the expected value of y_t via this filtering equations.\nSo the reason why I’m calling it the expected value of y_t via filtering it’s just the first component of the say that theta_t vectors. So that corresponds to the level of the series, the expected value of that y_t. And then, I can compute the smoothing equations using the backward smoothing. And again I have to pass the data, the structure of the model in terms of the matrices and the results that I obtained via the filtering equations. And I can compute credible intervals for this expected value via smoothing and as we mentioned before, it has the same structure the smoothing and the filtering is just that, we call the mean and the variance mt and Ct. In the case of the filtering equations for the smoothing equations we just call them mnt and Cnt. So now we can plot all the results here. I’m just going to plot the results that correspond to the smoothing distributions just for you to see. And we can see here that is this trend that is estimated here is capturing the structure of this linear increasing trend. And you can play with different values of the signal to noise ratio. So different values of the V and the W. And if you change the values so that there is more or less signal to noise ratio, you will see that you will capture more of the seasonal structure and less of this linear trend structure. If you were to change those values. So if I go back a little bit here you can see that I have a very low signal to noise ratio and I picked this on purpose, because I didn’t want to capture any of the seasonal behavior that I observe in the series through these parameters. So I’m assuming that a lot of the variation that I see now I’m just keeping it in the noise. Just because I want to just get a very smooth estimate for this linear trend through a second order polynomial model. In practice what we’re going to do later is we really want to construct a model in which we have a component for the linear trend using the second order polynomial model. And then we add another component that will allow us to capture also the seasonal behavior that we observe in this series using a Fourier component model. So we will illustrate that later, in a separate example here is just again to show you how to use the code for specifying a second order polynomial.",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#using-the-dlm-package-in-r",
    "href": "C4-L07.html#using-the-dlm-package-in-r",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.9 Using the dlm package in R 🎥",
    "text": "97.9 Using the dlm package in R 🎥\nThe dlm package in R is a powerful tool for working with dynamic linear models. The package provides a wide range of functions for filtering, smoothing, forecasting, and parameter estimation in DLMs. In this video, we walk through the code provided in Listing 97.7.\n\n\n\n\n\n\nNoteVideo Transcript\n\n\n\n\n\n\nSo here I’m going to show you how to use the dlm package to fit these dynamic linear models as well. So the dlm is package that is available from Cran. And it allows you to compute the filtering smoothing and forecasting equations for dynamic linear models. So I’m just going to show you how to do the same thing we’ve been doing with the code that I provided just using the dlm package. So I’m going to just run here the first examples that we ran. And I’m going to show you how to do the same again. So here, I’m just going through the Lake Huron data. So just setting up every_thing as we did before. And then going through the filtering and smoothing equations. And so we can now plot the results and just want to have all the results here. So we have the red line corresponds to the posterior mean for the distribution of \\theta_t given the Dt using a first order polynomial model to fit the data. And the blue line corresponds to the smoothing mean. So the mean of the posterior distribution of the smoothing equations here. So now we can look at how to fit this with the dlm package. So you have to call, install the package if you don’t have it installed. And then just call that library once you have installed the package. And the dlm package has a different set of functions to construct the model first.\nSo I’m going to use the function that is called the dlmModPoly, which allows you to fit polynomial models. So it constructs the polynomial models. The default function as you can see here is a function in that assumes that the polynomial model is of order 2. So here I want to polynomial model of all the 1. And then I’m going to specify the variance at the observational level, which is called dV in that package. dW is the variance at the evolution level. And then I have my prior mean for theta and the prior variance. I’m just using exactly the same prior distribution. And the package provides two functions of the dlm filter function allows you to providing the data. And the model that you just define computes the filtering recursions here. And then there is another function that is called the dlmSmooth that you essentially pass the results of the filtering equations. And then you obtain the smoothing distributions. So we’re just going to do that. And now I’m going to plot the results that I obtained from those filtering equations. One thing that you can see here, if I do names of, let’s say results_filter_dlm. You can see that the way in which the dlm functions from the dlm package keep the results. It has a particular format. So in the case of the dlm package, you’re going to have the information about what model you fitted. Then you have the mean of theta_t given Dt is kept in this m object. And then you have a is the prior mean of theta_t, given the t -1. And then f is the mean of the one step ahead forecast distribution. And then you have these U.C, D.C, U.R, D.R, those are just decompositions of the C variance matrix. So each of the Cs at time t. And then if you have also the composition of the R matrices. So the model, the way in which the functions are implemented in this dlm package. Assume used an SVD decomposition of all the matrices. So you have to keep in mind if you’re going to recover the structure here for the different components in the model. You have to keep this in mind. So for the filtering results, this is the structure. If you do names of the results, smooth, with the dlm package. You’re going to have again, here is the mean here that is called S and then you have the decomposition of the matrix as well. So, I’m just going to plot now for the filtering results. I’m just going to plot the mean here. And then for the smoothing distribution, I’m also going to plot that means. In this case, we’re working with the first order polynomial. So the dimension of the state vector is 1. So you can see that we obtain exactly the same results. And you can compare them numerically. The upper plot corresponds to the results we get with the code that we’ve been using. And the second block corresponds to just using the code from the dlm package. We can also run the example with the second order polynomial. So again, if I use the specification of the model that we use before with the functions that we described. I can keep my results there. And if I use the dlm package, I can use again, this is a second order polynomial model. I say that the order of the polynomial is 2, I use this dlmModPoly function. I specify the observational variance, the system variance m0 and C0. So I’m using exactly the same priors in this case. And then I use the dlm filter function and the dlm smooth just to compute the moments of the filtering and smoothing distributions. And then I can plot every_thing here. We are plotting just the first component here. The posterior distribution for the first component of the theta vector. Which also corresponds to the expected value of the y_t. And then if I do the same with the dlm package, you can see that you obtain the same results. So again, the upper plot corresponds to the results that we get from the code that we’ve been using. And then the bottom plot corresponds to the results that we get from the dlm package. So I just wanted to illustrate this. You’re welcome to always use the dlm package. Just keep in mind the structure in which the matrices are kept is a little bit different than what we have been discussing. Because the dlm package uses and SVD decomposition of the covariance matrices and keeps every_thing like that. So there are some differences. But you can also use this package to obtain inference in the case of dynamic linear models.",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L07.html#sec-dlm-package",
    "href": "C4-L07.html#sec-dlm-package",
    "title": "96  Bayesian Inference in the NDLM: Part 1 M3L7",
    "section": "97.10 Code: Using the dlm package 📖 ℛ",
    "text": "97.10 Code: Using the dlm package 📖 ℛ\n\n\n\n\nListing 97.1: Using the dlm package for dynamic linear models\n\n\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices &lt;- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states &lt;- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter &lt;- function(data, matrices, initial_states){\n  ## retrieve dataset\n  y_t &lt;- data$y_t\n  T &lt;- length(y_t)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF &lt;- matrices$FF  \n  GG &lt;- matrices$GG\n  VV &lt;- matrices$VV\n  WW &lt;- matrices$WW\n  \n  ## retrieve initial states\n  m0 &lt;- initial_states$m0\n  C0 &lt;- initial_states$C0\n  \n  ## create placeholder for results\n  d &lt;- dim(GG)[1]\n  at &lt;- matrix(NA, nrow=T, ncol=d)\n  Rt &lt;- array(NA, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(NA, nrow=T, ncol=d)\n  Ct &lt;- array(NA, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  \n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(m0)\n      Rt[, , i] &lt;- GG %*% C0 %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] &lt;- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(FF) %*% (at[i, ]) \n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% FF / Qt[i]\n    et[i] &lt;- y_t[i] - ft[i]\n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[,,i] &lt;- 0.5*Ct[,,i] + 0.5*t(Ct[,,i]) \n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = Rt, \n              ft = ft, Qt = Qt))\n}\n\nforecast_function &lt;- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  WW &lt;- matrices$WW\n  VV &lt;- matrices$VV\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] &lt;- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] &lt;- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval &lt;- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile &lt;- qnorm(quantile)\n  bound &lt;- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] &lt;- mu + z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] &lt;- mu + z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n### smoothing equations ###\nbackward_smoothing &lt;- function(data, matrices, \n                               posterior_states){\n  ## retrieve data \n  y_t &lt;- data$y_t\n  T &lt;- length(y_t) \n  \n  ## retrieve matrices\n  FF &lt;- matrices$FF\n  GG &lt;- matrices$GG\n  \n  ## retrieve matrices\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  at &lt;- posterior_states$at\n  Rt &lt;- posterior_states$Rt\n  \n  ## create placeholder for posterior moments \n  mnt &lt;- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt &lt;- array(NA, dim = dim(Ct))\n  fnt &lt;- numeric(T)\n  Qnt &lt;- numeric(T)\n  for(i in T:1){\n    # moments for the distributions of the state vector given D_T\n    if(i == T){\n      mnt[i, ] &lt;- mt[i, ]\n      Cnt[, , i] &lt;- Ct[, , i]\n      Cnt[, , i] &lt;- 0.5*Cnt[, , i] + 0.5*t(Cnt[, , i]) \n    }else{\n      inv_Rtp1&lt;-solve(Rt[,,i+1])\n      Bt &lt;- Ct[, , i] %*% t(GG) %*% inv_Rtp1\n      mnt[i, ] &lt;- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n      Cnt[, , i] &lt;- Ct[, , i] + Bt %*% (Cnt[, , i + 1] - Rt[, , i+1]) %*% t(Bt)\n      Cnt[,,i] &lt;- 0.5*Cnt[,,i] + 0.5*t(Cnt[,,i]) \n    }\n    # moments for the smoothed distribution of the mean response of the series\n    fnt[i] &lt;- t(FF) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] &lt;- t(FF) %*% t(Cnt[, , i]) %*% FF\n  }\n  cat(\"Backward smoothing is completed!\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n\n\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron) # 98 observations total \n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 97.2: Using the dlm package for dynamic linear models\n\n\nk=4\nT=length(LakeHuron)-k # We take the first \n                      # 94 observations only as our data\nts_data=LakeHuron[1:T]\nts_validation_data &lt;- LakeHuron[(T+1):98]\n\ndata &lt;- list(y_t = ts_data)\n\n## set up dlm matrices\nGG &lt;- as.matrix(1)\nFF &lt;- as.matrix(1)\nVV &lt;- as.matrix(1)\nWW &lt;- as.matrix(1)\nm0 &lt;- as.matrix(570)\nC0 &lt;- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices &lt;- set_up_dlm_matrices(FF, GG, VV, WW)\ninitial_states &lt;- set_up_initial_states(m0, C0)\n\n## filtering and smoothing \nresults_filtered &lt;- forward_filter(data, matrices, \n                                   initial_states)\n\n\n\n\nForward filtering is completed!\n\n\n\n\nListing 97.3: Using the dlm package for dynamic linear models\n\n\nresults_smoothed &lt;- backward_smoothing(data, matrices, \n                                       results_filtered)\n\n\n\n\nBackward smoothing is completed!\n\n\n\n\nListing 97.4: Using the dlm package for dynamic linear models\n\n\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\n\n\npar(mfrow=c(2,1), mar = c(3, 4, 2, 1))\nplot(index, LakeHuron, main = \"Lake Huron Level \",type='l',\n     xlab=\"time\",ylab=\"feet\",lty=3,ylim=c(575,583))\npoints(index,LakeHuron,pch=20)\nlines(index_filt, results_filtered$mt, type='l', \n      col='red',lwd=2)\nlines(index_filt, results_smoothed$mnt, type='l', \n      col='blue',lwd=2)\n\n\n# Now let's look at the DLM package \nlibrary(dlm)\nmodel=dlmModPoly(order=1,dV=1,dW=1,m0=570,C0=1e4)\nresults_filtered_dlm=dlmFilter(LakeHuron[1:T],model)\nresults_smoothed_dlm=dlmSmooth(results_filtered_dlm)\n\nplot(index_filt, LakeHuron[1:T], ylab = \"level\", \n     main = \"Lake Huron Level\",\n     type='l', xlab=\"time\",lty=3,ylim=c(575,583))\npoints(index_filt,LakeHuron[1:T],pch=20)\nlines(index_filt,results_filtered_dlm$m[-1],col='red',lwd=2)\nlines(index_filt,results_smoothed_dlm$s[-1],col='blue',lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 97.5: Using the dlm package for dynamic linear models\n\n\n# Similarly, for the second order polynomial and the co2 data:\nT=length(co2)\ndata=list(y_t = co2)\n\nFF &lt;- (as.matrix(c(1,0)))\nGG &lt;- matrix(c(1,1,0,1),ncol=2,byrow=T)\nVV &lt;- as.matrix(200)\nWW &lt;- 0.01*diag(2)\nm0 &lt;- t(as.matrix(c(320,0)))\nC0 &lt;- 10*diag(2)\n\n## wrap up all matrices and initial values\nmatrices &lt;- set_up_dlm_matrices(FF,GG, VV, WW)\ninitial_states &lt;- set_up_initial_states(m0, C0)\n\n## filtering and smoothing \nresults_filtered &lt;- forward_filter(data, matrices, \n                                   initial_states)\n\n\n\n\nForward filtering is completed!\n\n\n\n\nListing 97.6: Using the dlm package for dynamic linear models\n\n\nresults_smoothed &lt;- backward_smoothing(data, matrices, \n                                       results_filtered)\n\n\n\n\nBackward smoothing is completed!\n\n\n\n\nListing 97.7: Using the dlm package for dynamic linear models\n\n\n#### Now, using the DLM package: \nmodel=dlmModPoly(order=2,dV=200,dW=0.01*rep(1,2),\n                 m0=c(320,0),C0=10*diag(2))\n# filtering and smoothing \nresults_filtered_dlm=dlmFilter(data$y_t,model)\nresults_smoothed_dlm=dlmSmooth(results_filtered_dlm)\n\npar(mfrow=c(2,1), mar = c(3, 4, 2, 1))\nplot(as.vector(time(co2)),co2,type='l',xlab=\"time\",\n     ylim=c(300,380))\nlines(as.vector(time(co2)),results_filtered$mt[,1],\n      col='red',lwd=2)\nlines(as.vector(time(co2)),results_smoothed$mnt[,1],\n      col='blue',lwd=2)\n\nplot(as.vector(time(co2)),co2,type='l',xlab=\"time\",\n     ylim=c(300,380))\nlines(as.vector(time(co2)),results_filtered_dlm$m[-1,1],\n      col='red',lwd=2)\nlines(as.vector(time(co2)),results_smoothed_dlm$s[-1,1],\n      col='blue',lwd=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 1 M3L7</span>"
    ]
  },
  {
    "objectID": "C4-L08.html",
    "href": "C4-L08.html",
    "title": "97  Seasonal NDLMs M4L8",
    "section": "",
    "text": "97.1 Seasonal NDLMs",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#seasonal-ndlms",
    "href": "C4-L08.html#seasonal-ndlms",
    "title": "97  Seasonal NDLMs M4L8",
    "section": "",
    "text": "NoteLearning Objectives\n\n\n\n\n\n\nUse R for analysis and forecasting of time series using the NDLM (cases of known or unknown observational variance and unknown system variance specified using a discount factor)\nDerive the equations to obtain posterior inference and forecasting in the NDLM with unknown observational variance and system variance specified via discount factors\nDefine seasonal NDLMs\nApply the NDLM superposition principle and explain the role of the forecast function",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#fourier-representation",
    "href": "C4-L08.html#fourier-representation",
    "title": "97  Seasonal NDLMs M4L8",
    "section": "97.2 Fourier representation 🎥",
    "text": "97.2 Fourier representation 🎥\nTranscript:\n\nI will now describe how to incorporate seasonal components in a normal dynamic linear model. What we will do is we will first talk about the so-called single Fourier component representation . Just in case you have a single frequency and how to incorporate that single frequency in your model for the seasonality. Then using the superposition principle, you can incorporate several frequencies or a single frequency and the corresponding harmonics in your model.single Fourier component representation\nThere are other seasonal representations as well clarification needed. We will focus on the Fourier representation as is is flexible without needing too many parameters. E.g. if you want to consider, a fundamental frequency but you don’t want all the harmonics of that frequency. The Fourier representation, if you happen to have a single frequency.\nWe will discuss two cases with different component representations:\n\n\\omega \\in (0,\\pi)\n\\omega = \\pi \\implies \\{ 1,1,\\cdot, \\cdot\\}\n\nIn the case of any frequency \\omega \\in (0,\\pi), we will have a DLM that has this structure:\n\n\\{ \\underbrace {E_2}_{F},  \\underbrace {J_2(1,\\omega)}_{G}, \\underbrace{\\cdot}_{v_t}, \\underbrace{\\cdot}_{W_t}\\}\n\\tag{97.1}\nWe will have the F vector the 2-dimensional vector: \nE_2=(1,0)'\n\\tag{97.2}\nAs usual and the G matrix will be the 2 by 2 matrix:\n\nJ_2(1, \\omega) =\n\\begin{pmatrix}\n\\cos(\\omega) & \\sin(\\omega) \\\\\n-\\sin(\\omega) & \\cos(\\omega)\n\\end{pmatrix},\n\\tag{97.3}\nwhere \\omega is the frequency that we are considering.\nSince this is a 2 by 2 matrix the our state parameter vector will also be a vector of dimension 2.\nIf we think about the forecast function, f_t(h) h-steps ahead, (you are at time t and you want to look for h steps ahead).\nLet’s recall: the way we work with this is F* G^h * a_t\ngoing to be your E_2', then you have to take this G matrix, which is just this J_2(1,\\omega)^h, and then you have a vector, I’m going to call a_t and b_t, which is just going to be this vector value of your Theta t vector given the information up to the time t. It’s going to have two components, I’m just going to generically call them a_t and b_t. When you take this to the power of h using just trigonometric results, you’re going to get that J_2(1,\\omega)^h, is just going to give you cosine of Omega h sine of Omega h minus sine of Omega h cosine of Omega h. When you look at this expression, you get something that looks like this, and then you have, again, times these a_t, b_t.\n\n\\begin{aligned}\nf_t(h) &= E_2' [J_2(1, \\omega)]^h \\underbrace{\\begin{pmatrix} a_t \\\\\nb_t \\end{pmatrix}}_{\\mathbb{E}[\\theta\\mid \\mathcal{D}]} \\\\\n&= (1,0) \\begin{pmatrix} \\cos(\\omega h) & \\sin(\\omega h) \\\\\n-\\sin(\\omega h) & \\cos(\\omega h) \\end{pmatrix} \\begin{pmatrix} a_t \\\\\nb_t \\end{pmatrix} \\\\\n&= a_t \\cos(\\omega h) + b_t \\sin(\\omega h) \\\\\n&= A_t \\cos(\\omega h + B_t).\n\\end{aligned}\n\\tag{97.4}\n\nYou’re going to have the cosine and sine only multiplied by this. In the end, you’re going to have something that looks like this.\nYou have this sinusoidal form with the period Omega in your forecast function. You can also write this down in terms of an amplitude that I’m going to call A_t and then a phase that is B_t. Here again, you have your periodicity that appears in this cosine wave. This is again for the case in which you have a single frequency and the frequencies in this range. There was a second case that I mentioned, and that case is the case in which the Omega is exactly Pi. In this case, your Fourier representation is going to be your model that has a state vector that is just one dimensional. In the case where Omega is between zero and Pi, you have a two-dimensional state, vector here you’re going to have a one-dimensional state vector.\nThis is going to be your F and your G. Then you have again whatever you want to put here as your v_t and W_t. This gives me, if I think about the forecast function, h steps ahead is just going to be something that has the form -1^h \\times a_t. Now I have a single component here, is uni-dimensional. This is going to have an oscillatory behavior between a_t and -a_t if I were to look h steps ahead forward when I’m at time t. These two forms give me the single component Fourier representation and using the superposition principle, we will see that we can combine a single frequency and the corresponding harmonics or several different frequencies just using the superposition principle in the normal dynamic linear model. You can also incorporate more than one component in a full Fourier representation. Usually the way this works is you have a fundamental period, let’s say p. For example, if you are recording monthly data, p could be 12 and then you are going to incorporate in the model the fundamental frequency, and then all the harmonics that go with that fundamental frequency related to the period p.\n\n\n\n\n\nslide 1\n\n\nHere p, is the period and in this case, we are going to discuss essentially two different situations. One is when p is an odd number, the other one is when p is an even number. Let’s begin with the case of p is odd and in this particular scenario, we can write down p as 2 times m minus 1 for some value of m. This gives me a period that is odd. How many frequencies I’m going to incorporate in this model? I’m going to be able to write down \\omega_j = 2 \\pi \\times j / p, which is the fundamental period. j here goes from one all the way to m minus 1. Now we can use the superposition principle thinking we have a component DLM representation for each of these frequencies. They are all going to be between 0 and Pi. For each of them I’m going to have that two-dimensional DLM representation in terms of the state vector and then I can use the superposition principle to concatenate them all and get a model that has all these frequencies, the one related to the fundamental period and all the harmonics for that. Again, if I think about what is my F and my G here, I’m not writing down the t because both F and G are going to be constant over time. So my F is going to be again, I concatenate as many E_2 as I have frequencies in here. I’m going to have E_2 transpose and so on and I’m going to have m minus one of those. Times 2 gives me the dimension of \\theta_t. The vector here is 2 times m minus 1 dimensional vector.\nMy G is going to have that block diagonal structure where we are going to just have all those J_{2,1} \\omega_1, all the way down to the last harmonic. Each of these blocks is a two-by-two matrix and I’m going to put them together in a block diagonal form. This gives me the representation when the period is odd, what is the structure of the forecast function? Again, using the superposition principle, the forecast function is going to be just the sum of m minus 1 components, where each of those components is going to have an individual forecast function that has that cosine wave representation that we discussed before. Again, if I think about the forecast function at time t h steps ahead, I will be able to write it down like this.\nThis should be a B. B_{t,j}. Again here, I have an amplitude for each of the components and a phase for each of the components so it depends on time but does not depend on h. The h enters here, and this is my forecast function. In the case of P even the situation is slightly different. But again, it’s the same in terms of using the superposition principle. In this case, we can write down P as 2 times m because it’s an even number. Now I can write down these Omega j’s as a function of the fundamental period. Again, this goes from 1 up to m minus 1. But there is a last frequency here. When j is equal to m, this simplifies to be the Nyquist frequency. In this case, I have my Omega is equal to Pi. In this particular case, when I concatenate everything, I’m going to have again an F and a G that look like this. Once again, I concatenate all of these up to the component m minus 1. Then I have this 1 for the last frequency. Then my G is going to be the block diagonal.\nFor the last frequency I have that minus 1. This determines the dimension of the state vector, in this case I’m going to have 2 times m minus 1 plus 1.\nMy f function, my forecast function, is again a function of the number of steps ahead. I’m going to have the same structure I had before for the m minus 1 components. Then I have to add one more component that corresponds to the frequency Pi. This one appears with the power of h. As you can see, I’m using once again the superposition principle to go from component representation to the full Fourier representation. In practice, once we set the period, we can use a model that has the fundamental period and all the harmonics related to that fundamental period. We could also use, discard some of those harmonics and use a subset of them. This is one of the things that the Fourier representation allows. It allows you to be flexible in terms of how many components you want to add in this model. There are other representations that are also used in practice. One of them is the seasonal factors representation. In that case, you’re going to have a model in which the state vector has dimension p for a given period. It uses a G matrix that is a permutation matrix. There is a correspondence between this parameterization using the Fourier representation and that other parameterization. If you want to use that parameterization, the way to interpret the components of this state vector, since you have P of those, is going to be a representation in terms of factors. For example, if you think about monthly data, you will have the say January factor, February factor, March factor, and so on. You could think about those effects and do a correspondence with this particular model. We will always work in this class with these representations because it’s more flexible. But again, you can go back and forth between one and the other.\n\n\n\n\n\nslide 2",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#fourier-representation-example-1",
    "href": "C4-L08.html#fourier-representation-example-1",
    "title": "97  Seasonal NDLMs M4L8",
    "section": "97.3 Fourier Representation: Example 1 📖",
    "text": "97.3 Fourier Representation: Example 1 📖\n\n97.3.1 Seasonal Models\nExample: Full Fourier Model with p=5\nIn this case the Fourier frequencies are\n\nω_1 = 2π/5 and\nω_2 = 4π/5 and so\np = 2 × 3 − 1. Then,\nm = 3 and\n\\theta_t = (\\theta_{t,1}, \\ldots , \\theta_{t,4})′,\nF = (1, 0, 1, 0),\nG is given by:\n\n\nG = \\begin{bmatrix}\n\\cos(2\\pi/5) & \\sin(2\\pi/5) & 0 & 0 \\\\\n\\cos(4\\pi/5) & \\sin(4\\pi/5) & 0 & 0 \\\\\n0 & 0 & \\cos(2\\pi/5) & \\sin(2\\pi/5) \\\\\n0 & 0 & \\cos(4\\pi/5) & \\sin(4\\pi/5) \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n\nand the forecast function is:\n\nf_t(h) = A_{t,1} \\cos(2\\pi h/5 + \\gamma_t) + A_{t,2} \\cos(4\\pi h /5 + \\gamma_{t,2}) \\qquad\n\\tag{97.5}",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#building-ndlms-with-multiple-components-examples",
    "href": "C4-L08.html#building-ndlms-with-multiple-components-examples",
    "title": "97  Seasonal NDLMs M4L8",
    "section": "97.4 Building NDLMs with multiple components: Examples 🎥",
    "text": "97.4 Building NDLMs with multiple components: Examples 🎥\n\n\n\n\ntwo component model\n\n\nIn this second example, we are going to have two components; a linear trend plus a seasonal component where the fundamental period is four. The way to build this model, again, is using the superposition principle.\nFirst we need to think “what structure do we need, to get a linear trend in the forecast function?”\nThe linear trend is a linear function on the number of steps ahead.\nWhenever you have that structure, you will get a DLM that is the so-called polynomial model of order 2. So let’s discuss first the linear. Let’s say the linear trend part, and in this case, we have an F and a G, I’m going to call them 1, F_1 and G_1 to denote that this is the first component in the model.\nF_1 is just going to be 1, 0 transpose, and the G_1 is that upper triangular matrix, it’s a 2 by 2 matrix that has 1, 1 in the first row, 0, 1 in the second row, so this gives me a linear trend.\nMy forecast function, let’s call it f_{1,t} in terms of the number of steps ahead is just a linear function on h, is a linear polynomial order 1. Let’s say it’s a constant of K but depends on t0 plus K_{t_1}^h. This is the structure of the first component. Then I have to think about the seasonal component with period of four. If we are going to incorporate all the harmonics, we have to think again, is this an even period or a not period? In this example, this is an even period. I can write p, which is 4, as 2 times 2, so this gives me that m. I’m going to have one frequency, the first one, Omega 1, is related to the fundamental period of 4, so is 2 Pi over 4, which I can simplify and write down this as Pi over 2. This is the first frequency. The last one is going to correspond to the Nyquist.\nWe could obtain that doing 4Pi over 4, which is just Pi. As you remember, this component is going to require a two-dimensional DLM component model, this one is going to require a one-dimensional DLM component model in terms of the dimension here is the dimension of the state vectors. When we build this concatenating these components, we are going to have, again, let’s call it F_2 and G_2 for this particular component. I had called this here a, let’s call this b. My F_2 has that E_2 transpose and a 1, which gives me just 1, 0, 1. My G matrix is going to be a 3 by 3 matrix. The first component is\nthe component associated to that fundamental period. It’s a block diagonal again, and I’m going to have that J_2, 1 Omega 1, and then I have my minus 1 here. What this means is if I write this down as a matrix, let me write it here, G_2 is going to be cosine of that Pi halves,\nand then I have zeros here, I have my minus 1 here, 0, and 0. I can further simplify these to have this structure. The cosine of Pi halves is 0, the sine is 1, so I can write this down as 0, 1, 0, minus 1, 0, 0, and 0, 0 minus 1. Now if I want to go back to just having a model that has both components, I use the superposition principle again and combine this component with this component. The linear plus seasonal\nis a model that is going to have the representation F, G, with F is going to be just concatenate F_1 and F_2. G now has that block diagonal form again.\nIf I look at what I have, I have this block that is a 2 by 2, this block that is a 3 by 3. Therefore my model is going to be a five-dimensional model in terms of the state parameter vector, so this G is a 5 by 5, and this one is also a five-dimensional vector. Finally, if I think about the forecast function in this case, if I call here the forecast function f_{2,t} for the component that is seasonal, I’m going to have my A_t1 cosine of Pi halves h plus B_{t,1}, and then I have my A_{t,2} minus 1^h. My forecast function for the final model is going to be just the sum of these two components.\nYou can see how I can now put together all these blocks, so I have a block that is seasonal and a block that is a linear polynomial model, and I can put them together in a single model just to create a more flexible structure. You could add regression components, you could add autoregressive components and put together as many components as you need for the forecast function to have the form that you expect it to have. All of these models are using, again, the superposition principle and the fact that we’re working with a linear and Gaussian structure in terms of doing the posterior inference later.",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#summary-dlm-fourier-representation",
    "href": "C4-L08.html#summary-dlm-fourier-representation",
    "title": "97  Seasonal NDLMs M4L8",
    "section": "97.5 Summary: DLM Fourier representation 📖",
    "text": "97.5 Summary: DLM Fourier representation 📖\n\n97.5.1 Seasonal Models: Fourier Representation\nFor any frequency \\omega \\in (0, \\pi), a model of the form \\{E_2, J_2(1, \\omega), \\cdot, \\cdot\\} with a 2-dimensional state vector \\theta_t = (\\theta_{t,1}, \\theta_{t,2})' and\n\nJ_2(1, \\omega) =\n\\begin{pmatrix}\n\\cos(\\omega) & \\sin(\\omega) \\\\\n-\\sin(\\omega) & \\cos(\\omega)\n\\end{pmatrix},\n\nhas a forecast function\n\n\\begin{aligned}\nf_t(h) &= (1, 0) J_2^h(1, \\omega) (a_t, b_t) \\\\\n       &= a_t \\cos(\\omega h) + b_t \\sin(\\omega h) \\\\\n       &= A_t \\cos(\\omega h + B_t).\n\\end{aligned}\n\nFor \\omega = \\pi, the NDLM is \\{1, -1, \\cdot, \\cdot\\} and has a forecast function of the form\n\nf_t(h) = (-1)^h m_t\n\nThese are component Fourier models. Now, for a given period p, we can build a model that contains components for the fundamental period and all the harmonics of such a period using the superposition principle as follows:\n\n\n97.5.2 Case: p = 2m - 1 (odd)\nLet \\omega_j = 2\\pi j / p for j = 1 : (m - 1), F a (p - 1)-dimensional vector, or equivalently, a 2(m - 1)-dimensional vector, and G a (p - 1) \\times (p - 1) matrix with F = (E_2', E_2', \\dots, E_2')',\n\nG = \\text{blockdiag}[J_2(1, \\omega_1), \\dots, J_2(1, \\omega_{m-1})].\n\n\n\n97.5.3 Case: p = 2m (even)\nIn this case, F is again a (p - 1)-dimensional vector (or equivalently a (2m - 1)-dimensional vector), and G is a (p - 1) \\times (p - 1) matrix such that F = (E_2', \\dots, E_2', 1)' and\n\nG = \\text{blockdiag}[J_2(1, \\omega_1), \\dots, J_2(1, \\omega_{m-1}), -1].\n\nIn both cases, the forecast function has the general form:\n\nf_t(h) = \\sum_{j=1}^{m-1} A_{t,j} \\cos(\\omega_j h + \\gamma_{t,j}) + (-1)^h A_{t,m},\n\nwith A_{t,m} = 0 if p is odd.",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#examples",
    "href": "C4-L08.html#examples",
    "title": "97  Seasonal NDLMs M4L8",
    "section": "97.6 Examples",
    "text": "97.6 Examples\n\n97.6.1 Fourier Representation, p = 12:\nIn this case, p = 2 \\times 6 so \\theta_t is an 11-dimensional state vector,\n\nF = (1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1)',\n\nthe Fourier frequencies are \\omega_1 = 2\\pi/12, \\omega_2 = 4\\pi/12 = 2\\pi/6, \\omega_3 = 6\\pi/12 = 2\\pi/4, \\omega_4 = 8\\pi/12 = 2\\pi/3, \\omega_5 = 10\\pi/12 = 5\\pi/6, and \\omega_6 = 12\\pi/12 = \\pi (the Nyquist frequency).\n\nG = \\text{blockdiag}(J_2(1, \\omega_1), \\dots, J_2(1, \\omega_5), 1)\n\nand the forecast function is given by:\n\nf_t(h) = \\sum_{j=1}^{5} A_{t,j} \\cos(2\\pi j / 12 + \\gamma_{t,j}) + (-1)^h A_{t,6}.\n\n\n\n97.6.2 Linear Trend + Seasonal Component with p = 4\nWe can use the superposition principle to build more sophisticated models. For instance, assume that we want a model with the following 2 components:\n\nLinear trend: \\{F_1, G_1, \\cdot, \\cdot\\} with F_1 = (1, 0)',\n\n\nG_1 = J_2(1) =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n\n\nFull seasonal model with p = 4: \\{F_2, G_2, \\cdot, \\cdot\\}, p = 2 \\times 2 so m = 2 and \\omega = 2\\pi / 4 = \\pi / 2,\n\n\nF_2 = (1, 0, 1)',\n\nand\n\nG_2 =\n\\begin{pmatrix}\n\\cos(\\pi / 2) & \\sin(\\pi / 2) & 0 \\\\\n-\\sin(\\pi / 2) & \\cos(\\pi / 2) & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 1 & 0 \\\\\n-1 & 0 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}.\n\nThe resulting DLM is a 5-dimensional model \\{F, G, \\cdot, \\cdot\\} with\n\nF = (1, 0, 1, 0, 1)',\n\nand\n\nG =\n\\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & -1\n\\end{pmatrix}.\n\nThe forecast function is:\n\nf_t(h) = (k_{t,1} + k_{t,2} h) + k_{t,3} \\cos(\\pi h / 2) + k_{t,4} \\sin(\\pi h / 2) + k_{t,5} (-1)^h.",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L08.html#quiz-seasonal-models-and-superposition",
    "href": "C4-L08.html#quiz-seasonal-models-and-superposition",
    "title": "97  Seasonal NDLMs M4L8",
    "section": "97.7 Quiz: Seasonal Models and Superposition",
    "text": "97.7 Quiz: Seasonal Models and Superposition\nThis is omitted due to the Coursera honor code.",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Seasonal NDLMs M4L8</span>"
    ]
  },
  {
    "objectID": "C4-L09.html",
    "href": "C4-L09.html",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "",
    "text": "98.1 Filtering, Smoothing and Forecasting: Unknown observational variance 🎥\nIn this video we cover the following material also provided as a handout:\nInference in the NDLM with unknown but constant observational variance:\nLet v_t = v for all t, with v unknown and consider a DLM with the following structure: \n\\begin{aligned}\ny_t &= \\mathbf{F}_t' \\mathbf{\\theta}_t + \\nu_t, &\\nu_t &\\sim N (0, v)\\\\\n\\mathbf{\\theta}_t &= \\mathbf{G}_t \\mathbf{\\theta}_{t-1} + \\mathbf{\\omega}_t, & \\mathbf{\\omega}_t &\\sim N (0, v \\mathbf{W}^*_t)\n\\end{aligned}\n\\tag{98.1}\nwith conjugate prior distributions: \n(\\mathbf{\\theta}_0 \\mid D_0, v) \\sim N (\\mathbf{m}_0, v\\mathbf{C}^*_0), \\qquad (v \\mid D_0) \\sim IG(\\frac{n_0}{2}, \\frac{d_0}{2}),\n\\tag{98.2} and d_0 = n_0s_0",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#filtering-smoothing-and-forecasting-unknown-observational-variance",
    "href": "C4-L09.html#filtering-smoothing-and-forecasting-unknown-observational-variance",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "",
    "text": "98.1.1 Filtering\nAssuming (\\theta_{t-1} \\mid D_{t-1}, v) \\sim N (m_{t-1}, vC^*_{t-1}), we have the following results:\n\n(\\theta_t \\mid D_{t-1}, v) \\sim N (a_t, vR^*_t) with a_t = G_t m_{t-1} and R^*_t = G_t C^*_{t-1} G'_t + W^*_t, and unconditional on v, (\\theta_t \\mid D_{t-1}) \\sim T_{n_{t-1}} (a_t, R_t), with R_t = s_{t-1} R^*_t. The expression for s_t for all t is given below.\n(y_t \\mid D_{t-1}, v) \\sim N (f_t, vq^*_t), with f_t = F'_t a_t, and q^*_t = (1 + F'_t R^*_t F_t) and unconditional on v we have (y_t \\mid D_{t-1}) \\sim T_{n_{t-1}} (f_t, q_t), with q_t = s_{t-1} q^*_t.\n(v \\mid D_t) \\sim \\mathcal{IG}(n_t/2, s_t/2), with n_t = n_{t-1} + 1 and \ns_t = s_{t-1} + \\frac{s_{t-1}}{n_t} \\left ( \\frac{e^2_t}{q^*_t} - 1 \\right ),\n\\tag{98.3}\nwhere e_t = y_t - f_t\nθt|Dt, v) ∼ N (mt, vC∗ t ), with mt = at + Atet, and C∗ t = R∗ t − AtA′ tq∗ t . Similarly, unconditional on v we have (θt|Dt) ∼ Tnt (mt, Ct), with Ct = stC∗ t",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#summary-of-filtering-smoothing-and-forecasting-distributions-ndlm-unknown-observational-variance",
    "href": "C4-L09.html#summary-of-filtering-smoothing-and-forecasting-distributions-ndlm-unknown-observational-variance",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "98.2 Summary of Filtering, Smoothing and Forecasting Distributions, NDLM unknown observational variance 📖",
    "text": "98.2 Summary of Filtering, Smoothing and Forecasting Distributions, NDLM unknown observational variance 📖",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#specifying-the-system-covariance-matrix-via-discount-factors",
    "href": "C4-L09.html#specifying-the-system-covariance-matrix-via-discount-factors",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "98.3 Specifying the system covariance matrix via discount factors 🎥",
    "text": "98.3 Specifying the system covariance matrix via discount factors 🎥",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#ndlm-unknown-observational-variance-example",
    "href": "C4-L09.html#ndlm-unknown-observational-variance-example",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "98.4 NDLM, Unknown Observational Variance: Example 🎥",
    "text": "98.4 NDLM, Unknown Observational Variance: Example 🎥\nThis is a walk though of the R code for the example bellow.",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#rcode-ndlm-unknown-observational-variance-example",
    "href": "C4-L09.html#rcode-ndlm-unknown-observational-variance-example",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "98.5 Rcode: NDLM, Unknown Observational Variance Example 📖",
    "text": "98.5 Rcode: NDLM, Unknown Observational Variance Example 📖\nThis code allows time-varying F_t, G_t and W_t matrices and assumes an unknown but constant \\nu. It also allows the user to specify W_t using a discount factor \\delta \\in (0,1] or assume W_t known.\n\n## create list for matrices\nset_up_dlm_matrices_unknown_v &lt;- function(Ft, Gt, Wt_star){\n  if(!is.array(Gt)){\n    Stop(\"Gt and Ft should be array\")\n  }\n  if(missing(Wt_star)){\n    return(list(Ft=Ft, Gt=Gt))\n  }else{\n    return(list(Ft=Ft, Gt=Gt, Wt_star=Wt_star))\n  }\n}\n\n\n## create list for initial states\nset_up_initial_states_unknown_v &lt;- function(m0, C0_star, n0, S0){\n  return(list(m0=m0, C0_star=C0_star, n0=n0, S0=S0))\n}\n\nforward_filter_unknown_v &lt;- function(data, matrices, \n                              initial_states, delta){\n  ## retrieve dataset\n  yt &lt;- data$yt\n  T&lt;- length(yt)\n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  if(missing(delta)){\n    Wt_star &lt;- matrices$Wt_star\n  }\n  \n  ## retrieve initial state\n  m0 &lt;- initial_states$m0\n  C0_star &lt;- initial_states$C0_star\n  n0 &lt;- initial_states$n0\n  S0 &lt;- initial_states$S0\n  C0 &lt;- S0*C0_star\n  \n  ## create placeholder for results\n  d &lt;- dim(Gt)[1]\n  at &lt;- matrix(0, nrow=T, ncol=d)\n  Rt &lt;- array(0, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(0, nrow=T, ncol=d)\n  Ct &lt;- array(0, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  nt &lt;- numeric(T)\n  St &lt;- numeric(T)\n  dt &lt;- numeric(T)\n  \n  # moments of priors at t\n  for(i in 1:T){\n    if(i == 1){\n      at[i, ] &lt;- Gt[, , i] %*% m0\n      Pt &lt;- Gt[, , i] %*% C0 %*% t(Gt[, , i])\n      Pt &lt;- 0.5*Pt + 0.5*t(Pt)\n      if(missing(delta)){\n        Wt &lt;- Wt_star[, , i]*S0\n        Rt[, , i] &lt;- Pt + Wt\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }else{\n        Rt[, , i] &lt;- Pt/delta\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }\n      \n    }else{\n      at[i, ] &lt;- Gt[, , i] %*% t(mt[i-1, , drop=FALSE])\n      Pt &lt;- Gt[, , i] %*% Ct[, , i-1] %*% t(Gt[, , i])\n      if(missing(delta)){\n        Wt &lt;- Wt_star[, , i] * St[i-1]\n        Rt[, , i] &lt;- Pt + Wt\n        Rt[,,i]=0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }else{\n        Rt[, , i] &lt;- Pt/delta\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }\n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(Ft[, , i]) %*% t(at[i, , drop=FALSE]) \n    Qt[i] &lt;- t(Ft[, , i]) %*% Rt[, , i] %*% Ft[, , i] + \n      ifelse(i==1, S0, St[i-1])\n    et[i] &lt;- yt[i] - ft[i]\n    \n    nt[i] &lt;- ifelse(i==1, n0, nt[i-1]) + 1\n    St[i] &lt;- ifelse(i==1, S0, \n                    St[i-1])*(1 + 1/nt[i]*(et[i]^2/Qt[i]-1))\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% Ft[, , i] / Qt[i]\n    \n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- St[i]/ifelse(i==1, S0, \n                  St[i-1])*(Rt[, , i] - Qt[i] * At %*% t(At))\n    Ct[,,i] &lt;- 0.5*Ct[,,i]+0.5*t(Ct[,,i])\n  }\n  cat(\"Forward filtering is completed!\\n\")\n  return(list(mt = mt, Ct = Ct,  at = at, Rt = Rt, \n              ft = ft, Qt = Qt,  et = et, \n              nt = nt, St = St))\n}\n\n### smoothing function ###\nbackward_smoothing_unknown_v &lt;- function(data, matrices, \n                                posterior_states,delta){\n  ## retrieve data \n  yt &lt;- data$yt\n  T &lt;- length(yt) \n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  \n  ## retrieve matrices\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  Rt &lt;- posterior_states$Rt\n  nt &lt;- posterior_states$nt\n  St &lt;- posterior_states$St\n  at &lt;- posterior_states$at\n  \n  ## create placeholder for posterior moments \n  mnt &lt;- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt &lt;- array(NA, dim = dim(Ct))\n  fnt &lt;- numeric(T)\n  Qnt &lt;- numeric(T)\n  \n  for(i in T:1){\n    if(i == T){\n      mnt[i, ] &lt;- mt[i, ]\n      Cnt[, , i] &lt;- Ct[, , i]\n    }else{\n      if(missing(delta)){\n        inv_Rtp1 &lt;- chol2inv(chol(Rt[, , i+1]))\n        Bt &lt;- Ct[, , i] %*% t(Gt[, , i+1]) %*% inv_Rtp1\n        mnt[i, ] &lt;- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n        Cnt[, , i] &lt;- Ct[, , i] + Bt %*% (Cnt[, , i+1] - \n                                    Rt[, , i+1]) %*% t(Bt)\n        Cnt[,,i] &lt;- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])\n      }else{\n        inv_Gt &lt;- solve(Gt[, , i+1])\n        mnt[i, ] &lt;- (1-delta)*mt[i, ] + \n                delta*inv_Gt %*% t(mnt[i+1, ,drop=FALSE])\n        Cnt[, , i] &lt;- (1-delta)*Ct[, , i] + \n                delta^2*inv_Gt %*% Cnt[, , i + 1]  %*% t(inv_Gt)\n        Cnt[,,i] &lt;- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])\n      }\n    }\n    fnt[i] &lt;- t(Ft[, , i]) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] &lt;- t(Ft[, , i]) %*% t(Cnt[, , i]) %*% Ft[, , i]\n  }\n  for(i in 1:T){\n     Cnt[,,i]=St[T]*Cnt[,,i]/St[i] \n     Qnt[i]=St[T]*Qnt[i]/St[i]\n  }\n  cat(\"Backward smoothing is completed!\\n\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n\n## Forecast Distribution for k step\nforecast_function_unknown_v &lt;- function(posterior_states, k, \n                                        matrices, delta){\n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  if(missing(delta)){\n    Wt_star &lt;- matrices$Wt_star\n  }\n  \n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  St &lt;- posterior_states$St\n  at &lt;- posterior_states$at\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- Gt[, , T+i] %*% t(mt[T, , drop=FALSE])\n      \n      if(missing(delta)){\n       Rt[, , i] &lt;- Gt[, , T+i] %*% Ct[, , T] %*% \n         t(Gt[, , T+i]) + St[T]*Wt_star[, , T+i]\n      }else{\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Ct[, , T] %*% \n          t(Gt[, , T+i])/delta\n      }\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      \n    }else{\n      at[i, ] &lt;- Gt[, , T+i] %*% t(at[i-1, , drop=FALSE])\n      if(missing(delta)){\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Rt[, , i-1] %*% \n          t(Gt[, , T+i]) + St[T]*Wt_star[, , T + i]\n      }else{\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Rt[, , i-1] %*% \n          t(Gt[, , T+i])/delta\n      }\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n    }\n    \n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(Ft[, , T+i]) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(Ft[, , T+i]) %*% Rt[, , i] %*% Ft[, , T+i] + \n      St[T]\n  }\n  cat(\"Forecasting is completed!\\n\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval_unknown_v &lt;- function(ft, Qt, nt, \n                                   quantile = c(0.025, 0.975)){\n  bound &lt;- matrix(0, nrow=length(ft), ncol=2)\n\n  if ((length(nt)==1)){\n   for (t in 1:length(ft)){\n      t_quantile &lt;- qt(quantile[1], df = nt)\n      bound[t, 1] &lt;- ft[t] + t_quantile*sqrt(as.numeric(Qt[t])) \n  \n  # upper bound of 95% credible interval\n      t_quantile &lt;- qt(quantile[2], df = nt)\n      bound[t, 2] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t]))}\n  }else{\n  # lower bound of 95% credible interval\n    for (t in 1:length(ft)){\n      t_quantile &lt;- qt(quantile[1], df = nt[t])\n      bound[t, 1] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t])) \n  \n  # upper bound of 95% credible interval\n      t_quantile &lt;- qt(quantile[2], df = nt[t])\n      bound[t, 2] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t]))}\n  }\n  return(bound)\n\n}\n\n\n\n## Example: Nile River Level (in 10^8 m^3), 1871-1970 \n## Model: First order polynomial DLM\nplot(Nile) \n\n\n\n\n\n\n\nn=length(Nile) #n=100 observations \nk=5\nT=n-k\ndata_T=Nile[1:T]\ntest_data=Nile[(T+1):n]\ndata=list(yt = data_T)\n\n\n## set up matrices for first order polynomial model \nFt=array(1, dim = c(1, 1, n))\nGt=array(1, dim = c(1, 1, n))\nWt_star=array(1, dim = c(1, 1, n))\nm0=as.matrix(800)\nC0_star=as.matrix(10)\nn0=1\nS0=10\n\n## wrap up all matrices and initial values\nmatrices = set_up_dlm_matrices_unknown_v(Ft, Gt, Wt_star)\ninitial_states = set_up_initial_states_unknown_v(m0, \n                                      C0_star, n0, S0)\n\n## filtering \nresults_filtered = forward_filter_unknown_v(data, matrices, \n                                            initial_states)\n\nForward filtering is completed!\n\nci_filtered=get_credible_interval_unknown_v(results_filtered$mt, \n                                    results_filtered$Ct, \n                                     results_filtered$nt)\n\n## smoothing\nresults_smoothed=backward_smoothing_unknown_v(data, matrices, \n                                             results_filtered)\n\nBackward smoothing is completed!\n\nci_smoothed=get_credible_interval_unknown_v(results_smoothed$mnt, \n                                         results_smoothed$Cnt, \n                                         results_filtered$nt[T])\n\n## one-step ahead forecasting\nresults_forecast=forecast_function_unknown_v(results_filtered, \n                                                k,  matrices)\n\nForecasting is completed!\n\nci_forecast=get_credible_interval_unknown_v(results_forecast$ft, \n                                          results_forecast$Qt, \n                                     results_filtered$nt[T])\n\n\n## plot results\nindex=seq(1871, 1970, length.out = length(Nile))\nindex_filt=index[1:T]\nindex_forecast=index[(T+1):(T+k)]\n\nplot(index, Nile, main = \"Nile River Level \",type='l',\n     xlab=\"time\",ylab=\"feet\",lty=3,ylim=c(400,1500))\npoints(index,Nile,pch=20)\n\nlines(index_filt,results_filtered$mt, type='l', col='red',lwd=2)\nlines(index_filt,ci_filtered[, 1], type='l', col='red', lty=2)\nlines(index_filt,ci_filtered[, 2], type='l', col='red', lty=2)\nlines(index_filt,results_smoothed$mnt, type='l', col='blue',lwd=2)\nlines(index_filt, ci_smoothed[, 1], type='l', col='blue', lty=2)\nlines(index_filt, ci_smoothed[, 2], type='l', col='blue', lty=2)\n\nlines(index_forecast, results_forecast$ft, type='l', \n      col='green',lwd=2)\nlines(index_forecast, ci_forecast[, 1], type='l', \n      col='green', lty=2)\nlines(index_forecast, ci_forecast[, 2], type='l', \n      col='green', lty=2)",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#practice-graded-assignment-ndlm-data-analysis",
    "href": "C4-L09.html#practice-graded-assignment-ndlm-data-analysis",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "98.6 Practice Graded Assignment: NDLM data analysis",
    "text": "98.6 Practice Graded Assignment: NDLM data analysis\nThis peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you’ve learned in R and prepare you for your data analysis project in week 5.\nThe R code below fits a Normal Dynamic Linear Model to the monthly time series of Google trends “hits” for the term “time series”. The model has two components: (a) a polynomial model of order 2 and (b) a seasonal component with 4 frequencies: ω_1=2π/12, (annual cycle) ω_2=2π/6 (6 months cycle), ω_3=2π/4 and ω_4=2π/3. The model assumes that the observational variance v is unknown and the system variance-covariance matrix W_t is specified using a single discount factor. The discount factor is chosen using an optimality criterion as explained in the course.\nYou will be asked to modify the code in order to consider a DLM with two components: (a) a polynomial model of order 1 and (b) a seasonal component that contains a fundamental period of p = 12 and 2 additional harmonics for a total of 3 frequencies: ω1=2π/12, ω2=2π/6 and ω3=2π/4. You will also need to optimize the choice of the discount factor for this model. You will be asked to upload pictures summarizing your results.\nR code to fit the model: requires R packages gtrends,and dlm as well as the files “all_dlm_functions_unknown_v.R” and “discountfactor_selection_functions.R” also provided below.\n#| label: code-gtrendsR-data-analysis\n# download data \nlibrary(gtrendsR)\ntimeseries_data &lt;- gtrends(\"time series\",time=\"all\")\nplot(timeseries_data)\nnames(timeseries_data)\n\ntimeseries_data=timeseries_data$interest_over_time\ndata=list(yt=timeseries_data$hits)\n\nlibrary(dlm)\nmodel_seasonal=dlmModTrig(s=12,q=4,dV=0,dW=1)\nmodel_trend=dlmModPoly(order=2,dV=10,dW=rep(1,2),m0=c(40,0))\nmodel=model_trend+model_seasonal\nmodel$C0=10*diag(10)\nn0=1\nS0=10\nk=length(model$m0)\nT=length(data$yt)\n\nFt=array(0,c(1,k,T))\nGt=array(0,c(k,k,T))\nfor(t in 1:T){\n   Ft[,,t]=model$FF\n   Gt[,,t]=model$GG\n}\n\nsource('all_dlm_functions_unknown_v.R')\nsource('discountfactor_selection_functions.R')\n\nmatrices=set_up_dlm_matrices_unknown_v(Ft=Ft,Gt=Gt)\ninitial_states=set_up_initial_states_unknown_v(model$m0,\n                                               model$C0,n0,S0)\n\ndf_range=seq(0.9,1,by=0.005)\n\n## fit discount DLM\n## MSE\nresults_MSE &lt;- adaptive_dlm(data, matrices, \n               initial_states, df_range,\"MSE\",forecast=FALSE)\n\n## print selected discount factor\nprint(paste(\"The selected discount factor:\",results_MSE$df_opt))\n\n## retrieve filtered results\nresults_filtered &lt;- results_MSE$results_filtered\nci_filtered &lt;- get_credible_interval_unknown_v(\n  results_filtered$ft,results_filtered$Qt,results_filtered$nt)\n\n## retrieve smoothed results\nresults_smoothed &lt;- results_MSE$results_smoothed\nci_smoothed &lt;- get_credible_interval_unknown_v(\n  results_smoothed$fnt, results_smoothed$Qnt, \n  results_filtered$nt[length(results_smoothed$fnt)])\n\n## plot smoothing results \npar(mfrow=c(1,1), mar = c(3, 4, 2, 1))\nindex &lt;- timeseries_data$date\nplot(index, data$yt, ylab='Google hits',\n     main = \"Google Trends: time series\", type = 'l',\n     xlab = 'time', lty=3,ylim=c(0,100))\nlines(index, results_smoothed$fnt, type = 'l', col='blue', \n      lwd=2)\nlines(index, ci_smoothed[, 1], type='l', col='blue', lty=2)\nlines(index, ci_smoothed[, 2], type='l', col='blue', lty=2)\n\n# Plot trend and rate of change \npar(mfrow=c(2,1), mar = c(3, 4, 2, 1))\nplot(index,data$yt,pch=19,cex=0.3,col='lightgray',xlab=\"time\",\n     ylab=\"Google hits\",main=\"trend\")\nlines(index,results_smoothed$mnt[,1],lwd=2,col='magenta')\nplot(index,results_smoothed$mnt[,2],col='darkblue',lwd=2,\n     type='l', ylim=c(-0.6,0.6), xlab=\"time\",\n     ylab=\"rate of change\")\nabline(h=0,col='red',lty=2)\n\n# Plot seasonal components \npar(mfrow=c(2,2), mar = c(3, 4, 2, 1))\nplot(index,results_smoothed$mnt[,3],lwd=2,col=\"darkgreen\",\n     type='l', xlab=\"time\",ylab=\"\",main=\"period=12\",\n     ylim=c(-12,12))\nplot(index,results_smoothed$mnt[,5],lwd=2,col=\"darkgreen\",\n     type='l',xlab=\"time\",ylab=\"\",main=\"period=6\",\n     ylim=c(-12,12))\nplot(index,results_smoothed$mnt[,7],lwd=2,col=\"darkgreen\",\n     type='l', xlab=\"time\",ylab=\"\",main=\"period=4\",\n     ylim=c(-12,12))\nplot(index,results_smoothed$mnt[,9],lwd=2,col=\"darkgreen\",\n     type='l', xlab=\"time\",ylab=\"\",main=\"period=3\",\n     ylim=c(-12,12))\n\n#Estimate for the observational variance: St[T]\nresults_filtered$St[T]\n\n98.6.1 All dlm functions unknown v\n\n## create list for matrices\nset_up_dlm_matrices_unknown_v &lt;- function(Ft, Gt, Wt_star){\n  if(!is.array(Gt)){\n    Stop(\"Gt and Ft should be array\")\n  }\n  if(missing(Wt_star)){\n    return(list(Ft=Ft, Gt=Gt))\n  }else{\n    return(list(Ft=Ft, Gt=Gt, Wt_star=Wt_star))\n  }\n}\n\n\n## create list for initial states\nset_up_initial_states_unknown_v &lt;- function(m0, C0_star, n0, S0){\n  return(list(m0=m0, C0_star=C0_star, n0=n0, S0=S0))\n}\n\nforward_filter_unknown_v &lt;- function(data, matrices, \n                              initial_states, delta){\n  ## retrieve dataset\n  yt &lt;- data$yt\n  T&lt;- length(yt)\n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  if(missing(delta)){\n    Wt_star &lt;- matrices$Wt_star\n  }\n  \n  ## retrieve initial state\n  m0 &lt;- initial_states$m0\n  C0_star &lt;- initial_states$C0_star\n  n0 &lt;- initial_states$n0\n  S0 &lt;- initial_states$S0\n  C0 &lt;- S0*C0_star\n  \n  ## create placeholder for results\n  d &lt;- dim(Gt)[1]\n  at &lt;- matrix(0, nrow=T, ncol=d)\n  Rt &lt;- array(0, dim=c(d, d, T))\n  ft &lt;- numeric(T)\n  Qt &lt;- numeric(T)\n  mt &lt;- matrix(0, nrow=T, ncol=d)\n  Ct &lt;- array(0, dim=c(d, d, T))\n  et &lt;- numeric(T)\n  nt &lt;- numeric(T)\n  St &lt;- numeric(T)\n  dt &lt;- numeric(T)\n  \n  # moments of priors at t\n  for(i in 1:T){\n    if(i == 1){\n      at[i, ] &lt;- Gt[, , i] %*% m0\n      Pt &lt;- Gt[, , i] %*% C0 %*% t(Gt[, , i])\n      Pt &lt;- 0.5*Pt + 0.5*t(Pt)\n      if(missing(delta)){\n        Wt &lt;- Wt_star[, , i]*S0\n        Rt[, , i] &lt;- Pt + Wt\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }else{\n        Rt[, , i] &lt;- Pt/delta\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }\n      \n    }else{\n      at[i, ] &lt;- Gt[, , i] %*% t(mt[i-1, , drop=FALSE])\n      Pt &lt;- Gt[, , i] %*% Ct[, , i-1] %*% t(Gt[, , i])\n      if(missing(delta)){\n        Wt &lt;- Wt_star[, , i] * St[i-1]\n        Rt[, , i] &lt;- Pt + Wt\n        Rt[,,i]=0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }else{\n        Rt[, , i] &lt;- Pt/delta\n        Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      }\n    }\n    \n    # moments of one-step forecast:\n    ft[i] &lt;- t(Ft[, , i]) %*% t(at[i, , drop=FALSE]) \n    Qt[i] &lt;- t(Ft[, , i]) %*% Rt[, , i] %*% Ft[, , i] + \n      ifelse(i==1, S0, St[i-1])\n    et[i] &lt;- yt[i] - ft[i]\n    \n    nt[i] &lt;- ifelse(i==1, n0, nt[i-1]) + 1\n    St[i] &lt;- ifelse(i==1, S0, \n                    St[i-1])*(1 + 1/nt[i]*(et[i]^2/Qt[i]-1))\n    \n    # moments of posterior at t:\n    At &lt;- Rt[, , i] %*% Ft[, , i] / Qt[i]\n    \n    mt[i, ] &lt;- at[i, ] + t(At) * et[i]\n    Ct[, , i] &lt;- St[i]/ifelse(i==1, S0, \n                  St[i-1])*(Rt[, , i] - Qt[i] * At %*% t(At))\n    Ct[,,i] &lt;- 0.5*Ct[,,i]+0.5*t(Ct[,,i])\n  }\n  cat(\"Forward filtering is completed!\\n\")\n  return(list(mt = mt, Ct = Ct,  at = at, Rt = Rt, \n              ft = ft, Qt = Qt,  et = et, \n              nt = nt, St = St))\n}\n\n### smoothing function ###\nbackward_smoothing_unknown_v &lt;- function(data, matrices, \n                                posterior_states,delta){\n  ## retrieve data \n  yt &lt;- data$yt\n  T &lt;- length(yt) \n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  \n  ## retrieve matrices\n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  Rt &lt;- posterior_states$Rt\n  nt &lt;- posterior_states$nt\n  St &lt;- posterior_states$St\n  at &lt;- posterior_states$at\n  \n  ## create placeholder for posterior moments \n  mnt &lt;- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt &lt;- array(NA, dim = dim(Ct))\n  fnt &lt;- numeric(T)\n  Qnt &lt;- numeric(T)\n  \n  for(i in T:1){\n    if(i == T){\n      mnt[i, ] &lt;- mt[i, ]\n      Cnt[, , i] &lt;- Ct[, , i]\n    }else{\n      if(missing(delta)){\n        inv_Rtp1 &lt;- chol2inv(chol(Rt[, , i+1]))\n        Bt &lt;- Ct[, , i] %*% t(Gt[, , i+1]) %*% inv_Rtp1\n        mnt[i, ] &lt;- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n        Cnt[, , i] &lt;- Ct[, , i] + Bt %*% (Cnt[, , i+1] - \n                                    Rt[, , i+1]) %*% t(Bt)\n        Cnt[,,i] &lt;- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])\n      }else{\n        inv_Gt &lt;- solve(Gt[, , i+1])\n        mnt[i, ] &lt;- (1-delta)*mt[i, ] + \n                delta*inv_Gt %*% t(mnt[i+1, ,drop=FALSE])\n        Cnt[, , i] &lt;- (1-delta)*Ct[, , i] + \n                delta^2*inv_Gt %*% Cnt[, , i + 1]  %*% t(inv_Gt)\n        Cnt[,,i] &lt;- 0.5*Cnt[,,i]+0.5*t(Cnt[,,i])\n      }\n    }\n    fnt[i] &lt;- t(Ft[, , i]) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] &lt;- t(Ft[, , i]) %*% t(Cnt[, , i]) %*% Ft[, , i]\n  }\n  for(i in 1:T){\n     Cnt[,,i]=St[T]*Cnt[,,i]/St[i] \n     Qnt[i]=St[T]*Qnt[i]/St[i]\n  }\n  cat(\"Backward smoothing is completed!\\n\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n\n## Forecast Distribution for k step\nforecast_function_unknown_v &lt;- function(posterior_states, k, \n                                        matrices, delta){\n  \n  ## retrieve matrices\n  Ft &lt;- matrices$Ft\n  Gt &lt;- matrices$Gt\n  if(missing(delta)){\n    Wt_star &lt;- matrices$Wt_star\n  }\n  \n  mt &lt;- posterior_states$mt\n  Ct &lt;- posterior_states$Ct\n  St &lt;- posterior_states$St\n  at &lt;- posterior_states$at\n  \n  ## set up matrices\n  T &lt;- dim(mt)[1] # time points\n  d &lt;- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at &lt;- matrix(NA, nrow = k, ncol = d)\n  Rt &lt;- array(NA, dim=c(d, d, k))\n  ft &lt;- numeric(k)\n  Qt &lt;- numeric(k)\n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] &lt;- Gt[, , T+i] %*% t(mt[T, , drop=FALSE])\n      \n      if(missing(delta)){\n       Rt[, , i] &lt;- Gt[, , T+i] %*% Ct[, , T] %*% \n         t(Gt[, , T+i]) + St[T]*Wt_star[, , T+i]\n      }else{\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Ct[, , T] %*% \n          t(Gt[, , T+i])/delta\n      }\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n      \n    }else{\n      at[i, ] &lt;- Gt[, , T+i] %*% t(at[i-1, , drop=FALSE])\n      if(missing(delta)){\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Rt[, , i-1] %*% \n          t(Gt[, , T+i]) + St[T]*Wt_star[, , T + i]\n      }else{\n        Rt[, , i] &lt;- Gt[, , T+i] %*% Rt[, , i-1] %*% \n          t(Gt[, , T+i])/delta\n      }\n      Rt[,,i] &lt;- 0.5*Rt[,,i]+0.5*t(Rt[,,i])\n    }\n    \n    \n    ## moments of forecast distribution\n    ft[i] &lt;- t(Ft[, , T+i]) %*% t(at[i, , drop=FALSE])\n    Qt[i] &lt;- t(Ft[, , T+i]) %*% Rt[, , i] %*% Ft[, , T+i] + \n      St[T]\n  }\n  cat(\"Forecasting is completed!\\n\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval_unknown_v &lt;- function(ft, Qt, nt, \n                                   quantile = c(0.025, 0.975)){\n  bound &lt;- matrix(0, nrow=length(ft), ncol=2)\n\n  if ((length(nt)==1)){\n   for (t in 1:length(ft)){\n      t_quantile &lt;- qt(quantile[1], df = nt)\n      bound[t, 1] &lt;- ft[t] + t_quantile*sqrt(as.numeric(Qt[t])) \n  \n  # upper bound of 95% credible interval\n      t_quantile &lt;- qt(quantile[2], df = nt)\n      bound[t, 2] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t]))}\n  }else{\n  # lower bound of 95% credible interval\n    for (t in 1:length(ft)){\n      t_quantile &lt;- qt(quantile[1], df = nt[t])\n      bound[t, 1] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t])) \n  \n  # upper bound of 95% credible interval\n      t_quantile &lt;- qt(quantile[2], df = nt[t])\n      bound[t, 2] &lt;- ft[t] + \n        t_quantile*sqrt(as.numeric(Qt[t]))}\n  }\n  return(bound)\n\n}\n\n\n\n98.6.2 Discount factor selection functions\n\n##################################################\n##### using discount factor ##########\n##################################################\n## compute measures of forecasting accuracy\n## MAD: mean absolute deviation\n## MSE: mean square error\n## MAPE: mean absolute percentage error\n## Neg LL: Negative log-likelihood of disc,\n##         based on the one step ahead forecast distribution\nmeasure_forecast_accuracy &lt;- function(et, yt, Qt=NA, nt=NA, type){\n  if(type == \"MAD\"){\n    measure &lt;- mean(abs(et))\n  }else if(type == \"MSE\"){\n    measure &lt;- mean(et^2)\n  }else if(type == \"MAPE\"){\n    measure &lt;- mean(abs(et)/yt)\n  }else if(type == \"NLL\"){\n    measure &lt;- log_likelihood_one_step_ahead(et, Qt, nt)\n  }else{\n    stop(\"Wrong type!\")\n  }\n  return(measure)\n}\n\n\n## compute log likelihood of one step ahead forecast function\nlog_likelihood_one_step_ahead &lt;- function(et, Qt, nt){\n  ## et:the one-step-ahead error\n  ## Qt: variance of one-step-ahead forecast function\n  ## nt: degrees freedom of t distribution\n  T &lt;- length(et)\n  aux=0\n  for (t in 1:T){\n    zt=et[t]/sqrt(Qt[t])\n    aux=(dt(zt,df=nt[t],log=TRUE)-log(sqrt(Qt[t]))) + aux \n  } \n  return(-aux)\n}\n\n## Maximize log density of one-step-ahead forecast function to select discount factor\nadaptive_dlm &lt;- function(data, matrices, initial_states, df_range, type, \n                         forecast=TRUE){\n  measure_best &lt;- NA\n  measure &lt;- numeric(length(df_range))\n  valid_data &lt;- data$valid_data\n  df_opt &lt;- NA\n  j &lt;- 0\n  ## find the optimal discount factor\n  for(i in df_range){\n    j &lt;- j + 1\n    results_tmp &lt;- forward_filter_unknown_v(data, matrices, initial_states, i)\n     \n    measure[j] &lt;- measure_forecast_accuracy(et=results_tmp$et, yt=data$yt,\n                                  Qt=results_tmp$Qt, \n                                  nt=c(initial_states$n0,results_tmp$nt), type=type)\n    \n    \n    if(j == 1){\n      measure_best &lt;- measure[j]\n      results_filtered &lt;- results_tmp\n      df_opt &lt;- i\n    }else if(measure[j] &lt; measure_best){\n      measure_best &lt;- measure[j]\n      results_filtered &lt;- results_tmp\n      df_opt &lt;- i\n    }\n  }\n  results_smoothed &lt;- backward_smoothing_unknown_v(data, matrices, results_filtered, delta = df_opt)\n  if(forecast){\n    results_forecast &lt;- forecast_function(results_filtered, length(valid_data), \n                                          matrices, df_opt)\n    return(list(results_filtered=results_filtered, \n                results_smoothed=results_smoothed, \n                results_forecast=results_forecast, \n                df_opt = df_opt, measure=measure))\n  }else{\n    return(list(results_filtered=results_filtered, \n                results_smoothed=results_smoothed, \n                df_opt = df_opt, measure=measure))\n  }\n  \n}\n\n\n\n98.6.3 Grading Criteria\nThe assignment will be graded based on the uploaded pictures summarizing the results. Estimates of some of the model parameters and additional discussion will also be requested.",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#eeg-data",
    "href": "C4-L09.html#eeg-data",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "99.1 EEG data",
    "text": "99.1 EEG data",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L09.html#google-trends",
    "href": "C4-L09.html#google-trends",
    "title": "98  Bayesian Inference in the NDLM: Part 2 - M4L9",
    "section": "99.2 Google Trends",
    "text": "99.2 Google Trends",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Bayesian Inference in the NDLM: Part 2 - M4L9</span>"
    ]
  },
  {
    "objectID": "C4-L10.html",
    "href": "C4-L10.html",
    "title": "99  Final Project",
    "section": "",
    "text": "In this final project you will use normal dynamic linear models to analyze a time series dataset downloaded from Google trend.\n\n\n\n\n\n\nNoteObjectives\n\n\n\n\nUse R for analysis and forecasting of time series using NDLM (case of known observational and system variances)\nUse R for analysis and forecasting of time series using the NDLM (cases of known or unknown observational variance and unknown system variance specified using a discount factor)\n\n\n\n\n\n\n\n\n\nNoteInstructions\n\n\n\nSo far in this course, we have discussed the following aspects of Bayesian time series models:\n\nConcepts of stationarity, the autocorrelation function, definition and properties of autoregressive (AR) models;\nMaximum likelihood and Bayesian conjugate analysis of AR models;\nDetermination of the order of AR models using AIC or BIC as criteria;\nDefinition of Normal Dynamic Linear Models (NDLMs);\nNDLM building using polynomial trend, seasonal and regression components via the superposition principle;\nBayesian filtering, smoothing and forecasting in the NDLM with known observational variances and known system covariance matrices;\nBayesian filtering, smoothing and forecasting in the NDLM with unknown but constant observational variance and known system covariance matrix;\nBayesian filtering, smoothing and forecasting in the NDLM with known observational variances and unknown system covariance matrices using discount factors;\nBayesian filtering, smoothing and forecasting in the NDLM with unknown but constant observational variance and unknown system covariance matrices using discount factors.\n\nIn this project, you will download a dataset from Google trends. In order to do this you can type a term/terms of interest in Google trends, just like we did with the example with the term “time series” analyzed in the course. You could use any term such as “flu”, “cranberry” or any other term(s). Here is a tutorial on how to download data from Google trends:",
    "crumbs": [
      "<span class='chapter-number'>99</span>  <span class='chapter-title'>Final Project</span>"
    ]
  },
  {
    "objectID": "C4-L11.html",
    "href": "C4-L11.html",
    "title": "100  Week 0: Feynman Notebook on Bayesian Analysis",
    "section": "",
    "text": "100.1 A Feynman Notebook - For Bayesian Time Series Analysis",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Week 0: Feynman Notebook on Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "C4-L11.html#a-feynman-notebook---for-bayesian-time-series-analysis",
    "href": "C4-L11.html#a-feynman-notebook---for-bayesian-time-series-analysis",
    "title": "100  Week 0: Feynman Notebook on Bayesian Analysis",
    "section": "",
    "text": "ImportantTS Questions — A Feynman Notebook\n\n\n\nHere is where I can collect questions about TS analysis that I have as I go through the course. Hopefully I will be better equipped to answer many of these them by the end of the course.\n\n\n\nHow does Bayesian TS analysis differ from regular TS analysis?\n\nIn the NDLM we supply a Normal Prior.\nWe use Bayesian updating to update the model parameters from the data.\nRather than using a point estimate, we maintain a distributional view of the parameters. We can propagate these uncertainties into our forecasting, smoothing, and filtering distributions.\n\nFourier analysis is a powerful tool for time series analysis. How does it relate to Bayesian time series analysis?\n\nIt’s easier to discuss periodicity and frequency components rather than the fourier analysis which is a technique. However, Fourier analysis lets us consider the time series in the time domain and in the frequency domain.\nWe see in the intro that\nin a Bayesian framework. We can incorporate prior beliefs about the frequency of events.\nWe can incorporate the outcomes to incorporate seasonal elements into an NDLM\n\nHow and what type of prior knowledge into time series analysis?\n\nIn (West and Harrison 2013) they authors discuss both is the actual prior.\nBut they also talk about supporting interventions. E.g. when a major competitor goes out of business.The model should be able to handle this information and they make a big issues of how we need to incorporate into the next time step both new expected demand as well as an estimate of its variance which give better estimates of required production.\n\nAre there models that are unique to Bayesian time series analysis?\n\nHard to say but DLM seem to be.\n\nHow does distributional thinking affect time series analysis?\n\nIt gives us confidence bounds on future estimates.\n\nHow do we represent uncertainty in Bayesian time series analysis?\n\nWe have distribution and we can derive for any point estimate a corresponding credible interval.\nWe can use smoothing to try and reason about trend or seasonality separately.\n\nWill we learn about Gaussian Processes/Neural Networks in this course?\n\nThis is a type of Bayesian Non-parametric and we don’t cover these in the specialization. However Abel Rodriguez, the instructor of the third course on mixture model has a short course\nHerbert Lee wrote a Bayesian Nonparametrics via Neural Networks on the subject.\n\nWhat BTS models are most useful in economics and finance?\nIs there a cleanup procedure for time series data?\n\nUsing exponential smoothing\nUsing weighted averaging going back and forward enough steps can smooth seasonal effects.\nMore generally this is handled by smoothing\nGoing backwards this is can be done using filtering.\n\nIs there an Bayesian EDA for time series?\n\nwe can use differencing to make the time series stationary\nwe can use the ACF and PACF\nwe can decompose the time series into trend, seasonal, and residual components\nwe can visualize autocorrelation using a correlogram\nwe can visualize periodicity using a periodogram and spectral density.\nsee (Nielsen 2019)\n\nHow do we handle missing data in time series?\nHow do we handle non-stationary time series?\n\nBy applying differencing we can make the time series stationary.\n\nAre there processes for long term memory in time series?\n\nsee (Prado, Ferreira, and West 2023, 124)\nthe book also touches on EKF and MKF\n\nAre there processes for power laws.\n\n\nsee https://wiki.santafe.edu/images/5/52/Powerlaws.pdf\n\n\nCan BTS handle dynamic systems in time series?\nCan we model time series with multiple scales?\nWhat TS models are useful for regime switching?\n\nI recall this came up in Davidson-Pilon (2015)\nThis is also covered in module 3 of the course.\n\nHow can we simulate time series data?\nHow can we forecast time series data?\nHow can we find periodicity in time series data?\nIs the Kalman Filter a state-space or dynamic linear model\n\nsee (Prado, Ferreira, and West 2023, 141)\nthe book also touches on EKF and MKF\n\nParticle filters AKA Sequential Monte Carlo methods in the book.\n\nsee (Prado, Ferreira, and West 2023, 205)\n\nAre there Bayesian time series models of contagion?\nAre there Bayesian time series models of epidemics?\nWhat are Seasonal adjustments?\nHow to do a seasonal adjustment?\nWhat are the tools for wrangling and cleaning TS data.\n\nData Engineering using Wrangling and Cleaning c.f. (Woodward, Sadler, and Robertson 2022, sec. 1.4.1.3)\n\nHow to use the frequency domain as a key part of understanding and analyzing data?\nHow to assess whether an existing trend should be predicted to continue or whether caution should be used in forecasts?\nDo you really understand AR models or are they just a black box?\nWhat is a unit root?\nHow to use tools that would help you decide whether to use a seasonal model and/or include a unit root in models?\nHow to use predictor variables to help forecast some variable such as sales? (i.e. regression on time series data)\n\n\n100.1.1 Co-integration\n\nWhat are Cointegrated time series?\n\nCo-integration  is a technique used to find a long term correlation between time series processes that was introduced in 1987 by Nobel laureates Robert Engle and Clive Granger\nalso see (Pfaff 2008)\n\nWhat are some test for co-integration:\n\nEngle-Granger,\nJohansen Test,\nthe Phillips-Ouliaris test.\n\n\nmagic trick\n\n100.1.2 NN and Deep learning\n\nHow to use neural network methods to forecast time series?\n\nRNNs\nlstms\nconvolutions\nGRUs\ntransformers\nTS foundations models\nNeural Prophet citation needed\n\nDeep learning foundation models citation needed pre-train NN model with many time series. Is this a form of Bayesian time series analysis?\nHow does this BTS relate to deep learning?\n\nDiffusion models in DL are autoregressive citation needed\nthe recently the mamba architecture has been proposed which is an autoregressive state space model. citation needed\n\n\n\n\n100.1.3 Web scraping\n\nAny tips on scaraping time series data?\n\n\nWeb Scraping using Bots c.f (Woodward, Sadler, and Robertson 2022, sec. 1.4.1.4)\n\n\n\n100.1.4 Filtering & Smoothing\n\nWhat is smoothing in BTS\n\ndecomposing the series as a sum of two components: a smooth component, plus another component that includes all the features that are unexplained by the smooth component.\none way is to use a moving average.\nin the Bayesian context smoothing is the process of estimating the hidden states of a system given the observed data.\n\nWhat is filtering in Bayesian Time Series?\n\nin the Bayesian context filtering is the process of estimating the previous hidden states of a system given the observed data. I.e. a retrospective analysis to understand the process better\nwe want to sample \\mathbb{P}r(\\theta_t,k \\mid \\mathcal{D}_t)\n\nWhat is the Kalman filter?\n\nsee (Prado, Ferreira, and West 2023, 141)\nthe book also toches on EKF and MKF\n\nWhat is a particle filter?\n\nsee (Prado, Ferreira, and West 2023, sec. 6.2.2) on the The Auxiliary Particle Filter\n\nWhat is the Butterworth filter?\n\nThe Butterworth filter is a signal processing filter that has a flat response in passband (or as close as possible to flat) - making it good for cleaning up noise !?\n\n\n\n\n100.1.5 Models:\n\nWhite noise\nWiener process (random walk)\nAR(1): Autoregressive process order 1\nOrnstein–Uhlenbeck process a continuous-time version of the AR(1) process\nAR(p): Autoregressive process order p\nMA(q): Moving average process order q\nARMA(p,q): Autoregressive moving average\nSARMA: Seasonal ARMA\nARIMA: Autoregressive integrated moving average\nSARIMA: Seasonal ARIMA\nVAR: Vector autoregressive\nSVAR: structural vector autoregressive models (SVAR).\nVECM: Vector error correction models (VECM).\nGARCH: Generalized autoregressive conditional heteroskedasticity\nARCH: Autoregressive conditional heteroskedasticity\nSMC: Sequential Monte Carlo\nMDM: Multi-regression dynamic models\nLTMs: latent threshold models\nFFBS: Forward Filtering Backward Sampling\nDLM: Dynamic Linear Models\nBSTS: Bayesian Structural Time Series\n\nhttps://drive.google.com/file/d/14US56VzanuLt03XBkoAGzLy0gDEreZUc/view\n\n\nTVAR: Time-varying autoregressive models\nDGLM: Dynamic Generalized Linear Models\n\n\nWhat is the relation between Dynamic linear time series models (DLTS) models and Bayesian structural time series models (BSTS) models?\n\nDynamic Linear Time Series (DLTS) models and Bayesian Structural Time Series (BSTS) models are both frameworks for modeling time series data, and they share a strong connection, particularly in the way they approach model formulation and uncertainty. Here’s a breakdown of their relationship:\n\n\nDynamic Linear Time Series (DLTS) Models:\n\n\nDLTS models, often referred to as Dynamic Linear Models (DLMs), are a class of models where the parameters (such as the intercept or slope) evolve over time according to a stochastic process. They can be written in a state-space form, consisting of:\nObservation Equation: Relates the observed data to the hidden state.\nState Equation: Describes how the hidden state evolves over time.\nThese models use Kalman filtering for inference and prediction.\nDLTS models are flexible in handling non-stationarity and time-varying parameters.\n\n\nIn the course we primarily learned to use Bayesian methods to estimate the parameters of the model.\n\n\nBayesian Structural Time Series (BSTS) Models:\n\n\nBSTS models are a Bayesian approach to time series modeling, which generalizes the DLTS framework.\nLike DLTS, they use a state-space form, where the time series is decomposed into different components (e.g., trend, seasonality, regression effects).\nBSTS explicitly incorporates Bayesian inference, where prior distributions are placed on the model components and parameters, and inference is conducted using MCMC or other Bayesian methods.\nOne of the key advantages of BSTS is its ability to incorporate model uncertainty, allowing the user to specify structural components (such as trend or seasonality) with uncertainty about their presence or importance in the data.\n\n\nRelation Between DLTS and BSTS:\n\n\nBayesian Extension of DLTS: BSTS can be seen as a Bayesian extension of DLTS models. While DLTS uses Kalman filtering for deterministic inference, BSTS uses Bayesian methods to quantify and propagate uncertainty in model components and parameters.\nComponent Decomposition: Both models can represent the time series in terms of structural components (like trends, seasonal patterns, or covariates), but BSTS allows for more flexible modeling of these components using Bayesian priors and hierarchical structures.\nHandling of Uncertainty: DLTS models provide point estimates for parameters using Kalman filters, while BSTS incorporates full probabilistic estimates, enabling better uncertainty quantification in the presence of small data, model misspecification, or structural breaks.\nModel Complexity: BSTS models can handle more complex scenarios where the structure of the time series isn’t fully known (e.g., unknown seasonality or trends), whereas DLTS is typically used when the structure of the model (e.g., presence of trend or seasonality) is more defined.\nfor more information on BSTS",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Week 0: Feynman Notebook on Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "C4-L11.html#software",
    "href": "C4-L11.html#software",
    "title": "100  Week 0: Feynman Notebook on Bayesian Analysis",
    "section": "100.2 Software",
    "text": "100.2 Software\n\nWhat are the most useful packages in R for time series analysis?\n\nBOA Bayesian Output Analysis (BOA, Smith 2007) citation needed and\nCODA Convergence Diagnosis and Output Analysis for MCMC (CODA, Plummer, Best, Cowles, and Vines 2006). citation needed\nURCA Unit Root and Cointegration Tests for Time Series Data (URCA, Pfaff 2008)citation needed.\nVars VAR Modelling (Vars, Pfaff 2008). citation needed\nBSTS Bayesian Structural Time Series (BSTS, Scott and Varian 2014). citation needed\nCausalImpact Causal Impact Analysis (CausalImpact, Brodersen, Gallusser, Koehler, Remy, and Scott 2015). citation needed builds on BSTS and methods from this Inferring causal impact using Bayesian structural time-series models\nKFAS Kalman Filter and Smoother for Exponential Family State Space Models (KFAS, Helske 2017). citation needed\nMARSS Multivariate Autoregressive State-Space Models (MARSS, Holmes, Ward, and Scheuerell 2012). citation needed\nMCMCpack Markov Chain Monte Carlo (MCMCpack, Martin, Quinn, and Park 2011). citation needed\nMCMCglmm Markov Chain Monte Carlo Generalized Linear Mixed Models (MCMCglmm, Hadfield 2010). citation needed\nR-INLA Integrated Nested Laplace Approximations (R-INLA, Rue, Martino, and Chopin 2009). citation needed used approximate Bayesian inference for Latent Gaussian Models that can be expressed as latent Gaussian Markov random fields (GMRF)\n\nWhat about in python?\n\n\n\n\n\n\n\nDavidson-Pilon, Cameron. 2015. Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference. Addison-Wesley Data & Analytics Series. Pearson Education. https://books.google.co.il/books?id=rMKiCgAAQBAJ.\n\n\nNielsen, A. 2019. Practical Time Series Analysis: Prediction with Statistics and Machine Learning. O’Reilly Media. https://books.google.co.il/books?id=xNOwDwAAQBAJ.\n\n\nPfaff, B. 2008. Analysis of Integrated and Cointegrated Time Series with r. Use r! Springer New York. https://books.google.co.il/books?id=ca5MkRbF3fYC.\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.\n\n\nWoodward, W. A., B. P. Sadler, and S. Robertson. 2022. Time Series for Data Science: Analysis and Forecasting. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=_W16EAAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Week 0: Feynman Notebook on Bayesian Analysis</span>"
    ]
  },
  {
    "objectID": "C5-L01.html",
    "href": "C5-L01.html",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "",
    "text": "101.1 Introduction 🎥\nThis is a capstone project for the Bayesian Statistics course. The instructor is a doctoral student Chi Joe Kao. I should point out that while he doesn’t have the pedigree of the other instructors, he was on one of the best explainers of the complex material. One downside is that is english is a little broken but I’ve had much worse. However you can see from the first slide that it covers the key concepts in a clear and concise manner leaving out very little. Also it seems to me that Chi Joe Kao still remembers how hard this material is at first glance which is a great asset for teaching it effectively. You may notice that we revisit some old material this is because this course makes a great effort to be self contained.\nMany parts of this course are steps towards completing the capstone project. And perhaps just as exciting we will also cover a new type of model - a mixture version of the autoregressive model from the previous course.\nWe will cover the Bayesian conjugate analysis for autoregressive time series models. This sounds a bit bombastic, all it means is that we will be using a likelihood and a prior that leads to a conjugate posterior, keeping everything within one distributional family!",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#prerequisite-skill-checklist",
    "href": "C5-L01.html#prerequisite-skill-checklist",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.2 Prerequisite skill checklist 📖",
    "text": "101.2 Prerequisite skill checklist 📖\n\n\n\n\n\n\nNotePrerequisite skill checklist\n\n\n\n\n\n\n101.2.1 Bayesian Statistics\n\nInterpret and specify the components of Bayesian statistical models: likelihood, prior, posterior.\nExplain the basics of sampling algorithms, including sampling from standard distributions and using MCMC to sample from non-standard posterior distributions.\nAssess the performance of a statistical model and compare competing models using posterior samples.\nCoding in R to achieve the above tasks.\n\n\n\n101.2.2 Mixture Models\n\nDefine mixture model.\nExplain the likelihood function associated with a random sample from a mixture distribution.\nDerive Markov chain Monte Carlo algorithms for fitting mixture models.\nCoding in R to achieve the above tasks.\n\n\n\n101.2.3 Time Series Analysis\n\nDefine time series and stochastic processes (univariate, discrete-time, equally-spaced)\nDefine strong and weak stationarity\nDefine the auto-covariance function, the auto-correlation function (ACF), and the partial autocorrelation function (PACF)\nDefinition of the general class of autoregressive processes of order p.",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#read-data-ℛ",
    "href": "C5-L01.html#read-data-ℛ",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.3 Read Data 📖 ℛ",
    "text": "101.3 Read Data 📖 ℛ\n\n101.3.1 Earth Quake Data ℛ\n\n## read data, you need to make sure the data file is in your current working directory \nearthquakes.dat &lt;- read.delim(\"data/earthquakes.txt\")\nearthquakes.dat$Quakes=as.numeric(earthquakes.dat$Quakes)\n\ny.dat=earthquakes.dat$Quakes[1:100] ## this is the training data\ny.new=earthquakes.dat$Quakes[101:103] ## this is the test data\n\n\n\n101.3.2 Google Search Index Data ℛ\n\n## read data, you need to make sure the data file is in your current working directory \ncovid.dat &lt;- read.delim(\"data/GoogleSearchIndex.txt\")\ncovid.dat$Week=as.Date(as.character(covid.dat$Week),format = \"%Y-%m-%d\")\n\ny.dat=covid.dat$covid[1:57] ## this is the training data\ny.new=covid.dat$covid[58:60] ## this is the test data",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#model-formulation",
    "href": "C5-L01.html#model-formulation",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.4 Model Formulation 🎥",
    "text": "101.4 Model Formulation 🎥\n\n\n\n\n\n\n\nFigure 101.1: AR(p) Model Formulation 1\n\n\n\n\n\n\n\n\nFigure 101.2: Multiple Regression Formulation\n\n\n\n\n101.4.1 AR(p) model formulation\n$$ y_t t\n\\begin{cases}  T&gt;p \\\\ T=n+p \\\\ n&gt;1 \\end{cases}\n$$\nAR(p)\n\n\\begin{aligned}\ny_t = \\sum_{j=1}^p \\phi_j y_{t-j} + \\underbrace {\\varepsilon_t}_{\\text{innovations}}\n\\qquad\n\\begin{cases}\n\\varepsilon_t \\perp y_\\tau\\ & \\forall \\tau&lt;t \\\\\n\\varepsilon_t \\stackrel{iid}{\\sim} \\mathcal{N}(0,\\nu)\n\\end{cases}\n\\end{aligned}\n\\tag{101.1}\n\n\\begin{aligned}\ny_t &= \\sum_{j=1}^p \\phi_j y_{t-j} +  {\\varepsilon_t} & (\\text{ normality of innovation})\\\\\n    &= \\sum_{j=1}^p \\phi_j y_{t-j} +  \\mathcal N(0,\\nu) & (\\text{linearity}) \\\\\n    &= \\mathcal{N}(\\sum_{j=1}^p \\phi_j y_{t-j},\\nu) & (\\text{rewrite using design matrix})\\\\\n    &= \\mathcal{N}(f_t \\boldsymbol \\phi,\\nu) \\\\\n\\end{aligned}\n\n\n\\begin{aligned}\n\\mathbb{P}r(y_{1:T}\\mid \\phi_1, \\ldots, \\phi_p, \\nu )\n&= \\mathbb{P}r(y_{p+1:T})\\prod_{t=p+1}^T \\mathbb{P}r(y_t \\mid y_{t-1},\\ldots,y_{t-p},\\phi_1,\\ldots,\\phi_p,\\nu) \\\\\n&= \\prod_{t=p+1}^T \\mathcal{N}(f_t\\phi,\\nu) \\\\\n&= \\mathcal{N}(F^\\top \\boldsymbol{\\phi},\\nu\\mathbb{I}_n) \\\\\n\\end{aligned}\n\\tag{101.2}\n\nwhere\n\n\\boldsymbol{\\phi}=(\\phi_1,\\ldots,\\phi_p)^\\top is the vector of parameters, and\n\\nu is the scale parameter. and\nF is the design matrix\n\n\n\nF= \\begin{pmatrix}\ny_{p} & y_{p+1} & \\cdots & y_{T-1} \\\\\ny_{p-1} & y_{p} & \\cdots & y_{T-2} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_{1} & y_{2} & \\cdots & y_{T-p}\n\\end{pmatrix}\n\\tag{101.3}\n\n\n\n101.4.2 Multiple Linear regression model formulation\n\ny=(y_{p+1},\\ldots,y_T) \\in \\mathbb{R}^n\n\n\ny = F^\\top \\boldsymbol{\\phi} + \\varepsilon \\qquad \\varepsilon \\sim \\mathcal{N}(0,\\nu\\mathbb{I}_n)\n \n\\mathbb{P}r(y \\mid \\boldsymbol{\\phi},\\nu) = \\mathcal{N}(F^\\top \\boldsymbol{\\phi},\\nu\\mathbb{I}_n)\n \n\\mathbf{y}\\sim \\mathcal{N}(\\mathbf{F}^\\top \\boldsymbol{\\phi},\\nu\\mathbf{I}_n),\\quad \\boldsymbol{\\phi}\\sim \\mathcal{N}(\\mathbf{m}_0,\\nu\\mathbf{C}_0),\\quad \\nu\\sim IG(\\frac{n_0}{2},\\frac{d_0}{2})\n\\tag{101.4}\nwhere:\n\n\n\n\n\n\nNoteQuestion\n\n\n\n\n\nSuppose we have a multiple linear regression model y=\\mathbf{X}\\beta+\\varepsilon where \\varepsilon \\sim\\mathcal{N}(0,\\sigma^2I) what is the proper conditional conjugate prior for parameters \\beta and \\sigma^2?\nSolution\n\n\\mathbb{p}r(\\beta,\\sigma^2) \\quad \\propto \\frac{1}{σ^2}\n\\mathbb{p}r(\\beta,\\sigma^2) \\quad \\propto  \\mathcal{N}(\\beta \\mid \\mu,\\Sigma)\\mathcal{IG}(\\sigma^2\\mid a,b)\n\\mathbb{p}r(\\beta,\\sigma^2) \\quad \\propto  \\mathcal{N}(\\beta \\mid \\mu,\\sigma^2\\Sigma)\\mathcal{IG}(σ^2\\mid a,b)\n\\mathbb{p}r(\\beta,\\sigma^2) \\quad \\propto  1\n\n\n\n\nI got this right the first time by eyeballing it. So I’m not sure why it is right other than the the others are missing something.",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#review-useful-distributions-ℛ",
    "href": "C5-L01.html#review-useful-distributions-ℛ",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.5 Review: Useful Distributions 📖 ℛ",
    "text": "101.5 Review: Useful Distributions 📖 ℛ\nThis is material a review of three distributions we have seen frequently in relation to modeling with Normal distribution - our default model in many cases.\n\nThe multivariate normal distribution is used to model multiple correlated variables\nThe inverse-gamma distribution is used as a prior for the variance for Normally distributed data when the mean is known and the variance is unknown.\nThe multivariate Student-t distribution is a robust alternative to the multivariate normal distribution when the data has outliers or is not normally distributed.\n\n\n101.5.1 The multivariate normal distribution\nA k-dimensional random vector x=(x_1, \\cdots, x_k)^T that follows a multivariate normal distribution with mean \\mu and variance-covariance matrix \\Sigma is denoted as x \\sim \\mathcal{N}(\\mu, \\Sigma) or p(x) = \\mathcal{N}(x \\mid \\mu, \\Sigma) has a density function given by: \np(\\mathbf{x})=(2\\pi)^{-k/2}|\\boldsymbol{\\Sigma}|^{-1/2}\\exp[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})]\n\\tag{101.5}\nwhere \\mathbf{x} is a k-dimensional vector, |\\boldsymbol{\\Sigma}| is the determinant of the covariance matrix, and \\boldsymbol{\\Sigma}^{-1} is the inverse of the covariance matrix.\n\n\n101.5.2 The gamma and inverse-gamma distribution\nA random variable x that follows a gamma distribution with shape parameter \\alpha and inverse scale parameter \\beta , denoted as x \\sim \\mathcal{G}(\\alpha, \\beta), or \\mathbb{P}r(x) = \\mathcal{G}(x \\mid \\alpha, \\beta), has a density of the form \n\\mathbb{P}r(x)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x},\\quad x&gt;0\n\\tag{101.6}\nwhere \\alpha is the shape parameter, \\beta is the rate parameter, and \\Gamma(\\alpha) is the gamma function evaluated at \\alpha.\nIn addition, \n\\mathbb{E}[x]=\\frac{\\alpha}{\\beta}  \\qquad \\mathbb{V}ar[x]=\\frac{\\alpha}{\\beta^2}\n\nIf 1/x \\sim G(\\alpha, \\beta), then x follows an inverse-gamma distribution X\\sim \\mathcal{IG}(\\alpha, \\beta). or p(x) = \\mathcal{IG}(X\\mid \\alpha, \\beta). with\n\n\\mathbb{P}r(x)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{-(\\alpha+1)}e^{-\\beta/x},\\quad x&gt;0\n\\tag{101.7}\nwhere \\alpha is the shape parameter, \\beta is the scale parameter, and \\Gamma(\\alpha) is the gamma function evaluated at \\alpha.\nin this case\n\n\\mathbb{E}[x]=\\frac{\\beta}{\\alpha-1} \\quad \\text{for } \\alpha &gt; 1 \\qquad\n\\mathbb{V}ar[x]=\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)} \\quad \\text{for } \\alpha &gt; 2\n\\tag{101.8}\n\n\n101.5.3 The multivariate Student-t distribution\nA random vector x of dimension k follows a multivariate Student-t distribution with \\nu degrees of freedom, location \\boldsymbol{\\mu}, and scale matrix \\boldsymbol{\\Sigma}, denoted as x \\sim T_\\nu(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}), if its density is given by:\n\n\\mathbb{P}r(\\mathbf{x})=\\frac{\\Gamma(\\frac{\\nu+k}{2})}{\\Gamma(\\frac{\\nu}{2})(\\nu\\pi)^{k/2}}|\\boldsymbol{\\Sigma}|^{-1/2}[1+\\frac{1}{\\nu}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})]^{-(\\nu+k)/2}\n\n\n\\mathbb{E}[x]=\\mu \\text{ for } \\nu&gt;1  \\qquad \\mathbb{V}ar[x]=\\frac{\\nu\\Sigma}{\\nu−2} \\quad \\text{for } \\nu&gt;2",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#sec-capstone-posterior",
    "href": "C5-L01.html#sec-capstone-posterior",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.6 Posterior Distribution Derivation 📖",
    "text": "101.6 Posterior Distribution Derivation 📖\nIn this lesson, I present the derivation of the posterior distributions for \\boldsymbol{\\phi} and \\nu for the following model:\n\n\\mathbf{y}\\sim \\mathcal{N}(\\mathbf{F}^\\top \\boldsymbol{\\phi},\\nu\\mathbf{I}_n),\\quad \\boldsymbol{\\phi}\\sim \\mathcal{N}(\\mathbf{m}_0,\\nu\\mathbf{C}_0),\\quad \\nu\\sim IG(\\frac{n_0}{2},\\frac{d_0}{2})\n\\tag{101.9}\nwhere :\n\n\\mathbf{y} is the observed data,\n\\mathbf{F} is the design matrix,\n\\boldsymbol{\\phi} is the vector of parameters,\n\\nu is the scale parameter,\n\\mathbf{m}_0 is the prior mean of \\boldsymbol{\\phi},\n\\mathbf{C}_0 is the prior covariance matrix of \\boldsymbol{\\phi},\nn_0 and d_0 are the shape and scale parameters of the inverse-gamma prior for \\nu.\n\ni.e. we start with a Normal likelihood with a Normal prior for \\boldsymbol{\\phi} and an inverse-gamma prior for \\nu. Since we call the parameters \\phi and not \\theta it suggests that this isn’t a typical linear regression but rather an AR(p) model with\nI recall that we already did this derivation in the first course by Herbert Lee.\n\nUsing Bayes theorem, we have :\n\n\\begin{aligned}\n\\mathbb{p}r(\\boldsymbol{\\phi},\\nu\\mid\\mathbf{y}) & \\propto \\mathbb{p}r(\\mathbf{y} \\mid \\boldsymbol{\\phi},\\nu)\\mathbb{p}r(\\boldsymbol{\\phi}\\mid\\nu)\\mathbb{p}r(\\nu)\\\\\n&\\propto \\nu^{-n/2}\\exp \\left(-\\frac{(\\mathbf{y}-\\mathbf{F}^T\\boldsymbol{\\phi})^T(\\mathbf{y}-\\mathbf{F}^T\\boldsymbol{\\phi})}{2\\nu}\\right)\\\\\n&\\quad \\times \\nu^{-p/2}\\exp\\left(-\\frac{(\\boldsymbol{\\phi}-\\mathbf{m}_0)^T\\mathbf{C}_0^{-1}(\\boldsymbol{\\phi}-\\mathbf{m}_0)}{2\\nu}\\right)\\\\\n&\\quad \\times \\nu^{-(\\frac{n_0}{2}+1)}\\exp\\left(-\\frac{d_0}{2\\nu}\\right)\\\\\n&\\propto \\nu^{-(\\frac{n^*}{2}+1)}\\exp\\left(-\\frac{d^*}{2\\nu}\\right)\\\\\n&\\quad \\times \\nu^{-p/2}\\exp\\left(-\\frac{(\\boldsymbol{\\phi}-\\mathbf{m})^T\\mathbf{C}^{-1}(\\boldsymbol{\\phi}-\\mathbf{m})}{2\\nu}\\right)\\\\\n&\\propto \\mathbb{p}r(\\nu\\mid\\mathbf{y})\\mathbb{p}r(\\boldsymbol{\\phi}\\mid\\nu,\\mathbf{y})\n\\end{aligned}\n\\tag{101.10}\nwhere\n\n\\begin{aligned}\n\\mathbf{e}&=\\mathbf{y}-\\mathbf{F}^T\\mathbf{m}_0,\n&\\mathbf{Q}&=\\mathbf{F}^T\\mathbf{C}_0\\mathbf{F}+\\mathbf{I}_n,\\quad\n\\mathbf{A}=\\mathbf{C}_0\\mathbf{F}\\mathbf{Q}^{-1}\\\\\n\\mathbf{m}&=\\mathbf{m}_0+\\mathbf{A}\\mathbf{e},\\quad\n&\\mathbf{C}&=\\mathbf{C}_0-\\mathbf{A}\\mathbf{Q}\\mathbf{A}^{T}\\\\\nn^* &=n+n_0,\\quad\n&d^*&=(y-\\mathbf{F}^T\\mathbf{m}_0)^T\\mathbf{Q}^{-1}(y-\\mathbf{F}^T\\mathbf{m}_0)+d_0\n\\end{aligned}\n\\tag{101.11}\nTherefore, we have the posterior distribution of \\boldsymbol{\\phi} and \\nu as:\n\n\\begin{aligned}\n\\mathbb{P}r(\\boldsymbol{\\phi},\\nu \\mid \\mathbf{y}) &=\\mathbb{P}r(\\boldsymbol{\\phi} \\mid \\nu,\\mathbf{y})\\mathbb{P}r(\\nu\\mid \\mathbf{y}) \\\\\n&=\\mathcal{N}(\\boldsymbol{\\phi}\\mid \\mathbf{m},\\nu\\mathbf{C})\\ \\mathcal{IG}(\\nu\\mid \\frac{n^*}{2},\\frac{d^*}{2}) \\end{aligned}\n\\tag{101.12}\nTo get the sth sample of (\\boldsymbol{\\phi}^{(s)},\\nu^{(s)}) from the joint posterior distribution of \\boldsymbol{\\phi} and \\nu, we first sample \\nu^{(s)} from \\mathcal{IG}(\\frac{n^*}{2},\\frac{d^*}{2}), then sample \\boldsymbol{\\phi}^{(s)} from \\mathcal{N}(\\boldsymbol{m},\\nu^{(s)}\\boldsymbol{C}).\nOne can also obtain posterior samples of \\boldsymbol{\\phi} by directly sampling from its marginal posterior distribution. Integrate out \\nu from the joint posterior distribution, we have \\mathbb{P}r(\\boldsymbol{\\phi} \\mid \\mathbf{y})=T_{n^*}(\\boldsymbol{m},\\frac{d^*}{n^*}\\boldsymbol{C}).\nDenote the sth posterior sample of \\boldsymbol{\\phi} and \\nu as \\boldsymbol{\\phi}^{(s)} and \\nu^{(s)}, we can obtain the corresponding ith posterior sample of \\mathbf{y}^{(s)} by sampling from \\mathcal{N}(\\mathbf{F}^\\top \\boldsymbol{\\phi}^{(s)},\\nu^{(s)}). Therefore, we can obtain the posterior point and interval estimates for the time series.",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#sec-capstone-ar-fitting",
    "href": "C5-L01.html#sec-capstone-ar-fitting",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.7 AR model fitting example",
    "text": "101.7 AR model fitting example\n\n101.7.1 Bayesian Conjugate Analysis of AR Model\n\n101.7.1.1 Simulate Data\nWe give an example about fitting an AR(2) model using simulated data. We simulate 200 observations from the following model:\n\ny_t=0.5y_{t-1}+0.4y_{t-2}+\\varepsilon_t,\\quad \\varepsilon_t\\sim N(0,1)\n\\tag{101.13}\n\n## simulate data\n\nphi1=0.5\nphi2=0.4\nv=1\n\nset.seed(1)\ny.sample=arima.sim(n=200,model=list(order=c(2,0,0),ar=c(phi1,phi2),sd=sqrt(v)))\nplot.ts(y.sample,ylab=expression(italic(y)[italic(t)]),xlab=expression(italic(t)),\n        main='')\n\n\n\n\n\n\n\nFigure 101.3: Simulated AR(2) data\n\n\n\n\n\nNow, we perform a prior sensitivity analysis for the choice of hyperparameters in prior distribution. We choose three sets of prior hyperparameters and plot the posterior distribution for \\mathbb{P}r(\\phi_1, \\phi_2 \\mid y_{1:n}) below. The three sets of prior hyperparameters are :\n\n\\begin{aligned}\n& (1) \\quad \\mathbf{m}_0=(0,0)^T,\\mathbf{C}_0=I_2,n_0=2,d_0=2\\\\\n& (2) \\quad \\mathbf{m}_0=(0,0)^T,\\mathbf{C}_0=I_2,n_0=6,d_0=1\\\\\n& (3) \\quad \\mathbf{m}_0=(-0.5,-0.5)^T,\\mathbf{C}_0=I_2,n_0=6,d_0=1\n\\end{aligned}\n\nIn the plot, the left, middle and right panel correspond to the first, second and last sets of hyperparameters, respectively.\n\n## prior sensitivity analysis\n## plot posterior distribution of phi_1 and phi_2 on a grid \n\nlibrary(colorRamps)\nlibrary(leaflet)\nlibrary(fields)\n\nLoading required package: spam\n\n\nSpam version 2.11-1 (2025-01-20) is loaded.\nType 'help( Spam)' or 'demo( spam)' for a short introduction \nand overview of this package.\nHelp for individual functions is also obtained by adding the\nsuffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n\n\n\nAttaching package: 'spam'\n\n\nThe following objects are masked from 'package:base':\n\n    backsolve, forwardsolve\n\n\nLoading required package: viridisLite\n\n\n\nTry help(fields) to get started.\n\n\n\nAttaching package: 'fields'\n\n\nThe following object is masked from 'package:leaflet':\n\n    addLegend\n\nlibrary(mvtnorm)\n\n\nAttaching package: 'mvtnorm'\n\n\nThe following objects are masked from 'package:spam':\n\n    rmvnorm, rmvt\n\n## generate grid\ncoordinates_1=seq(-3,3,length.out = 100)\ncoordinates_2=seq(-3,3,length.out = 100)\ncoordinates=expand.grid(coordinates_1,coordinates_2)\ncoordinates=as.matrix(coordinates)\n\n## set up\nN=100\np=2  ## order of AR process\nn.all=length(y.sample) ## T, total number of data\n\nY=matrix(y.sample[3:n.all],ncol=1)\nFmtx=matrix(c(y.sample[2:(n.all-1)],y.sample[1:(n.all-2)]),nrow=p,byrow=TRUE)\nn=length(Y)\n\n## function to compute parameters for the posterior distribution of phi_1 and phi_2\n## the posterior distribution of phi_1 and phi_2 is a multivariate t distribution\n\ncal_parameters=function(m0=matrix(c(0,0),nrow=2),C0=diag(2),n0,d0){\n  e=Y-t(Fmtx)%*%m0\n  Q=t(Fmtx)%*%C0%*%Fmtx+diag(n)\n  Q.inv=chol2inv(chol(Q))  ## similar as solve, but more robust\n  A=C0%*%Fmtx%*%Q.inv\n  m=m0+A%*%e\n  C=C0-A%*%Q%*%t(A)\n  n.star=n+n0\n  d.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n  \n  params=list()\n  params[[1]]=n.star\n  params[[2]]=d.star\n  params[[3]]=m\n  params[[4]]=C\n  \n  return(params)\n}\n\n\n## evaluate density at the grid points\nget_density=function(param){\n  location=param[[3]]\n  scale=as.numeric(param[[2]]/param[[1]])*param[[4]]\n  density=rep(0,N^2)\n  \n  for (i in 1:N^2) {\n    xi=coordinates[i,]\n    density[i]=dmvt(xi,delta=location,sigma=scale,df=param[[1]])\n  }\n  \n  density_expand=matrix(density,nrow=N)\n  return(density_expand)\n}\n\n## calculate density for three sets of hyperparameters\nparams1=cal_parameters(n0=2,d0=2)\nparams2=cal_parameters(n0=6,d0=1)\nparams3=cal_parameters(m0=matrix(c(-0.5,-0.5),nrow=2),n0=6,d0=1)\n\ncol.list=matlab.like2(N)\nZ=list(get_density(params1),get_density(params2),get_density(params3))\n\nop &lt;- par(mfrow = c(1,3),\n          oma = c(5,4,0,0) + 0.1,\n          mar = c(4,4,0,0) + 0.2)\nimage(coordinates_1,coordinates_2,Z[[1]],col=col.list,\n      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))\nimage(coordinates_1,coordinates_2,Z[[2]],col=col.list,\n      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))\nimage(coordinates_1,coordinates_2,Z[[3]],col=col.list,\n      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))\n\n\n\n\n\n\n\nFigure 101.4: Prior sensitivity analysis of AR coefficients\n\n\n\n\n\nSince we change the value of hyperparameters but the posterior is almost the same, we can conclude that the posterior distribution of both AR coefficients and variance are robust w.r.t. the choice of hyperparameters.\nPosterior Inference We now sample 5000 sets of (\\phi_1, \\phi_2, \\nu) from their marginal posterior distributions and plot them. For prior hyperparameters, we take m_0=(0,0)^\\top, C_0=I_2, n_0=2, and d_0=2.\n\nm0=matrix(rep(0,p),ncol=1)\nC0=diag(p)\nn0=2\nd0=2\n\ne=Y-t(Fmtx)%*%m0\nQ=t(Fmtx)%*%C0%*%Fmtx+diag(n)\nQ.inv=chol2inv(chol(Q))\nA=C0%*%Fmtx%*%Q.inv\nm=m0+A%*%e\nC=C0-A%*%Q%*%t(A)\nn.star=n+n0\nd.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n\n\nn.sample=5000\n\nnu.sample=rep(0,n.sample)\nphi.sample=matrix(0,nrow=n.sample,ncol=p)\n\nfor (i in 1:n.sample) {\n  set.seed(i)\n  nu.new=1/rgamma(1,shape=n.star/2,rate=d.star/2)\n  nu.sample[i]=nu.new\n  phi.new=rmvnorm(1,mean=m,sigma=nu.new*C)\n  phi.sample[i,]=phi.new\n}\n\npar(mfrow=c(1,3))\nhist(phi.sample[,1],freq=FALSE,xlab=expression(phi[1]),main=\"\",ylim=c(0,6.4))\nlines(density(phi.sample[,1]),type='l',col='red')\nhist(phi.sample[,2],freq=FALSE,xlab=expression(phi[2]),main=\"\",ylim=c(0,6.4))\nlines(density(phi.sample[,2]),type='l',col='red')\nhist(nu.sample,freq=FALSE,xlab=expression(nu),main=\"\")\nlines(density(nu.sample),type='l',col='red')\n\n\n\n\n\n\n\nFigure 101.5: Posterior sampling of AR coefficients and variance\n\n\n\n\n\n\n\n\n101.7.2 Model Checking by In-sample Point and Interval Estimation\nTo check whether the model fits well, we plot the posterior point and interval estimate for each point.\n\n## get in sample prediction\npost.pred.y=function(s){\n  \n  beta.cur=matrix(phi.sample[s,],ncol=1)\n  nu.cur=nu.sample[s]\n  mu.y=t(Fmtx)%*%beta.cur\n  sapply(1:length(mu.y), function(k){rnorm(1,mu.y[k],sqrt(nu.cur))})\n  \n  \n}  \n\ny.post.pred.sample=sapply(1:5000, post.pred.y)\n\n\n## show the result\nsummary.vec95=function(vec){\n  c(unname(quantile(vec,0.025)),mean(vec),unname(quantile(vec,0.975)))\n}\n\nsummary.y=apply(y.post.pred.sample,MARGIN=1,summary.vec95)\n\nplot(Y,type='b',xlab='Time',ylab='',ylim=c(-7,7),pch=16)\nlines(summary.y[2,],type='b',col='grey',lty=2,pch=4)\nlines(summary.y[1,],type='l',col='purple',lty=3)\nlines(summary.y[3,],type='l',col='purple',lty=3)\nlegend(\"topright\",legend=c('Truth','Mean','95% C.I.'),lty=1:3,col=c('black','grey','purple'),\n       horiz = T,pch=c(16,4,NA))\n\n\n\n\n\n\n\nFigure 101.6: TS with prediction and credible interval",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#sec-capstone-prediction",
    "href": "C5-L01.html#sec-capstone-prediction",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.8 Prediction for AR Models 🎥",
    "text": "101.8 Prediction for AR Models 🎥\n AR(p) Model:\n\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_p y_{t-p} + \\varepsilon_t, \\qquad \\varepsilon_t\\sim \\mathcal{N}(0,\\nu)\n\nwhere (\\varepsilon_t) follows a normal distribution with mean zero and variance (\\nu).\n\nFirst Step Ahead Prediction, t&gt;T : \ny_{T+1} \\sim \\mathcal{N}(\\phi_1 y_T + \\ldots + \\phi_p y_{T+1-p}, \\nu)\n\nGeneral H-Step Ahead Prediction:\n\n\ny_{T+S}^s \\sim \\mathcal{N}\\left(\\sum_{j=1}^{p} \\phi_j^s y_{T+s-j}^s, \\nu^{(s)}\\right)\n\nwhere (\\phi_j^s) and (\\nu^{(s)}) are the posterior samples of the parameters.",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#sec-capstone-ar-prediction-example",
    "href": "C5-L01.html#sec-capstone-ar-prediction-example",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.9 AR model prediction example 📖 ℛ",
    "text": "101.9 AR model prediction example 📖 ℛ\nWe now give an example for coding the prediction algorithm in R. Consider the simulated data in the previous class, an AR(2) model with 200 observations obtained from y_t=0.5y_{t−1}+0.4y_{t−2}+\\varepsilon_t,\\varepsilon_t\\sim \\mathcal{N}(0,1).\nBefore doing the prediction, we have to obtain posterior samples of model parameters \\phi and \\nu, which has been taught in the previous class. Here we just copy the code from previous to obtain 5000 posterior samples of \\phi and \\nu\n\nlibrary(mvtnorm)\n\n## simulate data\n\nphi1&lt;-0.5\nphi2&lt;-0.4\nv&lt;-1\n\nset.seed(1)\ny.sample=arima.sim(n=200,model=list(order=c(2,0,0),ar=c(phi1,phi2),sd=sqrt(v)))\n\n\n## set up\nN&lt;-100\np&lt;-2 # order of AR process\nn.all=length(y.sample) ## T, total number of data\n\n\nY=matrix(y.sample[3:n.all],ncol=1)\nFmtx=matrix(c(y.sample[2:(n.all-1)],y.sample[1:(n.all-2)]),nrow=p,byrow=TRUE)\nn=length(Y)\n\n\n## posterior inference\n\n\n## set the prior\nm0=matrix(rep(0,p),ncol=1)\nC0=diag(p)\nn0=2\nd0=2\n\n\n## calculate parameters that will be reused in the loop\n\ne=Y-t(Fmtx)%*%m0\nQ=t(Fmtx)%*%C0%*%Fmtx+diag(n)\nQ.inv=chol2inv(chol(Q))\nA=C0%*%Fmtx%*%Q.inv\nm=m0+A%*%e\nC=C0-A%*%Q%*%t(A)\nn.star=n+n0\nd.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n\n\nn.sample=5000\n\n## store posterior samples\n\nnu.sample=rep(0,n.sample)\nphi.sample=matrix(0,nrow=n.sample,ncol=p)\n\n\nfor (i in 1:n.sample){\n  set.seed(i)\n  nu.new=1/rgamma(1,shape=n.star/2,rate=d.star/2)\n  nu.sample[i]=nu.new\n  phi.new=rmvnorm(1,mean=m,sigma=nu.new*C)\n  phi.sample[i,]=phi.new\n}\n\nAfter running the code, 5000 posterior samples of \\phi and \\nu are stored in phi.sample and nu.sample.\nThe h-step ahead prediction can be obtained by the following code:\n\n## the prediction function\n\n\ny_pred_h_step=function(h.step,s){\n phi.cur=matrix(phi.sample[s,],ncol=1)\n nu.cur=nu.sample[s]\n y.cur=c(y.sample[200],y.sample[199])\n y.pred=rep(0,h.step)\n for (i in 1:h.step) {\n  mu.y=sum(y.cur*phi.cur)\n  y.new=rnorm(1,mu.y,sqrt(nu.cur))\n  y.pred[i]=y.new\n  y.cur=c(y.new,y.cur)\n  y.cur=y.cur[-length(y.cur)]\n }\n return(y.pred)\n}\n\nFor illustration purposes, we obtain 3-step ahead predictions. The posterior distribution of Y_{201}, Y_{202}, Y_{203}, denoted as Y_{201}, Y_{202}, Y_{203}, is given by:\n\nset.seed(1)\ny.post.pred.ahead=sapply(1:5000, function(s){y_pred_h_step(h.step=3,s=s)})\n\n\npar(mfrow=c(1,3))\nhist(y.post.pred.ahead[1,],freq=FALSE,xlab=expression(y[201]),main=\"\")\nlines(density(y.post.pred.ahead[1,]),type='l',col='red')\nhist(y.post.pred.ahead[2,],freq=FALSE,xlab=expression(y[202]),main=\"\")\nlines(density(y.post.pred.ahead[2,]),type='l',col='red')\nhist(y.post.pred.ahead[3,],freq=FALSE,xlab=expression(y[203]),main=\"\")\nlines(density(y.post.pred.ahead[3,]),type='l',col='red')\n\n\n\n\n\n\n\n\n\nsummary.vec95=function(vec){\n c(unname(quantile(vec,0.025)),mean(vec),unname(quantile(vec,0.975)))\n}\n\n\napply(y.post.pred.ahead,MARGIN=1,summary.vec95)\n\n            [,1]        [,2]        [,3]\n[1,] -1.88717944 -2.12253036 -2.52266829\n[2,]  0.06139962  0.04574706  0.03225995\n[3,]  2.00420428  2.14813228  2.61098539\n\n\nPosterior mean and 95% confidence interval for \\phi_1 and \\phi_2 can be obtained by\n\nsummary.vec95=function(vec){\n c(unname(quantile(vec,0.025)),mean(vec),unname(quantile(vec,0.975)))\n}\n\napply(y.post.pred.ahead,MARGIN=1,summary.vec95)\n\n            [,1]        [,2]        [,3]\n[1,] -1.88717944 -2.12253036 -2.52266829\n[2,]  0.06139962  0.04574706  0.03225995\n[3,]  2.00420428  2.14813228  2.61098539",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#sec-capstone-extended-ar-model",
    "href": "C5-L01.html#sec-capstone-extended-ar-model",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "101.10 Extended AR model 📖",
    "text": "101.10 Extended AR model 📖\nThis is a five page Arima handout\nI have however added some callouts with question and counterexamples to assist my understanding of this and earlier material.\n\n101.10.1 Extending AR Models for Complex Time Series Data\nAfter we finished learning the materials from this module, you may wonder why we started with AR models. As we mentioned at the beginning, AR models are central to stationary time series data analysis and, as components of larger models or in suitably modified and generalized forms, underlie nonstationary time-varying models. This reading material will give examples about extending AR models for complex time series data. Hopefully these notes will help you build up confidence to this relatively simple model.\n\n\n101.10.2 Autoregressive Moving Average Models (ARMA)\n\n101.10.2.1 Characteristic polynomial of AR processes\n  An AR(p) process y_t is said to be causal if it can be written as a one-sided linear process dependent on the pastcausal Ar(p)\n\ny_t= \\Phi(\\operatorname{B})\\varepsilon_t= \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j}\n\\tag{101.14}\n Where \\operatorname{B} is the backshift operator, with \\operatorname{B}\\varepsilon_{t} = \\varepsilon_{t−1}, \\psi_0 = 1 and \\sum_{j=0}^{\\infty} |\\psi_j | &lt; \\infty.\ny_t is causal only when the autoregressive characteristic polynomial, defined as \n\\Phi(u) = 1 - \\sum_{j=1}^{p} \\phi_j u^j\n\\tag{101.15}\n  Has roots with moduli greater than unity. That is, y_t is causal if \\Phi(u) = 0 only when |u| &gt; 1. This causality condition implies stationarity, and so it is often referred as the stationary condition in the time series literature.  The autoregressive characteristic polynomial can also be written as \\Phi(u) = \\prod_{j=1}^{p}(1 - \\alpha_j u), so that its roots are the reciprocals of the \\alpha_j’s. The \\alpha_j’s may be real-valued or may appear as pairs of complex conjugates. Either way, if |\\alpha_j| &lt; 1 for all j, the process is stationary.causality \\implies stationaritystationary condition\n\n\n\n101.10.3 Structure of ARMA models\nConsider a time series y_t, for t = 1, 2, \\cdots, arising from the model\n\ny_t= \\sum_{i=1}^{p} \\phi_i y_{t-i} + \\sum_{j=1}^{q} \\theta_j \\varepsilon_{t-j} + \\varepsilon_t\n\\tag{101.16}\n With \\varepsilon_t \\sim \\mathcal{N}(0, \\nu). Then, \\{y_t\\} follows an autoregressive moving average model, or ARMA(p, q), where p and q are the orders of the autoregressive and moving average parts, respectively.  When p = 0, \\{y_t\\} is said to be a moving average process of order q or MA(q). Similarly, when q = 0, \\{y_t\\} is an autoregressive process of order p or AR(p).ARMA(p,q)\n\nExample 101.1 (MA(1) process) If \\{y_t\\} follows a MA(1) process, y_t = \\theta y_{t-1} + \\varepsilon_t, the process is stationary for all the values of \\theta. In addition, it is easy to see that the autocorrelation function has the following form:\n\n\\rho(h) =\n\\begin{cases}\n1 & h = 0 \\\\\n\\frac{\\theta}{1 + \\theta^2} & h = 1 \\\\\n0 & \\text{o.w.}\n\\end{cases}\n\\tag{101.17}\nNow, if we consider a MA(1) process with coefficient 1 instead of \\theta, we would obtain the same correlation function, and so it would be impossible to determine which of the two processes generated the data. Therefore, it is necessary to impose identifiability conditions on \\theta. In particular, \\frac{1}{\\theta} &gt; 1 is the identifiability condition for a MA(1), which is also known as the invertibility condition, given that it implies that the MA process can be “inverted” into an infinite order AR process. invertibility condition\n\nIn general, a MA(q) process is identifiable or invertible only when the roots of the MA characteristic polynomial \\Theta(u) = 1 + \\theta_1 u + \\ldots + \\theta_q u^q lie outside the unit circle. In this case it is possible to write the MA process as an infinite order AR process.\n\n\n\n\n\n\nWarningOrder \\neq Memory of MA(p) or AR(q) processes\n\n\n\nThroughout this unit and also before we are told about a finite order AR(p) \\eq MA(\\infty) and MA(q)=AR(\\infty). If like me you think this is a strange claim then perhaps you don’t understand the AR(1) and the MA(1) processes as well as you think.\nThe main issue here is understanding the relation of the parameters AR(p) and MA(q) and the memory of the process. If at some point you equate p or q in these processes to their memory you are due to a rude awakening. P and Q are just the number of parameters in the AR or MA process, they are the dimensions of matrices that represent the process.\nHowever, the actual memory of the process is determined by the behavior of the coefficients as the time goes to infinity. We can show how AR(1) and MA(1) can easily remember infinite number of past values.\nWhen we wish to represent such an AR(1) or MA(1) using their opposite we will quickly find counterexamples that require infinite number of parameters to represent the same process. In fact I would venture to say that only trivial or specially constructed AR and MA processes (e.g., white noise) are mutual finite-order representations. Most are infinite in one form or the other.\n\n\n\nExample 101.2 (🔴 AR(1) That Can’t Be Represented as Finite MA(q)) Consider the stationary AR(1) process:\n\nY_t = \\phi Y_{t-1} + \\varepsilon_t, \\quad |\\phi| &lt; 1\n\nThe infinite MA(∞) representation is:\n\nY_t = \\sum_{j=0}^{\\infty} \\phi^j \\varepsilon_{t-j}\n\nSuppose we ask: can we find any finite q such that:\n\nY_t = \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_q \\varepsilon_{t-q}\n\nThen we would need:\n\n\\phi^j = 0 \\quad \\text{for some finite } j &gt; q\n\nBut this is impossible unless \\phi = 0, which trivializes the AR(1) to white noise.\n👉 Therefore: A stationary AR(1) process (with \\phi \\neq 0) has infinite memory, and cannot be exactly represented as a finite MA(q).\n\nNow here’s a simple counterexample that shows a finite AR(p) can’t always be expressed exactly as a finite MA(q).\n\nExample 101.3 (🔴 MA(1) That Can’t Be Represented as Finite AR(p)) Now consider an MA(1):\n\nY_t = \\varepsilon_t + \\theta \\varepsilon_{t-1}, \\quad \\theta \\neq 0\n\nTo write it as a finite AR(p), we would need:\n\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\dots + \\varepsilon_t\n\nBut again, this fails except in special cases. For invertible \\theta, we can write an AR(∞):\n\nY_t = \\sum_{j=1}^\\infty \\phi_j Y_{t-j} + \\varepsilon_t\n\nBut truncating at any finite p yields approximation, not exact identity.\n\nFor an ARMA(p, q) process, the stationary condition is given in terms of the AR coefficients, i.e., the process is causal only when the roots of the AR characteristic polynomial \\Phi(u) = 1 − \\phi_1 u − \\ldots − \\phi_p u^p lie outside the unit circle. The ARMA process is invertible only when the roots of the MA characteristic polynomial lie outside the unit circle.\nSo, if the ARMA process is causal and invertible, it can be written either as a purely AR process of infinite order, or as a purely MA process of infinite order.\nIf \\{y_t\\} follows an ARMA(p,q) we can write \\Phi(B)y_t = \\Theta(B)\\varepsilon_t, with\n\n\\begin{aligned}\n\\Phi(B) &= 1 − \\phi_1\\operatorname{B} − \\ldots  − \\phi_p\\operatorname{B}^p \\\\\n\\Theta(B) &= 1 + \\theta_1\\operatorname{B} + \\ldots + \\theta_q\\operatorname{B}^q\n\\end{aligned}\n\\tag{101.18}\n\nwhere:\n\n\\operatorname{B} is the backshift operator.\n\\Phi(\\operatorname{B}) is the autoregressive characteristic polynomial of the ARMA model, with roots \\alpha_i and\n\\Theta(\\operatorname{B}) is the moving average characteristic polynomial of the ARMA model, with roots \\beta_j.\n\n\nIf the process is causal then we can write it as a purely MA process of infinite order\n\ny_t = \\Phi(\\operatorname{B})^{-1}\\Theta(\\operatorname{B})\\varepsilon_t = \\Psi(\\operatorname{B})\\varepsilon_t = \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j}\n\\tag{101.19}\nwith \\Psi(\\operatorname{B}) such that \\Phi(\\operatorname{B})\\Psi(\\operatorname{B}) = \\Theta(\\operatorname{B}). The \\psi_j values can be found by solving the homogeneous difference equations given by: \n\\psi_j − \\sum_{h=1}^{p} \\phi_h\\psi_{j−h} = \\theta_j, \\qquad j \\ge \\max (p, q + 1)\n\\tag{101.20}\nwhere \\phi_h are the AR coefficients and \\psi_j are the MA coefficients. with initial conditions:\n\n\\psi_j − \\sum_{h=1}^{j} \\phi_h\\psi_{j−h} = \\theta_j , \\qquad 0 ≤ j &lt; \\max(p, q + 1)\n\\tag{101.21}\nand θ_0. The general solution to the Equations Equation 101.19 and Equation 101.20 is given by\n\n\\psi_j = \\alpha_1^j p_1(j) + \\ldots + \\alpha_r^j p_r(j)\n\\tag{101.22}\nwhere \\alpha_1, \\ldots, \\alpha_r are the reciprocal roots of the characteristic polynomial \\Phi(u) = 0, with multiplicities m_1, \\ldots, m_r, respectively, and each p_i(j) is a polynomial of degree m_i - 1.\nSo the infinite decomposition yield a finite outcome!\n\n\n101.10.3.1 Inversion of AR components\nIn contexts where the time series has a reasonable length 1, we can fit long order AR models rather than ARMA or other, more complex forms. One key reason is that the statistical analysis, at least the conditional analysis based on fixed initial values, is much easier.\nIf this view is adopted in a given problem, it may be informative to use the results of an AR analysis to explore possible MA component structure using the device of inversion, or partial inversion, of the AR model.  Assume that \\{y_t\\} follows an AR(p) model with parameter vector \\phi = (\\phi_1, \\ldots, \\phi_p)^\\top, so we can writeinversion\n\n\\Phi(\\operatorname{B})y_t= \\prod_{i=1}^{p}(1 - \\alpha_iB)y_t = \\varepsilon_t\n\\tag{101.23}\nwhere:\n\n\\varepsilon_t \\sim \\mathcal{N}(0, \\nu) is a white noise process, and\n\\Phi(\\operatorname{B}) is the autoregressive characteristic polynomial of the AR model, with roots \\alpha_i and\n\\alpha_i are the autoregressive characteristic reciprocal roots.\n\nFor some positive integer r &lt; p, suppose that the final p − r reciprocal roots are identified as having moduli less than unity; some or all of the first r roots may also represent stationary components, though that is not necessary for the following development. Then, we can rewrite the model as\n\n\\prod_{i=1}^{r}(1 - \\alpha_iB) y_t = \\prod_{i=r+1}^{p}(1 - \\alpha_iB)^{-1} \\varepsilon_t = \\Psi^*(B) \\varepsilon_t\n\\tag{101.24}\nWhere the implicitly infinite order MA component has the coefficients of the infinite order polynomial.\n\n\\Psi^*(B) = \\prod_{i=1}^{r}(1 - \\alpha_iB)^{-1} \\prod_{i=r+1}^{p}(1 - \\alpha_iB)^{-1}\n\\tag{101.25}\nSo we have the representation\n\ny_t = \\sum \\phi^*_j y_{t−j} + \\varepsilon_t + \\sum_{j=1}^{\\infty} ψ^*_j \\varepsilon_{t−j}\n\\tag{101.26}\nwhere \\phi^*_j are the new AR coefficients and \\psi^*_j are the new MA coefficients.\nwhere the r new AR coefifients φ∗ j , for j = 1, · · · , r, are defined by the characteristic equation Φ∗(u) = (1 − αiu) = 0. The MA terms ψ∗ j can be easily calculated recursively, up to some appropriate upper bound on their number, say q.\nExplicitly, they are recursively computed as follows. 1. Initialize the algorithm by setting \\psi^*_i = 0 \\quad \\forall i \\in \\{1:q\\}. 2. For i = (r + 1) : p, update \\psi^*_1 = \\psi^*_1 + \\alpha_i, and then - for j = 2 : q, update \\psi^*_j = \\psi^*_j + \\alpha_i\\psi^*_j−1.\nSuppose \\phi is set at some estimate, such as a posterior mean, in the AR(p) model analysis. The above calculations can be performed for any specified value of r to compute the corresponding MA coefficients in an inversion to the approximating ARMA(r, q) model. If the posterior for \\phi is sampled in the AR analysis, the above computations can be performed repeated for all sampled \\phi vectors, so producing corresponding samples of the ARMA parameters \\phi^* and \\psi^*. Thus, inference in various relevant ARMA models can be directly, and quite easily, deduced by inversion of longer order AR models.\nTypically, various values of r will be explored. Guidance is derived from the estimated amplitudes and, in the case of complex roots, periods of the roots of the AR model. Analyses in which some components are persistent suggest that these components should be retained in the AR description. The remaining roots, typically corresponding to high frequency characteristics in the data with lower moduli, are then the candidates for inversion to what will often be a relatively low order MA component. The calculations can be repeated, sequentially increasing q and exploring inferences about the MA parameters, to assess a relevant approximating order.\n\n\n\n101.10.4 Smoothing and Differencing\nMany time series models are built under the stationary assumption. However, in many practical scenarios the data are realizations from one or several nonstationary processes. In this case, methods that aim to eliminate the nonstationary components are often used. The idea is to separate the nonstationary components, such as trends or seasonality, from the stationary ones so that the latter can be carefully studied via traditional time series models such as the aforementioned ARMA models. We briefly discuss two methods that are commonly used in practice for detrending and smoothing.\n\n101.10.4.1 Differencing\nDifferencing is used to remove trends in time series data. The first difference of a time series is defined in terms of the difference operator that we denoted as D, that produces the transformation Dyt = yt − yt−1. Higher order differences are obtained by successively applying the operator D.\nFor example\n\nD^2y_t = D(Dy_t) = D(y_t − y_{t−1}) = y_t − 2y_{t−1} + y_{t−2}\n\\tag{101.27}\nDifferencing can also be written in terms of the so called backshift operator B, with Byt = yt−1 so that Dy_t = (1 − B)y_t \\text{ and } D^dy_t = (1 − B)^dy_t.\n\n\n101.10.4.2 Moving Averages\nMoving averages is a method commonly used to ‘smooth’ a time series by removing certain features (e.g., seasonality) to highlight other features (e.g., trends). A moving average is a weighted average of the time series around a particular time t. In general, if we have data y_{1:T} , we could obtain a new time series such that\n\nz_t = \\sum_{j=−q}^p a_j y_{t+j}\n\\tag{101.28}\nwhere a_j are the weights, which can be chosen to be symmetric or asymmetric, and q and p are the number of past and future observations used in the average.\nfor t = (q + 1) : (T − p), with a_j \\ge 0 and \\sum_{j=−q}^p a_j = 1. Usually p = q and a_j = a_{−j}. For example, to remove seasonality in monthly data, one can use a moving average with p = 6, a_6 = a_{−6} = 1/24, and a_j = a_{−j} = 1/12 for k = 0, \\ldots , 5, resulting in:\n\nz_t = \\frac{1}{24} y_{t−6} + \\frac{1}{12} y_{t−5} + \\ldots + \\frac{1}{12} y_{t+5} + \\frac{1}{24} y_{t+6}\n\\tag{101.29}\n\n\n\n101.10.5 Epilogue\nWith the methodology we have discussed in this lesson, you should now be confident that the AR models can deal with a large class of time series data. In practice, one can first check the stationarity of the time series. If it contains nonstationary features, try using some detrending, deseasonalizing and smoothing method. Then for the resulting stationary series, using ARMA models and perform the inference by fitting a longer order AR model and inverting AR components.\n\n\n\n\n\n\nWarningQuasi-periodic data\n\n\n\nGreat that might help if the data is periodic or has a trend. But what if the data is quasi-periodic so that the model cannot be made stationary by differencing or smoothing?\nIn such a case a state-space methods should be used.",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01.html#footnotes",
    "href": "C5-L01.html#footnotes",
    "title": "101  Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1",
    "section": "",
    "text": "what is reasonable length? interpolation theorem lets us overfit to any length n with n+1 parameters↩︎",
    "crumbs": [
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01-Ex1.html",
    "href": "C5-L01-Ex1.html",
    "title": "102  Homework - Practice Quiz for Week 1 – M1L1",
    "section": "",
    "text": "102.1 Practice Quiz for Week 1",
    "crumbs": [
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Homework - Practice Quiz for Week 1 -- M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01-Ex1.html#practice-quiz-for-week-1",
    "href": "C5-L01-Ex1.html#practice-quiz-for-week-1",
    "title": "102  Homework - Practice Quiz for Week 1 – M1L1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Homework - Practice Quiz for Week 1 -- M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01-Ex2.html",
    "href": "C5-L01-Ex2.html",
    "title": "103  Homework - first-step-for-the-project – M1L1",
    "section": "",
    "text": "103.1 First step for the project",
    "crumbs": [
      "<span class='chapter-number'>103</span>  <span class='chapter-title'>Homework - first-step-for-the-project -- M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L01-Ex2.html#first-step-for-the-project",
    "href": "C5-L01-Ex2.html#first-step-for-the-project",
    "title": "103  Homework - first-step-for-the-project – M1L1",
    "section": "",
    "text": "Caution\n\n\n\nSection omitted to comply with the Honor Code",
    "crumbs": [
      "<span class='chapter-number'>103</span>  <span class='chapter-title'>Homework - first-step-for-the-project -- M1L1</span>"
    ]
  },
  {
    "objectID": "C5-L02.html",
    "href": "C5-L02.html",
    "title": "104  Model Selection Criteria - M2L2",
    "section": "",
    "text": "104.1 AIC and BIC in selecting the order of AR process 🎥\nNote: we have already covered Akaike information criteria (AIC) and Bayesian information criteria (BIC) a few times in this specialization. In Section 82.1 we discussed how to use BIC to select the number of components in a mixture model. In Section 104.1 we will see how to use AIC and BIC to select the order of an AR process. In Section 92.3 we discussed how to use BIC to select the order of an AR process.\nTo fit an AR(p) model to real data we need to determine the order p of the AR process.\nOne possible way is to repeat the analysis for different values of model order p and choose the best model based on some criteria. The two most widely known criteria are the so-called Akaike information criteria, AIC and the Bayesian Information criteria, BIC.\nRecall that our model in Equation 101.4 is:\n\\mathbf{y}\\sim \\mathcal{N}(\\mathbf{F}^\\top \\boldsymbol{\\phi},\\nu\\mathbf{I}_n),\\quad \\boldsymbol{\\phi}\\sim \\mathcal{N}(\\mathbf{m}_0,\\nu\\mathbf{C}_0),\\quad \\nu\\sim IG(\\frac{n_0}{2},\\frac{d_0}{2})\n\\tag{101.4}\nWe now describe how to select the order of AR process using AIC or BIC.",
    "crumbs": [
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>Model Selection Criteria - M2L2</span>"
    ]
  },
  {
    "objectID": "C5-L02.html#sec-aic-bic",
    "href": "C5-L02.html#sec-aic-bic",
    "title": "104  Model Selection Criteria - M2L2",
    "section": "",
    "text": "Figure 104.1: AIC and BIC\n\n\n\n\nOne approach is to treat this as a model selection problem , where we want to select the model with the best fit from multiple candidate models with different orders p.model selection problem\n\n\n\n\n\n\n\nNoteIs P a parameter of Hyper Parameter\n\n\n\n\n\nNote: I have pointed out before that while we can approach estimating the the number of component of a Mixture or the order of an AR(p) process as a hyperparameter tuning problem which involves training multiple models and using model selection criteria to pick the best one. Seems intuitive but is this the best way to do it?\nAnother approach might be to incorporate the number of components or the order of the AR process as a random variable in our model, and then use Bayesian inference to estimate its posterior distribution. This way we can avoid the need for model selection criteria and instead directly estimate the uncertainty around the number of components or the order of the AR process. I think this is also more or less how this is handled in bayesian switching models, where the number of different phases (stats) in the time series is treated as a random variable and the model is trained to infer the number of phases and their characteristics. C.f. (Davidson-Pilon 2015, chap. 2) where this is called switchpoint detection or change point analysis",
    "crumbs": [
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>Model Selection Criteria - M2L2</span>"
    ]
  },
  {
    "objectID": "C5-L02.html#sec-aic-bic-example",
    "href": "C5-L02.html#sec-aic-bic-example",
    "title": "104  Model Selection Criteria - M2L2",
    "section": "104.2 AIC and BIC example",
    "text": "104.2 AIC and BIC example\n\n104.2.1 Simulate Data\nWe simulate an AR process of order 2, and implement the AIC and BIC criteria to check if the best model selected has order 2.\nFirstly, the following code simulate an AR(2) process with 100 observations. The process is simulated from\n\ny_t=0.5y_{t−1}+0.4y_{t−2}+\\varepsilon_t,\\qquad \\varepsilon_t\\sim\\mathcal{N}(0,0.12)\n\n\n## simulate data\n\n\nset.seed(1)\nAR.model = list(order = c(2, 0, 0), ar = c(0.5,0.4))\ny.sample = arima.sim(n=100,model=AR.model,sd=0.1)\n\n\nplot(y.sample,type='l',xlab='time',ylab='')\n\n\n\n\n\n\n\n\nThe ACF and PACF plot of the simulated data is shown below.\n\npar(mfrow=c(1,2))\n\n\nacf(y.sample,main=\"\",xlab='Lag')\n\n\npacf(y.sample,main=\"\",xlab='Lag')\n\n\n\n\n\n\n\n\n\n\n104.2.2 Calculate AIC and BIC\nFor our case, we fix p^∗=15 as the maximum order of the AR process. The AIC and BIC are calculated for different values of p where 1≤p≤p^∗=15. \nT=100\n we will use the latter 85 observations for the analysis. We plot both AIC and BIC for different values of p where 1≤p≤p^∗=15 as follows:\n\nn.all=length(y.sample)\np.star=15\nY=matrix(y.sample[(p.star+1):n.all],ncol=1)\nsample.all=matrix(y.sample,ncol=1)\nn=length(Y)\np=seq(1,p.star,by=1)\n\ndesign.mtx=function(p_cur){\n  Fmtx=matrix(0,ncol=n,nrow=p_cur)\n  for (i in 1:p_cur) {\n    start.y=p.star+1-i\n    end.y=start.y+n-1\n    Fmtx[i,]=sample.all[start.y:end.y,1]\n  }\n  return(Fmtx)\n}\n\ncriteria.ar=function(p_cur){\n  Fmtx=design.mtx(p_cur)\n  beta.hat=chol2inv(chol(Fmtx%*%t(Fmtx)))%*%Fmtx%*%Y\n  R=t(Y-t(Fmtx)%*%beta.hat)%*%(Y-t(Fmtx)%*%beta.hat)\n  sp.square=R/(n-p_cur)\n  aic=2*p_cur+n*log(sp.square)\n  bic=log(n)*p_cur+n*log(sp.square)\n  result=c(aic,bic)\n  return(result)\n}\n\ncriteria=sapply(p,criteria.ar)\n\nplot(p,criteria[1,],type='p',pch='a',col='red',xlab='AR order p',ylab='Criterion',main='',\n    ylim=c(min(criteria)-10,max(criteria)+10))\npoints(p,criteria[2,],pch='b',col='blue')\n\n\n\n\n\n\n\n\nSince for AIC and BIC criteria, we both prefer model with smallest criterion, from the plot we will use p=2 which is the same as the order we used to simulate the data.",
    "crumbs": [
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>Model Selection Criteria - M2L2</span>"
    ]
  },
  {
    "objectID": "C5-L02.html#sec-dic",
    "href": "C5-L02.html#sec-dic",
    "title": "104  Model Selection Criteria - M2L2",
    "section": "104.3 Deviance information criterion (DIC)",
    "text": "104.3 Deviance information criterion (DIC)\n\n\n\n\n\n\nDavidson-Pilon, Cameron. 2015. Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference. Addison-Wesley Data & Analytics Series. Pearson Education. https://books.google.co.il/books?id=rMKiCgAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>Model Selection Criteria - M2L2</span>"
    ]
  },
  {
    "objectID": "A01.html",
    "href": "A01.html",
    "title": "105  Appendix: Notation",
    "section": "",
    "text": "105.1 The argmax function\nParameters describe the population and are usually designated with Greek letters and the preferred letter is \\theta\n\\theta\n\\tag{105.1}\nother common parameters are: \n\\mu, \\sigma^2, \\alpha, \\beta\n\\tag{105.2}\nwhere \\mu is the mean, \\sigma^2 is the variance, \\alpha is the intercept, and \\beta is the slope in a linear regression context.\nStatistics are population estimates of parameters and are usually designated with Latin letters, such as \\hat{\\theta}.\n\\hat{p}\n\\tag{105.3}\nthe Certain event\n\\Omega\nprobability of RV X taking value x\n\\mathbb{P}r(X=x)\nodds\n\\mathcal{O}(X)\nrandom variables \nX\nX is distributed as\nX\n\\sim N(\\mu, \\sigma^2)\nX is proportional to\nX\\propto N(\\mu, \\sigma^2)\nProbability of A and B\n\\mathbb{P}r(X \\cap Y)\nConditional probability\n\\mathbb{P}r(X \\mid Y)\nJoint probability\n\\mathbb{P}r(X,Y)\nY_i \\stackrel{iid}\\sim N(\\mu, \\sigma^2)\nApproximately distributed as (say using the CLT)\nY_i \\stackrel{.}\\sim N(\\mu, \\sigma^2)\n\\mathbb{E}[X_i]\nThe expected value of an RV X set to 0 (A.K.A. a fair bet)\n\\mathbb{E}[X_i]  \\stackrel{set} = 0\nThe variance of an RV\n\\mathbb{V}ar[X_i]\nlogical implication\n\\implies\nif and only if\n\\iff\ntherefore\n\\therefore\nindependence\nA \\perp \\!\\!\\! \\perp B \\iff \\mathbb{P}r(A \\mid B) = \\mathbb{P}r(A)\nIndicator function \n\\mathbb{I}_{\\{A\\}} = \\begin{cases}\n1 & \\text{if } A \\text{ is true} \\\\ 0 & \\text{otherwise} \\end{cases}\nDirichlet function\nThis is a continuous version of the indicator function, defined as a limit.\n\\delta(x) = \\lim_{\\varepsilon \\to 0} \\frac{1}{2\\varepsilon} \\mathbb{I}_{\\{|x| &lt; \\varepsilon\\}}\nThe Dirichlet function is used to represent a point mass at a point, often used as a component for zero inflated mixtures.\nThe following are from course 4\nWhen we want to maximize a function f(x), there are two things we may be interested in:",
    "crumbs": [
      "<span class='chapter-number'>105</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A01.html#sec-argmax-function",
    "href": "A01.html#sec-argmax-function",
    "title": "105  Appendix: Notation",
    "section": "",
    "text": "The value f(x) achieves when it is maximized, which we denote \\max_x f(x).\nThe x-value that results in maximizing f(x), which we denote \\hat x = \\arg \\max_x f(x). Thus \\max_x f(x) = f(\\hat x).",
    "crumbs": [
      "<span class='chapter-number'>105</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A01.html#sec-indicator-functions",
    "href": "A01.html#sec-indicator-functions",
    "title": "105  Appendix: Notation",
    "section": "105.2 Indicator Functions",
    "text": "105.2 Indicator Functions\nThe concept of an indicator function is a really useful one. This is a function that takes the value one if its argument is true, and the value zero if its argument is false. Sometimes these functions are called Heaviside functions or unit step functions. I write an indicator function as \\mathbb{I}_{A}(x), although sometimes they are written \\mathbb{1}_{A}(x). If the context is obvious, we can also simply write I{A}.\nExample:\n\n    \\mathbb{I}_{x&gt;3}(x)=\n    \\begin{cases}\n      0, & \\text{if}\\ x \\le 3 \\\\\n      1, & \\text{otherwise}\n    \\end{cases}\n\nNote: Indicator functions are easy to implement in code using a lambda function. They can be combined using a dictionary.\nBecause 0 · 1 = 0, the product of indicator functions can be combined into a single indicator function with a modified condition.\n\n105.2.1 Products of Indicator Functions:\n\n    \\mathbb{I}{x&lt;5} \\cdot \\mathbb{I}{x≥0} = \\mathbb{I}{0≤x&lt;5}\n\n\n    \\prod_{i=1}^N \\mathbb{I}_{(x_i&lt;2)} = \\mathbb{I}_{(x_i&lt;2) \\forall i} = \\mathbb{I}_{\\max_i x_i&lt;2}",
    "crumbs": [
      "<span class='chapter-number'>105</span>  <span class='chapter-title'>Appendix: Notation</span>"
    ]
  },
  {
    "objectID": "A02.html",
    "href": "A02.html",
    "title": "106  Appendix: Discrete Distributions",
    "section": "",
    "text": "106.1 Discrete Uniform",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-discrete-uniform",
    "href": "A02.html#sec-discrete-uniform",
    "title": "106  Appendix: Discrete Distributions",
    "section": "",
    "text": "106.1.1 Stories\n\n\n\n\n\n\nNoteDiscrete Uniform Finite set Parametrization\n\n\n\nLet C be a finite, nonempty set of numbers and X random variable associated with the event of choosing one of these numbers uniformly at random, that is all values being equally likely X(x=c)\nThen X is said to have the Discrete Uniform distribution with parameter C.\nWe denote this by X ∼ DUnif(C).\n\n\n\n\n\n\n\n\nNoteDiscrete Uniform with Lower and Upper bound Parametrization\n\n\n\nWhen the set C above is C=\\{c \\in \\mathbb{Z} \\mid a \\le c \\le b\\ \\}.\nThen X is said to have the Discrete Uniform distribution with lower bound parameter a and upper bound parameter b.\nWe denote this by X ∼ DUnif(a,b).\n\n\n\n\n\n\n\n\nNoteUrn Model\n\n\n\nSuppose we have an urn with n balls labeled with the numbers a 1, \\dots, a_n . One drawing from the urn produces a discrete uniform random variable on the set \\{a_1, \\dots, a_n \\}.\n\n\n\n\n106.1.2 Moments\n\n\\begin{aligned}\n    \\phi_X(t)&={\\displaystyle {\\frac {e^{at}-e^{(b+1)t}}{n(1-e^{t})}}}  && \\text{(MGF)}\\\\  \n    \\mathbb{E}[X] &= \\frac{a + b}{2} && \\text{(Expectation)} \\\\\n    \\mathbb{V}ar[X] &= \\frac{(b - a + 1)^2 - 1}{12} && \\text{(Variance)}\n\\end{aligned}\n\\tag{106.1}\n\n\n106.1.3 Probability mass function (PMF)\n\nf(x \\mid a, b) = \\frac{1}{b - a + 1}\n\n\n\n106.1.4 Cumulative distribution function (CDF)\n\nF(x \\mid a, b) = \\frac{\\lfloor x \\rfloor - a - 1}{b - a + 1} \\\\\n\\text{where} \\lfloor x \\rfloor \\text{ is the floor function (rounds down reals to nearest smaller integer)}\n\n\n\n106.1.5 Prior\nSince a number of families have the uniform as a special case we can use them as priors when we want a uniform prior:",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-bernoulli-distribution",
    "href": "A02.html#sec-bernoulli-distribution",
    "title": "106  Appendix: Discrete Distributions",
    "section": "106.2 Bernoulli Distribution",
    "text": "106.2 Bernoulli Distribution\n. \n\n106.2.1 Stories\n\n\n\n\n\n\nNote\n\n\n\nThe Bernoulli distribution arises when modeling the outcome of a binary event called a Bernoulli trial.\nLet X be the indicator variable corresponding to the success of getting “heads” in a “coin toss”, with a coin that has probability of success p for getting “heads”.\nThen X has a Bernoulli Distribution with parameter p\nWe denote this as X \\sim Bern(p)\n\n\n\n\n106.2.2 Parameters\nBecause of this story, the parameter p is often called the success probability of the Bern(p) distribution.\n\n\n106.2.3 Examples\n\n\n\n\n\n\nNote\n\n\n\n\nfair coin toss\nunfair coin toss\nad click\nweb site conversion\ndeath or survival of a patient in a medical trial\nindicator random variable\n\n\n\n\n\n106.2.4 Checklist\n\n\n\n\n\n\nNote\n\n\n\n\nDiscrete data\nA single trial\nOnly two trial outcomes: success and failure (These do not need to literally represent successes and failures, but this shorthand is typically used.)\n\n\n\n\n\\begin{aligned}\nX &\\sim Bernoulli(p)\\\\\n  & \\sim Bern(p)\\\\\n  & \\sim B(p)  \n\\end{aligned}\n\\tag{106.2}\n\n\n106.2.5 Moments\n\nM_X(t)=q+pe^{t} \\qquad \\text{(MGF)}\n\\tag{106.3}\n\n\\mathbb{E}[X]= p \\qquad \\text{(Expectation)}\n\\tag{106.4}\n\n\\mathbb{V}ar[x]= \\mathbb{P}r(1-p) \\qquad \\text{(Variance)}\n\\tag{106.5}\n\n\n106.2.6 PMF\nWhere parameter p is the probability of getting heads.\nThe probability for the two events is:\n\n\\mathbb{P}r(X=1) = p \\qquad \\mathbb{P}r(X=0)=1-p\n\n\n{\\displaystyle {\\begin{cases}1-p&{\\text{if }}k=0 \\\\\np&{\\text{if }}k=1\\end{cases}}}  \\qquad \\text{(PMF)}\n\\tag{106.6}\n\n\n106.2.7 CDF\n\n{\\displaystyle {\\begin{cases}0&{\\text{if }}k&lt;0  \\\\\n1-p&{\\text{if }}0\\leq k&lt;1 \\\\\n1&{\\text{if }}k\\geq 1\\end{cases}}} \\qquad \\text{(CDF)}\n\\tag{106.7}\n\n\n106.2.8 Likelihood\n\nL(\\theta) = \\prod p^x(1-p)^{1-x} \\mathbb{I}_{[0,1]}(x)  \\qquad \\text{(Likelihood)}\n\\tag{106.8}\n\n\\mathcal{L}(\\theta) =log(p) \\sum x + log(1-p)\\sum (1-x)  \\qquad \\text{(Log Likelihood)}\n\\tag{106.9}\n\n\n106.2.9 Entropy and Information\n\n\\mathbb{H}(x)= -q \\ln(q)- p \\ln(p) \\qquad \\text{(Entropy)}\n\\tag{106.10}\n\n\\mathcal{I}[X]\\frac{1}{\\mathbb{P}r(1-p)} \\qquad \\text{(Fisher Information)}\n\\tag{106.11}\n\nBeta(x) \\qquad \\text{(Conjugate Prior)}\n\\tag{106.12}\n\n\n106.2.10 Usage\n\n\n\nTable 106.1: Usage of Bernoulli\n\n\n\n\n\nPackage\nSyntax\n\n\n\n\nNumPy\nrg.choice([0, 1], p=[1-theta, theta])\n\n\nSciPy\nscipy.stats.bernoulli(theta)\n\n\nStan\nbernoulli(theta)\n\n\n\n\n\n\n\n\n106.2.11 Plots\n\nimport numpy as np\nfrom scipy.stats import bernoulli\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1)\np = 0.3\nmean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=0.30, var=0.21, skew=0.87, kurt=-1.24\n\nx = np.arange(bernoulli.ppf(0.01, p),\n              bernoulli.ppf(0.99, p))\nax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\nax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\n\nrv = bernoulli(p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n## Generate random numbers\nr = bernoulli.rvs(p, size=10)\nr\n\narray([0, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n\n\n\n\n\n\nA Swiss stamp issueed in 1994 depicting Mathematician Jakob Bernouilli and the formula and diagram of the law of large numbers\n\n\n\n\n\n\n\nTipBiographical note on Jacob Bernoulli\n\n\n\n\nIt seems that to make a correct conjecture about any event whatever, it is necessary to calculate exactly the number of possible cases and then to determine how much more likely it is that one case will occur than another. (Bernoulli 1713)\n\nThe Bernoulli distribution as well as The Binomial distribution are due to Jacob Bernoulli (1655-1705) who was a prominent mathematician in the Bernoulli family. He discovered the fundamental mathematical constant e. With his brother Johann, he was among the first to develop Leibniz’s calculus, introducing the word integral and applying it to polar coordinates and the study of curves such as the catenary, the logarithmic spiral and the cycloid\nHis most important contribution was in the field of probability, where he derived the first version of the law of large numbers (LLN). The LLN is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value and will tend to become closer to the expected value as more trials are performed. A special form of the LLN (for a binary random variable) was first proved by Jacob Bernoulli. It took him over 20 years to develop sufficiently rigorous mathematical proof.\n\nFor a more extensive biography visit the following link\n\n\nThe Bernoulli distribution is built on a trial of a coin toss (possibly biased).\n\nWe use the Bernoulli distribution to model a random variable for the probability of such a coin toss trial.\nWe use the Binomial distribution to model a random variable for the probability of getting k heads in N independent trails.",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-binomial-distribution",
    "href": "A02.html#sec-binomial-distribution",
    "title": "106  Appendix: Discrete Distributions",
    "section": "106.3 Binomial distribution",
    "text": "106.3 Binomial distribution\n\n106.3.1 Stories\n\n\n\n\n\n\nNote\n\n\n\n\n\\overbrace{\\underbrace{\\fbox{0}\\ \\ldots \\fbox{0}}_{N_0}\\ \\underbrace{\\fbox{1}\\ \\ldots \\fbox{1}}_{N_1}}^N\n\\tag{106.13}\nThe Binomial distribution arises when we conduct multiple independent Bernoulli trials and wish to model X the number of successes in Y_i\\mid \\theta identically distributed Bernoulli trials with the same probability of success \\theta. If n independent Bernoulli trials are performed, each with the same success probability p. The distribution of X is called the Binomial distribution with parameters n and p. We write X \\sim \\text{Bin}(n, p) to mean that X has the Binomial distribution with parameters n and p, where n is a positive integer and 0 &lt; p &lt; 1.\n\n\n\n\n106.3.2 Parameters\n\n\\theta - the probability of success in the Bernoulli trials\nN - the total number of trials being conducted\n\n\n\n106.3.3 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nDiscrete data\nTwo possible outcomes for each trial\nEach trial is independent and\nThe probability of success/failure is the same in each trial\nThe outcome is the aggregate number of successes\n\n\n\n\n\n106.3.4 Examples\n\nto model the aggregate outcome of clinical drug trials,\nto estimate the proportion of the population voting for each political party using exit poll data (where there are only two political parties).\n\n\nX \\sim Bin[n,p]\n\\tag{106.14}\n\nf(X=x \\mid \\theta) = {n \\choose x} \\theta^x(1-\\theta)^{n-x}\n\\tag{106.15}\n\nL(\\theta)=\\prod_{i=1}^{n} {n\\choose x_i}  \\theta ^ {x_i} (1− \\theta) ^ {(n−x_i)}\n\\tag{106.16}\n\n\\begin{aligned}\n\\ell( \\theta) &= \\log \\mathcal{L}( \\theta) \\\\\n              &= \\sum_{i=1}^n \\left[\\log {n\\choose x_i} + x_i \\log  \\theta + (n-x_i)\\log (1- \\theta) \\right]\n\\end{aligned}\n\\tag{106.17}\n\n\\mathbb{E}[X]= N \\times  \\theta\n\\tag{106.18}\n\n\\mathbb{V}ar[X]=N \\cdot \\theta \\cdot (1-\\theta)\n\\tag{106.19}\n\n\\mathbb{H}(X) = \\frac{1}{2}\\log_2 \\left (2\\pi n \\theta(1 - \\theta)\\right) + O(\\frac{1}{n})\n\\tag{106.20}\n\n\\mathcal{I}(\\theta)=\\frac{n}{ \\theta \\cdot (1- \\theta)}\n\\tag{106.21}\n\n\n106.3.5 Usage\n\n\n\nTable 106.2: Usage of Binomial\n\n\n\n\n\nPackage\nSyntax\n\n\n\n\nNumPy\nrg.binomial(N, theta)\n\n\nSciPy\nscipy.stats.binom(N, theta)\n\n\nStan\nbinomial(N, theta)\n\n\n\n\n\n\n\n\n106.3.6 Relationships\n\n\n\nbinomial distribution relations\n\n\n The Binomial Distribution is related to\n\nThe Binomial is a special case of the Multinomial distribution with K =2 (two categories).\nthe Poisson distribution distribution. If X \\sim Binomial(n, p) rv and Y \\sim Poisson(np) distribution then \\mathbb{P}r(X = n) ≈ \\mathbb{P}r(Y = n) for large n and small np.\nThe Bernoulli distribution is a special case of the the Binomial distribution \n\\begin{aligned}\nX & \\sim \\mathrm{Binomial}(n=1, p) \\\\\n\\implies X &\\sim \\mathrm{Bernoulli}(p)\n\\end{aligned}\n\nthe Normal distribution If X \\sim \\mathrm{Binomial}(n, p) RV and Y \\sim \\mathcal{N}(\\mu=np,\\sigma=n\\mathbb{P}r(1-p)) then for integers j and k, \\mathbb{P}r(j ≤ X ≤ k) ≈ \\mathbb{P}r(j – 1/2 ≤ Y ≤ k + 1/2). The approximation is better when p ≈ 0.5 and when n is large. For more information, see normal approximation to the Binomial\nThe Binomial is a limit of the Hypergeometric. The difference between a binomial distribution and a hypergeometric distribution is the difference between sampling with replacement and sampling without replacement. As the population size increases relative to the sample size, the difference becomes negligible. So If X \\sim \\mathrm{Binomial}(n, p) RV and Y \\sim \\mathrm{HyperGeometric}(N,a,b) then\n\n\\lim_{n\\to \\infty} X = Y\n\n\n\n106.3.7 Plots\n\nimport numpy as np\nfrom scipy.stats import binom\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\nn, p = 5, 0.4\nmean, var, skew, kurt = binom.stats(n, p, moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=2.00, var=1.20, skew=0.18, kurt=-0.37\n\nx = np.arange(binom.ppf(0.01, n, p),\n              binom.ppf(0.99, n, p))\nax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\nax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\nrv = binom(n, p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\n## generate random numbers\nr = binom.rvs(n, p, size=10)\nr\n\narray([1, 1, 2, 2, 4, 1, 3, 2, 1, 2])\n\n\nfrom __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nimport numpy as np\nimport scipy\nfrom scipy.special import gamma, factorial, comb\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\n#pyo.init_notebook_mode()\nINTERACT_FLAG=False\ndef binomial_vector_over_y(theta, n):\n    total_events = n\n    y =  np.linspace(0, total_events , total_events + 1)\n    p_y = [comb(int(total_events), int(yelem)) * theta** yelem * (1 - theta)**(total_events - yelem) for yelem in y]\n\n    fig = px.line(x=y, y=p_y, color_discrete_sequence=[\"steelblue\"], \n                  height=600, width=800, title=\" Binomial distribution for theta = %lf, n = %d\" %(theta, n))\n    fig.data[0].line['width'] = 4\n    fig.layout.xaxis.title.text = \"y\"\n    fig.layout.yaxis.title.text = \"P(y)\"\n    fig.show()\n    \nif(INTERACT_FLAG):    \n    interact(binomial_vector_over_y, theta=0.5, n=15)\nelse:\n    binomial_vector_over_y(theta=0.5, n=10)\n\n\n\n\n\n\n\nVideo 106.1: Binomial Distribution",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-hypergeometric-distribution",
    "href": "A02.html#sec-hypergeometric-distribution",
    "title": "106  Appendix: Discrete Distributions",
    "section": "106.4 Hypergeometric distribution",
    "text": "106.4 Hypergeometric distribution\n\n106.4.1 story 1 - Urn Model\nThe beta-binomial distribution with parameters \\alpha success rate and \\beta failure and n the number of trials can be motivated by an Pólya urn model.\nImagine a trial in which a ball is drawn without replacement from urn containing \\alpha white balls and \\beta black balls. If this is repeated n times, then the probability of observing x white balls follows a hypergeometric distribution with parameters n, \\alpha and \\beta.\nNote: is we used a\nIf the random draws are with simple replacement (no balls over and above the observed ball are added to the urn), then the distribution follows a binomial distribution and if the random draws are made without replacement, the distribution follows a hypergeometric distribution.\n\n\n\n\n\n\n\nVideo 106.2: The Hypergeometric Distribution\n\n\n\n\n106.4.2 Examples\n\nk white balls from an in Urn without replacement\ncapture-recapture\nAces in a poker hand\n\n\n\n106.4.3 Story\nConsider an urn with w white balls and b black balls. We draw n balls out of the urn at random without replacement, such that all w+b samples are equally likely. Let X be the number of white balls in n the sample. Then X is said to have the Hypergeometric distribution with parameters w, b, and n; we denote this by X \\sim \\mathcal{HG}(w, b, n) or X \\sim \\mathrm{HyperGeom}(w, b, n) or",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-poisson-distribution",
    "href": "A02.html#sec-poisson-distribution",
    "title": "106  Appendix: Discrete Distributions",
    "section": "106.5 Poisson distribution",
    "text": "106.5 Poisson distribution\n\n106.5.1 Stories\n\n\n\n\n\n\nNotePoisson Parametrization\n\n\n\nThe Poisson distribution arises when modeling the number of successes of independent and identically distributed (IID) events in a fixed interval of time or space, occurring at a constant rate \\lambda. Let X represent the count of the number of phone calls received at a call center in a given interval, such as an hour, with the parameter \\lambda corresponding to the average rate at which events occur in that interval. Then X is said to have the Poisson distribution with parameter \\lambda, and we denote this as X \\sim \\mathrm{Pois}(\\lambda).\n\nX \\sim \\mathrm{Pois}(\\lambda)\n\\tag{106.22}\n\n\n\n\n\n\n\n\n\nVideo 106.3: The Poisson Distribution\n\n\n\n\n106.5.2 Checklist\n\nCount of discrete events\nIndividual events occur at a given rate and independently of other events\nFixed amount of time or space in which the events can occur\n\n\n\n106.5.3 Examples\n\nThe number of emails you receive in an hour. There are a lot of people who could potentially email you at that hour, but it is unlikely that any specific person will actually email you at that hour. Alternatively, imagine subdividing the hour into milliseconds. There are 3.6×106 seconds in an hour, but in any specific millisecond, it is unlikely that you will get an email.\nThe number of chips in a chocolate chip cookie. Imagine subdividing the cookie into small cubes; the probability of getting a chocolate chip in a single cube is small, but the number of cubes is large.\nThe number of earthquakes in a year in some regions of the world. At any given time and location, the probability of an earthquake is small, but there are a large number of possible times and locations for earthquakes to occur over the course of the year.\nCount of component failures per week\nestimating the failure rate of artificial heart valves,\nestimating the prevalence of violent crimes in different districts,\napproximating the binomial which is, itself, being used to explain the prevalence of autism in the UK.\n\n\n\n106.5.4 Moments\n\n\\mathrm{E}(X) = \\lambda\n\n\n\\mathrm{V}ar(X) = \\lambda\n\n\n\n106.5.5 Probability mass function (PMF)\n\nf(x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\tag{106.23}\n\n\n106.5.6 Cumulative distribution function (CDF)\n\nF(x \\mid \\lambda) = \\frac{\\Gamma(\\lfloor x+1\\rfloor,\\lambda)}{\\lfloor x \\rfloor !} \\qquad \\text{CDF}\n\\tag{106.24}\n\n\\text{where }\\Gamma(u,v)=\\int_{v}^{\\infty}t^{u-1}e^{-t} \\mathrm{d}t \\text{ is the upper incomplete gamma function}\n\\tag{106.25}\n\n\\text{and } \\lfloor x \\rfloor \\text{ is the floor function (rounds down reals to nearest smaller integer)}\n\\tag{106.26}",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-geometric-distribution",
    "href": "A02.html#sec-geometric-distribution",
    "title": "106  Appendix: Discrete Distributions",
    "section": "106.6 Geometric distribution",
    "text": "106.6 Geometric distribution\n\n106.6.1 Stories\n\n\n\n\n\n\nNoteGeometric Distribution Failures before success\n\n\n\nConsider a sequence of independent Bernoulli trials, each with the same success probability p \\in (0, 1), with trials performed until a success occurs. Let X be the number of failures before the first successful trial. Then X has the Geometric distribution with parameter p; we denote this by X \\sim Geom(p).\nFor example, if we flip a fair coin until it lands Heads for the first time, then the number of Tails before the first occurrence of Heads is distributed as Geom(1/2).\nTo get the Geometric PMF from the story, imagine the Bernoulli trials as a string of 0’s (failures) ending in a single 1 (success). Each 0 has probability q = 1 − p and the final 1 has probability p, so a string of k failures followed by one success has probability q^kp.\n\n\n\n\n\n\n\n\nNoteGeometric distribution Failures and success\n\n\n\nConsider a sequence of independent Bernoulli trials, each with the same success probability p \\in (0, 1), with trials performed until a success occurs. Let X be the number of failures before the first successful trial. Then X has the Geometric distribution with parameter p; we denote this by X \\sim Geom(p).\nFor example, if we flip a fair coin until it lands Heads for the first time, then the number of Tails before the first occurrence of Heads is distributed as Geom(1/2).\nTo get the Geometric PMF from the story, imagine the Bernoulli trials as a string of 0’s (failures) ending in a single 1 (success). Each 0 has probability q = 1 − p and the final 1 has probability p, so a string of k failures followed by one success has probability q^kp.\n\n\n\n\n106.6.2 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nDiscrete data\nTwo possible outcomes for each trial\nEach trial is independent and\nThe probability of success/failure is the same in each trial\nThe outcome is the count of failures before the first success\n\n\n\n\n\n106.6.3 Examples\n\nConsider polymerization of an actin filament. At each time step, an actin monomer may add to the end of the filament (“failure”), or an actin monomer may fall off the end (“success”) with (usually very low) probability θ. The length of actin filaments, measured in a number of constitutive monomers, is Geometrically distributed.\n\nThe Geometric distribution arises when we want to know “What is the number of Bernoulli trials required to get the first success?”, i.e., the number of Bernoulli events until a success is observed, such as the probability of getting the first head when flipping a coin. It takes values on the positive integers starting with one (since at least one trial is needed to observe a success).\n\n\n\n\n\n\n\nVideo 106.4: The Geometric Distribution\n\n\n\nX \\sim Geo(p)\n\\tag{106.27}\n\n\n106.6.4 Moments\n\n\\mathbb{M}_X[t] = \\frac{pe^t}{1-(1-p)e^t} \\qquad t&lt;-ln(1-p)\n\\tag{106.28}\n\n\\mathbb{E}[X] = \\frac{1}{p}\n\\tag{106.29}\n\n\\mathbb{V}ar[X]=\\frac{1-p}{p^2}\n\\tag{106.30}\n\n\n106.6.5 PMF\n\n\\mathbb{P}r(X = x \\mid p) = \\mathbb{P}r(1-p)^{x-1} \\qquad \\forall x \\in N;\\quad 0\\le p \\le 1\n\\tag{106.31}\n\n\n106.6.6 CDF\n\n1-(1-p)^{\\lfloor x\\rfloor } \\qquad x&lt;1\n\\tag{106.32}\n\n\n106.6.7 Memoryless property\n\nThe geometric distribution is based on geometric series.\nThe geometric distribution has the memoryless property:\n\nP (X &gt; s \\mid X &gt;  t) = P (X &gt; s − t)\n\nOne can say that the distribution “forgets” what has occurred, so that The probability of getting an additional s − t failures, having already observed t failures, is the same as the probability of observing s − t failures at the start of the sequence. In other words, the probability of getting a run of failures depends only on the length of the run, not on its position.\nY=X-1 is the \\text{negative binomial}(1,p)\n\n\n106.6.8 Worked out Examples\n\nExample 106.1 (Geometric Distribution) The Geometric distribution arises when we consider how long we will have to “wait for a success” during repeated Bernoulli trials.\nWhat is the probability that we flip a fair coin four times and don’t see any heads?\nThis is the same as asking what is \\mathbb{P}r(X &gt; 4) where X ∼ Geo(1/2).\n\n  \\begin{aligned}\n    \\mathbb{P}r(X &gt; 4) &= 1 − \\mathbb{P}r(X =1)−\\mathbb{P}r(X = 2)−\\mathbb{P}r(X = 3)−\\mathbb{P}r(X = 4) \\\\\n    &= 1−(\\frac{1}{2})−(\\frac{1}{2})(\\frac{1}{2})−(\\frac{1}{2})(\\frac{1}{2})^2−(\\frac{1}{2})(\\frac{1}{2})^3  \\\\\n   &= \\frac{1}{16}\n    \\end{aligned}\n\nOf course, we could also have just computed it directly, but here we see an example of using the geometric distribution and we can also see that we got the right answer.\n\n\n\n106.6.9 Plots\n\nimport numpy as np\nfrom scipy.stats import geom\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\np = 0.5\nmean, var, skew, kurt = geom.stats(p,moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=2.00, var=2.00, skew=2.12, kurt=6.50\n\nx = np.arange(geom.ppf(0.01, p),\n              geom.ppf(0.99, p))\nax.plot(x, geom.pmf(x, p), 'bo', ms=8, label='geom pmf')\nax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=5, alpha=0.5)\n\nrv = geom(p)\nax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n        label='frozen pmf')\nax.legend(loc='best', frameon=False)\nplt.show()\n\n\n\n\n\n\n\nr = geom.rvs(p,size=10)\nr\n\narray([ 1, 10,  2,  2,  1,  2,  3,  1,  1,  2])",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-negative-binomial-distribution",
    "href": "A02.html#sec-negative-binomial-distribution",
    "title": "106  Appendix: Discrete Distributions",
    "section": "106.7 Negative Binomial Distribution",
    "text": "106.7 Negative Binomial Distribution\n\n106.7.1 Story\n\n\n\n\n\n\nNote\n\n\n\nIn a sequence of independent Bernoulli trials with success probability p, if X is the number of failures before the rth success, then X is said to have the Negative Binomial distribution with parameters r and p, denoted X \\sim NBin(r, p).\n\n\nBoth the Binomial and the Negative Binomial distributions are based on independent Bernoulli trials; they differ in the stopping rule and in what they are counting.\nThe Binomial counts the number of successes in a fixed number of trials; the Negative Binomial counts the number of failures until a fixed number of successes.\nIn light of these similarities, it comes as no surprise that the derivation of the Negative Binomial PMF bears a resemblance to the corresponding derivation for the Binomial.\n\n\n106.7.2 Parameters\n\nr the number of successes.\np the probability of the Bernoulli trial.\n\n\n\n106.7.3 Conditions\n\n\n\n\n\n\nTip\n\n\n\n\nCount of discrete events\nNon-independent events; it is sometimes said that the events can exhibit contagion, meaning that if one event occurs, it is more likely that another will also occur\nCan model a data-generating process where the variance exceeds the mean\nFixed amount of time or space in which the events can occur\n\n\n\n\n\n106.7.4 Examples\n\nStamp collection - Suppose there are n types of stamps, which you are collecting one by one, with the goal of getting a complete set. When collecting stamps, the stamp types are random. Assume that each time you collect a stamp, it is equally likely to be any of the n types. What is the expected number of toys needed until you have a complete set?\neverything the Poisson can do and more,\nto model the number of measles cases that occur on an island,\nthe number of banks that collapse in a financial crisis.\nthe length of a hospital stay\nthe probability you will have to visit Y houses if you must sell r cookies before returning home\n\n\n\n106.7.5 Moments\n\n\\mathrm{E}(X) = \\lambda\n\n\nvar(X) = \\lambda + \\frac{\\lambda^2}{\\kappa}\n\n\n\n106.7.6 Probability mass function (PMF)\n\nf(x \\mid \\lambda,\\kappa) = \\frac{\\Gamma(x+\\kappa)}{x!\\Gamma(\\kappa+1)}\\left(\\frac{\\lambda}{\\lambda+\\kappa}\\right)^x \\left(\\frac{\\kappa}{\\lambda+\\kappa}\\right)^\\kappa\n\n\n\n106.7.7 Cumulative distribution function (CDF)\n\nF(x \\mid \\lambda,\\kappa) =\n\\begin{cases}\n  I_{\\frac{\\kappa}{\\kappa+\\lambda}}(\\kappa,1+\\lfloor x \\rfloor), & x \\ge q 0 \\\\\n  0,                                                             & \\text{Otherwise}\n\\end{cases}\n\n\n\\text{where } I_w(u,v) \\text{ is the regularised incomplete beta function: }\nI_w(u,v) = \\frac{B(w; u, v)}{B(u,v)}\n\n\n\\text{where } B(w; u,v)=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the incomplete beta function and }\\\\\nB(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the complete beta function}",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#sec-multinomial-distribution",
    "href": "A02.html#sec-multinomial-distribution",
    "title": "106  Appendix: Discrete Distributions",
    "section": "106.8 Multinomial Distribution",
    "text": "106.8 Multinomial Distribution\nThe Multinomial distribution is a generalization of the Binomial. Whereas the Binomial distribution counts the successes in a fixed number of trials that can only be categorized as success or failure, the Multinomial distribution keeps track of trials whose outcomes can fall into multiple categories, such as excellent, adequate, poor; or red, yellow, green, blue.\n\n106.8.1 Story\nMultinomial distribution. Each of N objects is independently placed into one of k categories. An object is placed into category j with probability p_j ,P where the p_j are non-negative and \\sum^k_{j=1} p_j = 1. Let X_1 be the number of objects in category 1, X_2 the number of objects in category 2, etc., so that X_1 + \\dots + X_k = n. Then X = (X_1 , \\dots , X_k ) is said to have the Multinomial distribution with parameters n and p = (p_1 , \\dots , p_k ). We write this as X \\sim Mult_k(n, p).\nWe call X a random vector because it is a vector of random variables. The joint PMF of X can be derived from the story.\n\n\n106.8.2 Examples\n\nBlood type counts across n individuals\nNumbers of people voting for each party in a sample\n\n\n\n106.8.3 Moments\n\n\\mathrm{E}(X_i) = n p_i \\text{, }\\forall i\n\n\nvar(X_i) = n p_i (1-p_i) \\text{, }\\forall i\n\n\ncov(X_i,X_j) = -n p_i p_j \\text{, }\\forall i\\neq j\n\n\n\n106.8.4 Probability Mass Function (PMF)\n\nf(x_1,x_2,\\dots,x_d \\mid n,p_1,p_2,\\dots,p_d) = \\frac{n!}{x_1 ! x_2 ! \\dots x_d !} p_1^{x_1} p_2^{x_2}\\dots p_d^{x_d}",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A02.html#beta-binomial",
    "href": "A02.html#beta-binomial",
    "title": "106  Appendix: Discrete Distributions",
    "section": "106.9 Beta Binomial",
    "text": "106.9 Beta Binomial\n\n106.9.1 Story 1 - Polya Urn Model\nThe beta-binomial distribution with parameters \\alpha success rate and \\beta failure and n the number of trials can be motivated by an Pólya urn model.\nImagine an urn containing \\alpha red balls and \\beta black balls, where random draws are made. If a red ball is observed, then two red balls are returned to the urn. Likewise, if a black ball is drawn, then two black balls are returned to the urn. If this is repeated n times, then the probability of observing x red balls follows a beta-binomial distribution with parameters n, \\alpha and \\beta.\nIf the random draws are with simple replacement (no balls over and above the observed ball are added to the urn), then the distribution follows a binomial distribution and if the random draws are made without replacement, the distribution follows a hypergeometric distribution.\n\n\n106.9.2 Story 2 compound distribution\nThe Beta distribution is a conjugate distribution of the binomial distribution. This fact leads to an analytically tractable compound distribution constructed in a hierarchical fashion where one can think of the p parameter in the binomial distribution as being randomly drawn from a beta distribution.\nSuppose we were interested in predicting the number of heads, x in n future trials. This is given by\n\n{\\displaystyle {\n    \\begin{aligned}\n        f(x\\mid n,\\alpha ,\\beta )&=\\int _{0}^{1}\\mathrm {Bin} (x \\mid n,p)\\mathrm {Beta} (p\\mid \\alpha ,\\beta )\\,dp\\\\\n                            [6pt]&={n \\choose x}{\\frac {1}{\\mathrm {B} (\\alpha ,\\beta )}}\\int _{0}^{1}p^{x+\\alpha -1}(1-p)^{n-x+\\beta -1}\\,dp \\\\\n                            [6pt]&={n \\choose x}{\\frac {\\mathrm {B} (x+\\alpha ,n-x+\\beta )}{\\mathrm {B} (\\alpha ,\\beta )}}.\n    \\end{aligned}}}\n\n\n{\\displaystyle f(x\\mid n,\\alpha ,\\beta )={\\frac {\\Gamma (n+1)}{\\Gamma (x+1)\\Gamma (n-x+1)}}{\\frac {\\Gamma (x+\\alpha )\\Gamma (n-x+\\beta )}{\\Gamma (n+\\alpha +\\beta )}}{\\frac {\\Gamma (\\alpha +\\beta )}{\\Gamma (\\alpha )\\Gamma (\\beta )}}.}\n\n\n\n106.9.3 Moments\n\n\\mathrm{E}(X) = \\frac{n\\alpha}{\\alpha+\\beta}\n \nvar(X) = \\frac{n\\alpha\\beta(\\alpha+\\beta+n)}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\n\n\n\n106.9.4 Probability mass function (PMF)\n\nf(x \\mid n,\\alpha,\\beta) = \\binom{n}{x}\\frac{B(x+\\alpha,n-x+\\beta)}{B(\\alpha,\\beta)}\n\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the (complete) beta function }\n\n\n\n106.9.5 Cumulative distribution function (CDF)\n\nF(x\\mid n,\\alpha,\\beta) = \\begin{cases}\n0, & x&lt;0 \\\\\n\\binom{n}{x}\\frac{B(x+\\alpha,n-x+\\beta)}{B(\\alpha,\\beta)} {}_{3}F_2(1,-x,n-x+\\beta;n-x-1,1-x-\\alpha;1), & 0\\leq x \\leq n \\\\\n1, & x&gt;n \\end{cases}\n\n\n\\text{where } {}_{3}F_2(a,b,x) \\text{ is the generalised hypergeometric function}\n\n\n\n106.9.6 Relations\n\nThe Pascal distribution (after Blaise Pascal) is special cases of the negative binomial distribution. Used with an integer-valued stopping-time parameter r\nThe Pólya distribution (for George Pólya) is special cases of the negative binomial distribution. Used with a real-valued-valued stopping-time parameter r\n\n\n\n\n\nA photo of Hungarian Mathematician George Pólya\n\n\n\n\n\n\n\nTipBiographical note on George Pólya\n\n\n\n\nThe cookbook gives a detailed description of ingredients and procedures but no proofs for its prescriptions or reasons for its recipes; the proof of the pudding is in the eating … Mathematics cannot be tested in exactly the same manner as a pudding; if all sorts of reasoning are debarred, a course of calculus may easily become an incoherent inventory of indigestible information. (Polya 1945)\n\nPólya was arguably the most influential mathematician of the 20th century. His basic research contributions span complex analysis, mathematical physics, probability theory, geometry, and combinatorics. He was a teacher par excellence who maintained a strong interest in pedagogical matters throughout his long career.\nHe was awarded a doctorate in mathematics having studied, essentially without supervision, a problem in the theory of geometric probability. Later Pólya looked at the Fourier transform of a probability measure, showing in 1923 that it was a characteristic function. He wrote on the normal distribution and coined the term “central limit theorem” in 1920 which is now standard usage.\nIn 1921 he proved his famous theorem on random walks on an integer lattice. He considered a d-dimensional array of lattice points where a point moves to any of its neighbors with equal probability. He asked whether given an arbitrary point A in the lattice, a point executing a random walk starting from the origin would reach A with probability 1. Pólya’s surprising answer was that it would for d=1 and for d=2, but it would not for d\\ge 3. In later work he looked at two points executing independent random walks and also at random walks satisfying the condition that the moving point never passed through the same lattice point twice.\nOne of Pólya’s notable achievements was his collaboration with the economist Abraham Wald during World War II. They developed statistical techniques to solve military problems, including estimating enemy troop movements and predicting the effectiveness of bombing missions. These contributions played a vital role in aiding the Allies during the war.\nHis book “How to Solve It,” published in 1945, presented problem-solving heuristics applicable to various mathematical domains, including probability and statistics. This influential work emphasized the importance of understanding the problem, devising a plan, executing the plan, and reflecting on the results. Pólya’s problem-solving strategies continue to be widely taught and practiced.\n\nFor a more extensive biography visit the following link\n\n\n\n\n\n\n\n\nBernoulli, J. 1713. Ars Conjectandi [the Art of Conjecturing]. Impensis Thurnisiorum. https://books.google.co.il/books?id=Ba5DAAAAcAAJ.\n\n\nPolya, G. 1945. How to Solve It. Princeton University Press. https://doi.org/10.1515/9781400828678.",
    "crumbs": [
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>Appendix: Discrete Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html",
    "href": "A03.html",
    "title": "107  Appendix: Continuous Distributions",
    "section": "",
    "text": "107.1 The Continuous Uniform\nFollowing a subjective view of distribution, which is more amenable to reinterpretation I use an indicator function to place restrictions on the range of parameter of the PDF.",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-continuous-uniform",
    "href": "A03.html#sec-continuous-uniform",
    "title": "107  Appendix: Continuous Distributions",
    "section": "",
    "text": "107.1.1 Stories\n\n\n\n\n\n\nNoteDiscrete Uniform Finite set Parametrization\n\n\n\n\n\n\n\nX \\sim U[\\alpha,\\beta]\n\\tag{107.1}\n\n\n107.1.2 Moments\n\n\\mathbb{E}[X]=\\frac{(\\alpha+\\beta)}{2}\n\\tag{107.2}\n\n\\mathbb{V}ar[X]=\\frac{(\\beta-\\alpha)^2}{12}\n\\tag{107.3}\n\n\n107.1.3 Probability mass function (PDF)\n\nf(x)= \\frac{1}{\\alpha-\\beta} \\mathbb{I}_{\\{\\alpha \\le x \\le \\beta\\}}(x)\n\\tag{107.4}\n\n\n107.1.4 Cumulative distribution function (CDF)\n\nF(x\\mid \\alpha,\\beta)=\\begin{cases}\n  0,  & \\text{if }x &lt; \\alpha \\\\\n  \\frac{x-\\alpha}{\\beta-\\alpha}, & \\text{if } x\\in [\\alpha,\\beta]\\\\\n  1, & \\text{if } x &gt; \\beta\n  \\end{cases}\n\\tag{107.5}\n\n\n107.1.5 Prior\nSince a number of families have the uniform as a special case we can use them as priors when we want a uniform prior:\nNormal(0,1)= Beta(1,1)",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-beta-distribution",
    "href": "A03.html#sec-the-beta-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.2 The Beta Distribution",
    "text": "107.2 The Beta Distribution\n\n\n107.2.1 Story\nThe Beta distribution is used for random variables which take on values between 0 and 1. For this reason (and other reasons we will see later in the course), the Beta distribution is commonly used to model probabilities.\n\nX \\sim Beta(\\alpha, \\beta)\n\\tag{107.6}\n\n\n107.2.2 PDF & CDF\n\nf(x \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha−1}(1 − x)^{\\beta−1}\\mathbb{I}_{x\\in(0,1)}\\mathbb{I}_{\\alpha\\in\\mathbb{R}^+}\\mathbb{I}_{\\beta\\in\\mathbb{R}^+} \\qquad \\text{(PDF)}\n\\tag{107.7}\n\n\\begin{aligned}\n                 & F(x \\mid \\alpha,\\beta) &= I_x(\\alpha,\\beta) && \\text{(CDF)} \\\\\n   \\text{where } & I_w(u,v) & &&\\text{ is the regularized beta function: } \\\\\n                 & I_w(u,v) &= \\frac{B(w; u, v)}{B(u,v)} \\\\\n    \\text{where }& B(w; u,v) &=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t  && \\text{ is the incomplete beta function  }\\\\\n    \\text{and }  & B(u,v)& && \\text{ is the (complete) beta function}\n\\end{aligned}\n\\tag{107.8}\n\n\n107.2.3 Moments\n\n\\mathbb{E}[X] = \\frac{\\alpha}{\\alpha + \\beta} \\qquad (\\text{expectation})\n\\tag{107.9}\n\n\\mathbb{V}ar[X] = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)} \\qquad (\\text{variance})\n\\tag{107.10}\n\n\\mathbb{M}_X(t) = 1+ \\sum^\\infty_{i=1} \\left ( {\\prod^\\infty_{j=0} \\frac{\\alpha+j}{\\alpha + \\beta + j} } \\right ) \\frac{t^i}{i!}\n\\tag{107.11}\nwhere \\Gamma(·) is the Gamma function introduced with the gamma distribution.\nNote also that \\alpha &gt; 0 and \\beta &gt; 0.\n\n\n107.2.4 Relations\n\n\n\nRelations of the Beta distribution\n\n\nThe standard Uniform(0, 1) distribution is a special case of the beta distribution with \\alpha = \\beta = 1.\n\nUniform(0, 1) = Beta(1,1)\n\\tag{107.12}\n\n\n107.2.5 As a prior\nThe Beta distribution is often used as a prior for parameters that are probabilities,since it takes values from 0 and 1.\nDuring prior elicitation the parameters can be set using\n\nthe mean: \\alpha \\over \\alpha +\\beta which I would interpret here as count of successes over trials prior to seeing the data.\nvariance: Equation 107.10 or\nThe effective sample size which is \\alpha+\\beta (see course 1 lesson 7.3 for the derivation).",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-cauchy",
    "href": "A03.html#sec-cauchy",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.3 The Cauchy Distribution",
    "text": "107.3 The Cauchy Distribution\n\n\n107.3.1 PDF\n\n\\text{Cauchy}(y\\mid\\mu,\\sigma) = \\frac{1}{\\pi \\sigma} \\\n\\frac{1}{1 + \\left((y - \\mu)/\\sigma\\right)^2} \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{107.13}\n\n\n107.3.2 CDF\n\nF(x \\mid \\mu, \\sigma) = \\frac{1}{2} + \\frac{1}{\\pi}\\text{arctan}\\left(\\frac{x-\\mu}{\\sigma}\\right) \\qquad \\text{(CDF)}\n\\tag{107.14}\n\n\\mathbb{E}(X) = \\text{ undefined}\n\n\n\\mathbb{V}ar[X] = \\text{ undefined}\n\n\n\n107.3.3 As a prior\n\nThe Cauchy despite having no mean or variance is recommended as a prior for regression coefficients in Logistic regression. see (Gelman et al. 2008) this is analyzed and discussed in (Ghosh, Li, and Mitra 2018)",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-double-exponential",
    "href": "A03.html#sec-double-exponential",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.4 Double Exponential Distribution (Laplace)",
    "text": "107.4 Double Exponential Distribution (Laplace)\n\n\n\\text{DoubleExponential}(y \\mid \\mu,\\sigma) =\n\\frac{1}{2\\sigma} \\exp \\left( - \\, \\frac{\\|y - \\mu\\|}{\\sigma} \\right)\n\\qquad \\text (PDF)\n\\tag{107.15}",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-gamma-distribution",
    "href": "A03.html#sec-the-gamma-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.5 The Gamma Distribution",
    "text": "107.5 The Gamma Distribution\n\n\n107.5.1 Story\nIf X_1, X_2, ..., X_n are independent (and identically distributed \\mathrm{Exp}(\\lambda)) waiting times between successive events, then the total waiting time for all n events to occur Y = \\sum X_i will follow a gamma distribution with shape parameter \\alpha = n and rate parameter \\beta = \\lambda:\nWe denote this as:\n\nY =\\sum^N_{i=0} \\mathrm{Exp}_i(\\lambda) \\sim \\mathrm{Gamma}(\\alpha = N, \\beta = \\lambda)\n\\tag{107.16}\n\n\n107.5.2 PDF\n\nf(y \\mid \\alpha , \\beta) = \\frac{\\beta^\\alpha} {\\Gamma(\\alpha)} y^{\\alpha−1} e^{− \\beta y} \\mathbb{I}_{y \\ge \\theta }(y)\n\\tag{107.17}\n\n\n107.5.3 Moments\n\n\\mathbb{E}[Y] = \\frac{\\alpha}{ \\beta}\n\\tag{107.18}\n\n\\mathbb{V}ar[Y] = \\frac{\\alpha}{ \\beta^2}\n\\tag{107.19}\nwhere \\Gamma(·) is the gamma function, a generalization of the factorial function which can accept non-integer arguments. If n is a positive integer, then \\Gamma(n) = (n − 1)!.\nNote also that \\alpha &gt; 0 and $ &gt; 0$.\n\n\n107.5.4 Relations\n\n\n\nRelations of the Gamma Distribution\n\n\nThe exponential distribution is a special case of the Gamma distribution with \\alpha = 1. The gamma distribution commonly appears in statistical problems, as we will see in this course. It is used to model positive-valued, continuous quantities whose distribution is right-skewed. As \\alpha increases, the gamma distribution more closely resembles the normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#inverse-gamma-distribution",
    "href": "A03.html#inverse-gamma-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.6 Inverse Gamma Distribution",
    "text": "107.6 Inverse Gamma Distribution\n\n\n107.6.1 PDF\n\n\\mathcal{IG}(y\\mid\\alpha,\\beta) =\n\\frac{1} {\\Gamma(\\alpha)}\\frac{\\beta^{\\alpha}}{y^{\\alpha + 1}}  e^{- \\frac{ \\beta}{y}}\n   \\ \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{\\beta \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)\n\\tag{107.20}\n\n\n107.6.2 Moments\n\n\\mathbb{E}[X]=\\frac{\\beta}{\\alpha - 1} \\qquad \\text{Expectation}\n\\tag{107.21}\n\n\\mathbb{V}ar[X]=\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}\\qquad \\text{Variance}\n\\tag{107.22}",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#the-z-or-standard-normal-distribution",
    "href": "A03.html#the-z-or-standard-normal-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.7 The Z or Standard normal distribution",
    "text": "107.7 The Z or Standard normal distribution\n· The Standard normal distribution is given by:\n\nZ \\sim \\mathcal{N}[1,0]\n\\tag{107.23}\n\nf(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{z^2}{2}}\n\\tag{107.24}\n\n\\mathcal{L}(\\mu,\\sigma)=\\prod_{i=1}^{n}{1 \\over 2 \\pi \\sigma}e^{−(x_i−\\mu)^2 \\over 2 \\sigma^2}\n\\tag{107.25}\n\n\\begin{aligned}\n\\ell(\\mu, \\sigma) &= \\log \\mathcal{L}(\\mu, \\sigma) \\\\\n&= -\\frac{n}{2}\\log(2\\pi) - n\\log\\sigma - \\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2}\n\\end{aligned}\\sigma^2\n\\tag{107.26}\n\n\\begin{aligned}\n  \\mathbb{E}(Z)&= 0 \\quad \\text{(Expectation)} \\qquad  \\mathbb{V}ar(Z)&= 1 \\quad \\text{(Variance)}\n\\end{aligned}\n\\tag{107.27}",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-normal-distribution",
    "href": "A03.html#sec-the-normal-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.8 The Normal Distribution",
    "text": "107.8 The Normal Distribution\n The normal, or Gaussian distribution is one of the most important distributions in statistics.\nIt arises as the limiting distribution of sums (and averages) of random variables. This is due to the Section 110.1. Because of this property, the normal distribution is often used to model the “errors,” or unexplained variations of individual observations in regression models.\nNow consider X = \\sigma Z+\\mu where \\sigma &gt; 0 and \\mu is any real constant. Then \\mathbb{E}[X] = \\mathbb{E}[\\sigma Z+\\mu] = \\sigma E[Z] + \\mu = \\sigma_0 + \\mu = \\mu and \\mathbb{V}ar[X] = Var(\\sigma^2 Z + \\mu) = \\sigma^2 Var(Z) + 0 = \\sigma^2 \\cdot 1 = \\sigma^2\nThen, X follows a normal distribution with mean \\mu and variance \\sigma^2 (standard deviation \\sigma) denoted as\n\nX \\sim N[\\mu,\\sigma^2]\n\\tag{107.28}\n\n107.8.1 PDF\n\nf(x\\mid\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}  e^{-\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}(x-\\mu)^2}\n\\tag{107.29}\n\n\n107.8.2 Moments\n\n\\mathbb{E}(x)= \\mu\n\\tag{107.30}\n\nVar(x)= \\sigma^2\n\\tag{107.31}\n\nThe normal distribution is symmetric about the mean \\mu and is often described as a bell-shaped curve.\nAlthough X can take on any real value (positive or negative), more than 99% of the probability mass is concentrated within three standard deviations of the mean.\n\nThe normal distribution has several desirable properties.\nOne is that if X_1 \\sim N(\\mu_1, \\sigma^2_1) and X_2 ∼ N(\\mu_2, \\sigma^2_2) are independent, then X_1+X_2 \\sim N(\\mu_1+\\mu_2, \\sigma^2_1+\\sigma^2_2).\nConsequently, if we take the average of n Independent and Identically Distributed (IID) Normal random variables we have:\n\n\\bar X = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim N(\\mu, \\frac{\\sigma^2}{n})\n\\tag{107.32}\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = norm.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=0.00, var=1.00, skew=0.00, kurt=0.00\n\nx = np.linspace(norm.ppf(0.01),\n                norm.ppf(0.99), 100)\nax.plot(x, norm.pdf(x),\n       'r-', lw=5, alpha=0.6, label='norm pdf')\n\nrv = norm()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\nr = norm.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n(array([0.00737665, 0.00368832, 0.01106497, 0.05901319, 0.06270151,\n       0.10696141, 0.12909135, 0.23236443, 0.25080606, 0.35776746,\n       0.31719589, 0.35039081, 0.36145579, 0.37989741, 0.32088422,\n       0.2581827 , 0.20285784, 0.12909135, 0.05532487, 0.03688324,\n       0.02581827, 0.0147533 , 0.00368832, 0.00737665, 0.00368832]), array([-3.19698904, -2.92586321, -2.65473738, -2.38361155, -2.11248572,\n       -1.84135989, -1.57023406, -1.29910823, -1.0279824 , -0.75685656,\n       -0.48573073, -0.2146049 ,  0.05652093,  0.32764676,  0.59877259,\n        0.86989842,  1.14102425,  1.41215008,  1.68327591,  1.95440174,\n        2.22552757,  2.4966534 ,  2.76777923,  3.03890506,  3.31003089,\n        3.58115672]), [&lt;matplotlib.patches.Polygon object at 0x73ba9cfc1e40&gt;])\n\nax.set_xlim([x[0], x[-1]])\n\n(-2.3263478740408408, 2.3263478740408408)\n\nax.legend(loc='best', frameon=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-t-distribution",
    "href": "A03.html#sec-the-t-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.9 The t-Distribution",
    "text": "107.9 The t-Distribution\n If we have normal data, we can use (Equation 107.32) to help us estimate the mean \\mu. Reversing the transformation from the previous section, we get:\n\n\\frac {\\hat X - \\mu}{\\sigma / \\sqrt(n)} \\sim N(0, 1)\n\\tag{107.33}\nHowever, we may not know the value of \\sigma. If we estimate it from data, we can replace it with S = \\sqrt{\\sum_i \\frac{(X_i-\\hat X)^2}{n-1}}, the sample standard deviation. This causes the expression (Equation 107.33) to no longer be distributed as a Standard Normal; but as a standard t-distribution with ν = n − 1 degrees of freedom\n\nX \\sim t[\\nu]\n\\tag{107.34}\nf(t\\mid\\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}}\\left (1 + \\frac{t^2}{\\nu}\\right)^{-(\\frac{\\nu+1}{2})}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{107.35}\n\n\\text{where }\\Gamma(w)=\\int_{0}^{\\infty}t^{w-1}e^{-t}\\mathrm{d}t \\text{ is the gamma function}\n\nf(t\\mid\\nu)={\\frac {1}{{\\sqrt {\\nu }}\\,\\mathrm {B} ({\\frac {1}{2}},{\\frac {\\nu }{2}})}}\\left(1+{\\frac {t^{2}}{\\nu }}\\right)^{-(\\nu +1)/2}\\mathbb{I}_{t\\in\\mathbb{R}} \\qquad \\text{(PDF)}\n\\tag{107.36}\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the beta function}\n\n\\begin{aligned}\n&& F(t)&=\\int _{-\\infty }^{t}f(u)\\,du=1-{\\tfrac {1}{2}}I_{x(t)}\\left({\\tfrac {\\nu }{2}},{\\tfrac {1}{2}}\\right) &&\\text{(CDF)} \\\\\n   \\text{where } && I_{x(t)}&= \\frac{B(x; u, v)}{B(u,v)}                 &&\\text{is the regularized Beta function} \\\\\n   \\text{where } && B(w; u,v)&=\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t &&  \\text{ is the incomplete Beta function } \\\\\n   \\text {and }  && B(u,v)&=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t    && \\text{ is the (complete) beta function}\n\\end{aligned}\n\\tag{107.37}\n\\int _{-\\infty }^{t}f(u)\\,du={\\tfrac {1}{2}}+t{\\frac {\\Gamma \\left({\\tfrac {1}{2}}(\\nu +1)\\right)}{{\\sqrt {\\pi \\nu }}\\,\\Gamma \\left({\\tfrac {\\nu }{2}}\\right)}}\\,{}_{2}F_{1}\\left({\\tfrac {1}{2}},{\\tfrac {1}{2}}(\\nu +1);{\\tfrac {3}{2}};-{\\tfrac {t^{2}}{\\nu }}\\right)\n\n\n\\mathcal{L}(\\mu, \\sigma, \\nu) = \\prod_{i=1}^n \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right)^{-\\frac{\\nu+1}{2}}\n\\tag{107.38}\n\n\\begin{aligned}\n\\ell(\\mu, \\sigma, \\nu) &= \\log \\mathcal{L}(\\mu, \\sigma, \\nu) \\\\\n&= \\sum_{i=1}^n \\left[\\log\\Gamma\\left(\\frac{\\nu+1}{2}\\right) - \\log\\sqrt{\\nu\\pi} - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\frac{\\nu+1}{2}\\log\\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right)\\right] \\\\\n&= n\\log\\Gamma\\left(\\frac{\\nu+1}{2}\\right) - n\\log\\sqrt{\\nu\\pi} - n\\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\frac{\\nu+1}{2}\\sum_{i=1}^n\\log\\left(1+\\frac{(x_i-\\mu)^2}{\\sigma^2\\nu}\\right).\n\\end{aligned}\n\\tag{107.39}\n\n\\mathbb{E}[Y] = 0 \\qquad \\text{ if } \\nu &gt; 1\n\\tag{107.40}\n\n\\mathbb{V}ar[Y] = \\frac{\\nu}{\\nu - 2} \\qquad \\text{ if } \\nu &gt; 2\n\\tag{107.41}",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#location-scale-parametrization-t-distribution",
    "href": "A03.html#location-scale-parametrization-t-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.10 Location Scale Parametrization t-distribution",
    "text": "107.10 Location Scale Parametrization t-distribution\n\nX=\\mu+\\sigma T\n\nThe resulting distribution is also called the non-standardized Student’s t-distribution.\nthis is another parameterization of the student-t with:\n\nlocation \\mu \\in \\mathbb{R}^+\nscale \\sigma \\in \\mathbb{R}^+\ndegrees of freedom \\nu \\in \\mathbb{R}^+\n\n\nf(x \\mid \\mu, \\sigma, \\nu) = \\frac{\\left(\\frac{\\nu }{\\nu +\\frac{(x-\\mu )^2}{\\sigma ^2}}\\right)^{\\frac{\\nu+1}{2}}}{\\sqrt{\\nu } \\sigma  B\\left(\\frac{\\nu }{2},\\frac{1}{2} \\right)}\n\\tag{107.42}\n\n\\text{where } B(u,v)=\\int_{0}^{1}t^{u-1}(1-t)^{v-1}\\mathrm{d}t \\text{ is the beta function}\n\n\nF(\\mu, \\sigma, \\nu) =\n\\begin{cases}\n\\frac{1}{2} I_{\\frac{\\nu  \\sigma ^2}{(x-\\mu )^2+\\nu  \\sigma  ^2}}\\left(\\frac{\\nu }{2},\\frac{1}{2}\\right),                & x\\leq \\mu  \\\\\n\\frac{1}{2} \\left(I_{\\frac{(x-\\mu )^2}{(x-\\mu )^2+\\nu  \\sigma   ^2}}\\left(\\frac{1}{2},\\frac{\\nu }{2}\\right)+1\\right), & \\text{Otherwise}\n\\end{cases}\n\\tag{107.43}\nwhere I_w(u,v) is the regularized incomplete beta function:\n\nI_w(u,v) = \\frac{B(w; u, v)}{B(u,v)}\n\nwhere B(w; u,v) is the incomplete beta function:\n\nB(w; u,v) =\\int_{0}^{w}t^{u-1}(1-t)^{v-1}\\mathrm{d}t\n\nAnd B(u,v) is the (complete) beta function\n\n\\mathbb{E}[X] = \\begin{cases}\n  \\mu,               & \\text{if }\\nu &gt; 1  \\\\\n  \\text{undefined} & \\text{ otherwise}\n\\end{cases}\n\\tag{107.44}\n\n\\mathbb{V}ar[X] = \\frac{\\nu \\sigma^2}{\\nu-2}\n\\tag{107.45}\nThe t distribution is symmetric and resembles the Normal Distribution but with thicker tails. As the degrees of freedom increase, the t distribution looks more and more like the standard normal distribution.\n\n\n\n\n\n\n\nFigure 107.1: William Sealy Gosset AKA Student\n\n\n\n\n\n\n\n\nTipHistorical Note on The William Sealy Gosset A.K.A Student\n\n\n\n The student-t distribution is due to Gosset, William Sealy (1876-1937) who was an English statistician, chemist and brewer who served as Head Brewer of Guinness and Head Experimental Brewer of Guinness and was a pioneer of modern statistics. He is known for his pioneering work on small sample experimental designs. Gosset published under the pseudonym “Student” and developed most famously Student’s t-distribution – originally called Student’s “z” – and “Student’s test of statistical significance”.\nHe was told to use a Pseudonym and choose ‘Student’ after a predecessor at Guinness published a paper that leaked trade secrets. Gosset was a friend of both Karl Pearson and Ronald Fisher. Fisher suggested a correction to the student-t using the degrees of freedom rather than the sample size. Fisher is also credited with helping to publicize its use.\nfor a full biography see (Pearson et al. 1990)",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-the-exponential-distribution",
    "href": "A03.html#sec-the-exponential-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.11 The Exponential Distribution",
    "text": "107.11 The Exponential Distribution\n\n\n107.11.1 Story\nThe Exponential distribution models the waiting time between events for events with a rate lambda. Those events, typically, come from a Poisson process\nThe exponential distribution is often used to model the waiting time between random events. Indeed, if the waiting times between successive events are independent then they form an Exp(λ) distribution. Then for any fixed time window of length t, the number of events occurring in that window will follow a Poisson distribution with mean tλ.\n\nX \\sim Exp[\\lambda]\n\\tag{107.46}\n\n\n107.11.2 PDF\n\nf(x \\mid \\lambda) = \\frac{1}{\\lambda} e^{- \\frac{x}{\\lambda}}(x)\\mathbb{I}_{\\lambda\\in\\mathbb{R}^+ } \\mathbb{I}_{x\\in\\mathbb{R}^+_0 } \\quad \\text{(PDF)}\n\\tag{107.47}\n\n\n107.11.3 CDF\n\nF(x \\mid \\lambda) = 1 - e^{-\\lambda x} \\qquad \\text{(CDF)}\n\n\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda x_i}\n\\tag{107.48}\n\n\\begin{aligned} \\ell(\\lambda) &= \\log \\mathcal{L}(\\lambda) \\\\\n&= \\sum_{i=1}^n \\log(\\lambda) - \\lambda x_i \\\\\n&= n\\log(\\lambda) - \\lambda\\sum_{i=1}^n x_i \\end{aligned}\n\\tag{107.49}\n\n\n107.11.4 Moments\n\n\\mathbb{E}(x)= \\lambda\n\\tag{107.50}\n\n\\mathbb{V}ar[X]= \\lambda^2\n\\tag{107.51}\n\n\\mathbb{M}_X(t)= \\frac{1}{1-\\lambda t} \\qquad t &lt; \\frac{1}{\\gamma}\n\\tag{107.52}\n\n\n107.11.5 Special cases:\n \n\nWeibull Y = X^{\\frac{1}{\\gamma}}\nRayleigh Y = \\sqrt{\\frac{2X}{\\lambda}}\nGumbel Y=\\alpha - \\gamma \\log(\\frac{X}{\\lambda})\n\n\n\n107.11.6 Properties:\n\nmemoryless\n\n\nimport numpy as np\nfrom scipy.stats import expon\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 1)\n\nn, p = 5, 0.4\nmean, var, skew, kurt = expon.stats(moments='mvsk')\nprint(f'{mean=:1.2f}, {var=:1.2f}, {skew=:1.2f}, {kurt=:1.2f}')\n\nmean=1.00, var=1.00, skew=2.00, kurt=6.00\n\nx = np.linspace(expon.ppf(0.01), expon.ppf(0.99), 100)\nax.plot(x, expon.pdf(x), 'r-', lw=5, alpha=0.6, label='expon pdf')\n\nrv = expon()\nax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n\nr = expon.rvs(size=1000)\n\nax.hist(r, density=True, bins='auto', histtype='stepfilled', alpha=0.2)\n\n(array([0.79628554, 0.68381583, 0.60283764, 0.44088126, 0.37789822,\n       0.32391276, 0.29692003, 0.15745759, 0.15745759, 0.12596607,\n       0.10347213, 0.05848425, 0.06298304, 0.04498788, 0.08547698,\n       0.02249394, 0.02249394, 0.02699273, 0.0404891 , 0.00899758,\n       0.        , 0.01349637, 0.00899758, 0.00449879, 0.        ,\n       0.00449879, 0.01349637, 0.        , 0.        , 0.        ,\n       0.00899758, 0.        , 0.        , 0.00449879]), array([2.39229972e-03, 2.24674371e-01, 4.46956443e-01, 6.69238515e-01,\n       8.91520587e-01, 1.11380266e+00, 1.33608473e+00, 1.55836680e+00,\n       1.78064887e+00, 2.00293095e+00, 2.22521302e+00, 2.44749509e+00,\n       2.66977716e+00, 2.89205923e+00, 3.11434130e+00, 3.33662338e+00,\n       3.55890545e+00, 3.78118752e+00, 4.00346959e+00, 4.22575166e+00,\n       4.44803373e+00, 4.67031581e+00, 4.89259788e+00, 5.11487995e+00,\n       5.33716202e+00, 5.55944409e+00, 5.78172617e+00, 6.00400824e+00,\n       6.22629031e+00, 6.44857238e+00, 6.67085445e+00, 6.89313652e+00,\n       7.11541860e+00, 7.33770067e+00, 7.55998274e+00]), [&lt;matplotlib.patches.Polygon object at 0x73ba9cedc940&gt;])\n\nax.set_xlim([x[0], x[-1]])\n\n(0.010050335853501442, 4.605170185988091)\n\nax.legend(loc='best', frameon=False)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-lognormal-distribution",
    "href": "A03.html#sec-lognormal-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.12 LogNormal Distribution",
    "text": "107.12 LogNormal Distribution\nThe long normal arises when the a log transform is applied to the normal distribution.\n\n\n\\text{LogNormal}(y\\mid\\mu,\\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\ \\sigma} \\, \\frac{1}{y} \\ \\exp \\! \\left( - \\, \\frac{1}{2} \\, \\left( \\frac{\\log y - \\mu}{\\sigma} \\right)^2 \\right) \\ \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)\n\\tag{107.53}",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-pareto-distribution",
    "href": "A03.html#sec-pareto-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.13 Pareto Distribution",
    "text": "107.13 Pareto Distribution\n\n\n\\text{Pareto}(y|y_{\\text{min}},\\alpha) = \\frac{\\displaystyle\n\\alpha\\,y_{\\text{min}}^\\alpha}{\\displaystyle y^{\\alpha+1}} \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{y_{min} \\in \\mathbb{R}^+}\\mathbb{I}_{y\\ge y_{min} \\in \\mathbb{R}^+}\n\\qquad \\text (PDF)\n\\tag{107.54}\n\n\\mathrm{Pareto\\_Type\\_2}(y|\\mu,\\lambda,\\alpha) = \\\n\\frac{\\alpha}{\\lambda} \\, \\left( 1+\\frac{y-\\mu}{\\lambda}\n\\right)^{-(\\alpha+1)} \\! \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\lambda \\in \\mathbb{R}^+}\\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{y\\ge \\mu \\in \\mathbb{R}}\n\\qquad \\text (PDF)\n\\tag{107.55}\n\n\\mathbb{E}[X]=\\displaystyle{\\frac{\\alpha y_\\mathrm{min}}{\\alpha - 1}}\\mathbb{I}_{\\alpha&gt;1} \\qquad \\text (expectation)\n\\tag{107.56}\n\n\\mathbb{V}ar[X]=\\displaystyle{\\frac{\\alpha y_\\mathrm{min}^2}{(\\alpha - 1)^2(\\alpha - 2)}}\\mathbb{I}_{\\alpha&gt;2} \\qquad \\text (variance)\n\\tag{107.57}",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-weibull-distribution",
    "href": "A03.html#sec-weibull-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.14 Weibull Distribution",
    "text": "107.14 Weibull Distribution\n\n\n107.14.1 PDF\n\n\\text{Weibull}(y|\\alpha,\\sigma) =\n\\frac{\\alpha}{\\sigma} \\, \\left( \\frac{y}{\\sigma} \\right)^{\\alpha - 1} \\, e^{ - \\left( \\frac{y}{\\sigma} \\right)^{\\alpha}} \\mathbb{I}_{\\alpha \\in \\mathbb{R}^+}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}^+} \\qquad \\text (PDF)",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-chi-squared-distribution",
    "href": "A03.html#sec-chi-squared-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.15 Chi Squared Distribution",
    "text": "107.15 Chi Squared Distribution\n\nThe chi squared distribution is a special case of the gamma. It is widely used in hypothesis testing and the construction of confidence intervals. It is parameterized using parameter \\nu for the degrees of predom\n\n107.15.1 PDF:\n\n\\text{ChiSquare}(y\\mid\\nu) = \\frac{2^{-\\nu/2}}     {\\Gamma(\\nu / 2)} \\,\ny^{\\nu/2 - 1} \\, \\exp \\! \\left( -\\, \\frac{1}{2} \\, y \\right) \\mathbb{I}_{\\nu \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}} \\qquad \\text (PDF)\n\\tag{107.58}\n\n\n107.15.2 CDF:\n\n{\\frac {1}{\\Gamma (\\nu/2)}}\\;\\gamma \\left({\\frac {\\nu}{2}},\\,{\\frac {x}{2}}\\right) \\qquad \\text (CDF)\n\\tag{107.59}\n\n\n107.15.3 MOMENTS\n\n\\mathbb{E}[X]=\\nu\n\\tag{107.60}\n\n\\mathbb{V}ar[X] = 2\\nu\n\\tag{107.61}",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-logistic-distribution",
    "href": "A03.html#sec-logistic-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.16 Logistic Distribution",
    "text": "107.16 Logistic Distribution\n\n\\text{Logistic}(y|\\mu,\\sigma) = \\frac{1}{\\sigma} \\\n\\exp\\!\\left( - \\, \\frac{y - \\mu}{\\sigma} \\right) \\ \\left(1 + \\exp\n\\!\\left( - \\, \\frac{y - \\mu}{\\sigma} \\right) \\right)^{\\!-2} \\mathbb{I}_{\\mu \\in \\mathbb{R}}\\mathbb{I}_{\\sigma \\in \\mathbb{R}^+}\\mathbb{I}_{y \\in \\mathbb{R}}  \\qquad \\text (PDF)\n\\tag{107.62}",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A03.html#sec-f-distribution",
    "href": "A03.html#sec-f-distribution",
    "title": "107  Appendix: Continuous Distributions",
    "section": "107.17 F Distribution",
    "text": "107.17 F Distribution\n   The F-distribution or F-ratio, arises frequently as the null distribution of a test statistic, in the analysis of variance (ANOVA) and other F-tests.F DistributionF-ratio\n\n107.17.1 PDF\n\n\\frac {\\sqrt {\\frac {(d_{1}x)^{d_{1}}d_{2}^{d_{2}}}{(d_{1}x+d_{2})^{d_{1}+d_{2}}}}}{x\\,\\mathrm {B} \\!\\left({\\frac {d_{1}}{2}},{\\frac {d_{2}}{2}}\\right)}\n\\tag{107.63}\n\n\n107.17.2 CDF\n\n\\mathbb{I}_{\\frac {d_{1}x}{d_{1}x+d_{2}}}\\left({\\tfrac {d_{1}}{2}},{\\tfrac {d_{2}}{2}}\\right)\n\\tag{107.64}\n\n\n107.17.3 Moments\n\n\\mathbb{E}[X]=\\frac {d_{2}}{d_{2}-2}\n\\tag{107.65}\n\n\\mathbb{V}ar[X] = {\\frac {2\\,d_{2}^{2}\\,(d_{1}+d_{2}-2)}{d_{1}(d_{2}-2)^{2}(d_{2}-4)}}\n\\tag{107.66}\n\n\n\n\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” The Annals of Applied Statistics 2 (4). https://doi.org/10.1214/08-aoas191.\n\n\nGhosh, Joyee, Yingbo Li, and Robin Mitra. 2018. “On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression.” Bayesian Analysis 13 (2). https://doi.org/10.1214/17-ba1051.\n\n\nPearson, E. S., W. S. Gosset, R. L. Plackett, and G. A. Barnard. 1990. Student: A Statistical Biography of William Sealy Gosset. Clarendon Press. https://books.google.co.il/books?id=LBDvAAAAMAAJ.",
    "crumbs": [
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>Appendix: Continuous Distributions</span>"
    ]
  },
  {
    "objectID": "A05.html",
    "href": "A05.html",
    "title": "108  Appendix: Exponents & Logarithms",
    "section": "",
    "text": "108.1 Exponents\nExponents are of the form a^x where:\nRecall that a^0 = 1. Exponents have the following useful properties\nNote: that the first property requires that both terms have the same base a.\nWe cannot simplify a^x ·b^y if a \\ne b.",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A05.html#sec-exponents",
    "href": "A05.html#sec-exponents",
    "title": "108  Appendix: Exponents & Logarithms",
    "section": "",
    "text": "a (called the base) and\nx (called the exponent) is any real number.\n\n\n\na^x· a^y = a^{x+y}\n(a^x)^y = a^{x·y}\n\n\n\n\nOne common base is the number e which is approximately equal to 2.7183.\nThe function e^x is so common in mathematics and has its own symbol e^x = \\exp(x).\nBecause e &gt; 0 we have e^x &gt; 0 for all real numbers x\n\\lim_{x \\to \\infty} x = e^{−x} = 0.",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A05.html#sec-natural-logarithms",
    "href": "A05.html#sec-natural-logarithms",
    "title": "108  Appendix: Exponents & Logarithms",
    "section": "108.2 Natural Logarithms",
    "text": "108.2 Natural Logarithms\nWe will need to manipulate long products of probabilities. Since there often comprise small fractions, their calculation on computers can be problematic due to the underflow of floats. We will therefore prefer to convert these products into sums of logarithms.\n\nDefinition 108.1 (The Logarithm) A log is the inverse of a power. We can use (Equation 108.1).\n\ny = a^x \\implies log_a(y) = x\n\\tag{108.1}\n\n\nDefinition 108.2 (The Natural log) The natural logarithm function has base e and is written without the subscript\n\nlog_e(y) = log(y)\n\\tag{108.2}\n\n\nTheorem 108.1 (Logs take positive values) logs only exist for values greater than 0\n\n\\forall x(e^x &gt; 0) \\implies \\exists \\log(y) \\iff {y &gt; 0}\n\n\nWe can use the properties of exponents from the previous section to obtain some important properties of logarithms:\n\nDefinition 108.3 (Log of a product) we can use Equation 108.3 to convert a log of a product to a sum of logs.\n\n\\log(x·y) = \\log(x) + \\log(y)\n\\tag{108.3}\n\n\nDefinition 108.4 (Log of a quotient) we can use Equation 108.4 to convert a log of a quotient to a difference of logs.\n\n\\log(\\frac{x}{y}) = log(x) − log(y)\n\\tag{108.4}\n\n\nDefinition 108.5 (Log of a power) we can use Equation 108.5 to convert a log of a variable raised to a power into the product.\n\n    \\log(x^b) = b \\cdot log(x)\n\\tag{108.5}\n\n\nDefinition 108.6 (Log of one) we can use (Equation 108.6) to replace a log of 1 with zero since $x(x^0 = 1) $\n\n    \\log(1)=0\n\\tag{108.6}\n\n\n108.2.1 Log of exponent\nwe can use (Equation 108.7) to cancel a log of an exponent since the log is the inverse function of the exponent.\n\nexp(log(y)) = log(exp(y)) = y\n\\tag{108.7}\n\nExample 108.1 (Logarithm) \n    \\begin{aligned}\n    log \\frac{5^2}{10}= 2 log(5) − log(10) ≈ 0.916.\n    \\end{aligned}\n\n\n\nDefinition 108.7 (Change of base for a log) we can use (Equation 108.8) to change the base of a logarithm.\n\n    \\log_b(a)=\\frac{\\log_c(a)}{\\log_c(n)}\n\\tag{108.8}\n\n\nDefinition 108.8 (Derivative of a Log) we can use (Equation 108.9) to differentiate a log.\n\n    \\frac{d}{dx} \\log_(x)=\\frac{1}{x}\n\\tag{108.9}\n\n\nBecause the natural logarithm is a monotonically increasing one-to-one function, finding the x which maximizes any (positive-valued function) f(x) is equivalent to maximizing log(f(x)).\nThis is useful because we often take derivatives to maximize functions.\nIf f(x) has product terms, then log(f(x)) will have summation terms, which are usually simpler when taking derivatives.",
    "crumbs": [
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>Appendix: Exponents & Logarithms</span>"
    ]
  },
  {
    "objectID": "A07.html",
    "href": "A07.html",
    "title": "109  Appendix: The Law of Large Numbers",
    "section": "",
    "text": "109.1 Law of large numbers\nSuppose we observe data D=\\{x_1, \\ldots, x_n\\} with each x_i \\sim F .\nBy the strong law of large numbers the empirical distribution \\hat{F}_n based on data D=\\{x_1, \\ldots, x_n\\} converges to the true underlying distribution F as n \\rightarrow \\infty almost surely:\n\\hat{F}_n\\overset{a. s.}{\\to} F\nThe Glivenko–Cantelli asserts that the convergence is uniform. Since the strong law implies the weak law we also have convergence in probability:\n\\hat{F}_n\\overset{P}{\\to} F\nCorrespondingly, for n \\rightarrow \\infty the average \\text{E}_{\\hat{F}_n}(h(x)) = \\frac{1}{n} \\sum_{i=1}^n h(x_i) converges to the expectation \\text{E}_{F}(h(x)) .",
    "crumbs": [
      "<span class='chapter-number'>109</span>  <span class='chapter-title'>Appendix: The Law of Large Numbers</span>"
    ]
  },
  {
    "objectID": "A08.html",
    "href": "A08.html",
    "title": "110  Appendix: The Central Limit Theorem",
    "section": "",
    "text": "110.1 Central Limit Theorem\nThe Central Limit Theorem is one of the most important results in statistics, stating that with sufficiently large sample sizes, the sample average approximately follows a normal distribution. This underscores the importance of the normal distribution, as well as most of the methods commonly used which make assumptions about the data being normally distributed.\nLet’s first stop and think about what it means for the sample average to have a distribution. Imagine going to the store and buying a bag of your favorite brand of chocolate chip cookies. Suppose the bag has 24 cookies in it. Will each cookie have the exact same number of chocolate chips in it? It turns out that if you make a batch of cookies by adding chips to dough and mixing it really well, then putting the same amount of dough onto a baking sheet, the number of chips per cookie closely follows a Poisson distribution. (In the limiting case of chips having zero volume, this is exactly a Poisson process.) Thus we expect there to be a lot of variability in the number of chips per cookie. We can model the number of chips per cookie with a Poisson distribution. We can also compute the average number of chips per cookie in the bag. For the bag we have, that will be a particular number. But there may be more bags of cookies in the store. Will each of those bags have the same average number of chips? If all of the cookies in the store are from the same industrial-sized batch, each cookie will individually have a Poisson number of chips. So the average number of chips in one bag may be different from the average number of chips in another bag. Thus we could hypothetically find out the average number of chips for each bag in the store. And we could think about what the distribution of these averages is, across the bags in the store, or all the bags of cookies in the world. It is this distribution of averages that the central limit theorem says is approximately a normal distribution, with the same mean as the distribution for the individual cookies, but with a standard deviation that is divided by the square root of the number of samples in each average (i.e., the number of cookies per bag).",
    "crumbs": [
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>Appendix: The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "A08.html#sec-cl-theorem",
    "href": "A08.html#sec-cl-theorem",
    "title": "110  Appendix: The Central Limit Theorem",
    "section": "",
    "text": "Theorem 110.1 (Central Limit Theorem) Let X_1, ..., X_n be independent and identically distributed (IID) with \\mathbb{E}(X_i) = \\mu and Var(X_i) = \\sigma^2 &lt;\\infty\nThen:\n\n\\lim_{n\\to\\infty} \\sqrt{n} \\sum_{i=0}^{n} \\frac{1}{n}\\frac{(X_i-\\mu)}{\\sigma} = \\sum_{i=0}^{n} \\frac{X_i-\\mu}{\\sqrt{n} \\sigma} = N(0, 1)\n\nThat is, \\hat{X_n} is approximately normally distributed with mean µ and variance \\frac{\\sigma}{2/n} or standard deviation \\frac{\\sigma}{\\sqrt{n}}.",
    "crumbs": [
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>Appendix: The Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "A09.html",
    "href": "A09.html",
    "title": "111  Appendix: Conjugate Priors",
    "section": "",
    "text": "111.1 Conjugate Priors",
    "crumbs": [
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>Appendix: Conjugate Priors</span>"
    ]
  },
  {
    "objectID": "A09.html#sec-conjugate-priors",
    "href": "A09.html#sec-conjugate-priors",
    "title": "111  Appendix: Conjugate Priors",
    "section": "",
    "text": "Table 111.1: Conjugate prior\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\nConjugate prior\nPosterior\nPosterior predictive\n\n\n\n\n\\text{Bernoulli}(p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left( \\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +n-\\sum _{i=1}^{n}x_{i}\\right)}\n{\\displaystyle \\mathbb{P}r({\\tilde {x}}=1)={\\frac {\\alpha '}{\\alpha '+\\beta '}}}\n\n\n\\text{Binomial}(trials=m,p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left(\\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}\\right)}\n{\\displaystyle \\operatorname {BetaBin} ({\\tilde {x}}|\\alpha ',\\beta ')}\n\n\n\\text{NegBinomial}(fails=r)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle \\text{Beta}\\left( \\alpha +rn,\\beta + \\sum _{i=1}^{n} x_{i}\\right)}\n{\\displaystyle \\operatorname {BetaNegBin} ({\\tilde {x}}|\\alpha ',\\beta ')}\n\n\n\\text{Poisson}(rate=\\lambda)\n\\text{Gamma}(k,\\theta)\n{\\displaystyle \\text{Gamma}\\left( k+\\sum _{i=1}^{n}x_{i},\\ {\\frac {\\theta }{n\\theta +1}}\\!\\right)}\n{\\displaystyle \\operatorname {NB} \\left({\\tilde {x}}\\mid k',{\\frac {1}{\\theta '+1}}\\right)}\n\n\n\\text{Poisson}(rate=\\lambda)\n\\text{Gamma}(\\alpha,\\beta)\n{\\displaystyle\\text{Gamma}\\left( \\alpha +\\sum _{i=1}^{n}x_{i},\\ \\beta +n\\!\\right)}\n{\\displaystyle \\operatorname {NB} \\left({\\tilde {x}}\\mid \\alpha ',{\\frac {\\beta '}{1+\\beta '}}\\right)}\n\n\n\\text{Categorical}(probs=p,cats=k)\n\\text{Dir}(\\alpha_k)\\mathbb{I}_{k\\ge1}\n{\\displaystyle \\text{Dir}\\left({ {\\boldsymbol {\\alpha }}+(c_{1},\\ldots ,c_{k})}\\right)}\n{\\displaystyle {\\begin{aligned}\\mathbb{P}r({\\tilde {x}}=i)&={\\frac {{\\alpha _{i}}'}{\\sum _{i}{\\alpha _{i}}'}} \\\\ &={\\frac {\\alpha _{i}+c_{i}}{\\sum _{i}\\alpha _{i}+n}}\\end{aligned}}}\n\n\n\\text{Multinomial}(probs=p,cats=k)\n\\text{Dir}(\\alpha_k)\\mathbb{I}_{k\\ge1}\n{\\displaystyle \\text{Dir}\\left({ {\\boldsymbol {\\alpha }}+\\sum _{i=1}^{n}\\mathbf {x} _{i}\\!}\\right)}\n{\\displaystyle \\operatorname {DirMult} ({\\tilde {\\mathbf {x} }}\\mid {\\boldsymbol {\\alpha }}')}\n\n\n\\text{Hypergeometric}(pop=n)\n\\text{BetaBinomial}(\\alpha,\\beta,n=N)\n{\\displaystyle \\text{BetaBinomial}\\left({\\displaystyle \\alpha +\\sum _{i=1}^{n}x_{i},\\,\\beta +\\sum _{i=1}^{n}N_{i}-\\sum _{i=1}^{n}x_{i}}\\right)}\n\n\n\n\\text{Geometric}(p)\n\\text{Beta}(\\alpha,\\beta)\n{\\displaystyle\\text{Beta}\\left( \\alpha +n,\\,\\beta +\\sum _{i=1}^{n}x_{i}\\right)}",
    "crumbs": [
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>Appendix: Conjugate Priors</span>"
    ]
  },
  {
    "objectID": "A10.html",
    "href": "A10.html",
    "title": "112  Appendix: Link Function",
    "section": "",
    "text": "112.1 Link functions\nIn statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\nThe link function provides the relationship between the linear predictor and the mean of the distribution function. There are many commonly used link functions, and their choice is informed by several considerations. There is always a well-defined canonical link function which is derived from the exponential of the response’s density function. However, in some cases, it makes sense to try to match the domain of the link function to the range of the distribution function’s mean, or use a non-canonical link function for algorithmic purposes, for example Bayesian probit regression.\nWhen using a distribution function with a canonical parameter \\theta , the canonical link function is the function that expresses \\theta in terms of \\mu i.e. \\theta =b(\\mu) . For the most common distributions, the mean μ is one of the parameters in the standard form of the distribution’s Density function, and then b(\\mu ) is the function as defined above that maps the density function into its canonical form. When using the canonical link function, b(\\mu )=\\theta =\\mathbf {X} {\\boldsymbol {\\beta }} , which allows \\mathbf{X}^{T}\\mathbf{Y} to be a sufficient statistic for \\beta .\nFollowing is a table of several exponential-family distributions in common use and the data they are typically used for, along with the canonical link functions and their inverses (sometimes referred to as the mean function, as done here).\nIn the cases of the exponential and gamma distributions, the domain of the canonical link function is not the same as the permitted range of the mean. In particular, the linear predictor may be positive, which would give an impossible negative mean. When maximizing the likelihood, precautions must be taken to avoid this. An alternative is to use a non-canonical link function.\nIn the case of the Bernoulli, binomial, categorical and multinomial distributions, the support of the distributions is not the same type of data as the parameter being predicted. In all of these cases, the predicted parameter is one or more probabilities, i.e. real numbers in the range [0,1]. The resulting model is known as Logistic regression (or Multinomial logistic regression in the case that K-way rather than binary values are being predicted).\nFor the Bernoulli and binomial distributions, the parameter is a single probability, indicating the likelihood of occurrence of a single event. The Bernoulli still satisfies the basic condition of the generalized linear model in that, even though a single outcome will always be either 0 or 1, the expected value will nonetheless be a real-valued probability, i.e. the probability of occurrence of a “yes” (or 1) outcome. Similarly, in a binomial distribution, the expected value is Np, i.e. the expected proportion of “yes” outcomes will be the probability to be predicted.\nFor categorical and multinomial distributions, the parameter to be predicted is a K-vector of probabilities, with the further restriction that all probabilities must add up to 1. Each probability indicates the likelihood of occurrence of one of the K possible values. For the multinomial distribution, and for the vector form of the categorical distribution, the expected values of the elements of the vector can be related to the predicted probabilities similarly to the binomial and Bernoulli distributions.",
    "crumbs": [
      "<span class='chapter-number'>112</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A10.html#sec-link-function",
    "href": "A10.html#sec-link-function",
    "title": "112  Appendix: Link Function",
    "section": "",
    "text": "Common distributions with typical uses and canonical link functions\n\n\n\n\n\n\n\n\n\n\nDistribution\nSupport\nUses\nLink name\nLink fn\nMean fn\n\n\n\n\nNormal\n\\mathbb{R}\n\nIdentity\n\\mathbf {X} {\\boldsymbol {\\beta }}=\\mu\n\n\n\nExpoential\n\\mathbb{R}^+_0\n\nNegative inverse\n\\mathbf {X} {\\boldsymbol {\\beta }}=-\\mu^{-1}\n\n\n\nGamma\n\\mathbb{R}^+_0\n\nNegative inverse\n\\mathbf {X} {\\boldsymbol {\\beta }}=-\\mu^{-1}\n\n\n\nInverse Gamma\n\\mathbb{R}^+_0\n\nInverse squared\n\\mathbf {X} {\\boldsymbol {\\beta }}=\\mu^{-2}\n\n\n\nPoisson\n\\mathbb{N}_0\n\nLog\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\mu)\n\n\n\nBernuolli\n\\{0,1\\}\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})\n\n\n\nBinomial\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{n-\\mu})\n\n\n\nCategorical\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})\n\n\n\nMultinomial\n\n\nLogit\n\\mathbf {X} {\\boldsymbol {\\beta }}=ln(\\frac{\\mu}{1-\\mu})",
    "crumbs": [
      "<span class='chapter-number'>112</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A10.html#credits",
    "href": "A10.html#credits",
    "title": "112  Appendix: Link Function",
    "section": "Credits:",
    "text": "Credits:\nThis page is based on the Generalized linear model article on Wikipedia, which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. By Wikimedia contributors, available under CC BY-SA 3.0.\nThe text has been modified for clarity and conciseness.",
    "crumbs": [
      "<span class='chapter-number'>112</span>  <span class='chapter-title'>Appendix: Link Function</span>"
    ]
  },
  {
    "objectID": "A11.html",
    "href": "A11.html",
    "title": "113  Appendix: Bayes by backprop",
    "section": "",
    "text": "113.1 Introduction\nThis appendix reviews of a method to introduce weight uncertainty into neural networks called the “Bayes by Backprop” method introduced in (Blundell et al. 2015). where, the main question is how to determine the parameters of the distribution for each network weight. I learned about it from Probabilistic Deep Learning with TensorFlow 2 by Dr Kevin Webster, S reading from based this on\nThe authors note that prior work which considered uncertainty at the hidden unit (H_i) an approach that allows to state the uncertainty with respect to a particular observation and which is an easier problem since the number of weight is greater by two orders of magnitude. But considering the uncertainty in the weights is complementary, in the sense that it captures uncertainty about which neural network is appropriate, leading to regularization of the weights and model averaging. This weight uncertainty can be used to drive the exploration/exploitation in contextual bandit problems using Thompson sampling . Weights with greater uncertainty introduce more variability into the decisions made by the network, naturally leading to exploration. As more data are observed, the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the environment is better understood.\nIn a traditional neural network, as shown in the upper left, each weight has a single value. But in the true value for these weights is not certain. Much of this uncertainty comes from imperfect training data, which is an approximation of the the distribution of the data generating process from which the data were sampled. Recall that this is called epistemic uncertainty, which we expect to decrease as the amount of training data increases.\nIn this method, we want to include such uncertainty in deep learning models. This is done by changing each weight from a single deterministic value to a probability distribution. We then learn the parameters of this distribution. Consider a neural network weight w_i . In a standard (deterministic) neural network, this has a single value \\hat{w}_i , learnt via backpropagation. In a neural network with weight uncertainty, each weight is represented by a probability distribution, and the parameters of this distribution are learned via backpropagation. Suppose, for example, that each weight has a normal distribution. This has two parameters: a mean \\mu_i and a standard deviation \\sigma_i .\nSince the weights are uncertain, the feedforward value of some input x_i is not constant. A single feedforward value is determined in two steps: 1. Sample each network weight from their respective distributions – this gives a single set of network weights. 2. Use these weights to determine a feedforward value \\hat{y}_i .\nHence, the key question is how to determine the parameters of the distribution for each network weight. The paper introduces exactly such a scheme, called Bayes by Backprop.",
    "crumbs": [
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#introduction",
    "href": "A11.html#introduction",
    "title": "113  Appendix: Bayes by backprop",
    "section": "",
    "text": "Fig. 1 from (Blundell et al. 2015) contrasting traditional and Bayesian neural networks\n\n\n\n\n\nClassic deterministic NN: w_i = \\hat{w}_i\nNN with weight uncertainty represented by normal distribution: w_i \\sim N(\\hat{\\mu}_i, \\hat{\\sigma}_i) .",
    "crumbs": [
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#bayesian-learning",
    "href": "A11.html#bayesian-learning",
    "title": "113  Appendix: Bayes by backprop",
    "section": "113.2 Bayesian learning",
    "text": "113.2 Bayesian learning\nNote: We use the notation P to refer to a probability density. For simplicity, we’ll only consider continuous distributions (which have a density). In the case of discrete distributions, P would represent a probability mass and integrals should be changed to sums. However, the formulae are the same.\nWhat you need to know now is that Bayesian methods can be used to calculate the distribution of a model parameter given some data. In the context of weight uncertainty in neural networks, this is convenient, since we are looking for the distribution of weights (model parameters) given some (training) data. The key step relies on Bayes’ theorem. This theorem states, in mathematical notation, that\n\n\\mathbb{P}r(w \\mid D) = \\frac{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)}{\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'}\n\nwhere the terms mean the following:\n\nD is some data, e.g. x and y value pairs: D = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\} . This is sometimes called the evidence.\nw is the value of a model weight.\n\\mathbb{P}r(w) is called the prior. This is our “prior” belief on the probability density of a model weight, i.e. the distribution that we postulate before seeing any data.\n\\mathbb{P}r(D \\mid w) is the likelihood of having observed data D given weight w . It is precisely the same likelihood used to calculate the negative log-likelihood.\n\\mathbb{P}r(w \\mid D) is the posterior density of the distribution of the model weight at value w , given our training data. It is called posterior since it represents the distribution of our model weight after taking the training data into account.\n\nNote that the term {\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'} = \\mathbb{P}r(D) does not depend on w (as the w' is an integration variable). It is only a normalization term. For this reason, we will from this point on write Bayes’ theorem as\n\n\\mathbb{P}r(w \\mid D) = \\frac{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)}{\\mathbb{P}r(D)}.\n\nBayes’ theorem gives us a way of combining data with some “prior belief” on model parameters to obtain a distribution for these model parameters that considers the data, called the posterior distribution.",
    "crumbs": [
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#bayesian-neural-network-with-weight-uncertainty-in-principle",
    "href": "A11.html#bayesian-neural-network-with-weight-uncertainty-in-principle",
    "title": "113  Appendix: Bayes by backprop",
    "section": "113.3 Bayesian neural network with weight uncertainty – in principle",
    "text": "113.3 Bayesian neural network with weight uncertainty – in principle\nThe above formula gives a way to determine the distribution of each weight in the neural network:\n\nPick a prior density \\mathbb{P}r(w) .\nUsing training data D , determine the likelihood \\mathbb{P}r(D \\mid w) .\nDetermine the posterior density \\mathbb{P}r(w \\mid D) using Bayes’ theorem.\n\nThis is the distribution of the NN weight.\nWhile this works in principle, in many practical settings it is difficult to implement. The main reason is that the normalization constant {\\int \\mathbb{P}r(D \\mid w') \\mathbb{P}r(w') \\text{d}w'} = \\mathbb{P}r(D) may be very difficult to calculate, as it involves solving or approximating a complicated integral. For this reason, approximate methods, such as Variational Bayes described below, are often employed.",
    "crumbs": [
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#variational-bayes",
    "href": "A11.html#variational-bayes",
    "title": "113  Appendix: Bayes by backprop",
    "section": "113.4 Variational Bayes",
    "text": "113.4 Variational Bayes\n Variational Bayes methods approximate the posterior distribution with a second function, called a variational posterior. This function has a known functional form, and hence avoids the need to determine the posterior \\mathbb{P}r(w \\mid D) exactly. Of course, approximating a function with another one has some risks, since the approximation may be very bad, leading to a posterior that is highly inaccurate. In order to mediate this, the variational posterior usually has a number of parameters, denoted by \\theta , that are tuned so that the function approximates the posterior as well as possible. Let’s see how this works below.\nInstead of \\mathbb{P}r(w \\mid D) , we assume the network weight has density q(w \\mid \\theta) , parameterized by \\theta . q(w \\mid \\theta) is known as the variational posterior . We want q(w \\mid \\theta) to approximate \\mathbb{P}r(w \\mid D) , so we want the “difference” between q(w \\mid \\theta) and \\mathbb{P}r(w \\mid D) to be as small as possible. This “difference” between the two distributions is measured by the Kullback-Leibler divergence D_{\\text{KL}} (note that this is unrelated to the D we use to denote the data). The Kullback-Leibler divergence between two distributions with densities f(x) and g(x) respectively is defined as\n\nD_{KL} (f(x) \\parallel g(x)) = \\int f(x) \\log \\left( \\frac{f(x)}{g(x)} \\right) \\text{d} x\n\nNote that this function has value 0 (indicating no difference) when f(x) \\equiv g(x) , which is the result we expect. We use the convention that \\frac{0}{0} = 1 here.\n Viewing the data D as a constant, the Kullback-Leibler divergence between q(w \\mid \\theta) and \\mathbb{P}r(w \\mid D) is hence:\n\n\\begin{aligned}\n  D_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D)) &= \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta)}{\\mathbb{P}r(w \\mid D)} \\right) \\text{d} w \\\\\n&= \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta) \\mathbb{P}r(D)}{\\mathbb{P}r(D \\mid w) \\mathbb{P}r(w)} \\right) \\text{d} w \\\\\n&= \\int q(w \\mid \\theta) \\log \\mathbb{P}r(D) \\text{d} w + \\int q(w \\mid \\theta) \\log \\left( \\frac{q(w \\mid \\theta)}{\\mathbb{P}r(w)} \\right) \\text{d} w - \\int q(w \\mid \\theta) \\log \\mathbb{P}r(D \\mid w) \\text{d} w \\\\\n&= \\log \\mathbb{P}r(D) + D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w))\n\\end{aligned}\n\nwhere, in the last line, we have used\n\n\\int q(w \\mid \\theta) \\log \\mathbb{P}r(D) \\text{d}w = \\log \\mathbb{P}r(D) \\int q(w \\mid \\theta) \\text{d} w = \\log \\mathbb{P}r(D)\n\nsince q(w \\mid \\theta) is a probability distribution and hence integrates to 1. If we consider the data D to be constant, the first term is a constant also, and we may ignore it when minimizing the above. Hence, we are left with the function\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w))\n\\end{aligned}\n\nNote that this function depends only on \\theta and D , since w is an integration variable. This function has a nice interpretation as the sum of: - The Kullback-Leibler divergence between the variational posterior q(w \\mid \\theta) and the prior \\mathbb{P}r(w) . This is called the complexity cost, and it depends on \\theta and the prior but not the data D . - The expectation of the negative log likelihood \\log \\mathbb{P}r(D \\mid w) under the variational posterior q(w \\mid \\theta) . This is called the likelihood cost and it depends on \\theta and the data but not the prior.\nL(\\theta \\mid D) is the loss function that we minimize to determine the parameter \\theta . Note also from the above derivation, that we have\n\n\\begin{aligned}\n\\log \\mathbb{P}r(D) &= \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) - D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) + D_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D))\\\\\n&\\ge \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) - D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) =: ELBO\n\\end{aligned}\n\nwhich follows because\n\nD_{KL} (q(w \\mid \\theta) \\parallel \\mathbb{P}r(w \\mid D))\n\nis non negative. The final expression on the right hand side is therefore a lower bound on the log-evidence, and is called the evidence lower bound, often shortened to ELBO. The ELBO is the negative of our loss function, so minimizing the loss function is equivalent to maximizing the ELBO.\nMaximizing the ELBO requires a trade off between the KL term and expected log-likelihood term. On the one hand, the divergence between q(w \\mid \\theta) and \\mathbb{P}r(w) should be kept small, meaning the variational posterior shouldn’t be too different to the prior. On the other, the variational posterior parameters should maximize the expectation of the log-likelihood \\log \\mathbb{P}r(D \\mid w) , meaning the model assigns a high likelihood to the data.",
    "crumbs": [
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A11.html#a-backpropagation-scheme",
    "href": "A11.html#a-backpropagation-scheme",
    "title": "113  Appendix: Bayes by backprop",
    "section": "113.5 A backpropagation scheme",
    "text": "113.5 A backpropagation scheme\n\n113.5.1 The idea\nWe can use the above ideas to create a neural network with weight uncertainty, which we will call a Bayesian neural network. From a high level, this works as follows. Suppose we want to determine the distribution of a particular neural network weight w .\n\nAssign the weight a prior distribution with density \\mathbb{P}r(w) , which represents our beliefs on the possible values of this network before any training data. This may be something simple, like a unit Gaussian. Furthermore, this prior distribution will usually not have any trainable parameters.\nAssign the weight a variational posterior with density q(w \\mid \\theta) with some trainable parameter \\theta .\nq(w \\mid \\theta) is the approximation for the weight’s posterior distribution. Tune \\theta to make this approximation as accurate as possible as measured by the ELBO.\n\nThe remaining question is then how to determine \\theta . Recall that neural networks are typically trained via a backpropagation algorithm, in which the weights are updated by perturbing them in a direction that reduces the loss function. We aim to do the same here, by updating \\theta in a direction that reduces L(\\theta \\mid D) .\nHence, the function we want to minimise is\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) \\\\\n&= \\int q(w \\mid \\theta) ( \\log q(w \\mid \\theta) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w) ) \\text{d}w.\n\\end{aligned}\n\nIn principle, we could take derivatives of L(\\theta \\mid D) with respect to \\theta and use this to update its value. However, this involves doing an integral over w , and this is a calculation that may be impossible or very computationally expensive. Instead, we want to write this function as an expectation and use a Monte Carlo approximation to calculate derivatives. At present, we can write this function as\n\nL(\\theta \\mid D) = \\mathbb{E}_{q(w \\mid \\theta)} ( \\log q(w \\mid \\theta) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w) )\n\nHowever, taking derivatives with respect to \\theta is difficult because the underlying distribution the expectation is taken with respect to depends on \\theta . One way we can handle this is with the reparameterization trick.\n\n\n113.5.2 The reparameterization trick\nThe reparameterization trick is a way to move the dependence on \\theta around so that an expectation may be taken independently of it. It’s easiest to see how this works with an example. Suppose q(w \\mid \\theta) is a Gaussian, so that \\theta = (\\mu, \\sigma) . Then, for some arbitrary f(w; \\mu, \\sigma) , we have\n\n\\begin{aligned}\n\\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\int q(w \\mid \\mu, \\sigma) f(w; \\mu, \\sigma) \\text{d}w \\\\\n&= \\int \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{1}{2 \\sigma^2} (w - \\mu)^2 \\right) f(w; \\mu, \\sigma) \\text{d}w \\\\\n&= \\int \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left( -\\frac{1}{2} \\varepsilon^2 \\right) f \\left( \\mu + \\sigma \\varepsilon; \\mu, \\sigma \\right) \\text{d}\\varepsilon \\\\\n&= \\mathbb{E}_{\\varepsilon \\sim N(0, 1)} (f \\left( \\mu + \\sigma \\varepsilon; \\mu, \\sigma \\right) )\n\\end{aligned}\n\nwhere we used the change of variable w = \\mu + \\sigma \\varepsilon . Note that the dependence on \\theta = (\\mu, \\sigma) is now only in the integrand and we can take derivatives with respect to \\mu and \\sigma:\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\frac{\\partial}{\\partial \\mu} \\mathbb{E}_{\\varepsilon \\sim N(0, 1)} (f \\left( w; \\mu, \\sigma \\right) ) = \\mathbb{E}_{\\varepsilon \\sim N(0, 1)} \\frac{\\partial}{\\partial \\mu} f \\left( \\mu + \\sigma \\varepsilon; \\mu, \\sigma \\right)\n\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\sigma} \\mathbb{E}_{q(w \\mid \\mu, \\sigma)} (f(w; \\mu, \\sigma) ) &= \\frac{\\partial}{\\partial \\sigma} \\mathbb{E}_{\\varepsilon \\sim N(0, 1)} (f \\left( w; \\mu, \\sigma \\right) ) = \\mathbb{E}_{\\varepsilon \\sim N(0, 1)} \\frac{\\partial}{\\partial \\sigma} f \\left( \\mu + \\sigma \\varepsilon; \\mu, \\sigma \\right)\n\\end{aligned}\n\nFinally, note that we can approximate the expectation by its Monte Carlo estimate:\n\n\\begin{aligned}\n\\mathbb{E}_{\\varepsilon \\sim N(0, 1)}  \\frac{\\partial}{\\partial \\theta} f \\left( \\mu + \\sigma \\varepsilon; \\mu, \\sigma \\right) \\approx \\sum_{i}  \\frac{\\partial}{\\partial \\theta} f \\left( \\mu + \\sigma \\varepsilon_i; \\mu, \\sigma \\right),\\qquad \\varepsilon_i \\sim N(0, 1).\n\\end{aligned}\n\nThe above reparameterization trick works in cases where we can write the w = g(\\varepsilon, \\theta) , where the distribution of the random variable \\varepsilon is independent of \\theta .\n\n\n113.5.3 Implementation\nPutting this all together, for our loss function L(\\theta \\mid D) \\equiv L(\\mu, \\sigma \\mid D) , we have\n\nf(w; \\mu, \\sigma) = \\log q(w \\mid \\mu, \\sigma) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w)\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mu} L(\\mu, \\sigma \\mid D) \\approx \\sum_{i} \\left( \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\mu} \\right)\n\\end{aligned}\n\n\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\sigma} L(\\mu, \\sigma \\mid D) \\approx \\sum_{i} \\left( \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} \\varepsilon_i + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\sigma} \\right)\n\\end{aligned}\n\n\nf(w; \\mu, \\sigma) = \\log q(w \\mid \\mu, \\sigma) - \\log \\mathbb{P}r(D \\mid w) - \\log \\mathbb{P}r(w)\n\nwhere w_i = \\mu + \\sigma \\varepsilon_i, \\, \\varepsilon_i \\sim N(0, 1) . In practice, we often only take a single sample \\varepsilon_1 for each training point. This leads to the following backpropagation scheme:\n\nSample \\varepsilon_i \\sim N(0, 1) . 2. Let w_i = \\mu + \\sigma \\varepsilon_i\nCalculate\n\n\n\\nabla_{\\mu}f = \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\mu} \\hspace{3em} \\nabla_{\\sigma}f = \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial w_i} \\varepsilon_i + \\frac{\\partial f(w_i; \\mu, \\sigma)}{\\partial \\sigma}\n\n\nUpdate the parameters with some gradient-based optimizer using the above gradients.\n\nThis is how we learn the parameters of the distribution for each neural network weight.\n\n\n113.5.4 Minibatches\nNote that the loss function (or negative of the ELBO) is\n\n\\begin{aligned}\nL(\\theta \\mid D) &= D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\mathbb{E}_{q(w \\mid \\theta)}(\\log \\mathbb{P}r(D \\mid w)) \\\\\n& = D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\sum_{j=1}^N \\log \\mathbb{P}r(y_j, x_j \\mid w_j)\n\\end{aligned}\n\nwhere j runs over all the data points in the training data (N in total) and w_j = \\mu + \\sigma \\varepsilon_j is sampled using \\varepsilon_j \\sim N(0, 1) (we assume a single sample from the approximate posterior per data point for simplicity).\nIf training occurs in minibatches of size B , typically much smaller than N , we instead have a loss function\n\n\\begin{aligned}\nD_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\sum_{j=1}^{B} \\log \\mathbb{P}r(y_j, x_j \\mid w_j).\n\\end{aligned}\n\nNote that the scaling factors between the first and second terms have changed, since before the sum ran from 1 to N , but it now runs from 1 to B . To correct for this, we should add a correction factor \\frac{N}{B} to the second term to ensure that its expectation is the same as before. This leads to the loss function, after dividing by N to take the average per training value, of\n\n\\begin{aligned}\n\\frac{1}{N} D_{KL} ( q(w \\mid \\theta) \\parallel \\mathbb{P}r(w) ) - \\frac{1}{B} \\sum_{j=1}^{B} \\log \\mathbb{P}r(y_j, x_j \\mid w_j).\n\\end{aligned}\n\nBy default, when Tensorflow calculates the loss function, it calculates the average across the minibatch. Hence, it already uses the factor \\frac{1}{B} present on the second term. However, it does not, by default, divide the first term by N . In an implementation, we will have to specify this. You’ll see in the next lectures and coding tutorials how to do this.\n\n\n113.5.5 Conclusion\nWe introduced the Bayes by Backpropagation method, which can be used to embed weight uncertainty into neural networks. Good job getting through it, as the topic is rather advanced. This approach allows the modelling of epistemic uncertainty on the model weights. We expect that, as the number of training points increases, the uncertainty on the model weights decreases. This can be shown to be the case in many settings. In the next few lectures and coding tutorials, you’ll learn how to apply these methods to your own models, which will make the idea much clearer.\n\n\n113.5.6 Further reading and resources\n\nBayes by backprop paper (Blundell et al. 2015)\nWikipedia article on Bayesian inference\n\n\n\n\n\n\n\nBlundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015. “Weight Uncertainty in Neural Networks.” https://doi.org/10.48550/ARXIV.1505.05424.",
    "crumbs": [
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>Appendix: Bayes by backprop</span>"
    ]
  },
  {
    "objectID": "A12.html",
    "href": "A12.html",
    "title": "114  Bayesian Books in R & Python",
    "section": "",
    "text": "114.1 Introduction to Probability\nA number of books on Bayesian statistics and time series analysis are available in both R and Python. A number of these are introduced in the specilization and many others are worth mentioning.\nThere are many books in R and Python that can help you learn more about these languages and how to use them for data analysis.\nHere are some of the most popular books on R and Python:",
    "crumbs": [
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#introduction-to-probability",
    "href": "A12.html#introduction-to-probability",
    "title": "114  Bayesian Books in R & Python",
    "section": "",
    "text": "Introduction to Probability by Dennis L. Sun",
    "crumbs": [
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#sec-bayesian-books",
    "href": "A12.html#sec-bayesian-books",
    "title": "114  Bayesian Books in R & Python",
    "section": "114.2 Books in R",
    "text": "114.2 Books in R\n\nR for Data Science by Hadley Wickham & Garrett Grolemund\nAdvanced R by Hadley Wickham\nggplot2: Elegant Graphics for Data Analysis (3e)\nR Graphics Cookbook, 2nd edition\nAn Introduction to Statistical Learning\nEngineering Production-Grade Shiny Apps\nForecasting: Principles and Practice (3rd ed)\nExploratory Data Analysis with R Roger D. Peng\nModern R with the tidyverse by Bruno Rodrigues\nModern Statistics with R by Benjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton\nMastering Shiny by Hadley Wickham, Winston Chang, and Joe Cheng\nLearning Statistics with R by Danielle Navarro\nText Mining with R by Julia Silge and David Robinson",
    "crumbs": [
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#books-in-python",
    "href": "A12.html#books-in-python",
    "title": "114  Bayesian Books in R & Python",
    "section": "114.3 Books in Python",
    "text": "114.3 Books in Python\n\n(James et al. 2013) An Introduction to Statistical Learning with python by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani\n(McKinney 2022) Python for Data Analysis by Wes McKinney of Pandas infamy parquet and Apache Arrow\n(VanderPlas 2016) Python Data Science Handbook by Jake VanderPlas\nThink Stats by Allen B. Downey\nThink Bayes by Allen B. Downey\nProbabilistic Programming & Bayesian Methods for Hackers by Cameron Davidson-Pilon",
    "crumbs": [
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#bayesian-statistics-techniques-and-models",
    "href": "A12.html#bayesian-statistics-techniques-and-models",
    "title": "114  Bayesian Books in R & Python",
    "section": "114.4 Bayesian Statistics: Techniques and Models",
    "text": "114.4 Bayesian Statistics: Techniques and Models\nAt the end of the course there is a handout called further reading. In this course the following titles are recommended for further reading and reference, unfortunately some of the links were out of date.\n\n\n\n\n\n\n\nFigure 114.1: Doing Bayesian Data Analysis\n\n\n(Kruschke 2011) Doing Bayesian Data Analysis this is the favorite on Bayesian statistics. It is a great book for learning Bayesian statistics and covers a wide range of topics in Bayesian modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. This text is recommended for further reading in the course 2\n\n\n\n\n\n\n\nFigure 114.2: Statistical Rethinking\n\n\n(McElreath 2015) Statistical Rethinking: A Bayesian Course with Examples in R and Stan by Richard McElreath is, the runner up in terms of a popular book on Bayesian Statistics. This book is a great resource for learning Bayesian statistics and covers a wide range of topics in Bayesian modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. McElreath is a gifted and entertaining explainer but he is not a full-fledged statistician. I find that in most cases the examples in this book are rather weak and challenging to adapt to real life settings. On the other hand his use of metaphors and his ability to pass on his intuition is unparalleled and has helped to shape my own thinking about statistics as is evidenced by the many references to his work in these notes. This text is recommended for further reading in the course 2\n\n\n\n\n\n\n\nFigure 114.3: Bayesian Data Analysis\n\n\n(Gelman et al. 2013) Bayesian Data Analysis by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, Donald B. Rubin. This book is considered the Graduate level text on Bayesian statistics and covers a wide range of topics in Bayesian data analysis. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. This book is harder to read than the previous two books. I’d recommend reading and viewing McElreath’s book and videos first and only then moving to this book later. This text is recommended for further reading in the course 2 but with a cautionary caveat that a solid background in calculus-based probability will be useful, as many details are left to the reader. I took an undergraduate course in probability and statistics that was based on measure theory but I’m not sure this was the kind of background these authors have in mind.\n\n\n\n\n\n\n\nFigure 114.4: The BUGS book\n\n\n(Lunn et al. 2012) The BUGS book: A Practical Introduction to Bayesian Analysis is a classic text on Bayesian statistics and covers a wide range of topics in Bayesian modeling, computation, and inference. IT focuses on simulation using BUGS. This text is recommended for further reading in the course 2\nSome addition books that were mentioned in the course bibliography:\n(Carlin and Louis 2008) Bayesian Methods for Data Analysis\n(Agresti 2012) This is a graduate level book on categorical data which often poses extra challenges for Bayesian statistics. Section 17.5 covers Bayesian methods for Categorical Data.\n(Spiegelhalter et al. 2002) “Measures of Model Complexity and Fit” by David Spiegelhalter, Andrew Thomas, Nicky Best, and W. Brian Gilks. This paper considers the problem of model selection in Bayesian Hierarchical Models and proposes a new measure of model complexity and fit, the Deviance Information Criterion (DIC).\n(Banerjee, Gelfand, and Carlin 2026) Hierarchical Modeling and Analysis for Spatial Data by Sudipto Banerjee, Bradley P. Carlin, and Alan E. Gelfand. This book is a comprehensive introduction to hierarchical modeling and analysis for spatial data and covers a wide range of topics in spatial statistics, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. data etc\n(Carlin and Louis 2008) Bayesian Methods for Data Analysis\n(Gelman and Hill 2006) This is too is a classic book on data analysis using regression and multilevel/hierarchical models. I read this one may years ago and is fairly approachable and recommended Unless the next edition has come out?",
    "crumbs": [
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#mixture-models",
    "href": "A12.html#mixture-models",
    "title": "114  Bayesian Books in R & Python",
    "section": "114.5 Mixture Models",
    "text": "114.5 Mixture Models\n\n\n\n\n\n\n\nFigure 114.5: Finite Mixture and Markov Switching Models\n\n\nThere were no books recommended in this course but (Frühwirth-Schnatter 2006) was in the bibliography from the previous course. Finite Mixture and Markov Switching Models by Sylvia Frühwirth-Schnatter. This book is a comprehensive introduction to finite mixture and Markov switching models and covers a wide range of topics in mixture modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\n(Fruhwirth-Schnatter, Celeux, and Robert 2019) Handbook of Mixture Analysis by Christian P. Robert, Gilles Celeux, Sylvia Fruhwirth-Schnatter\n(McLachlan and Peel 2004) Finite Mixture Models by Geoffrey J. McLachlan and David Peel. A comprehensive account of major issues in finite mixture modeling. Advanced text on finite mixture models. Lacks exercises and requires deep diving into papers.\n(Chen 2023) Statistical Inference Under Mixture Models by Jiahua Chen. A more recent introduction to mixture models with recent developments in testing hypothesis for the order of the mixture and insights on inference for mixture models.\n(Yao and Xiang 2024) Mixture Models: Parametric, Semiparametric, and New Directions by Weixin Yao and Sijia Xiang.\n(Visser and Speekenbrink 2022) Mixture and Hidden Markov Models with R by Visser, I. and Speekenbrink, M.",
    "crumbs": [
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#non-parametric-bayesian-statistics",
    "href": "A12.html#non-parametric-bayesian-statistics",
    "title": "114  Bayesian Books in R & Python",
    "section": "114.6 Non-parametric Bayesian Statistics",
    "text": "114.6 Non-parametric Bayesian Statistics\n(Bayesian Nonparametrics 2010) “Bayesian Nonparametrics” by Peter M. Hjort, Chris Holmes, Maria E. Müller, and Stephen G. Walker. This book is a comprehensive introduction to Bayesian nonparametric methods and covers a wide range of topics in nonparametric modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.",
    "crumbs": [
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A12.html#bayesian-time-series-bibliography",
    "href": "A12.html#bayesian-time-series-bibliography",
    "title": "114  Bayesian Books in R & Python",
    "section": "114.7 Bayesian Time Series Bibliography",
    "text": "114.7 Bayesian Time Series Bibliography\nWe start with some books from the course, I collected here both the recommended books and some others that I found useful.\n\n\n\n\n\n\n\nFigure 114.6: Bayesian Forecasting and Dynamic Models\n\n\n(West and Harrison 2013) Bayesian Forecasting and Dynamic Models by Mike West and Jeff Harrison. This book is a classic text on Bayesian statistics and covers a wide range of topics in Bayesian forecasting and dynamic models. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. This is a much easier read than the following book and is a great introduction to Bayesian time series analysis. It soon became my goto reference for understanding the more challenging aspects of the course. It provides simple explanations with better motivations.\nI have some criticism. The authors sometimes become very meta discussing their models at very abstract level that is more relevant to philosophy majors than statistics. For example a dynamic model is a defined as a set of models whose parameters may change at any index of time series. That’s a very poor definition - its like asking us to use a 1:1 map for navigation. When I took a course on function approximation in RL we got a much neater exposition to parameterization of to very similar markov state space models. I found that the white noise model getting a very extensive treatment, perhaps, because it is easier to analyze. This is a random walk and the rudiments of which are easily taught in a single high-school lesson. They also digress into statistical war stories type anecdotes that rather than instil confidence that the authors know what they are talking about they are effectively making a case that they failed to communicate effectively with their clients with disastrous effect. Mathematically the book is very much self contained and this shows great integrity. I’ve seen some talks by Mike West in which he highlights research by his students and this are great if very challenging to follow.\nThe authors also published a second book [Applied Bayesian Forecasting and Time Series Analysis]](https://www2.stat.duke.edu/~mw/bats.html) this one is based on the BATS software package which is no longer maintained. However it does contain a large number of datasets\n\n\n\n\n\n\n\nFigure 114.7: Time Series: Modeling, Computation, and Inference\n\n\n(Prado, Ferreira, and West 2023) Time Series: Modeling, Computation, and Inference by course instructor Raquel Prado and Marco A. R. Ferreira, Mike West. This book, now in its second edition is a comprehensive introduction to time series analysis and covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\nWhile learning this course I found some of the material harder to follow than I expected. The book helped to clarify definitions and so on however the book is rather comprehensive and mathematically advanced unlike some other books on statistics.\nThe teacher frequently point out that many aspects of Times series and are beyond the scope of the course. Yet this book covers much more ground like unequally spaced time series and vector valued time series.\nFor example we look at EKG data which the authors have been working on for years. However we look at it in this course in terms of a univariate time series while in reality EKG is usually sampled at 12 sites simultaneously yielding a multi-variate time series.\nOnce this course is done I will probably want to dive deeper into the subject and try to devote more time to other models in the book.\nc.f. (Nielsen 2019)\n\n\n\n\n\n\n\nFigure 114.8: Practical Times Series Analysis\n\n\n(Nielsen 2019) Practical Time Series Analysis: Prediction with Statistics and Machine Learning by Aileen Nielsen. Is a good resource for practitioners getting started with time series analysis. I also recommend any videos by Aileen Nielsen on the subject. In many ways this is a recommended introductory book on time series analysis. It covers, finding and wrangle time series data. EDA for TS. How to store temporal data. Simulate time series data. Generate and select features for a time series. Estimate errors, evaluate accuracy and performance. Forecast and classify time series with machine or deep learning.\n\n\n\n\n\n\n\nFigure 114.9: Statistical Analysis in Climate Research\n\n\n(Storch and Zwiers 2002) Statistical Analysis in Climate Research I came across this book while looking into the Durban-Levinson recursion and the Yule-Walker equations. So far I haven’t had time to read it but it looks promising. Here is the description from the publisher:\n\nClimatology is, to a large degree, the study of the statistics of our climate. The powerful tools of mathematical statistics therefore find wide application in climatological research. The purpose of this book is to help the climatologist understand the basic precepts of the statistician’s art and to provide some of the background needed to apply statistical methodology correctly and usefully. The book is self contained: introductory material, standard advanced techniques, and the specialized techniques used specifically by climatologists are all contained within this one source. There are a wealth of real-world examples drawn from the climate literature to demonstrate the need, power and pitfalls of statistical analysis in climate research. Suitable for graduate courses on statistics for climatic, atmospheric and oceanic science, this book will also be valuable as a reference source for researchers in climatology, meteorology, atmospheric science, and oceanography.\n\n\nHans von Storch is Director of the Institute of Hydrophysics of the GKSS Research Centre in Geesthacht, Germany and a Professor at the Meteorological Institute of the University of Hamburg.\n\n\nFrancis W. Zwiers is Chief of the Canadian Centre for Climate Modelling and Analysis, Atmospheric Environment Service, Victoria, Canada, and an Adjunct Professor at the Department of Mathematics and Statistics of the University of Victoria.\n\n\n\n\n\n\n\n\nFigure 114.10: Bayesian Modeling and Computation in Python\n\n\n(Martin, Kumar, and Lao 2021) Bayesian Modeling and Computation in Python by Osvaldo Martin is a great book for learning Bayesian statistics and covers a wide range of topics in Bayesian modeling, computation, and inference. I found the chapter on state space modeling and the Kalman filter particularly useful. The book is a great resource for translating what we learned in the course to Python. The book is suitable for undergraduate students in statistics, computer science, and related fields.\n\n\n\n\n\n\n\nFigure 114.11: Introductory Time Series with R\n\n\n(Cowpertwait and Metcalfe 2009) Introductory Time Series with R by Cowpertwait and Metcalfe\n\nYearly global mean temperature and ocean levels, daily share prices, and the signals transmitted back to Earth by the Voyager space craft are all examples of sequential observations over time known as time series. This book gives you a step-by-step introduction to analyzing time series using the open source software R. Each time series model is motivated with practical applications, and is defined in mathematical notation. Once the model has been introduced it is used to generate synthetic data, using R code, and these generated data are then used to estimate its parameters. This sequence enhances understanding of both the time series model and the R function used to fit the model to data. Finally, the model is used to analyze observed data taken from a practical application. By using R, the whole procedure can be reproduced by the reader.\nAll the data sets used in the book are available on the website at datasets\nThe book is written for undergraduate students of mathematics, economics, business and finance, geography, engineering and related disciplines, and postgraduate students who may need to analyze time series as part of their taught programme or their research.\n\nPaul Cowpertwait is an associate professor in mathematical sciences (analytics) at Auckland University of Technology with a substantial research record in both the theory and applications of time series and stochastic models.\nAndrew Metcalfe is an associate professor in the School of Mathematical Sciences at the University of Adelaide, and an author of six statistics text books and numerous research papers. Both authors have extensive experience of teaching time series to students at all levels.\n\n\n\n\n\n\n\nFigure 114.12: Analysis of Integrated and Cointegrated Time Series with R\n\n\n(Pfaff 2008) “Analysis of Integrated and Cointegrated Time Series with R” by Bernhard Pfaff. Its been a long time since I read this book and rather than do it an injustice I direct you to the review by Dirk Eddelbuettel in the Journal of Statistical Software is available at review. Or the book’s website at Analysis of Integrated and Cointegrated Time Series with R.\n\nThe analysis of integrated and co-integrated time series can be considered as the main methodology employed in applied econometrics. This book not only introduces the reader to this topic but enables him to conduct the various unit root tests and co-integration methods on his own by utilizing the free statistical programming environment R. The book encompasses seasonal unit roots, fractional integration, coping with structural breaks, and multivariate time series models. The book is enriched by numerous programming examples to artificial and real data so that it is ideally suited as an accompanying text book to computer lab classes.\nThe second edition adds a discussion of vector auto-regressive, structural vector auto-regressive, and structural vector error-correction models.\n\n(Broemeling 2019) Bayesian Analysis of Time Series by Lyle D. Broemeling\nIn many branches of science relevant observations are taken sequentially over time. Bayesian Analysis of Time Series discusses how to use models that explain the probabilistic characteristics of these time series and then utilizes the Bayesian approach to make inferences about their parameters. This is done by taking the prior information and via Bayes theorem implementing Bayesian inferences of estimation, testing hypotheses, and prediction. The methods are demonstrated using both R and WinBUGS. The R package is primarily used to generate observations from a given time series model, while the WinBUGS packages allows one to perform a posterior analysis that provides a way to determine the characteristic of the posterior distribution of the unknown parameters.\nThe book covers pretty much the material in the course. It uses R and WinBUGS to demonstrate the models and methods. Models considered include: white noise, Wiener process (random walk), AR(p),ARMA(p,q), ARIMA, Regression, Regression with MA and Seasonal effects, DLM , TAR\n\n\n\n\n\n\n\nFigure 114.13: Bayesian Inference for Stochastic Processes by Lyle D. Broemeling\n\n\n(Broemeling 2019) Bayesian Inference for Stochastic Processes by Lyle D. Broemeling is a comprehensive introduction to the analysis of stochastic processes using Bayesian methods. The book covers a wide range of topics in stochastic process modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields. The code for R and WinBUGS is available at code It is based on WinBUGS which is a bit dated but still useful and a bit dated but it covers a lot of the material in the course.\n\n\n\n\n\n\n\nFigure 114.14: Dynamic Time Series Models using R-INLA: An Applied Perspective\n\n\n(Ravishanker, Raman, and Soyer 2022) Dynamic Time Series Models using R-INLA: An Applied Perspective is a new book that covers the use of the R-INLA package for fitting dynamic time series models. The book is available online gitbook\nThis is a very interesting book which covers a new approach to fitting time series models using the R-INLA package. INLA stands for Integrated Nested Laplace Approximation and is a method for fitting Bayesian models that is faster than MCMC. The book covers a wide range of topics in time series modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\n\n\n\n\n\n\n\nFigure 114.15: Statistics for Spatio-Temporal Data\n\n\n(Cressie and Wikle 2011) Statistics for Spatio-Temporal Data is a book I came across when I tried to understand the NDLM model. NDLMs have a two level hierarchical form and it seems possible to extend this formulation will non-normally distributed shocks and possibly non linear relation. In this book the authors take an interesting approach of not only looking at NDLM as a hierarchical model but they also extend the time series model into a spatio-temporal model.\nThis book is a comprehensive introduction to the analysis of spatio-temporal data and covers a wide range of topics in spatio-temporal statistics. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\na newer title from the authors is:\n\n\n\n\n\n\n\nFigure 114.16: Spatio-Temporal Statistics with R\n\n\n(Wikle, Zammit-Mangion, and Cressie 2019) Spatio-Temporal Statistics with R\n\n\n\n\n\n\n\nFigure 114.17: Bayesian Analysis of Stochastic Process Models\n\n\n(Rios Insua, Ruggeri, and Wiper 2012) Bayesian Analysis of Stochastic Process Models by David Rios Insua, Fabrizio Ruggeri, Michael P. Wiper. This book is a comprehensive introduction to the analysis of stochastic process models using Bayesian methods. It covers a wide range of topics in stochastic process modeling, computation, and inference. The book is suitable for graduate students and researchers in statistics, computer science, and related fields.\nThere are also a number of books on NDLM that I’ve come across:\nDynamic linear model tutorial matlab\nForecasting, structural time series and the Kalman filter by Andrew C. Harvey\nDynamic Linear Models with R by Giovanni Petris Sonia Petrone Patrizia Campagnoli\nTime Series Analysis by State Space Methods by J. Durbin and S.J. Koopman\n\n\n\n\n\n\n\nFigure 114.18: Machine Learning: A Bayesian and Optimization Perspective\n\n\n(Theodoridis 2015) Machine Learning: A Bayesian and Optimization Perspective Has two chapters on bayesian learning which are summarized in this summer school slide deck. I came across this book while looking into the Durban-Levinson recursion and the Yule-Walker equations. So far I haven’t had time to read it but it looks like a good book on machine learning and the slide deck indicates it covers some of the essential material missing from this specialization like particle filtering.\n\n\n\n\n\n\nAgresti, A. 2012. Categorical Data Analysis. Wiley Series in Probability and Statistics. Wiley. https://books.google.co.il/books?id=UOrr47-2oisC.\n\n\nBanerjee, S., A. E. Gelfand, and B. P. Carlin. 2026. Hierarchical Modeling and Analysis for Spatial Data. CRC Press LLC. https://books.google.co.il/books?id=GRFT0QEACAAJ.\n\n\nBayesian Nonparametrics. 2010. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\n\n\nBroemeling, Lyle D. 2019. Bayesian Analysis of Time Series. CRC Press.\n\n\nCarlin, B. P., and T. A. Louis. 2008. Bayesian Methods for Data Analysis. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=GTJUt8fcFx8C.\n\n\nChen, J. 2023. Statistical Inference Under Mixture Models. ICSA Book Series in Statistics. Springer Nature Singapore. https://books.google.co.il/books?id=sBXlEAAAQBAJ.\n\n\nCowpertwait, P. S. P., and A. V. Metcalfe. 2009. Introductory Time Series with r. Use r! Springer New York. https://books.google.co.il/books?id=QFiZGQmvRUQC.\n\n\nCressie, N., and C. K. Wikle. 2011. Statistics for Spatio-Temporal Data. CourseSmart Series. Wiley. https://books.google.co.il/books?id=-kOC6D0DiNYC.\n\n\nFruhwirth-Schnatter, S., G. Celeux, and C. P. Robert. 2019. Handbook of Mixture Analysis. Chapman & Hall/CRC Handbooks of Modern Statistical Methods. CRC Press. https://books.google.co.il/books?id=N3yCDwAAQBAJ.\n\n\nFrühwirth-Schnatter, S. 2006. Finite Mixture and Markov Switching Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=f8KiI7eRjYoC.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Analytical Methods for Social Research. Cambridge University Press. https://books.google.co.il/books?id=c9xLKzZWoZ4C.\n\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning: With Applications in r. Springer Texts in Statistics. Springer New York. https://books.google.co.il/books?id=qcI_AAAAQBAJ.\n\n\nKruschke, John K. 2011. Doing Bayesian Data Analysis: A Tutorial with R and BUGS. Burlington, MA: Academic Press. http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855.\n\n\nLunn, D., C. Jackson, N. Best, A. Thomas, and D. Spiegelhalter. 2012. The BUGS Book: A Practical Introduction to Bayesian Analysis. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis. https://books.google.co.il/books?id=Cthz3XMa_VQC.\n\n\nMartin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. Bayesian Modeling and Computation in Python. Boca Raton.\n\n\nMcElreath, Richard. 2015. Statistical Rethinking, a Course in r and Stan.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and Jupyter. 3rd ed. O’Reilly Media.\n\n\nMcLachlan, G. J., and D. Peel. 2004. Finite Mixture Models. Wiley Series in Probability and Statistics. Wiley. https://books.google.co.il/books?id=c2_fAox0DQoC.\n\n\nNielsen, A. 2019. Practical Time Series Analysis: Prediction with Statistics and Machine Learning. O’Reilly Media. https://books.google.co.il/books?id=xNOwDwAAQBAJ.\n\n\nPfaff, B. 2008. Analysis of Integrated and Cointegrated Time Series with r. Use r! Springer New York. https://books.google.co.il/books?id=ca5MkRbF3fYC.\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series: Modeling, Computation, and Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nRavishanker, N., B. Raman, and R. Soyer. 2022. Dynamic Time Series Models Using r-INLA: An Applied Perspective. CRC Press.\n\n\nRios Insua, David, Fabrizio Ruggeri, and Michael P Wiper. 2012. Bayesian Analysis of Stochastic Process Models. John Wiley & Sons.\n\n\nSpiegelhalter, David J., Nicola G. Best, Bradley P. Carlin, and Angelika Van Der Linde. 2002. “Bayesian Measures of Model Complexity and Fit.” Journal of the Royal Statistical Society Series B: Statistical Methodology 64 (4): 583–639. https://doi.org/10.1111/1467-9868.00353.\n\n\nStorch, H. von, and F. W. Zwiers. 2002. Statistical Analysis in Climate Research. Cambridge University Press.\n\n\nTheodoridis, S. 2015. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. 1st ed. O’Reilly Media, Inc. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nVisser, I., and M. Speekenbrink. 2022. Mixture and Hidden Markov Models with r. Use r! Springer International Publishing. https://books.google.co.il/books?id=Eep3EAAAQBAJ.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.\n\n\nWikle, C. K., A. Zammit-Mangion, and N. Cressie. 2019. Spatio-Temporal Statistics with r. Chapman & Hall/CRC the r Series. CRC Press. https://books.google.co.il/books?id=FD-IDwAAQBAJ.\n\n\nYao, W., and S. Xiang. 2024. Mixture Models: Parametric, Semiparametric, and New Directions. Chapman & Hall/CRC Monographs on Statistics and Applied Probability. CRC Press. https://books.google.co.il/books?id=scP9EAAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>Bayesian Books in R & Python</span>"
    ]
  },
  {
    "objectID": "A13.html",
    "href": "A13.html",
    "title": "115  Appendix: Yule-Walker Equations & Durbin-Levinson Recursion",
    "section": "",
    "text": "115.1 Durbin-Levinson recursion\nDurbin-Levinson recursion is a method in linear algebra for computing the solution to an equation involving a Toeplitz matrix AKA a diagonal-constant matrix where descending diagonals are constant. The recursion runs in O(n^2) time rather then O(n^3) time required by Gauss-Jordan elimination.\nIn the course on Bayesian time series analysis, the Professor mentions the Durbin-Levinson recursion several times without explaining what this is. It is a shame as it is a very elegant bit of linear algebra for solving the Yule-Walker equations more efficiently. I tried to find a good explanation in the context of the course, However I wrote some notes that can help you understand this topic. One final point is that Durbin-Levinson recursion is not the last word on solving this system of equations. There are today numerous improvements which are both faster and more numerically stable!\nLike me, you might be curious about the Durbin-Levinson recursion mentioned above. This is not covered in the course, and turned out to be an enigma wrapped in a mystery.\nI present my finding in the note below - much of it is due to (Wikipedia contributors 2024b) and (Wikipedia contributors 2024a)\nIn (Yule 1927) and (Walker 1931), Yule and Walker proposed a method for estimating the parameters of an autoregressive model. The method is based on the Yule-Walker equations which are a set of linear equations that can be used to estimate the parameters of an autoregressive model.\nDue to the autoregressive structure of the model, the matrix for these equations is sparse and of a well known form called a Toeplitz matrix. \n\\begin{array}{c} {\\displaystyle \\qquad {\\begin{bmatrix}a&b&c&d&e\\\\f&a&b&c&d\\\\g&f&a&b&c\\\\h&g&f&a&b\\\\i&h&g&f&a\\end{bmatrix}}.} \\end{array}\n\\tag{115.1}\nIn the 1930s, Yule and Walker would have had to solve these equations using Gauss-Jordan elimination which has an O(n^3) time complexity.\nThis is where Durbin and Levinson come in. A decade or two later in (Levinson 1946) and (Durbin 1960) the authors came up for with a weakly stable yet more efficient algorithm for solving these autocorrelated system of equations which requires only O(n^2) in time complexity. Later their work was further refined in (Trench 1964) and (Zohar 1969) to just 3\\times n^2 multiplication.\nA cursory search reveals that Toeplitz matrix inversion is still an area of active research with papers covering parallel algorithms and stability studies. Not surprising as most of the more interesting deep learning models, including LLMs are autoregressive!\nSo the Durbin-Levinson recursion is just an elegant bit of linear algebra for solving the Yule-Walker equations more efficiently.\nHere is what I dug up:",
    "crumbs": [
      "<span class='chapter-number'>115</span>  <span class='chapter-title'>Appendix: Yule-Walker Equations & Durbin-Levinson Recursion</span>"
    ]
  },
  {
    "objectID": "A13.html#sec-durbin-levinson",
    "href": "A13.html#sec-durbin-levinson",
    "title": "115  Appendix: Yule-Walker Equations & Durbin-Levinson Recursion",
    "section": "",
    "text": "115.1.1 Durbin-Levinson and the Yule-Walker equations (Off-Course Reading)\nThe Durbin-Levinson recursion is a method in linear algebra for computing the solution to an equation involving a Toeplitz matrix AKA a diagonal-constant matrix where descending diagonals are constant. The recursion runs in O(n^2) time rather then O(n^3) time required by Gauss-Jordan elimination.\nThe recursion can be used to compute the coefficients of the autoregressive model of a stationary time series. It is based on the Yule-Walker equations and is used to compute the PACF of a time series.\nThe Yule-Walker equations can be stated as follows for an AR(p) process:\n\n\\gamma_m = \\sum_{k=1}^p \\phi_k \\gamma_{m-k} + \\sigma_\\varepsilon^2\\delta_{m,0} \\qquad \\text{(Yule-Walker equations)}\n\\tag{115.2}\nwhere:\n\n\\gamma_m is the autocovariance function of the time series,\n\\phi_k are the AR coefficients,\n\\sigma_\\varepsilon^2 is the variance of the white noise process, and\n\\delta_{m,0} is the Kronecker delta function.\n\nwhen m=0 the equation simplifies to:\n\n\\gamma_0 = \\sum_{k=1}^p \\phi_k \\gamma_{-k} + \\sigma_\\varepsilon^2 \\qquad \\text{(Yule-Walker equations for m=0)}\n\\tag{115.3}\nfor m &gt; 0 the equation simplifies to:\n\n  \\begin{bmatrix}\n    \\gamma_1 \\\\\n    \\gamma_2 \\\\\n    \\gamma_3 \\\\\n    \\vdots   \\\\\n    \\gamma_p\n  \\end{bmatrix} =  \n  \\begin{bmatrix}\n    \\gamma_0     & \\gamma_{-1}  & \\gamma_{-2}  & \\cdots \\\\\n    \\gamma_1     & \\gamma_0     & \\gamma_{-1}  & \\cdots \\\\\n    \\gamma_2     & \\gamma_1     & \\gamma_0     & \\cdots \\\\\n    \\vdots       & \\vdots       & \\vdots       & \\ddots \\\\\n    \\gamma_{p-1} & \\gamma_{p-2} & \\gamma_{p-3} & \\cdots\n\\end{bmatrix}  \n\\begin{bmatrix}\n    \\phi_{1} \\\\\n    \\phi_{2} \\\\\n    \\phi_{3} \\\\\n    \\vdots   \\\\\n    \\phi_{p}\n\\end{bmatrix}\n\n and since this matrix is Toeplitz, we can use Durbin-Levinson recursion to efficiently solve the system for \\phi_k \\forall k.\nOnce \\{\\phi_m \\qquad m=1,2, \\dots ,p \\} are known, we can consider m=0 and solved for \\sigma_\\varepsilon^2 by substituting the \\phi_k into Equation 115.3 Yule-Walker equations.\nOf course the Durbin-Levinson recursion is not the last word on solving this system of equations. There are today numerous improvements which are both faster and more numerically stable.\nThe Yule-Walker equations are a set of p linear equations in the p unknowns \\phi_1, \\phi_2, \\ldots, \\phi_p that can be used to estimate the parameters of an autoregressive model of order p. The Yule-Walker equations are derived by setting the sample autocorrelation function equal to the theoretical autocorrelation function of an AR(p) model and then solving for the unknown parameters. The Yule-Walker equations are given by:\n\n\\begin{aligned}\n\\gamma(0) & = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + \\ldots + \\phi_p \\gamma(p) \\\\\n\\gamma(1) & = \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) + \\ldots + \\phi_p \\gamma(p-1) \\\\\n\\gamma(2) & = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0) + \\ldots + \\phi_p \\gamma(p-2) \\\\\n\\vdots \\\\\n\\gamma(p) & = \\phi_1 \\gamma(p-1) + \\phi_2 \\gamma(p-2) + \\ldots + \\phi_p \\gamma(0) \\\\\n\\end{aligned}\n\nwhere \\gamma(k) is the sample autocorrelation function at lag k. The Yule-Walker equations can be solved using matrix algebra to obtain the estimates of the AR parameters \\phi_1, \\phi_2, \\ldots, \\phi_p.\n\n\n\n\n\n\nDurbin, J. 1960. “The Fitting of Time-Series Models.” Revue de l’Institut International de Statistique / Review of the International Statistical Institute 28 (3): 233–44. http://www.jstor.org/stable/1401322.\n\n\nLevinson, Norman. 1946. “The Wiener (Root Mean Square) Error Criterion in Filter Design and Prediction.” Journal of Mathematics and Physics 25 (1-4): 261–78. https://doi.org/https://doi.org/10.1002/sapm1946251261.\n\n\nTrench, William F. 1964. “An Algorithm for the Inversion of Finite Toeplitz Matrices.” Journal of the Society for Industrial and Applied Mathematics 12 (3): 515–22. http://ramanujan.math.trinity.edu/wtrench/research/papers/TRENCH_RP_6.PDF.\n\n\nWalker, Gilbert Thomas. 1931. “On Periodicity in Series of Related Terms.” Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character 131 (818): 518–32. https://doi.org/10.1098/rspa.1931.0069.\n\n\nWikipedia contributors. 2024a. “Autoregressive Model — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Autoregressive_model&oldid=1233171855#Estimation_of_AR_parameters.\n\n\n———. 2024b. “Levinson Recursion — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Levinson_recursion&oldid=1229942891.\n\n\nYule, George Udny. 1927. “VII. On a Method of Investigating Periodicities Disturbed Series, with Special Reference to Wolfer’s Sunspot Numbers.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 226 (636-646): 267–98. https://doi.org/10.1098/rsta.1927.0007.\n\n\nZohar, Shalhav. 1969. “Toeplitz Matrix Inversion: The Algorithm of w. F. Trench.” J. ACM 16: 592–601. https://api.semanticscholar.org/CorpusID:3115290.",
    "crumbs": [
      "<span class='chapter-number'>115</span>  <span class='chapter-title'>Appendix: Yule-Walker Equations & Durbin-Levinson Recursion</span>"
    ]
  },
  {
    "objectID": "A14.html",
    "href": "A14.html",
    "title": "116  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "",
    "text": "116.1 Properties of transpose\nThe Moore-Penrose inversion and the Cholesky decomposition are two important methods in linear algebra for solving linear equations efficiently. They are widely used in various applications, including statistics, machine learning, and numerical analysis.\ntranspose of a row vector is a column vector and vice versa:\nkA^{T} = {kA}^T \\qquad \\text{scalar multiplication }\n\\tag{116.1}\n(A^{T})^T = A \\qquad \\text { involution }\n\\tag{116.2}\n(A+B)^{T} = A^T + B^T \\qquad\\text {  distributivity under addition }\n\\tag{116.3}\n(AB)^T = B^T A^T \\qquad\\text {   anti }\n\\tag{116.4}\nnote that we swap the order of the matrices in the product when taking the transpose.\nif A is a square matrix, then the following are equivalent: \nSq = A^t*A = A*A^t\n where Sq is a symmetric positive definite matrix.\nSk = A^t*A = A*A^t",
    "crumbs": [
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A14.html#sec-full-rank",
    "href": "A14.html#sec-full-rank",
    "title": "116  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "116.2 Full Rank",
    "text": "116.2 Full Rank\nA matrix is said to be of full row rank if its rows are linearly independent, and it is of full column rank if its columns are linearly independent. A matrix is said to be of full rank if it is either of full row rank or full column rank.",
    "crumbs": [
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A14.html#sec-moore-penrose",
    "href": "A14.html#sec-moore-penrose",
    "title": "116  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "116.3 Generelised (Moore-Penrose) Inverse",
    "text": "116.3 Generelised (Moore-Penrose) Inverse\nThe Moore-Penrose inversion is a method for computing the pseudoinverse of a matrix. The pseudoinverse is a generalization of the inverse of a matrix that can be used to solve linear equations when the matrix is rectangular, not-invertible or even singular.\n\nDefinition 116.1 (Definition of the Moore-Penrose Inverse 1) The Moore–Penrose inverse of the m × n matrix A is the n × m matrix, denoted by A^+, which satisfies the conditions\n\nAA+ A = A\n\\tag{116.5}\n  \nA^+AA^+ = A^+\n\\tag{116.6}\n\n(AA^+ )' = AA^+\n\\tag{116.7}\n\n(A^+A)' = A^+A\n\\tag{116.8}\n\nAn important features of the Moore–Penrose inverse, is that it is uniquely defined.\nCorresponding to each m × n matrix A, one and only one n × m matrix A^+ exists satisfying conditions (Equation 116.5)–(Equation 116.8).\nDefinition Definition 116.1 is the definition of a generalized inverse given by Penrose (1955).\nThe following alternative definition, which we will find useful on some occasions, utilizes properties of the Moore–Penrose inverse that were first illustrated by Moore (1935).\n\nDefinition 116.2 (Definition of the Moore-Penrose Inverse 2) Let A be an m × n matrix. Then the Moore–Penrose inverse of A is the unique n × m matrix A^+ satisfying\n\nAA^+ = P_{R(A)}\n\\tag{116.9}\n\nA^+ A = P_{R(A^+)}\n\\tag{116.10}\nwhere P_{R(A)} and P_{R(A^+)} are the projection matrices of the range spaces of A and A^+, respectively.\n\n\nTheorem 116.1 (Properties of the Moore-Penrose inverse) Let A be an m \\times n matrix. Then:\n\n(αA)^+ = α^{-1} A^+ , \\text{ if } \\alpha \\ne 0 \\text{ is a scalar}\n(A^T)^+ = (A^+)^T\n(A^+)^+ = A\nA^+ = A^{-1} ,\\text{if A is square and nonsingular}\n(A^T A)^+ = A^+ A^T and (AA^T)^+ = A^T A^+\n(AA^+)^+ = AA^+ and (A^+ A)^+ = A^+ A\nA^+ = (A^T A)^+ A^T = A^T (AA^T)^+\nA^+ = (A^T A)^{-1} A^T and A^+ A = I_n , \\text{ if } rank(A) = n\nA^+ = A^T (AA^T)^{-1} and AA^+ = I_m , \\text{ if } rank(A) = m\nA^+ = A^T if the columns of A are orthogonal, that is, A^T A = I_n\n\n\n\nTheorem 116.2 (Rank of Moore-Penrose Inverse) For any m \\times n matrix A, \\text{rank}(A) = \\text{rank}(A^+) = \\text{rank}(AA^+) = \\text{rank}(A^+ A).\n\n\nTheorem 116.3 (Symmetric Moore-Penrose Inverse) Let A be an m × m symmetric matrix. Then a. A^+ is also symmetric, b. AA^+ = $A^+A, c. A^+ = A, if A is idempotent.\n\n\nThe Moore-Penrose inverse is particularly useful in maximum likelihood estimation (MLE) for linear models. In MLE, we often need to solve linear equations of the form Ax = b, where A is the design matrix and b is the response vector. If A is not full rank or is not square, we can use the Moore-Penrose inverse to find a solution that minimizes the residual sum of squares.\nIn the context of MLE, the Moore-Penrose inverse allows us to obtain parameter estimates even when the design matrix is singular or when there are more predictors than observations. This is achieved by projecting the response vector onto the column space of the design matrix, leading to a solution that is consistent and has desirable statistical properties.\nwe start with:\n\ny = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol\\varepsilon \\qquad \\boldsymbol\\varepsilon \\sim \\mathcal{N} (0, v\\mathbf{I})\n\nwe want MLE of \\boldsymbol{\\beta}, which is given by: \n\\hat{\\boldsymbol{\\beta}}_{NKE} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T y\n\nAlso in the baysian setting we can show that MLE is equivalent to minimizing the negative log-likelihood function, under a uniform prior on \\boldsymbol{\\beta}, which is equivalent to minimizing the residual sum of squares.\nwe can show that if we use least squares AKA l_2 norm minimization, we will end up with the Moore-Penrose inverse to find the solution:\nwe can write this explicitly as:\n\n\\mathbb{E}_{l_2}(\\boldsymbol{\\beta}) = \\frac{1}{2} \\sum (y - \\boldsymbol{\\beta}^T \\mathbf{X})^2",
    "crumbs": [
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A14.html#sec-cholesky",
    "href": "A14.html#sec-cholesky",
    "title": "116  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "116.4 Cholesky Decomposition",
    "text": "116.4 Cholesky Decomposition\nThe Cholesky decomposition is a method for factorizing a positive definite matrix into the product of a lower triangular matrix and its transpose. It is particularly useful for solving systems of linear equations and for generating samples from multivariate normal distributions.\n\nDefinition 116.3 (Definition of the Cholesky Decomposition) André-Louis Cholesky (1875–1918) was a cartographer in the French army, who introduced a method for decomposing a symmetric positive definite matrix into the product of a lower triangular matrix and its transpose. This decomposition is known as the Cholesky decomposition.\nLet A be a symmetric positive definite matrix. The Cholesky decomposition of A is a factorization of the form: \nA = LL^T\n\\tag{116.11} where L is a lower triangular matrix with positive diagonal entries.\n\nThe Cholesky decomposition is unique for a given positive definite matrix, and it can be computed efficiently using algorithms such as the Doolittle algorithm or the Crout algorithm.\n\n116.4.1 Doolittle Algorithm\n\ndef doolittle(A):\n    \"\"\"Performs Doolittle LU decomposition: A = LU, with L unit lower triangular and U upper triangular.\"\"\"\n    n = len(A)\n    L = [[0.0]*n for _ in range(n)]\n    U = [[0.0]*n for _ in range(n)]\n    \n    for i in range(n):\n        # Upper Triangular\n        for k in range(i, n):\n            U[i][k] = A[i][k] - sum(L[i][j]*U[j][k] for j in range(i))\n        \n        # Lower Triangular\n        L[i][i] = 1.0\n        for k in range(i+1, n):\n            if U[i][i] == 0:\n                raise ZeroDivisionError(\"Zero pivot encountered.\")\n            L[k][i] = (A[k][i] - sum(L[k][j]*U[j][i] for j in range(i))) / U[i][i]\n    \n    return L, U\n\n\ndef print_matrix(M, name=\"Matrix\"):\n    print(f\"{name} =\")\n    for row in M:\n        print(\"  [\" + \"  \".join(f\"{val:8.3f}\" for val in row) + \"]\")\n    print()\n\n\nA = [\n    [2, 3, 1],\n    [4, 7, 7],\n    [6, 18, 22]\n]\n\nprint_matrix(A, name=\"A\")\n\n\nL, U = doolittle(A)\nprint_matrix(L, name=\"L\")\nprint_matrix(U, name=\"U\")\n\nA =\n  [   2.000     3.000     1.000]\n  [   4.000     7.000     7.000]\n  [   6.000    18.000    22.000]\n\nL =\n  [   1.000     0.000     0.000]\n  [   2.000     1.000     0.000]\n  [   3.000     9.000     1.000]\n\nU =\n  [   2.000     3.000     1.000]\n  [   0.000     1.000     5.000]\n  [   0.000     0.000   -26.000]\n\n\n\n\n\n116.4.2 Doolittle’s Algorithm with Partial Pivoting\nWhen performing LU decomposition, it is often necessary to use partial pivoting to ensure numerical stability and to handle cases where the matrix may be singular or nearly singular. Partial pivoting involves swapping rows of the matrix to place the largest absolute value in the pivot position.\nAdding partial pivoting is algebraically equivalent to multiplying the original matrix by a permutation matrix P, such that PA = LU, where P is a permutation matrix, L is a lower triangular matrix, and U is an upper triangular matrix.\n\ndef doolittle_partial_pivoting(A):\n    \"\"\"Performs LU decomposition with partial pivoting: PA = LU.\"\"\"\n    n = len(A)\n    # Deep copy of A\n    A = [row[:] for row in A]\n    P = list(range(n))\n    L = [[0.0]*n for _ in range(n)]\n    U = [[0.0]*n for _ in range(n)]\n\n    for k in range(n):\n        # Partial pivoting: find row with max abs value in column k\n        pivot_row = max(range(k, n), key=lambda i: abs(A[i][k]))\n        if A[pivot_row][k] == 0:\n            raise ZeroDivisionError(\"Matrix is singular.\")\n\n        # Swap rows in A and record permutation\n        A[k], A[pivot_row] = A[pivot_row], A[k]\n        P[k], P[pivot_row] = P[pivot_row], P[k]\n        for i in range(k):\n            L[k][i], L[pivot_row][i] = L[pivot_row][i], L[k][i]\n\n        # Compute U[k][k:] and L[k+1:][k]\n        L[k][k] = 1.0\n        for j in range(k, n):\n            U[k][j] = A[k][j] - sum(L[k][s]*U[s][j] for s in range(k))\n        for i in range(k+1, n):\n            L[i][k] = (A[i][k] - sum(L[i][s]*U[s][k] for s in range(k))) / U[k][k]\n\n    # Permutation matrix P as a 2D matrix\n    P_matrix = [[1 if j == P[i] else 0 for j in range(n)] for i in range(n)]\n    return P_matrix, L, U\n\nDemo for Doolittle’s algorithm with partial pivoting:\n\nA = [\n    [0, 3, 1],\n    [4, 7, 7],\n    [6, 18, 22]\n]\n\nprint_matrix(A, name=\"A\")\n\n\nP, L, U = doolittle_partial_pivoting(A)\nprint_matrix(P, name=\"P\")\nprint_matrix(L, name=\"L\")\nprint_matrix(U, name=\"U\")\n\nA =\n  [   0.000     3.000     1.000]\n  [   4.000     7.000     7.000]\n  [   6.000    18.000    22.000]\n\nP =\n  [   0.000     0.000     1.000]\n  [   0.000     1.000     0.000]\n  [   1.000     0.000     0.000]\n\nL =\n  [   1.000     0.000     0.000]\n  [   0.667     1.000     0.000]\n  [   0.000    -0.600     1.000]\n\nU =\n  [   6.000    18.000    22.000]\n  [   0.000    -5.000    -7.667]\n  [   0.000     0.000    -3.600]\n\n\n\n\nA = [\n    [2, 1, 1, 3, 2],\n    [1, 2, 2, 1, 1],\n    [3, 2, 3, 2, 1],\n    [2, 1, 2, 2, 1],\n    [1, 1, 1, 1, 1]\n]\n\nP, L, U = doolittle_partial_pivoting(A)\n\nprint_matrix(P, \"P\")\nprint_matrix(L, \"L\")\nprint_matrix(U, \"U\")\n\ndef matmul(A, B):\n    return [[sum(A[i][k] * B[k][j] for k in range(len(B)))\n             for j in range(len(B[0]))]\n            for i in range(len(A))]\n\ndef permute(A, P):\n    \"\"\"P is a permutation matrix; return PA.\"\"\"\n    return matmul(P, A)\n\nPA = permute(A, P)\nLU = matmul(L, U)\n\n# Print comparison\nprint_matrix(PA, \"PA\")\nprint_matrix(LU, \"LU\")\n\nP =\n  [   0.000     0.000     1.000     0.000     0.000]\n  [   0.000     1.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     1.000     0.000]\n  [   1.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     1.000]\n\nL =\n  [   1.000     0.000     0.000     0.000     0.000]\n  [   0.333     1.000     0.000     0.000     0.000]\n  [   0.667    -0.250     1.000     0.000     0.000]\n  [   0.667    -0.250    -3.000     1.000     0.000]\n  [   0.333     0.250    -1.000     0.250     1.000]\n\nU =\n  [   3.000     2.000     3.000     2.000     1.000]\n  [   0.000     1.333     1.000     0.333     0.667]\n  [   0.000     0.000     0.250     0.750     0.500]\n  [   0.000     0.000     0.000     4.000     3.000]\n  [   0.000     0.000     0.000     0.000     0.250]\n\nPA =\n  [   3.000     2.000     3.000     2.000     1.000]\n  [   1.000     2.000     2.000     1.000     1.000]\n  [   2.000     1.000     2.000     2.000     1.000]\n  [   2.000     1.000     1.000     3.000     2.000]\n  [   1.000     1.000     1.000     1.000     1.000]\n\nLU =\n  [   3.000     2.000     3.000     2.000     1.000]\n  [   1.000     2.000     2.000     1.000     1.000]\n  [   2.000     1.000     2.000     2.000     1.000]\n  [   2.000     1.000     1.000     3.000     2.000]\n  [   1.000     1.000     1.000     1.000     1.000]\n\n\n\n\n\n116.4.3 Vectorization of the doolittle algorithm\n\nimport numpy as np\n\ndef doolittle_numpy(A):\n    \"\"\"LU decomposition with partial pivoting using NumPy. Returns P, L, U such that PA = LU.\"\"\"\n    A = np.array(A, dtype=float)\n    n = A.shape[0]\n    P = np.eye(n)\n    L = np.zeros((n, n))\n    U = A.copy()\n\n    for k in range(n):\n        # Partial pivoting\n        pivot = np.argmax(abs(U[k:, k])) + k\n        if U[pivot, k] == 0:\n            raise ZeroDivisionError(\"Matrix is singular.\")\n        if pivot != k:\n            U[[k, pivot]] = U[[pivot, k]]\n            P[[k, pivot]] = P[[pivot, k]]\n            L[[k, pivot], :k] = L[[pivot, k], :k]\n\n        L[k, k] = 1.0\n        L[k+1:, k] = U[k+1:, k] / U[k, k]\n        U[k+1:] -= np.outer(L[k+1:, k], U[k])\n\n    return P, L, U\n\n\ndef random_sign_matrix(n, seed=None):\n    \"\"\"Generate an n×n matrix with random entries in {-1, 0, 1}.\"\"\"\n    rng = np.random.default_rng(seed)\n    return rng.choice([-1, 0, 1], size=(n, n))\n\nA = [\n    [2, 3, 1],\n    [4, 7, 7],\n    [6, 18, 22]\n]\nA = random_sign_matrix(16, seed=42)\n\nprint_matrix(A, name=\"A\")\n\nP, L, U = doolittle_numpy(A)\n\nprint_matrix(P, name=\"P\")\n\nprint_matrix(L, name=\"L\")\n\nprint_matrix(U, name=\"U\")\n\nA =\n  [  -1.000     1.000     0.000     0.000     0.000     1.000    -1.000     1.000    -1.000    -1.000     0.000     1.000     1.000     1.000     1.000     1.000]\n  [   0.000    -1.000     1.000     0.000     0.000     0.000    -1.000     1.000     1.000     0.000     0.000     1.000     0.000     0.000     0.000    -1.000]\n  [  -1.000     0.000     1.000    -1.000     1.000     1.000    -1.000     0.000    -1.000     1.000     1.000     0.000    -1.000     1.000     0.000     1.000]\n  [   1.000     1.000     1.000    -1.000     0.000     0.000     0.000    -1.000     0.000    -1.000     1.000     1.000     1.000     1.000     0.000     1.000]\n  [   0.000    -1.000     1.000     0.000    -1.000     0.000     1.000    -1.000     0.000    -1.000     1.000     0.000    -1.000    -1.000     0.000     1.000]\n  [   1.000     0.000    -1.000     1.000     0.000     1.000    -1.000    -1.000     1.000     1.000     0.000     1.000     1.000     0.000     1.000    -1.000]\n  [  -1.000     1.000     0.000    -1.000     1.000    -1.000     1.000    -1.000     1.000     1.000     1.000     0.000     0.000     1.000    -1.000     1.000]\n  [   0.000     0.000     0.000     0.000    -1.000    -1.000    -1.000    -1.000     0.000     1.000     0.000     0.000     1.000     0.000    -1.000     1.000]\n  [   0.000     0.000     0.000     0.000    -1.000     0.000     1.000    -1.000     0.000    -1.000     0.000     0.000     1.000    -1.000    -1.000     0.000]\n  [   1.000     1.000    -1.000    -1.000     1.000    -1.000     1.000    -1.000     1.000    -1.000     0.000     0.000    -1.000     0.000     0.000     1.000]\n  [   1.000     0.000     0.000     0.000     0.000     1.000    -1.000    -1.000     0.000    -1.000    -1.000    -1.000     1.000     1.000     1.000     0.000]\n  [   1.000    -1.000     1.000     0.000     1.000    -1.000     0.000     1.000     0.000     0.000    -1.000     0.000    -1.000    -1.000     1.000     0.000]\n  [   0.000     0.000     1.000    -1.000     0.000    -1.000     0.000     1.000     0.000     1.000     0.000     1.000     0.000    -1.000     1.000     1.000]\n  [  -1.000     1.000    -1.000     1.000     1.000     0.000     1.000    -1.000    -1.000    -1.000     0.000     1.000    -1.000     0.000     1.000    -1.000]\n  [   1.000    -1.000     1.000     0.000     0.000    -1.000     0.000     1.000    -1.000     1.000     0.000     1.000     0.000     0.000    -1.000     0.000]\n  [  -1.000     0.000    -1.000     0.000     0.000    -1.000     1.000     0.000     1.000    -1.000    -1.000     0.000    -1.000    -1.000     0.000    -1.000]\n\nP =\n  [   1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.000]\n\nL =\n  [   1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     1.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.500     0.750     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.000    -0.500     0.000     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   1.000    -0.500    -0.750    -1.000     0.667     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -0.000     0.000    -0.000     0.000    -0.667    -0.571     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   1.000     0.000     0.500     0.667     0.667    -0.714    -0.938     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [   1.000     0.000    -0.000    -0.667     0.333    -0.143    -0.187     0.537     1.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -0.000    -0.500    -0.750    -0.333     0.333     0.286     0.375    -0.232     0.670     1.000     0.000     0.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.500     0.250     0.333     0.000     0.429     0.125    -0.189    -0.393     0.619     1.000     0.000     0.000     0.000     0.000     0.000]\n  [  -0.000     0.000    -0.500    -0.667     0.000     0.000     0.438    -0.242     0.214    -0.636    -0.177     1.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.000    -0.500     0.000     0.333    -0.143     0.250    -0.211    -0.625    -0.996    -0.786     0.408     1.000     0.000     0.000     0.000]\n  [  -0.000     0.000    -0.000     0.000    -0.667    -0.143    -0.188     0.032    -0.161     0.360     0.480     0.771     0.733     1.000     0.000     0.000]\n  [  -0.000    -0.500    -0.750    -0.333    -0.333     0.143    -0.250     0.042    -0.045     0.933     0.070     0.108     0.143     0.431     1.000     0.000]\n  [   1.000    -0.500     0.250    -0.333    -0.333    -0.286     0.062     0.158     0.554     0.895     0.568     0.802     0.025     0.140    -0.221     1.000]\n\nU =\n  [  -1.000     1.000     0.000     0.000     0.000     1.000    -1.000     1.000    -1.000    -1.000     0.000     1.000     1.000     1.000     1.000     1.000]\n  [   0.000     2.000     1.000    -1.000     0.000     1.000    -1.000     0.000    -1.000    -2.000     1.000     2.000     2.000     2.000     1.000     2.000]\n  [   0.000     0.000    -2.000     0.000     1.000    -1.000     1.000     0.000     1.000     0.000    -1.000    -1.000    -2.000    -1.000     0.000     0.000]\n  [   0.000     0.000     0.000     1.500    -0.750     2.250    -2.250     0.000    -0.250     1.000     0.250     1.750     2.500     0.750     1.500    -1.000]\n  [   0.000     0.000     0.000     0.000     1.500    -0.500    -0.500     2.000    -0.500    -1.000    -1.500     0.500    -1.000    -0.500     2.000     1.000]\n  [   0.000     0.000     0.000     0.000     0.000     2.333    -1.667    -2.333     0.333     2.667     2.000     0.667     0.667     1.333    -0.333    -0.667]\n  [   0.000     0.000     0.000     0.000     0.000     0.000    -2.286    -1.000    -0.143     1.857     0.143     0.714     0.714     0.429     0.143     1.286]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000    -5.938     0.104     3.646     2.896     0.146    -0.854     0.688    -2.437    -1.271]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.965     1.772     0.425     0.151     1.688     0.568    -0.379    -0.172]\n  [   0.000     0.000     0.000     0.000     0.000    -0.000     0.000     0.000     0.000    -2.134     0.095     1.141    -1.120    -0.096     0.064    -1.137]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000    -1.551    -2.328     1.486     0.788     0.474     0.854]\n  [   0.000     0.000     0.000     0.000     0.000    -0.000     0.000     0.000     0.000     0.000     0.000     1.671    -0.663    -1.065     1.553    -1.072]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.449     2.208    -1.697    -0.149]\n  [   0.000     0.000     0.000     0.000     0.000    -0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000    -2.134     0.123     1.760]\n  [   0.000     0.000     0.000     0.000     0.000    -0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     1.765     2.843]\n  [   0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.802]\n\n\n\n\n\n116.4.4 Crout’s Algorithm\nCrout’s algorithm is another method for performing LU decomposition, similar to Doolittle’s algorithm. It constructs the lower triangular matrix L directly, while the upper triangular matrix U is obtained from the original matrix A.\nthe key parts are the formula for computing the entries of L:\n\nL[i][j] = A[i][j] - \\sum_{k=0}^{j-1} L[i][k] U[k][j]\n\nand using this to computing the entries of U:\n\nU[j][i] = \\frac{A[j][i] - \\sum_{k=0}^{j-1} L[j][k] U[k][i]}{L[j][j]}\n\n\nimport numpy as np\n\ndef crout_lu(A, *, pivot=True, rtol=1e-9, atol=None):\n    \"\"\"\n    Robust Crout LU with partial pivoting.\n\n    Improvements\n    ------------\n    1. Pivot is selected from the *updated* column, eliminating the\n       “false zero-pivot” issue.\n    2. Test `abs(pivot) &lt; max(atol, rtol*‖A‖∞)` so tolerance scales with data.\n\n    Returns L, U, P such that  P @ A = L @ U.\n    \"\"\"\n    A = np.asarray_chkfinite(A, dtype=float)\n    n  = A.shape[0]\n    if A.ndim != 2 or n != A.shape[1]:\n        raise ValueError(\"square matrix required\")\n\n    L = np.zeros_like(A)\n    U = np.eye(n, dtype=A.dtype)\n    P = np.eye(n, dtype=A.dtype)\n    rows = np.arange(n)\n\n    if atol is None:\n        atol = np.finfo(A.dtype).eps * np.linalg.norm(A, np.inf) * 10\n\n    for k in range(n):\n        # current residual column\n        col = A[rows[k:], k] - L[k:, :k] @ U[:k, k]\n\n        if pivot:\n            j = k + np.argmax(np.abs(col))        # best row\n            if np.abs(col[j - k]) &lt; max(atol, rtol*np.abs(col).max()):\n                raise np.linalg.LinAlgError(\"matrix is numerically singular\")\n            if j != k:                            # swap logical rows\n                rows[[k, j]] = rows[[j, k]]\n                L[[k, j], :k] = L[[j, k], :k]\n                P[[k, j]] = P[[j, k]]\n                col[[0, j - k]] = col[[j - k, 0]]\n\n        L[k:, k] = col\n        if np.abs(L[k, k]) &lt; max(atol, rtol*np.abs(col).max()):\n            raise np.linalg.LinAlgError(\"zero pivot encountered\")\n\n        # row k of U (unit diagonal)\n        U[k, k+1:] = (A[rows[k], k+1:] - L[k, :k] @ U[:k, k+1:]) / L[k, k]\n\n    return L, U, P\n\nTest the Crout LU decomposition with a random full rank sign matrix:\n\ndef random_full_rank_sign_matrix(n, seed=42):\n    rng = np.random.default_rng(seed)\n    while True:\n        A = rng.choice([-1,0, 1], size=(n, n))\n        if np.linalg.matrix_rank(A) == n:\n            return A\n\nA = random_full_rank_sign_matrix(12, seed=42)\nprint_matrix(A, name=\"A\")\nA_orig = A.copy()  # Keep original for verification\nL, U, P = crout_lu(A)\n\n# Check correctness\nassert np.allclose(P @ A_orig, L @ U, atol=1e-8)\n\nA =\n  [  -1.000     1.000     0.000     0.000     0.000     1.000    -1.000     1.000    -1.000    -1.000     0.000     1.000]\n  [   1.000     1.000     1.000     1.000     0.000    -1.000     1.000     0.000     0.000     0.000    -1.000     1.000]\n  [   1.000     0.000     0.000     1.000     0.000     0.000     0.000    -1.000    -1.000     0.000     1.000    -1.000]\n  [   1.000     1.000    -1.000     0.000    -1.000     1.000     1.000     0.000    -1.000     1.000     0.000     1.000]\n  [   1.000     1.000     1.000    -1.000     0.000     0.000     0.000    -1.000     0.000    -1.000     1.000     1.000]\n  [   1.000     1.000     0.000     1.000     0.000    -1.000     1.000     0.000    -1.000     0.000     1.000    -1.000]\n  [   0.000    -1.000     1.000     0.000    -1.000    -1.000     0.000     1.000     1.000     0.000    -1.000     1.000]\n  [   0.000     1.000    -1.000    -1.000     1.000     1.000     0.000     1.000     1.000     0.000     1.000    -1.000]\n  [  -1.000     1.000     0.000    -1.000     1.000    -1.000     1.000    -1.000     1.000     1.000     1.000     0.000]\n  [   0.000     1.000    -1.000     1.000     0.000     0.000     0.000     0.000    -1.000    -1.000    -1.000    -1.000]\n  [   0.000     1.000     0.000     0.000     1.000     0.000    -1.000     1.000     0.000     0.000     0.000     0.000]\n  [  -1.000     0.000     1.000    -1.000     0.000    -1.000     0.000     0.000     1.000    -1.000    -1.000     0.000]\n\n\n\nThe Cholesky decomposition has several important properties:\n\nIf A is positive definite, then L is unique and has positive diagonal entries.\nThe Cholesky decomposition can be used to solve linear systems of equations of the form Ax = b by first solving Ly = b for y, and then solving L^Tx = y.\nThe Cholesky decomposition can be used to generate samples from a multivariate normal distribution by transforming samples from a standard normal distribution using the Cholesky factor L.\n\nThe Cholesky decomposition is widely used in various applications, including numerical optimization, Bayesian inference, and machine learning. It is particularly useful for solving linear systems efficiently and for generating samples from multivariate normal distributions.",
    "crumbs": [
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A14.html#sec-kron-hadamard",
    "href": "A14.html#sec-kron-hadamard",
    "title": "116  Moore-Penrose Inversion & Cholesky Decomposition",
    "section": "116.5 Kronecker Product and Hadamard Product",
    "text": "116.5 Kronecker Product and Hadamard Product\nThe Kronecker product and Hadamard product are two important operations in linear algebra that are used to manipulate matrices in various ways. These operations are particularly useful in applications such as signal processing, image processing, and machine learning, where they can be used as short hand notation for certain matrix operations used in many algorithms.\nThe Kronecker product is a matrix operation that takes two matrices and produces a block matrix by multiplying each element of the first matrix by the entire second matrix.\nThe Hadamard product, on the other hand, is an element-wise multiplication of two matrices of the same dimensions.\n\nDefinition 116.4 (Definition of the Kronecker Product) The Kronecker product of two matrices A and B, denoted by A \\otimes B, is defined as the block matrix formed by multiplying each element of A by the entire matrix B. If A is an m \\times n matrix and B is a p \\times q matrix, then the Kronecker product A \\otimes B is an (mp) \\times (nq) matrix given by:\n\nA \\otimes B = \\begin{bmatrix}\na_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\\na_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1}B & a_{m2}B & \\cdots & a_{mn}B\n\\end{bmatrix}\n\\tag{116.12}\nwhere a_{ij} are the elements of matrix A.\n\nsome properties of the Kronecker product include:\n\nTheorem 116.4 (Properties of the Kronecker Product) Let A be an m \\times n matrix and B be a p \\times q matrix. Then:\n\na \\otimes A = A\\otimes a for any scalar a (scalar multiplication property)\n(aA) \\otimes (bB) = ab(A \\otimes B) for any scalars a and b (scalar multiplication property)\n(A \\otimes B) \\otimes C =  A \\otimes (B\\otimes C) (associative property)\n(A+B) \\otimes C = (A \\otimes C) + (B \\otimes C) when A,B,C are matrices of the same size (distributive property)\nA \\otimes (B+C) = (A \\otimes B) + (A \\otimes C) when A,B,C are matrices of the same size (distributive property)\n(A \\otimes B)^{\\prime} = A^{\\prime} \\otimes B^{\\prime} (transpose property)\n(\\mathbf{a} \\otimes \\mathbf{b})^{\\prime} = \\mathbf{a}^{\\prime} \\otimes \\mathbf{b}^{\\prime} (commutativity property for vectors)\n\\text{det}(A \\otimes B) = \\text{det}(A)^{p} \\cdot \\text{det}(B)^{m} (determinant property)\n\\text{rank}(A \\otimes B) = \\text{rank}(A) \\cdot \\text{rank}(B) (rank property)\n(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}, if both A and B are invertible (inverse property)\n(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD) (multiplication property) ;. \\text{tr}(A \\otimes B) = \\text{tr}(A) \\cdot \\text{tr}(B) (trace property)",
    "crumbs": [
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>Moore-Penrose Inversion & Cholesky Decomposition</span>"
    ]
  },
  {
    "objectID": "A15.html",
    "href": "A15.html",
    "title": "117  Appendix: Inequalities",
    "section": "",
    "text": "117.1 Markov’s inequality\nMarkov’s inequality is a foundational result in measure theory and probability theory that provides an upper bound on the probability that a non-negative random variable exceeds a certain threshold. It is particularly useful for establishing the existence of moments and for proving other inequalities.",
    "crumbs": [
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-markov-inequality",
    "href": "A15.html#sec-markov-inequality",
    "title": "117  Appendix: Inequalities",
    "section": "",
    "text": "Theorem 117.1 (Markov’s Inequality) Let X be a non-negative random variable and let a &gt; 0. Then,\n\n\\Pr(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}.",
    "crumbs": [
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-chebyshev-inequality",
    "href": "A15.html#sec-chebyshev-inequality",
    "title": "117  Appendix: Inequalities",
    "section": "117.2 Chebyshev’s inequality",
    "text": "117.2 Chebyshev’s inequality\nChebyshev’s inequality is a powerful tool in probability theory that provides an upper bound on the probability that a random variable deviates from its mean. It is particularly useful for establishing the concentration of measure and for proving other inequalities.\n\nTheorem 117.2 (Chebyshev’s Inequality) Let X be a random variable with mean \\mu = \\mathbb{E}[X] and variance \\sigma^2 = \\operatorname{Var}(X). Then for any k &gt; 0, \n\\Pr(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}.\n\n\n\nTheorem 117.3 (Measure-theoretic Chebyshev’s Inequality) Let (\\Omega, \\mathcal{F}, \\mathbb{P}) be a probability space and let X be a random variable measurable with respect to \\mathcal{F}. If \\mu = \\mathbb{E}[X] and \\sigma^2 = \\operatorname{Var}(X), then for any k &gt; 0, \n\\Pr(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}.",
    "crumbs": [
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-cantellis-inequality",
    "href": "A15.html#sec-cantellis-inequality",
    "title": "117  Appendix: Inequalities",
    "section": "117.3 Cantelli’s inequality",
    "text": "117.3 Cantelli’s inequality\nCantelli’s inequality is a refinement of Chebyshev’s inequality that provides a one-sided bound on the probability that a random variable deviates from its mean. It is particularly useful in statistical inference and hypothesis testing.\n\nTheorem 117.4 (Cantelli’s Inequality) Let X be a random variable with mean \\mu = \\mathbb{E}[X] and variance \\sigma^2 = \\operatorname{Var}(X). Then for any k &gt; 0, \n\\Pr(X - \\mu \\geq k\\sigma) \\leq \\frac{1}{1 + k^2}.",
    "crumbs": [
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-bhattacharyyas-inequality",
    "href": "A15.html#sec-bhattacharyyas-inequality",
    "title": "117  Appendix: Inequalities",
    "section": "117.4 Bhattacharyya’s inequality",
    "text": "117.4 Bhattacharyya’s inequality\nBhattacharyya’s inequality is a refinement of Cantelli’s inequality that provides a two-sided bound on the probability that a random variable deviates from its mean. It is particularly useful in statistical inference and hypothesis testing.\nThe neat idea is that it uses the third and fourth moments of the distribution to do this.\n\nTheorem 117.5 (Bhattacharyya’s Inequality) Let X be a random variable with mean \\mu = \\mathbb{E}[X] and variance \\sigma^2 = \\operatorname{Var}(X). Then for any k &gt; 0, \n\\Pr(|X - \\mu| \\geq k\\sigma) \\leq \\frac{2}{1 + k^2}.",
    "crumbs": [
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A15.html#sec-kolmogorov-inequality",
    "href": "A15.html#sec-kolmogorov-inequality",
    "title": "117  Appendix: Inequalities",
    "section": "117.5 Kolmogorov’s inequality",
    "text": "117.5 Kolmogorov’s inequality\nKolmogorov’s inequality is a fundamental result in probability theory that provides an upper bound on the probability of the maximum absolute value of a sum of independent random variables exceeding a certain threshold. It is particularly useful in the context of stochastic processes and random walks.\n\nIt can be used to prove the weak law of large numbers.\nIt can be used like the empirical rule but for a broad class of distributions. Stating that at least 75% of the values lie within two standard deviations of the mean and at least 89% of the values lie within three standard deviations of the mean.\n\n\nTheorem 117.6 (Kolmogorov’s Inequality) Let X_1, X_2, \\ldots, X_n be a sequence of independent random variables with zero expectation and finite variances. Then for any \\lambda \\geq 0,\n\n\\Pr \\left(\\max _{1\\leq k\\leq n}|S_{k}|\\geq \\lambda \\right)\\leq {\\frac {1}{\\lambda ^{2}}}\\operatorname {Var} [S_{n}]\\equiv {\\frac {1}{\\lambda ^{2}}}\\sum _{k=1}^{n}\\operatorname {Var} [X_{k}]={\\frac {1}{\\lambda ^{2}}}\\sum _{k=1}^{n}{\\text{E}}[X_{k}^{2}],\n\\tag{117.1}\nwhere S_k = X_1 + X_2 + \\ldots + X_k is the partial sum of the first k random variables.",
    "crumbs": [
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>Appendix: Inequalities</span>"
    ]
  },
  {
    "objectID": "A16.html",
    "href": "A16.html",
    "title": "118  Appendix: Wold’s theorem",
    "section": "",
    "text": "118.1 Wold’s theorem - (extra curricular) circa 1939\nWhenever we talk about innovation in the TS course we are alluding to Wold’s theorem, which is a fundamental result in time series analysis that provides a representation of stationary time series as a sum of deterministic and stochastic components.\nI researched this appendix to understand the Moving Average (MA) representation of the AR(p) process, which is a key result in this course. I think that this short appendix help provide a historical perspective on the development of time series analysis and helps other students put this and some other more esoteric concepts within a historical rather than seeing them in a purely mathematical context.\nI found that this approach helped me to understand and remember many results in a number of fields like geometry Ostermann and Wanner (2012), analysis Hairer and Wanner (2008), Ebbinghaus and Ewing (1991), complex analysis Remmert and Burckel (2012) integration Bressoud (2008) and algebra Golan (2012)\nIn the 1920s George Udny Yule and Soviet economists Eugen Slutsky were researching time series and they came up with two different ways to represent a time series.\nwe can use the two schemes together and get the ARMA(p,q) model:\n\\begin{aligned}\nY_{t} & = \\sum _{j=1}^{p} \\phi _{j} Y_{t-j} + u_{t} + \\sum _{j=0}^{q} \\theta _{j} u_{t-j}\n\\end{aligned}\n\\tag{118.3}\nwhere:\nThe following is due in part to Wikipedia contributors (2025)\nWold’s decomposition AKA called the Wold representation theorem states that:\nFormally:\n\\begin{aligned}\nY_{t} & =\\sum _{j=0}^{\\infty }  \\underbrace{b_{j}\\varepsilon _{t-j}}_{\\text{stochastic}} + \\underbrace{\\eta _{t}}_{\\text{deterministic}} \\\\\n&= \\sum _{j=0}^{\\infty } b_{j}\\varepsilon _{t-j} + \\phi_{j} y_{t-j}\n\\end{aligned}\nwhere:\nThe moving average coefficients have these properties:\nAny stationary process has this seemingly special representation. Not only is the existence of such a simple linear and exact representation remarkable, but even more so is the special nature of the moving average model.\nThis result is used without stating its name in the course when we are show the AR(p) representation in terms of moving averages.",
    "crumbs": [
      "<span class='chapter-number'>118</span>  <span class='chapter-title'>Appendix: Wold's theorem</span>"
    ]
  },
  {
    "objectID": "A16.html#wolds-theorem---extra-curricular-circa-1939",
    "href": "A16.html#wolds-theorem---extra-curricular-circa-1939",
    "title": "118  Appendix: Wold’s theorem",
    "section": "",
    "text": "Yule’s researches led to the notion of the autoregressive scheme. \n\\begin{aligned}\nY_{t} & = \\sum _{j=1}^{p} \\phi _{j} Y_{t-j} + u_{t}\n\\end{aligned}\n\\tag{118.1}\nSlutsky’s researches led to the notion of a moving average scheme. \n\\begin{aligned}\nY_{t} & =\\sum _{j=0}^{q} \\theta _{j} u_{t-j}\n\\end{aligned}\n\\tag{118.2}\n\n\n\n\n\n\n\nEvery covariance-stationary time series Y_{t} can be written as the sum of two time series, one deterministic and one stochastic.\n\n\n\n\n\n{Y_{t}} is the time series being considered,\n{\\varepsilon _{t}} is an white noise sequence called innovation process that acts as an input to the linear filter {\\{b_{j}\\}}.\n{b} is the possibly infinite vector of moving average weights (coefficients or parameters)\n{\\eta _{t}} is a “deterministic” time series, in the sense that it is completely determined as a linear combination of its past values It may include “deterministic terms” like sine/cosine waves of {t}, but it is a stochastic process and it is also covariance-stationary, it cannot be an arbitrary deterministic process that violates stationarity.\n\n\n\nStable, that is, square summable \\sum _{j=1}^{\\infty } \\mid b_{j}|^{2} &lt; \\infty\nCausal (i.e. there are no terms with j &lt; 0)\nMinimum delay\nConstant (b_j independent of t)\nIt is conventional to define b_0=1\n\n\n\n\n\n\n\n\n\nBressoud, D. M. 2008. A Radical Approach to Lebesgue’s Theory of Integration. Classroom Resource Materials. Cambridge University Press. https://books.google.co.il/books?id=TxxMoGjXC-wC.\n\n\nEbbinghaus, H. D., and J. H. Ewing. 1991. Numbers. Graduate Texts in Mathematics. Springer New York. https://books.google.co.il/books?id=OKcKowxXwKkC.\n\n\nGolan, J. S. 2012. The Linear Algebra a Beginning Graduate Student Ought to Know. Mathematics and Statistics. Springer Netherlands. https://books.google.co.il/books?id=inwjR-k1dlkC.\n\n\nHairer, E., and G. Wanner. 2008. Analysis by Its History. Undergraduate Texts in Mathematics. Springer New York. https://books.google.co.il/books?id=ncxGAAAAQBAJ.\n\n\nOstermann, A., and G. Wanner. 2012. Geometry by Its History. Undergraduate Texts in Mathematics. Springer Berlin Heidelberg. https://books.google.co.il/books?id=eOSqPHwWJX8C.\n\n\nRemmert, R., and R. B. Burckel. 2012. Theory of Complex Functions. Graduate Texts in Mathematics. Springer New York. https://books.google.co.il/books?id=kIHlBwAAQBAJ.\n\n\nWikipedia contributors. 2025. “Wold’s Theorem — Wikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Wold%27s_theorem&oldid=1295347901.",
    "crumbs": [
      "<span class='chapter-number'>118</span>  <span class='chapter-title'>Appendix: Wold's theorem</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "119  References",
    "section": "",
    "text": "119.1 Bibliography",
    "crumbs": [
      "<span class='chapter-number'>119</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "references.html#bibliography",
    "href": "references.html#bibliography",
    "title": "119  References",
    "section": "",
    "text": "Agresti, A. 2012. Categorical Data Analysis. Wiley Series in\nProbability and Statistics. Wiley. https://books.google.co.il/books?id=UOrr47-2oisC.\n\n\nAldrich, John. 2008. “R. A. Fisher on Bayes and Bayes’\nTheorem.” Bayesian Analysis 3 (March). https://doi.org/10.1214/08-BA306.\n\n\nAutolatry. 2015. “Why Square Brackets for Expectation.”\nMathematics Stack Exchange. https://math.stackexchange.com/q/1302543.\n\n\nBanerjee, S., A. E. Gelfand, and B. P. Carlin. 2026. Hierarchical\nModeling and Analysis for Spatial Data. CRC Press LLC. https://books.google.co.il/books?id=GRFT0QEACAAJ.\n\n\nBayesian Nonparametrics. 2010. Cambridge Series in Statistical\nand Probabilistic Mathematics. Cambridge University Press.\n\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. 1980. Regression\nDiagnostics. John Wiley & Sons, Inc. https://doi.org/10.1002/0471725153.\n\n\nBernoulli, J. 1713. Ars Conjectandi [the Art of Conjecturing].\nImpensis Thurnisiorum. https://books.google.co.il/books?id=Ba5DAAAAcAAJ.\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning.\nInformation Science and Statistics. Springer (India) Private Limited. https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf.\n\n\nBlundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan\nWierstra. 2015. “Weight Uncertainty in Neural Networks.” https://doi.org/10.48550/ARXIV.1505.05424.\n\n\nBressoud, D. M. 2008. A Radical Approach to Lebesgue’s Theory of\nIntegration. Classroom Resource Materials. Cambridge University\nPress. https://books.google.co.il/books?id=TxxMoGjXC-wC.\n\n\nBrockwell, Peter J, and Richard A Davis. 1991. Time Series: Theory\nand Methods. Springer science & business media.\n\n\nBroemeling, Lyle D. 2019. Bayesian Analysis of Time Series. CRC\nPress.\n\n\nCarlin, B. P., and T. A. Louis. 2008. Bayesian Methods for Data\nAnalysis. Chapman & Hall/CRC Texts in Statistical Science. CRC\nPress. https://books.google.co.il/books?id=GTJUt8fcFx8C.\n\n\nCasella, G., and R. L. Berger. 2002. Statistical Inference.\nDuxbury Advanced Series in Statistics and Decision Sciences. Thomson\nLearning. http://home.ustc.edu.cn/~zt001062/MaStmaterials/George%20Casella&Roger%20L.Berger--Statistical%20Inference.pdf.\n\n\nChen, J. 2023. Statistical Inference Under Mixture Models. ICSA\nBook Series in Statistics. Springer Nature Singapore. https://books.google.co.il/books?id=sBXlEAAAQBAJ.\n\n\nCook, R. Dennis. 1977. “Detection of Influential Observation in\nLinear Regression.” Technometrics 19 (1): 15. https://doi.org/10.2307/1268249.\n\n\nCowpertwait, P. S. P., and A. V. Metcalfe. 2009. Introductory Time\nSeries with r. Use r! Springer New York. https://books.google.co.il/books?id=QFiZGQmvRUQC.\n\n\nCressie, N., and C. K. Wikle. 2011. Statistics for Spatio-Temporal\nData. CourseSmart Series. Wiley. https://books.google.co.il/books?id=-kOC6D0DiNYC.\n\n\nDavidson-Pilon, Cameron. 2015. Bayesian Methods for Hackers:\nProbabilistic Programming and Bayesian Inference. Addison-Wesley\nData & Analytics Series. Pearson Education. https://books.google.co.il/books?id=rMKiCgAAQBAJ.\n\n\nDempster, Arthur P, Nan M Laird, and Donald B Rubin. 1977.\n“Maximum Likelihood from Incomplete Data via the EM\nAlgorithm.” Journal of the Royal Statistical Society: Series\nB (Methodological) 39 (1): 1–22.\n\n\nDurbin, J. 1960. “The Fitting of Time-Series Models.”\nRevue de l’Institut International de Statistique / Review of the\nInternational Statistical Institute 28 (3): 233–44. http://www.jstor.org/stable/1401322.\n\n\nEbbinghaus, H. D., and J. H. Ewing. 1991. Numbers. Graduate\nTexts in Mathematics. Springer New York. https://books.google.co.il/books?id=OKcKowxXwKkC.\n\n\nFinetti, Bruno de. 1937. “La Prévision: Ses Lois Logiques, Ses\nSources Subjectives.” Annales de l’Institut Henri\nPoincaré 7 (1): 1–68.\n\n\n———. 2017. “Theory of Probability.” Edited by Antonio Machí\nand Adrian Smith. Wiley Series in Probability and Statistics,\nJanuary. https://doi.org/10.1002/9781119286387.\n\n\nFisher, R. A. 1925. Statistical Methods for Research Workers.\n1st ed. Edinburgh Oliver & Boyd.\n\n\nFruhwirth-Schnatter, S., G. Celeux, and C. P. Robert. 2019. Handbook\nof Mixture Analysis. Chapman & Hall/CRC Handbooks of Modern\nStatistical Methods. CRC Press. https://books.google.co.il/books?id=N3yCDwAAQBAJ.\n\n\nFrühwirth-Schnatter, S. 2006. Finite Mixture and Markov Switching\nModels. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=f8KiI7eRjYoC.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third\nEdition. Chapman & Hall/CRC Texts in Statistical Science.\nTaylor & Francis.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Analytical Methods\nfor Social Research. Cambridge University Press. https://books.google.co.il/books?id=c9xLKzZWoZ4C.\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su.\n2008. “A Weakly Informative Default Prior Distribution for\nLogistic and Other Regression Models.” The Annals of Applied\nStatistics 2 (4). https://doi.org/10.1214/08-aoas191.\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation,\nGibbs Distributions, and the Bayesian Restoration of Images.”\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nPAMI-6 (6): 721–41. https://doi.org/10.1109/tpami.1984.4767596.\n\n\nGhosh, Joyee, Yingbo Li, and Robin Mitra. 2018. “On the Use of\nCauchy Prior Distributions for Bayesian Logistic Regression.”\nBayesian Analysis 13 (2). https://doi.org/10.1214/17-ba1051.\n\n\nGolan, J. S. 2012. The Linear Algebra a Beginning Graduate Student\nOught to Know. Mathematics and Statistics. Springer Netherlands. https://books.google.co.il/books?id=inwjR-k1dlkC.\n\n\nHairer, E., and G. Wanner. 2008. Analysis by Its History.\nUndergraduate Texts in Mathematics. Springer New York. https://books.google.co.il/books?id=ncxGAAAAQBAJ.\n\n\nHärdle, Wolfgang Karl, and Léopold Simar. 2019. Applied Multivariate\nStatistical Analysis. Springer International Publishing. https://doi.org/10.1007/978-3-030-26006-4.\n\n\nHobbs, N. Thompson, and Mevin B. Hooten. 2015. Bayesian Models: A\nStatistical Primer for Ecologists. STU - Student edition. Princeton\nUniversity Press. http://www.jstor.org/stable/j.ctt1dr36kz.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical\nMethods. Springer New York. https://doi.org/10.1007/978-0-387-92407-6.\n\n\nJackman, Simon. 2009. “Bayesian Analysis for the Social\nSciences.” Wiley Series in Probability and Statistics,\nOctober. https://doi.org/10.1002/9780470686621.\n\n\nJames, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An\nIntroduction to Statistical Learning: With Applications in r.\nSpringer Texts in Statistics. Springer New York. https://books.google.co.il/books?id=qcI_AAAAQBAJ.\n\n\nJeffreys, H. 1983. Theory of Probability. International Series\nof Monographs on Physics. Clarendon Press.\n\n\nJohnson, R. A., and D. W. Wichern. 2001. Applied Multivariate\nStatistical Analysis. Pearson Modern Classics for Advanced\nStatistics Series. Prentice Hall. https://books.google.co.il/books?id=QBqlswEACAAJ.\n\n\nKruschke, John K. 2011. Doing Bayesian Data Analysis: A Tutorial\nwith R and BUGS. Burlington, MA: Academic\nPress. http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855.\n\n\nLevinson, Norman. 1946. “The Wiener (Root Mean Square) Error\nCriterion in Filter Design and Prediction.” Journal of\nMathematics and Physics 25 (1-4): 261–78. https://doi.org/https://doi.org/10.1002/sapm1946251261.\n\n\nLunn, D., C. Jackson, N. Best, A. Thomas, and D. Spiegelhalter. 2012.\nThe BUGS Book: A Practical Introduction to Bayesian Analysis.\nChapman & Hall/CRC Texts in Statistical Science. Taylor &\nFrancis. https://books.google.co.il/books?id=Cthz3XMa_VQC.\n\n\nMartin, Osvaldo A., Ravin Kumar, and Junpeng Lao. 2021. Bayesian Modeling and Computation in Python.\nBoca Raton.\n\n\nMcElreath, Richard. 2015. Statistical Rethinking, a Course in r and\nStan.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with\nPandas, NumPy, and Jupyter. 3rd ed. O’Reilly Media.\n\n\nMcLachlan, G. J., and D. Peel. 2004. Finite Mixture Models.\nWiley Series in Probability and Statistics. Wiley. https://books.google.co.il/books?id=c2_fAox0DQoC.\n\n\nMoivre, Abraham De. 1718. The Doctrine of Chances. H. Woodfall.\nhttps://tellingstorieswithdata.com.\n\n\nMorita, Satoshi, Peter F Thall, and Peter Müller. 2008.\n“Determining the Effective Sample Size of a Parametric\nPrior.” Biometrics 64 (2): 595–602.\n\n\nNielsen, A. 2019. Practical Time Series Analysis: Prediction with\nStatistics and Machine Learning. O’Reilly Media. https://books.google.co.il/books?id=xNOwDwAAQBAJ.\n\n\nOstermann, A., and G. Wanner. 2012. Geometry by Its History.\nUndergraduate Texts in Mathematics. Springer Berlin Heidelberg. https://books.google.co.il/books?id=eOSqPHwWJX8C.\n\n\nPearson, E. S., W. S. Gosset, R. L. Plackett, and G. A. Barnard. 1990.\nStudent: A Statistical Biography of William Sealy Gosset.\nClarendon Press. https://books.google.co.il/books?id=LBDvAAAAMAAJ.\n\n\nPfaff, B. 2008. Analysis of Integrated and Cointegrated Time Series\nwith r. Use r! Springer New York. https://books.google.co.il/books?id=ca5MkRbF3fYC.\n\n\nPoisson, S. -D. 2019. “English Translation of Poisson’s\n\"Recherches Sur La Probabilité Des Jugements En Matière Criminelle Et En\nMatière Civile\" / \"Researches into the Probabilities of Judgements in\nCriminal and Civil Cases\".” https://arxiv.org/abs/1902.02782.\n\n\nPolya, G. 1945. How to Solve It. Princeton University Press. https://doi.org/10.1515/9781400828678.\n\n\nPrado, Raquel, Gabriel Huerta, and Mike West. 2000. “Bayesian\nTime-Varying Autoregressions: Theory, Methods and Applications.”\nResenhas Do Instituto de Matemática e\nEstatı́stica Da Universidade de São Paulo\n4 (4): 405–22. https://www2.stat.duke.edu/~mw/MWextrapubs/Prado2001.pdf.\n\n\nPrado, R., M. A. R. Ferreira, and M. West. 2023. Time Series:\nModeling, Computation, and Inference. Chapman & Hall/CRC Texts\nin Statistical Science. CRC Press. https://books.google.co.il/books?id=pZ6lzgEACAAJ.\n\n\nRamsey, Frank P. 1926. “Truth and Probability.” In The\nFoundations of Mathematics and Other Logical Essays, edited by R.\nB. Braithwaite, 156–98. McMaster University Archive for the History of\nEconomic Thought. https://EconPapers.repec.org/RePEc:hay:hetcha:ramsey1926.\n\n\nRavishanker, N., B. Raman, and R. Soyer. 2022. Dynamic Time Series\nModels Using r-INLA: An Applied Perspective. CRC Press.\n\n\nRemmert, R., and R. B. Burckel. 2012. Theory of Complex\nFunctions. Graduate Texts in Mathematics. Springer New York. https://books.google.co.il/books?id=kIHlBwAAQBAJ.\n\n\nRios Insua, David, Fabrizio Ruggeri, and Michael P Wiper. 2012.\nBayesian Analysis of Stochastic Process Models. John Wiley\n& Sons.\n\n\nSchott, James R. 2016. Matrix Analysis for Statistics. Wiley\nSeries in Probability and Statistics. Wiley. https://books.google.co.il/books?id=Y2PpCgAAQBAJ.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with r.\nSpringer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nSpanos, A. 2019. Probability Theory and Statistical Inference.\nCambridge University Press. https://books.google.co.il/books?id=9nCiDwAAQBAJ.\n\n\nSpiegelhalter, David J., Nicola G. Best, Bradley P. Carlin, and Angelika\nVan Der Linde. 2002. “Bayesian Measures of Model Complexity and\nFit.” Journal of the Royal Statistical Society Series B:\nStatistical Methodology 64 (4): 583–639. https://doi.org/10.1111/1467-9868.00353.\n\n\nStorch, H. von, and F. W. Zwiers. 2002. Statistical Analysis in\nClimate Research. Cambridge University Press.\n\n\nTheodoridis, S. 2015. Machine Learning: A Bayesian and Optimization\nPerspective. Elsevier Science.\n\n\nTrench, William F. 1964. “An Algorithm for the Inversion of Finite\nToeplitz Matrices.” Journal of the Society for Industrial and\nApplied Mathematics 12 (3): 515–22. http://ramanujan.math.trinity.edu/wtrench/research/papers/TRENCH_RP_6.PDF.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential\nTools for Working with Data. 1st ed. O’Reilly Media, Inc. https://jakevdp.github.io/PythonDataScienceHandbook/.\n\n\nVisser, I., and M. Speekenbrink. 2022. Mixture and Hidden Markov\nModels with r. Use r! Springer International Publishing. https://books.google.co.il/books?id=Eep3EAAAQBAJ.\n\n\nWalker, Gilbert Thomas. 1931. “On Periodicity in Series of Related\nTerms.” Proceedings of the Royal Society of London. Series A,\nContaining Papers of a Mathematical and Physical Character 131\n(818): 518–32. https://doi.org/10.1098/rspa.1931.0069.\n\n\nWeckerle, Melissa. 2022. “Statistics\nprofessor wins prestigious professional statistics society award\n Baskin School of Engineering.” https://engineering.ucsc.edu/news/statistics-professor-wins-zellner-medal.\n\n\nWest, M., and J. Harrison. 2013. Bayesian Forecasting and Dynamic\nModels. Springer Series in Statistics. Springer New York. https://books.google.co.il/books?id=NmfaBwAAQBAJ.\n\n\nWiesenfarth, Manuel, and Silvia Calderazzo. 2020. “Quantification\nof Prior Impact in Terms of Effective Current Sample Size.”\nBiometrics 76 (1): 326–36. https://doi.org/https://doi.org/10.1111/biom.13124.\n\n\nWikipedia contributors. 2023a. “68–95–99.7 Rule —\nWikipedia.” https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule.\n\n\n———. 2023b. “Functional (Mathematics) —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Functional_(mathematics)&oldid=1148699341.\n\n\n———. 2024a. “Autoregressive Model —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Autoregressive_model&oldid=1233171855#Estimation_of_AR_parameters.\n\n\n———. 2024b. “Levinson Recursion —\nWikipedia, the Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Levinson_recursion&oldid=1229942891.\n\n\n———. 2025. “Wold’s Theorem — Wikipedia,\nthe Free Encyclopedia.” https://en.wikipedia.org/w/index.php?title=Wold%27s_theorem&oldid=1295347901.\n\n\nWikle, C. K., A. Zammit-Mangion, and N. Cressie. 2019.\nSpatio-Temporal Statistics with r. Chapman & Hall/CRC the r\nSeries. CRC Press. https://books.google.co.il/books?id=FD-IDwAAQBAJ.\n\n\nWoodward, W. A., B. P. Sadler, and S. Robertson. 2022. Time Series\nfor Data Science: Analysis and Forecasting. Chapman & Hall/CRC\nTexts in Statistical Science. CRC Press. https://books.google.co.il/books?id=_W16EAAAQBAJ.\n\n\nYao, W., and S. Xiang. 2024. Mixture Models: Parametric,\nSemiparametric, and New Directions. Chapman & Hall/CRC\nMonographs on Statistics and Applied Probability. CRC Press. https://books.google.co.il/books?id=scP9EAAAQBAJ.\n\n\nYule, George Udny. 1927. “VII. On a Method of Investigating\nPeriodicities Disturbed Series, with Special Reference to Wolfer’s\nSunspot Numbers.” Philosophical Transactions of the Royal\nSociety of London. Series A, Containing Papers of a Mathematical or\nPhysical Character 226 (636-646): 267–98. https://doi.org/10.1098/rsta.1927.0007.\n\n\nZohar, Shalhav. 1969. “Toeplitz Matrix Inversion: The Algorithm of\nw. F. Trench.” J. ACM 16: 592–601. https://api.semanticscholar.org/CorpusID:3115290.",
    "crumbs": [
      "<span class='chapter-number'>119</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "C1-L10.html",
    "href": "C1-L10.html",
    "title": "24  Normally distributed Data - M4L10",
    "section": "",
    "text": "24.1 Normal Likelihood with known variance\nCharles Zaiontz provides pro types of conjugate priors for normally distributed data:\nIn each case, the unknown refer to population statistics. Since we are able to estimate sample parameters such as the mean and variance quite easily. A key question to consider is how well does our posterior distribution of the parameter representative of the unknown population statistic?\nIdeally, I will update the notes below with proofs of conjugate, prior and posterior and marginal distribution.\nSome of the proofs are in here as well\nNote: this material is also covered in (Hoff 2009, sec. 5.2) Let’s suppose the standard deviation or variance \\sigma^2 is known and we’re only interested in learning about the mean. This is a situation that often arises in monitoring industrial production processes.\nX_i \\stackrel{iid}\\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\tag{24.1}\nIt turns out that the Normal distribution is conjugate for itself when looking for the mean parameter\nPrior\n\\mu \\sim \\mathcal{N}(m_0,S_0^2)\n\\tag{24.2}\nBy Bayes rule:\nf(\\mu \\mid x ) \\propto f(x \\mid \\mu)f(\\mu)\n\\mu \\mid x \\sim \\mathcal{N} \\left (\\frac{\\frac{n\\bar{x}}{\\sigma_0^2} + \\frac{m_0}{s_0^2} }{\\frac{n}{\\sigma_0^2} +\\frac{1}{s_0^2}}, \\frac{1}{\\frac{n}{\\sigma_0^2} + \\frac{1}{s_0^2}}\\right )\n\\tag{24.3}\nwhere:\nLet’s look at the posterior mean\n\\begin{aligned}\nposterior_{\\mu} &= \\frac{\n          \\frac{n}{\\sigma_0^2}}\n       {\\frac{n}{\\sigma_0^2}s + \\frac{1}{s_0^2}}\\bar{x} +     \n          \\frac{ \\frac{1}{s_0^2} }{ \\frac{n}{\\sigma_0^2} + \\frac{1}{s_0^2}\n        }m \\\\\n&= \\frac{n}{n + \\frac{\\sigma_0^2}{s_0^2} }\\bar{x} + \\frac{ \\frac{\\sigma_0^2}{s_0^2} }{n + \\frac{\\sigma_0^2}{s_0^2}}m\n\\end{aligned}\n\\tag{24.4}\nThus we see, that the posterior mean is a weighted average of the prior mean and the data mean. And indeed that the effective sample size for this prior is the ratio of the variance for the data to the variance in the prior.\nPrior\\ ESS= \\frac{\\sigma_0^2}{s_0^2}\n\\tag{24.5}\nThis makes sense, because the larger the variance of the prior, the less information that’s in it.\nThe marginal distribution for Y is\n\\mathcal{N}(m_0, s_0^2 + \\sigma^2)\n\\tag{24.6}",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normally distributed Data - M4L10</span>"
    ]
  },
  {
    "objectID": "C1-L10.html#sec-normal-likelihood-with-unknown-mean",
    "href": "C1-L10.html#sec-normal-likelihood-with-unknown-mean",
    "title": "24  Normally distributed Data - M4L10",
    "section": "",
    "text": "Figure 24.1: Normal likelihood with variance known\n\n\n\n\n\n\n\n\n\n\n\n\nn is the sample size\n\\bar{x}=\\frac{1}{n}\\sum x_i is the sample mean\n\\sigma =\\frac{1}{n} \\sum (x_i-\\bar{x})^2 is the sample variance\nindexing parameters with 0 seems to be a convention that they are from the prior:\ns_0 is the prior variance\nm_0 is the prior mean\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nshould we use n-1 in the sample variance?\n\n\n\n\n\n\n\n\n24.1.1 Prior (and posterior) predictive distribution\nThe prior (and posterior) predictive distribution for data is particularly simple in the conjugate normal model .\nIf \ny \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^2)\n and \n\\theta \\sim \\mathcal{N}(m, s_0^2)\n\nthen the marginal distribution for Y, obtained as\n\n\\int f(y,\\theta) d\\theta = \\mathcal{N}(m_0,s_0^2)\n\\tag{24.7}\n\nExample 24.1 Suppose your data are normally distributed with \\mu=\\theta and \\sigma^2=1.\n\ny \\mid \\theta \\sim \\mathcal{N}(\\theta,1)\n\nYou select a normal prior for \\theta with mean 0 and variance 2.\n\n\\theta \\sim \\mathcal{N}(0, 2)\n\nThen the prior predictive distribution for one data point would be N(0, a). What is the value of a?\nSince, m_0 =0, and s^2_0=2 and \\sigma^2=1, the predictive distribution is N(0,2+1).",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normally distributed Data - M4L10</span>"
    ]
  },
  {
    "objectID": "C1-L10.html#sec-normal-likelihood-with-expectation-and-variance-unknown",
    "href": "C1-L10.html#sec-normal-likelihood-with-expectation-and-variance-unknown",
    "title": "24  Normally distributed Data - M4L10",
    "section": "24.2 Normal likelihood with expectation and variance unknown",
    "text": "24.2 Normal likelihood with expectation and variance unknown\n\n\n\n\n\n\n\nFigure 24.2: Normal likelihood with a unknown variance\n\n\n\n\n\n\n\n\nTipChallenging\n\n\n\nThis section is challenging.\n\nThe updating derivation is skipped,\nthe posterior\nupdating rule values are introduced without motivations and explanation.\nThe model is also the most complicated in the course, the note at the end says this can be extended hierarchically if we want to specify hyper priors for m, w and \\beta\nOther text discuss this case using a inverse chi squared distribution\n\nIf we can understand the model the homework is going to make sense. Also this is probably the level needed for the other courses in the specialization.\nIt can help to review some of the books:\n\nSee (Hoff 2009, sec. 5.3) which has some R examples.\nSee (Gelman et al. 2013, sec. 5)\n\n\n\n If both \\mu and \\sigma^2 are unknown, we can specify a conjugate prior in a hierarchical fashion.\n\nX_i \\mid \\mu, \\sigma^2 \\stackrel{iid}\\sim \\mathcal{N}(\\mu, \\sigma^2) \\qquad \\text{(the data given the params) }\n\n\nThis is the level 1 hierarchically model - X_i model our observations.\nWe state on the left, that the RV X is conditioned on the \\mu and \\sigma^2.\nBut the variables \\mu and \\sigma^2 are unknown population statistics which we will need to infer from the data. We can call them latent variables.\n\nNext we add a prior from \\mu conditional on the value for \\sigma^2\n\n\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(m, \\frac{\\sigma^2}{w}) \\qquad \\text{(prior of the mean conditioned on the variance)}\n\nwhere:\n\nw is going to be the ratio of \\sigma^2 and some variance for the Normal distribution. This is the effective sample size of the prior.\nWhy is the mean conditioned on the variance. We can have a model where they are independent too?\nlater on (in the homework) we are told that w can express the confidence in the prior.\nI think this means that Since this is a knowledge of m, i.e. giving w a weight of 1/10 expresses that we value\n\nPerhaps this is due to Central Limit Theorem ?\nThis is level 2 of the model\n\nFinally, the last step is to specify a prior for \\sigma^2. The conjugate prior here is an inverse gamma distribution with parameters \\alpha and \\beta.\n\n\\sigma^2 \\sim \\mathrm{Gamma}^{-1}(\\alpha, \\beta)  \\qquad \\text{prior of the variance}\n\nAfter many calculations… we get the posterior distribution\n\n\\sigma^2 \\mid x \\sim \\mathrm{Gamma}^{-1}(\\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2}\\sum_{i = 1}^n{(x-\\bar{x}^2 + \\frac{nw}{2(n+2)}(\\bar{x} - m)^2)})\n\\tag{24.8}\n\n\\mu \\mid \\sigma^2,x \\sim \\mathcal{N}(\\frac{n\\bar{x}+wm}{n+w}, \\frac{\\sigma^2}{n + w})\n\\tag{24.9}\nWhere the posterior mean can be written as the weighted average of the prior mean and the data mean.\n\n\\frac{n\\bar{x}+wm}{n+w} = \\frac{w}{n + w}m + \\frac{n}{n + w}\\bar{x} \\qquad \\text{post. mean}\n\\tag{24.10}\nIn some cases, we only care about \\mu. We want some inference on \\mu and we may want it such that it does not depend on \\sigma^2. We can marginalize that \\sigma^2 integrating it out. The posterior for \\mu marginally follows a t distribution.\n\n\\mu \\mid x \\sim t\n\nSimilarly, the posterior predictive distribution also is a t distribution.\nFinally, note that we can extend this in various directions, this can be extended to the multivariate normal case that requires matrix vector notations and can be extended hierarchically if we want to specify priors for m, w, \\beta\n\n\n\n\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis.\n\n\nHoff, Peter D. 2009. A First Course in Bayesian Statistical Methods. Springer New York. https://doi.org/10.1007/978-0-387-92407-6.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Normally distributed Data - M4L10</span>"
    ]
  },
  {
    "objectID": "C5-L03.html",
    "href": "C5-L03.html",
    "title": "105  Bayesian location mixture of AR(p) models - M3L3",
    "section": "",
    "text": "105.1 Prediction for Location Mixture of AR Models 🎥",
    "crumbs": [
      "<span class='chapter-number'>105</span>  <span class='chapter-title'>Bayesian location mixture of AR(p) models - M3L3</span>"
    ]
  },
  {
    "objectID": "C3-L06.html",
    "href": "C3-L06.html",
    "title": "75  Clustering - M4L6",
    "section": "",
    "text": "75.1 Mixture Models for Clustering 🎥\nClustering, or unsupervised classification, aims to partition heterogeneous data into homogeneous groups (clusters). Common in biology and other domains, clustering helps identify underlying structure, such as species based on physiological features.\nA widely-used method is K-means clustering, which:\nThis process iterates until assignments stabilize.\nK-means is closely related to a mixture of Gaussians with:\nIf components are well-separated, EM approximates hard assignments similar to K-means.",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  },
  {
    "objectID": "C3-L06.html#sec-mixture-clustering",
    "href": "C3-L06.html#sec-mixture-clustering",
    "title": "75  Clustering - M4L6",
    "section": "",
    "text": "Figure 75.1: Clustering Mixture Models 1\n\n\n\n\n\n\n\n\nFigure 75.2: Clustering Mixture Models 2\n\n\n\n\n\n\n\n\nFigure 75.3: Clustering Mixture Models 3\n\n\n\n\n\n\n\n\nFigure 75.4: Clustering Mixture Models 4\n\n\n\n\n\n\n\n\nFixes the number of clusters K\nAlternates between:\n\nAssignment step: Assigns each data point to its nearest cluster center.\nUpdate step: Recomputes centers as means of assigned points.\n\n\n\n\n\nEqual component weights \\omega_k = 1/K\nShared spherical covariance \\Sigma_k = \\sigma^2 I\nFitted via the EM algorithm, where:\n\nE-step: compute soft assignment probabilities V_{ik}\nM-step: update cluster means using weighted averages\n\n\n\n\n75.1.1 Limitations of K-means:\n\nAssumes equal-sized spherical clusters\nFails with:\n\nCorrelated features\nDifferent variances per dimension\nUnequal cluster sizes\n\n\n\n\n75.1.2 Advantages of Mixture Models:\n\nAllow flexible covariances \\Sigma_k\nEstimate weights \\omega_k\nCan use alternative kernels (e.g., t-distributions)\nEnable Bayesian clustering via MCMC\n\nThus, viewing K-means as a special case of Gaussian mixture models clarifies its assumptions and guides principled extensions.",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  },
  {
    "objectID": "C3-L06.html#sec-mixture-clustering-example",
    "href": "C3-L06.html#sec-mixture-clustering-example",
    "title": "75  Clustering - M4L6",
    "section": "75.2 Clustering example 🎥",
    "text": "75.2 Clustering example 🎥\nThis video demonstrates clustering using the iris dataset, comparing K-means and a location and scale mixture model with K normals. It highlights how mixture models can flexibly adapt to data structure, unlike K-means’ rigid assumptions. The code is provided in",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  },
  {
    "objectID": "C3-L06.html#sec-clustering-code",
    "href": "C3-L06.html#sec-clustering-code",
    "title": "75  Clustering - M4L6",
    "section": "75.3 Clustering example 📖 ℛ",
    "text": "75.3 Clustering example 📖 ℛ\n\n\n## Using mixture models for clustering in the iris dataset\n## Compare k-means clustering and a location and scale mixture model with K normals\n\n### Loading data and setting up global variables\nrm(list=ls())\nlibrary(mclust)\n\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nlibrary(mvtnorm)\n\n\nAttaching package: 'mvtnorm'\n\n\nThe following object is masked from 'package:mclust':\n\n    dmvnorm\n\n### Defining a custom function to create pair plots\n### This is an alternative to the R function pairs() that allows for \n### more flexibility. In particular, it allows us to use text to label \n### the points\npairs2 = function(x, col=\"black\", pch=16, labels=NULL, names = colnames(x)){\n  n = dim(x)[1]\n  p = dim(x)[2]\n  par(mfrow=c(p,p))\n  for(k in 1:p){\n    for(l in 1:p){\n      if(k!=l){\n        par(mar=c(3,3,1,1)+0.1)\n        plot(x[,k], x[,l], type=\"n\", xlab=\"\", ylab=\"\")\n        if(is.null(labels)){\n          points(x[,k], x[,l], pch=pch, col=col)\n        }else{\n          text(x[,k], x[,l], labels=labels, col=col)\n        }\n      }else{\n        plot(seq(0,5), seq(0,5), type=\"n\", xlab=\"\", ylab=\"\", axes=FALSE)\n        text(2.5,2.5,names[k], cex=1.2)\n      }\n    }\n  }\n}\n\n## Setup data\ndata(iris)\nx       = as.matrix(iris[,-5])\nn       = dim(x)[1]\np       = dim(x)[2]       # Number of features\nKK      = 3\nepsilon = 0.0000001\npar(mfrow=c(1,1))\npar(mar=c(4,4,1,1))\ncolscale = c(\"black\",\"blue\",\"red\")\nshortnam  = c(\"s\",\"c\",\"g\")\npairs2(x, col=colscale[iris[,5]], labels=shortnam[as.numeric(iris[,5])])\n\n\n# Initialize the parameters of the algorithm\nset.seed(63252)\nnumruns = 15\nv.sum   = array(0, dim=c(numruns, n, KK))\nQQ.sum  = rep(0, numruns)\n\nfor(ss in 1:numruns){\n  w   = rep(1,KK)/KK  #Assign equal weight to each component to start with\n  mu  = rmvnorm(KK, apply(x,2,mean), 3*var(x))   #Cluster centers randomly spread over the support of the data\n  Sigma      = array(0, dim=c(KK,p,p))  #Initial variances are assumed to be the same\n  Sigma[1,,] = var(x)\n  Sigma[2,,] = var(x)\n  Sigma[3,,] = var(x)\n  \n  sw     = FALSE\n  QQ     = -Inf\n  QQ.out = NULL\n  s      = 0\n  \n  while(!sw){\n    ## E step\n    v = array(0, dim=c(n,KK))\n    for(k in 1:KK){  #Compute the log of the weights\n      v[,k] = log(w[k]) + mvtnorm::dmvnorm(x, mu[k,], Sigma[k,,], log=TRUE) \n    }\n    for(i in 1:n){\n      v[i,] = exp(v[i,] - max(v[i,]))/sum(exp(v[i,] - max(v[i,])))  #Go from logs to actual weights in a numerically stable manner\n    }\n    \n    ## M step\n    w = apply(v,2,mean)\n    mu = matrix(0, nrow=KK, ncol=p)\n    for(k in 1:KK){\n      for(i in 1:n){\n        mu[k,]    = mu[k,] + v[i,k]*x[i,]\n      }\n      mu[k,] = mu[k,]/sum(v[,k])\n    }\n    Sigma = array(0,dim=c(KK, p, p))\n    for(k in 1:KK){\n      for(i in 1:n){\n        Sigma[k,,] = Sigma[k,,] + v[i,k]*(x[i,] - mu[k,])%*%t(x[i,] - mu[k,])\n      }\n      Sigma[k,,] = Sigma[k,,]/sum(v[,k])\n    }\n    \n    ##Check convergence\n    QQn = 0\n    for(i in 1:n){\n      for(k in 1:KK){\n        QQn = QQn + v[i,k]*(log(w[k]) + mvtnorm::dmvnorm(x[i,],mu[k,],Sigma[k,,],log=TRUE))\n      }\n    }\n    if(abs(QQn-QQ)/abs(QQn)&lt;epsilon){\n      sw=TRUE\n    }\n    QQ = QQn\n    QQ.out = c(QQ.out, QQ)\n    s = s + 1\n  }\n  \n  v.sum[ss,,] = v\n  QQ.sum[ss]  = QQ.out[s]\n  print(paste(\"ss =\", ss))\n}\n\n[1] \"ss = 1\"\n[1] \"ss = 2\"\n[1] \"ss = 3\"\n[1] \"ss = 4\"\n[1] \"ss = 5\"\n[1] \"ss = 6\"\n[1] \"ss = 7\"\n[1] \"ss = 8\"\n[1] \"ss = 9\"\n[1] \"ss = 10\"\n[1] \"ss = 11\"\n[1] \"ss = 12\"\n[1] \"ss = 13\"\n[1] \"ss = 14\"\n[1] \"ss = 15\"\n\n## Cluster reconstruction under the mixture model\ncc = apply(v.sum[which.max(QQ.sum),,], 1 ,which.max)\ncolscale = c(\"black\",\"blue\",\"red\")\npairs2(x, col=colscale[cc], labels=cc)\nARImle = adjustedRandIndex(cc, as.numeric(iris[,5]))  # Higher values indicate larger agreement\n\n## Cluster reconstruction under the K-means algorithm\nirisCluster &lt;- kmeans(x, 3, nstart = numruns)\ncolscale = c(\"black\",\"blue\",\"red\")\npairs2(x, col=colscale[irisCluster$cluster], labels=irisCluster$cluster)\nARIkmeans = adjustedRandIndex(irisCluster$cluster, as.numeric(iris[,5]))\n\n\n\n\n\n\n\nFigure 75.5: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.\n\n\n\n\n\n\n\n\n\n\n\nFigure 75.6: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.\n\n\n\n\n\n\n\n\n\n\n\nFigure 75.7: Clustering the iris dataset using k-means clustering and a location and scale mixture model with K normals.",
    "crumbs": [
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>Clustering - M4L6</span>"
    ]
  }
]