<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">

<title>Mixture Models – Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-37379e3bbb48fc9e4c504b22bdb22879.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-e12ace95bce8268b44b48d9ebb190a16.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-37379e3bbb48fc9e4c504b22bdb22879.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      Bayesian Statistics
      </li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Intro</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Bayesian Statistics</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">1. From Concept to Data Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on paradigms of probability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayes’ Theorem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conditional Probability and Bayes’ Law</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conditional Probability and Bayes’ Law</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Random Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Frequentist Inference</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework - Frequentist MLE</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Inference</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Likelihoods and MLEs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Bayesian Inference</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Priors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework Posterior Probabilities</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Binomial Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Priors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Poisson Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Poisson Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Honors Quiz - Beta Bernoulli</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exponential Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework exponential data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Normally distributed Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework Normal data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Non-Informative Priors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework Alternative Priors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Brief Review of Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">—</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Honnors Homework On Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">2. Techniques and Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Modeling and Monte Carlo estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Modeling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Monte Carlo estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Metropolis-Hastings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on the Metropolis-Hastings algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gibbs sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HW - Gibbs-Sampling algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assessing Convergence</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on the Gibbs-Sampling algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Honnors Homework on M-H algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notes - Linear regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HW on Linear Regression Model Part 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HW - Deviance information criterion (DIC)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ANOVA</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HW on ANOVA</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HW+ - Multiple Factor ANOVA</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Logistic regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Logistic Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Poisson regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Poisson regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hierarchical modeling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Hierarchical Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Non-Normal Hierarchical Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Capstone Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homework on Predictive distributions and mixture models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">3. Mixture Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Basic Concepts of Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Basic Concepts of Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mixtures of Gaussians</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Definition of Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Zero inflated distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Basic Concepts of Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Basic Concepts of Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The EM algorithm for zero-inflated mixtures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The EM algorithm for Zero-Inflated Mixtures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The EM algorithm for Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MCMC for Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The MCMC algorithm for Zero-Inflated Mixtures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Markov chain Monte Carlo algorithms for Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Applications of Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Old Faithful eruptions density estimation with the EM algorithm</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Old Faithful eruptions density estimation with the MCMC algorithms</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Mixture Models for Classification of Banknotes</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computational considerations for Mixture Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Information Criteria (BIC)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Estimating the number of components in Bayesian settings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Estimating the partition structure in Bayesian models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The MCMC algorithm for Zero-Inflated Mixtures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">4. Time series Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 0: Introductions to time series analysis and the AR(1) process</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introductions to Time Series analysis &amp; the AR(1) process</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The AR(p) process</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Normal Dynamic Linear Models, Part 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Normal Dynamic Linear Models, Part 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Final Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 0: Feyman Notebook on Bayesian Time Series Analysis</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">5. Capstone Project</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
 <span class="menu-text">C5-L01.qmd</span>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">6. Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix: Notation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix: Discrete Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix: Continuous Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix: Exponents &amp; Logarithms</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix: The Law of Large Numbers</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix: The Central Limit Theorem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix: Conjugate Priors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Appendix: Link Function</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayes by backprop</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Books in R &amp; Python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Summary</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#basic-concepts" id="toc-basic-concepts" class="nav-link active" data-scroll-target="#basic-concepts"><span class="header-section-number">1</span> 1. Basic Concepts:</a>
  <ul class="collapse">
  <li><a href="#definition-of-a-finite-mixture-model" id="toc-definition-of-a-finite-mixture-model" class="nav-link" data-scroll-target="#definition-of-a-finite-mixture-model"><span class="header-section-number">1.1</span> 1.1 Definition of a finite mixture model</a></li>
  <li><a href="#why-finite-mixture-models" id="toc-why-finite-mixture-models" class="nav-link" data-scroll-target="#why-finite-mixture-models"><span class="header-section-number">1.2</span> 1.2 Why finite mixture models?</a></li>
  <li><a href="#hierarchical-representation-of-finite-mixtures" id="toc-hierarchical-representation-of-finite-mixtures" class="nav-link" data-scroll-target="#hierarchical-representation-of-finite-mixtures"><span class="header-section-number">1.3</span> 1.3 hierarchical representation of finite mixtures</a></li>
  <li><a href="#the-likelihood-function-for-mixture-models" id="toc-the-likelihood-function-for-mixture-models" class="nav-link" data-scroll-target="#the-likelihood-function-for-mixture-models"><span class="header-section-number">1.4</span> 1.4 The likelihood function for mixture models</a></li>
  <li><a href="#parameter-identifiability" id="toc-parameter-identifiability" class="nav-link" data-scroll-target="#parameter-identifiability"><span class="header-section-number">1.5</span> 1.5 parameter identifiability</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood-estimation-for-mixture-models" id="toc-maximum-likelihood-estimation-for-mixture-models" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-for-mixture-models"><span class="header-section-number">2</span> 2. Maximum Likelihood Estimation For Mixture Models:</a>
  <ul class="collapse">
  <li><a href="#em-algorithm-for-mixture-models" id="toc-em-algorithm-for-mixture-models" class="nav-link" data-scroll-target="#em-algorithm-for-mixture-models"><span class="header-section-number">2.1</span> 2.1 EM Algorithm for Mixture Models:</a></li>
  </ul></li>
  <li><a href="#the-em-algorithm-for-a-location-mixture-of-two-gaussians" id="toc-the-em-algorithm-for-a-location-mixture-of-two-gaussians" class="nav-link" data-scroll-target="#the-em-algorithm-for-a-location-mixture-of-two-gaussians"><span class="header-section-number">3</span> 2.2 The EM algorithm for a Location Mixture of Two Gaussians</a>
  <ul class="collapse">
  <li><a href="#general-location-and-scale-mixtures-of-p-variate-gaussian-distributions" id="toc-general-location-and-scale-mixtures-of-p-variate-gaussian-distributions" class="nav-link" data-scroll-target="#general-location-and-scale-mixtures-of-p-variate-gaussian-distributions"><span class="header-section-number">3.1</span> 2.3 general location and scale mixtures of p-variate gaussian distributions</a></li>
  </ul></li>
  <li><a href="#mcmc" id="toc-mcmc" class="nav-link" data-scroll-target="#mcmc"><span class="header-section-number">4</span> 3. MCMC</a>
  <ul class="collapse">
  <li><a href="#markov-chain-monte-carlo-algorithms-for-mixture-mod-" id="toc-markov-chain-monte-carlo-algorithms-for-mixture-mod-" class="nav-link" data-scroll-target="#markov-chain-monte-carlo-algorithms-for-mixture-mod-"><span class="header-section-number">4.1</span> 3.1 Markov chain monte carlo algorithms for mixture mod-</a></li>
  <li><a href="#the-mcmc-algorithm-for-a-location-mixture-of-two-gaussian-distributions" id="toc-the-mcmc-algorithm-for-a-location-mixture-of-two-gaussian-distributions" class="nav-link" data-scroll-target="#the-mcmc-algorithm-for-a-location-mixture-of-two-gaussian-distributions"><span class="header-section-number">4.2</span> 3.2 The MCMC algorithm for a location mixture of two gaussian distributions</a></li>
  <li><a href="#general-location-and-scale-mixtures-of-p-variate-gaus-" id="toc-general-location-and-scale-mixtures-of-p-variate-gaus-" class="nav-link" data-scroll-target="#general-location-and-scale-mixtures-of-p-variate-gaus-"><span class="header-section-number">4.3</span> 3.3 general location and scale mixtures of p-variate gaus-</a></li>
  </ul></li>
  <li><a href="#applications-of-mixture-models" id="toc-applications-of-mixture-models" class="nav-link" data-scroll-target="#applications-of-mixture-models"><span class="header-section-number">5</span> 4. Applications of Mixture Models</a>
  <ul class="collapse">
  <li><a href="#density-estimation" id="toc-density-estimation" class="nav-link" data-scroll-target="#density-estimation"><span class="header-section-number">5.1</span> 4.1 density estimation</a></li>
  <li><a href="#clustering-unsupervised-classification" id="toc-clustering-unsupervised-classification" class="nav-link" data-scroll-target="#clustering-unsupervised-classification"><span class="header-section-number">5.2</span> 4.2 clustering (unsupervised classification</a></li>
  <li><a href="#supervised-classification" id="toc-supervised-classification" class="nav-link" data-scroll-target="#supervised-classification"><span class="header-section-number">5.3</span> 4.3 (supervised) classification</a></li>
  </ul></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations"><span class="header-section-number">6</span> 5. Practical Considerations</a>
  <ul class="collapse">
  <li><a href="#ensuring-numerical-stability-when-computing-class-probabilities" id="toc-ensuring-numerical-stability-when-computing-class-probabilities" class="nav-link" data-scroll-target="#ensuring-numerical-stability-when-computing-class-probabilities"><span class="header-section-number">6.1</span> 5.1 Ensuring numerical stability when computing class probabilities</a></li>
  <li><a href="#numerical-consequences-of-multimodality" id="toc-numerical-consequences-of-multimodality" class="nav-link" data-scroll-target="#numerical-consequences-of-multimodality"><span class="header-section-number">6.2</span> 5.2 numerical consequences of multimodality</a></li>
  <li><a href="#selecting-the-number-components" id="toc-selecting-the-number-components" class="nav-link" data-scroll-target="#selecting-the-number-components"><span class="header-section-number">6.3</span> 5.3 selecting the number components</a></li>
  <li><a href="#fully-bayesian-inference-on-the-number-of-components" id="toc-fully-bayesian-inference-on-the-number-of-components" class="nav-link" data-scroll-target="#fully-bayesian-inference-on-the-number-of-components"><span class="header-section-number">6.4</span> 5.4 fully Bayesian inference on the number of components</a></li>
  <li><a href="#fully-bayesian-inference-on-the-partition-structure" id="toc-fully-bayesian-inference-on-the-partition-structure" class="nav-link" data-scroll-target="#fully-bayesian-inference-on-the-partition-structure"><span class="header-section-number">6.5</span> 5.5 fully Bayesian inference on the partition structure</a></li>
  </ul></li>
  <li><a href="#section-4.2-clustering-unsupervised-classification-formulas" id="toc-section-4.2-clustering-unsupervised-classification-formulas" class="nav-link" data-scroll-target="#section-4.2-clustering-unsupervised-classification-formulas"><span class="header-section-number">7</span> Section 4.2: Clustering (Unsupervised Classification) — Formulas</a>
  <ul class="collapse">
  <li><a href="#hard-assignment-mode-assignment-for-em" id="toc-hard-assignment-mode-assignment-for-em" class="nav-link" data-scroll-target="#hard-assignment-mode-assignment-for-em"><span class="header-section-number">7.1</span> 1. Hard assignment (mode assignment for EM):</a></li>
  <li><a href="#posterior-probability-of-cluster-assignment-bayesian-approach" id="toc-posterior-probability-of-cluster-assignment-bayesian-approach" class="nav-link" data-scroll-target="#posterior-probability-of-cluster-assignment-bayesian-approach"><span class="header-section-number">7.2</span> 2. Posterior Probability of Cluster Assignment (Bayesian approach):</a></li>
  </ul></li>
  <li><a href="#section-4.3-supervised-classification" id="toc-section-4.3-supervised-classification" class="nav-link" data-scroll-target="#section-4.3-supervised-classification"><span class="header-section-number">8</span> Section 4.3: (Supervised) Classification</a>
  <ul class="collapse">
  <li><a href="#posterior-probability-of-class-membership-ldaqdamixture-discriminant-analysis" id="toc-posterior-probability-of-class-membership-ldaqdamixture-discriminant-analysis" class="nav-link" data-scroll-target="#posterior-probability-of-class-membership-ldaqdamixture-discriminant-analysis"><span class="header-section-number">8.1</span> 3. Posterior probability of class membership (LDA/QDA/Mixture discriminant analysis):</a></li>
  <li><a href="#for-a-mixture-model" id="toc-for-a-mixture-model" class="nav-link" data-scroll-target="#for-a-mixture-model"><span class="header-section-number">8.2</span> 4. For a mixture model:</a></li>
  <li><a href="#posterior-probability-for-classification-under-mixture-model" id="toc-posterior-probability-for-classification-under-mixture-model" class="nav-link" data-scroll-target="#posterior-probability-for-classification-under-mixture-model"><span class="header-section-number">8.3</span> 5. Posterior probability for classification under mixture model:</a></li>
  </ul></li>
  <li><a href="#section-5-practical-considerations" id="toc-section-5-practical-considerations" class="nav-link" data-scroll-target="#section-5-practical-considerations"><span class="header-section-number">9</span> Section 5: Practical Considerations</a>
  <ul class="collapse">
  <li><a href="#log-sum-exp-trick-for-numerical-stability" id="toc-log-sum-exp-trick-for-numerical-stability" class="nav-link" data-scroll-target="#log-sum-exp-trick-for-numerical-stability"><span class="header-section-number">9.1</span> 6. Log-sum-exp trick (for numerical stability):</a></li>
  <li><a href="#bayesian-information-criterion-bic" id="toc-bayesian-information-criterion-bic" class="nav-link" data-scroll-target="#bayesian-information-criterion-bic"><span class="header-section-number">9.2</span> 7. Bayesian Information Criterion (BIC):</a></li>
  <li><a href="#dirichlet-process-partition-probability" id="toc-dirichlet-process-partition-probability" class="nav-link" data-scroll-target="#dirichlet-process-partition-probability"><span class="header-section-number">9.3</span> 8. Dirichlet process partition probability:</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Mixture Models</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Oren Bochman </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="basic-concepts" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="basic-concepts"><span class="header-section-number">1</span> 1. Basic Concepts:</h2>
<section id="definition-of-a-finite-mixture-model" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="definition-of-a-finite-mixture-model"><span class="header-section-number">1.1</span> 1.1 Definition of a finite mixture model</h3>
<ol type="1">
<li>Mixture Model (CDF):</li>
</ol>
<p><span id="eq-mixture-cdf"><span class="math display">
F(x) = \sum_{k=1}^K \omega_k G_k(x)
\tag{1}</span></span></p>
<p>where <span class="math inline">G_k(x)</span> is the CDF of the <span class="math inline">k</span>-th component distribution and <span class="math inline">\omega_k</span> is the weight for the <span class="math inline">k</span>-th component.</p>
<ol start="2" type="1">
<li>Mixture Model (PDF/PMF):</li>
</ol>
<p><span id="eq-mixture-pdf"><span class="math display">
f(x) = \sum_{k=1}^K \omega_k g_k(x)
\tag{2}</span></span></p>
<p>where <span class="math inline">g_k(x)</span> is the PDF/PMF of the <span class="math inline">k</span>-th component distribution and <span class="math inline">\omega_k</span> is the weight for the <span class="math inline">k</span>-th component.</p>
<ol start="3" type="1">
<li>Example: Exponential Mixture CDF:</li>
</ol>
<p><span id="eq-exponential-mixture-cdf"><span class="math display">
F(x) = \omega_1 \left(1 - \exp\left\{\frac{x}{\theta_1}\right\}\right)
     + \omega_2 \left(1 - \exp\left\{\frac{x}{\theta_2}\right\}\right)
     + \omega_3 \left(1 - \exp\left\{\frac{x}{\theta_3}\right\}\right)
\tag{3}</span></span></p>
<ol start="4" type="1">
<li>Example: Exponential Mixture PDF:</li>
</ol>
<p><span id="eq-exponential-mixture-pdf"><span class="math display">
f(x) = \frac{\omega_1}{\theta_1} \exp\left\{\frac{x}{\theta_1}\right\}
    + \frac{\omega_2}{\theta_2} \exp\left\{\frac{x}{\theta_2}\right\}
    + \frac{\omega_3}{\theta_3} \exp\left\{\frac{x}{\theta_3}\right\}
\tag{4}</span></span></p>
<ol start="5" type="1">
<li>Gamma Mixture PDF:</li>
</ol>
<p><span id="eq-gamma-mixture-pdf"><span class="math display">
f(x) =
\begin{cases}
    \omega \frac{x^{\nu_1-1}}{\Gamma(\nu_1)\lambda_1^{\nu_1}}\ \exp \left\{\frac{x}{\lambda_1}\right\}
    + (1-\omega) \frac{x^{\nu_2-1}}{\Gamma(\nu_2)\lambda_2^{\nu_2}}\ \exp \left\{\frac{x}{\lambda_2}\right\} &amp; x &gt; 0 \\
    0 &amp; \text{otherwise}
\end{cases}
\tag{5}</span></span></p>
<ol start="6" type="1">
<li>Mean and Variance of a Mixture:</li>
</ol>
<p><span id="eq-mixture-mean"><span class="math display">
\mathbb{E}_F(X) = \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}[X]
\tag{6}</span></span></p>
<p><span id="eq-mixture-variance"><span class="math display">
\begin{aligned}
\operatorname{Var}_F(X) &amp; = \mathbb{E}_F(X^2) - \{\mathbb{E}_F(X)\}^2 \\
&amp; = \sum_{k=1}^K \omega_k \left\{ \mathbb{E}_{G_k}(X^2) \right\} - \left\{ \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}(X) \right\}^2 \\
&amp; = \sum_{k=1}^K \omega_k \left\{ \operatorname{Var}_{G_k}(X) + [\mathbb{E}_{G_k}(X)]^2 \right\} - \left\{ \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}(X) \right\}^2
\end{aligned}
\tag{7}</span></span></p>
<ol start="7" type="1">
<li>Special Case (Component means zero):</li>
</ol>
<p><span id="eq-mixture-variance-zero-mean"><span class="math display">
\operatorname{Var}_F(X) = \sum_{k=1}^K \omega_k \operatorname{Var}_{G_k}(X)
\tag{8}</span></span></p>
</section>
<section id="why-finite-mixture-models" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="why-finite-mixture-models"><span class="header-section-number">1.2</span> 1.2 Why finite mixture models?</h3>
<p>Finite mixtures of distributions within a single family provide a lot of flexibility. For example, a mixture of Gaussian distributions can have a bimodal density.</p>
<ol start="8" type="1">
<li>Example: Bimodal Mixture:</li>
</ol>
<p><span id="eq-bimodal-mixture"><span class="math display">
f(x) = 0.6 \frac{1}{\sqrt{2\pi}} \exp\left\{ -\frac{1}{2}x^2 \right\}
     + 0.4 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left\{ -\frac{1}{2}\frac{(x-5)^2}{4} \right\}
\tag{9}</span></span></p>
<ol start="9" type="1">
<li>Example: Skewed Unimodal Mixture:</li>
</ol>
<p><span id="eq-skewed-unimodal-mixture"><span class="math display">
f(x) = 0.55 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left\{ -\frac{1}{2} \frac{x^2}{2} \right\}
    + 0.45 \frac{1}{\sqrt{2\pi} 2} \exp\left\{ -\frac{1}{2}\left(\frac{x-3}{2}\right)^2 \right\}
\tag{10}</span></span></p>
<ol start="10" type="1">
<li>Example: Symmetric Heavy-tailed Mixture:</li>
</ol>
<p><span id="eq-symmetric-heavy-tailed-mixture"><span class="math display">
f(x) = 0.4 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left\{ -\frac{1}{2} \frac{x^2}{2} \right\}
    + 0.4 \frac{1}{\sqrt{2\pi} 4} \exp\left\{ -\frac{1}{2} \frac{x^2}{16} \right\}
    + 0.2 \frac{1}{\sqrt{2\pi} \sqrt{20}} \exp\left\{ -\frac{1}{2} \frac{x^2}{20} \right\}
\tag{11}</span></span></p>
<ol start="11" type="1">
<li>Zero-inflated Negative Binomial PMF:</li>
</ol>
<p><span id="eq-zero-inflated-negative-binomial-pmf"><span class="math display">
\mathbb{P}r(x) =
\begin{cases}
    \omega_1 + (1-\omega_1)\theta^r &amp; x=0 \\
    (1-\omega_1) \binom{x+r-1}{x} \theta^r (1-\theta)^x &amp; x&gt;1
\end{cases}
\tag{12}</span></span></p>
<ol start="12" type="1">
<li>Regular Negative Binomial PMF:</li>
</ol>
<p><span id="eq-regular-negative-binomial-pmf"><span class="math display">
p^*(x) = \binom{x+r-1}{x} \theta^r (1-\theta)^x
\tag{13}</span></span></p>
<ol start="13" type="1">
<li>Zero-inflated Log-Gaussian PDF:</li>
</ol>
<p><span id="eq-zero-inflated-log-gaussian-pdf"><span class="math display">
f(x) =
\begin{cases}
    \omega_1 \delta_0(x) + (1-\omega_1)\frac{1}{\sqrt{2\pi}\sigma x}\exp\left\{ -\frac{(\ln x - \mu)^2}{2\sigma^2} \right\} &amp; x &gt; 0 \\
    0 &amp; \text{otherwise}
\end{cases}
\tag{14}</span></span></p>
<p>where <span class="math inline">\delta_0(x)</span> is the Dirac delta function at <span class="math inline">x=0</span>.</p>
</section>
<section id="hierarchical-representation-of-finite-mixtures" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="hierarchical-representation-of-finite-mixtures"><span class="header-section-number">1.3</span> 1.3 hierarchical representation of finite mixtures</h3>
<ol start="14" type="1">
<li>Mixture Model (Hierarchical):</li>
</ol>
<p><span id="eq-hierarchical-mixture"><span class="math display">
X \mid c \sim G_c, \quad \mathbb{P}r(c = k) = \omega_k
\tag{15}</span></span></p>
<p>where <span class="math inline">G_c</span> is the distribution of the <span class="math inline">k</span>-th component and <span class="math inline">\omega_k</span> is the weight for the <span class="math inline">k</span>-th component.</p>
</section>
<section id="the-likelihood-function-for-mixture-models" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="the-likelihood-function-for-mixture-models"><span class="header-section-number">1.4</span> 1.4 The likelihood function for mixture models</h3>
<ol start="15" type="1">
<li>Observed-data Likelihood for a mixture Model</li>
</ol>
<p><span id="eq-observed-data-likelihood"><span class="math display">
L_O(\theta, \omega; x) \propto \mathbb{P}r(x \mid \theta, \omega) = \prod_{i=1}^n \sum_{k=1}^K \omega_k g_k(x_i \mid \theta_k)
\tag{16}</span></span></p>
<p>where <span class="math inline">g_k(x_i \mid \theta_k)</span> is the PDF/PMF of the <span class="math inline">k</span>-th component distribution evaluated at <span class="math inline">x_i</span> with parameter <span class="math inline">\theta_k</span>.</p>
<ol start="16" type="1">
<li>Mixture Model (Likelihood, complete-data):</li>
</ol>
<p><span id="eq-complete-data-likelihood"><span class="math display">
L(\theta, \omega; x, c) = \mathbb{P}r(x, c \mid \theta, \omega) = \prod_{i=1}^n \prod_{k=1}^K [\omega_k g_k(x_i \mid \theta_k)]^{1(c_i = k)}
\tag{17}</span></span></p>
<p>where <span class="math inline">1(c_i = k)</span> is an indicator function that is 1 if <span class="math inline">c_i = k</span> and 0 otherwise.</p>
<ol start="17" type="1">
<li>Alternative complete-data likelihood decomposition:</li>
</ol>
<p><span id="eq-complete-data-likelihood-decomposition"><span class="math display">
\mathbb{P}r(x, c \mid \theta, \omega) = \mathbb{P}r(x \mid c, \theta) \mathbb{P}r(c \mid \omega)
\tag{18}</span></span></p>
<p>with</p>
<p><span id="eq-complete-data-likelihood-x"><span class="math display">
\mathbb{P}r(x \mid c, \theta) = \prod_{i=1}^n g_{c_i}(x_i \mid \theta_{c_i})
\tag{19}</span></span></p>
<p><span id="eq-complete-data-likelihood-c"><span class="math display">
\mathbb{P}r(c \mid \omega) = \prod_{k=1}^K \omega_k^{\sum_{i=1}^n 1(c_i = k)}
\tag{20}</span></span></p>
<p>where <span class="math inline">1(c_i = k)</span> is an indicator function that is 1 if <span class="math inline">c_i = k</span> and 0 otherwise.</p>
</section>
<section id="parameter-identifiability" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="parameter-identifiability"><span class="header-section-number">1.5</span> 1.5 parameter identifiability</h3>
<p>Label switching</p>
<p>TODO : missing formula</p>
<p><span id="eq-parameter-identifiability-mix1"><span class="math display">
f(x) = ...
\tag{21}</span></span></p>
<p>TODO : missing formula</p>
<p><span id="eq-parameter-identifiability-mix2"><span class="math display">
f(x) = ...
\tag{22}</span></span></p>
<p>Number of components</p>
<p><span id="eq-parameter-identifiability-mix3"><span class="math display">
f(x) = ...
\tag{23}</span></span></p>
</section>
</section>
<section id="maximum-likelihood-estimation-for-mixture-models" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="maximum-likelihood-estimation-for-mixture-models"><span class="header-section-number">2</span> 2. Maximum Likelihood Estimation For Mixture Models:</h2>
<ol start="18" type="1">
<li>Maximum Likelihood Estimator (MLE) for Mixture:</li>
</ol>
<p><span id="eq-max-observed-data-likelihood"><span class="math display">
(\hat{\omega}, \hat{\theta}) = \arg\max_{\omega, \theta} \prod_{i=1}^n \sum_{k=1}^K \omega_k g_k(x_i \mid \theta_k)
\tag{24}</span></span></p>
<p>where <span class="math inline">\hat{\omega}</span> and <span class="math inline">\hat{\theta}</span> are the MLEs for the weights and parameters of the mixture components, respectively.</p>
<section id="em-algorithm-for-mixture-models" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="em-algorithm-for-mixture-models"><span class="header-section-number">2.1</span> 2.1 EM Algorithm for Mixture Models:</h3>
<ol start="19" type="1">
<li>EM algorithm Set Q-function in E step:</li>
</ol>
<p>E step:</p>
<p><span id="eq-em-Q-function"><span class="math display">
Q(\omega, \theta \mid \omega^{(t)}, \theta^{(t)}, x) = \mathbb{E}_{c \mid \omega^{(t)}, \theta^{(t)}, x} [\log \mathbb{P}r(x, c \mid \omega, \theta)]
\tag{25}</span></span></p>
<p>where <span class="math inline">Q(\omega, \theta \mid \omega^{(t)}, \theta^{(t)}, x)</span> is the expected complete-data log-likelihood given the current estimates of the parameters <span class="math inline">\omega^{(t)}</span> and <span class="math inline">\theta^{(t)}</span> and the observed data <span class="math inline">x</span>.</p>
<ol start="19" type="1">
<li>EM algorithm Set parameters in M step:</li>
</ol>
<p><span id="eq-em-m-step"><span class="math display">
(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)}) = \arg\max_{\omega, \theta} Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, y)   
\tag{26}</span></span></p>
<p>where <span class="math inline">(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)})</span> are the updated estimates of the parameters after the M step.</p>
<!-- missing em stuff here -->
<p><span id="eq-conditional-independence-of-components"><span class="math display">
\mathbb{P}r(c_i = k \mid \omega, \theta, x) = \frac{\omega_k g_k(x_i \mid \theta_k)}{\sum_{l=1}^K \omega_l g_l(x_i \mid \theta_l)} = v_{i,k}(\omega, \theta)
\tag{27}</span></span></p>
<p>where <span class="math inline">v_{i,k}(\omega, \theta)</span> is the posterior probability of the <span class="math inline">k</span>-th component given the observed data <span class="math inline">x_i</span> and the current estimates of the parameters <span class="math inline">\omega</span> and <span class="math inline">\theta</span>.</p>
<p>and <span class="math inline">(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)})</span> are the updated estimates of the parameters after the M step.</p>
<p>Also, remember that:</p>
<p><span id="eq-em-m-step"><span class="math display">
\mathbb{P}r(x, c | \theta, \omega) =
\prod_{i=1}^n
\prod_{k=1}^K
[\omega_k g_k(x_i | \theta_k)]^{\mathbb{1}(c_i=k)}
\tag{28}</span></span></p>
<p>The value of <span class="math inline">v_{i,k}(\omega, \theta)</span> can be interpreted as the probability that observation <span class="math inline">i</span> was generated from component <span class="math inline">k</span> if we assume that the true parameters of the mixture model are <span class="math inline">\omega</span> and <span class="math inline">\theta</span>.</p>
<!-- missing em stuff here -->
<p><span id="eq-conditional-independence-of-components"><span class="math display">
\mathbb{P}r(c_i = k \mid \omega, \theta, x) = \frac{\omega_k g_k(x_i \mid \theta_k)}{\sum_{l=1}^K \omega_l g_l(x_i \mid \theta_l)} = v_{i,k}(\omega, \theta)
\tag{29}</span></span></p>
<p>where <span class="math inline">v_{i,k}(\omega, \theta)</span> is the posterior probability of the <span class="math inline">k</span>-th component given the observed data <span class="math inline">x_i</span> and the current estimates of the parameters <span class="math inline">\omega</span> and <span class="math inline">\theta</span>.</p>
<p>which implies that</p>
<p><span id="eq-log-likelihood"><span class="math display">
\log \mathbb{P}r(x_i,c \mid \theta, \omega) = \sum_{n=1}^N \sum_{k=1}^K \mathbb{1}(c_i=k) [\log(\omega_k) + \log(g_k(x_i \mid \theta_k))]
\tag{30}</span></span></p>
<p>where <span class="math inline">\mathbb{1}(c_i=k)</span> is an indicator function that is 1 if <span class="math inline">c_i = k</span> and 0 otherwise.</p>
<p>Hence</p>
<p><span id="eq-em-Q-function"><span class="math display">
Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x) = \sum_{i=1}^n \sum_{k=1}^K v_{i,k} \mathbb{E}_{c \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x} \left [\mathbb{1}_{(c_i=k)} \log(\omega_k) + \log(g_k(x_i \mid \theta_k))\right]
\tag{31}</span></span></p>
<p>where <span class="math inline">v_{i,k}(\omega^{(t)}, \theta^{(t)})</span> is the posterior probability of the <span class="math inline">k</span>-th component given the observed data <span class="math inline">x_i</span> and the current estimates of the parameters <span class="math inline">\omega^{(t)}</span> and <span class="math inline">\theta^{(t)}</span>.</p>
<p>and therefore <span id="eq-em-Q-function"><span class="math display">
Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x) = \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)}\left(\hat{\omega}^{(t)}, \hat{\theta}^{(t)}\right) [\log(\omega_k) + \log(g_k(x_i \mid \theta_k))]
\tag{32}</span></span></p>
<p>where <span class="math inline">v_{i,k}(\hat{\omega}^{(t)}, \hat{\theta}^{(t)})</span> is the posterior probability of the <span class="math inline">k</span>-th component given the observed data <span class="math inline">x_i</span> and the current estimates of the parameters <span class="math inline">\hat{\omega}^{(t)}</span> and <span class="math inline">\hat{\theta}^{(t)}</span>.</p>
<p>(Remember that the term that is constant with respect to c can come out of the expectation, and that the expected value of an indicator function is just the probability of the event inside the indicator).</p>
<p>Hence, roughly speaking, we can see that the Q function is in this case equivalent to a weighted average of the log likelihoods associated with each of the components in the mixture.</p>
</section>
</section>
<section id="the-em-algorithm-for-a-location-mixture-of-two-gaussians" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="the-em-algorithm-for-a-location-mixture-of-two-gaussians"><span class="header-section-number">3</span> 2.2 The EM algorithm for a Location Mixture of Two Gaussians</h2>
<p><span id="eq-gaussian-location-mixture"><span class="math display">
f(x\mid \omega, \mu_1,\mu_2, \sigma) = \omega \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{ -\frac{(x - \mu_1)^2}{2\sigma^2} \right\} + (1-\omega) \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{ -\frac{(x - \mu_2)^2}{2\sigma^2} \right\}
\tag{33}</span></span></p>
<p>where <span class="math inline">\omega_1</span> and <span class="math inline">\omega_2</span> are the weights of the two components, <span class="math inline">\mu_1</span> and <span class="math inline">\mu_2</span> are the means of the two components, and <span class="math inline">\sigma^2</span> is the common variance.</p>
<p>The expected weights are:</p>
<p><span id="eq-location-mixture-weights"><span class="math display">
v^{(t+1)}_{i,1} =v^{(t+1)}_{i,1} \left (\hat{\omega}^{(t)},\hat{\mu}^{(t)},\hat{\sigma}^{(t)}\right)=\frac{\hat{\omega}^{(t)}_1 \frac{1}{\sqrt{2\pi}\hat{\sigma}^{(t)}} \exp\left\{ -\frac{(x_i - \hat{\mu}^{(t)}_1)^2}{2\hat{\sigma}^{(t)}} \right\}}{\sum_{k=1}^K \hat{\omega}^{(t)}_k \frac{1}{\sqrt{2\pi}\hat{\sigma}^{(t)}} \exp\left\{ -\frac{(x_i - \hat{\mu}^{(t)}_k)^2}{2\hat{\sigma}^{(t)}} \right\}}
\tag{34}</span></span></p>
<p>and</p>
<p><span class="math display">
v^{(t+1)}_{i,2} = 1 - v^{(t+1)}_{i,1}
</span></p>
<p>Therefore, the Q function is</p>
<p><span id="eq-Q-function"><span class="math display">
\begin{aligned}
Q(\omega, \mu_1, \mu_2, \sigma \mid \hat{\omega}^{(t)}, \hat{\mu}^{(t)}_1,\hat{\mu}^{(t)}_2, \hat{\sigma}^{(t)}, x) = \sum_{i=1}^n &amp; v_{i,1}^{(t+1)} \left [\log(\omega) - \frac{1}{2} \log(2\pi) - \log(\sigma) - \frac{(x_i - \mu_1)^2}{2\sigma^2}\right ] \\
+ &amp; v_{i,2}^{(t+1)} \left [\log(1-\omega) - \frac{1}{2} \log(2\pi) - \log(\sigma) - \frac{(x_i - \mu_2)^2}{2\sigma^2}\right]
\end{aligned}
\tag{35}</span></span></p>
<p>where <span class="math inline">v_{i,1}^{(t+1)}</span> and <span class="math inline">v_{i,2}^{(t+1)}</span> are the posterior probabilities of the first and second components given the observed data <span class="math inline">x_i</span> and the current estimates of the parameters <span class="math inline">\hat{\omega}^{(t)}</span>, <span class="math inline">\hat{\mu}^{(t)}_1</span>, <span class="math inline">\hat{\mu}^{(t)}_2</span>, and <span class="math inline">\hat{\sigma}^{(t)}</span>.</p>
<p>The Q function is a function of the parameters <span class="math inline">\omega</span>, <span class="math inline">\mu_1</span>, <span class="math inline">\mu_2</span>, and <span class="math inline">\sigma</span> and is used to update the estimates of these parameters in the M step of the EM algorithm.</p>
<p>to maximize Q we compute its partial derivatives with respect to <span class="math inline">\omega</span>, <span class="math inline">\mu_k</span>, and <span class="math inline">\sigma</span> and set them equal to zero. The partial derivative with respect to <span class="math inline">\omega</span> is:</p>
<p><span id="eq-partial-derivative-of-Q-wrt-omega"><span class="math display">
\frac{\partial Q}{\partial \omega} = \sum_{i=1}^n \left [\frac{v_{i,1}^{(t+1)}}{\omega} - \frac{v_{i,2}^{(t+1)}}{1-\omega} \right ] = 0
\tag{36}</span></span></p>
<p><span id="eq-partial-derivative-of-Q-wrt-mu"><span class="math display">
\frac{\partial Q}{\partial \mu_k} = \sum_{i=1}^n v_{i,k}^{(t+1)} \frac{1}{\sigma^2} (x_i - \mu_k) = 0
\tag{37}</span></span></p>
<p><span id="eq-partial-derivative-of-Q-wrt-sigma"><span class="math display">
\frac{\partial Q}{\partial \sigma} = \sum_{i=1}^n \left [\frac{v_{i,1}^{(t+1)}}{\sigma} - \frac{(x_i - \mu_1)^2}{\sigma^3} + \frac{v_{i,2}^{(t+1)}}{\sigma} - \frac{(x_i - \mu_2)^2}{\sigma^3} \right ] = 0
\tag{38}</span></span></p>
<p>By setting (<a href="#eq-partial-derivative-of-Q-wrt-omega" class="quarto-xref">Equation&nbsp;36</a>) equal to zero we get:</p>
<p><span id="eq-omega-update"><span class="math display">
\begin{aligned}
&amp;\left \{ \sum_{i=1}^n v_{i,2}^{(t+1)} \right \}  \omega^{(t+1)} &amp; = &amp;
\left \{ \sum_{i=1}^n v_{i,1}^{(t+1)} \right \} \left (1 - \omega^{(t+1)}\right)
\\ \implies &amp; \left \{ \sum_{i=1}^n v_{i,1}^{(t+1)} \right \} &amp; = &amp;
\left \{ \sum_{i=1}^n v_{i,1}^{(t+1)} + \sum_{i=1}^n v_{i,2}^{(t+1)} \right \} \omega^{(t+1)}
\\ \implies &amp; \omega^{(t+1)} &amp; = &amp; \frac{\sum_{i=1}^n v_{i,1}^{(t+1)}}{\sum_{i=1}^n v_{i,1}^{(t+1)} + v_{i,2}^{(t+1)}}
\\ &amp;  &amp;= &amp; \frac{1}{n} \sum_{i=1}^n v_{i,1}^{(t+1)}
\end{aligned}
\tag{39}</span></span></p>
<p>where <span class="math inline">\omega^{(t+1)}</span> is the updated estimate of the weight for the first component after the M step.</p>
<p>where we used the fact that <span class="math inline">\sum_{i=1}^n v_{i,1}^{(t+1)} + \sum_{i=1}^n v_{i,2}^{(t+1)} = n</span>.</p>
<p>This means that the new estimate of <span class="math inline">\omega</span> is the average of the posterior probabilities of the first component over all observations.</p>
<p>by setting (<a href="#eq-partial-derivative-of-Q-wrt-mu" class="quarto-xref">Equation&nbsp;37</a>) equal to zero we get:</p>
<p><span id="eq-em-solving-for-mu"><span class="math display">\begin{aligned}
            &amp;&amp; 0 &amp;&amp; = &amp; \sum_{i=1}^n v_{i,k}^{(t+1)} (x_i - \mu_k)
\\ \implies &amp;&amp; \sum_{i=1}^n v_{i,k}^{(t+1)} x_i &amp;&amp; = &amp; \sum_{i=1}^n v_{i,k}^{(t+1)} \mu_k
\\ \implies &amp;&amp; \mu_k^{(t+1)} &amp;&amp; = &amp; \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} x_i}{\sum_{i=1}^n v_{i,k}^{(t+1)}}
\end{aligned}
\tag{40}</span></span></p>
<p>where we used the fact that <span class="math inline">\sum_{i=1}^n v_{i,k}^{(t+1)} = n</span>.</p>
<p>Note that the partial estimator for μk is a weighted average of the xis, with the weight associated with observation i being proportional to the probability that such observation was generated by component k. Again, if the components are well separated and values of vi,k are all close to either 0 or 1, this weighted average is roughly the average of the observations coming from component k</p>
<p>Finally, making the same argument for the partial derivative with respect to σ, we get:</p>
<p><span id="eq-em-solving-for-sigma"><span class="math display">
\begin{aligned}
&amp;
    \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} &amp;=&amp; \left ( \frac {1}{\sigma^{(t+1)}} \right ) ^ 2  \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)})^2
\\ \implies &amp;
\sigma^{(t+1)} &amp;=&amp; \sqrt{\frac{\sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)})^2}{\sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)}}}
\end{aligned}
\tag{41}</span></span></p>
<section id="general-location-and-scale-mixtures-of-p-variate-gaussian-distributions" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="general-location-and-scale-mixtures-of-p-variate-gaussian-distributions"><span class="header-section-number">3.1</span> 2.3 general location and scale mixtures of p-variate gaussian distributions</h3>
<p><span id="eq-general-location-scale-mixture"><span class="math display">
f(x) =
\sum_{k=1}^K
\omega_k
\left( \frac{1}{(2\pi)} \right) ^\frac{q}{2}
|\Sigma_k|-\frac{1}{2} \exp
\left\{
(x - \theta_k)^T \Sigma_k^{-1}(x - \theta_k)
\right\}
\tag{42}</span></span></p>
<p>where <span class="math inline">\theta_k</span> is the mean of the <span class="math inline">k</span>-th component, <span class="math inline">\Sigma_k</span> is the covariance matrix of the <span class="math inline">k</span>-th component, and <span class="math inline">\omega_k</span> is the weight for the <span class="math inline">k</span>-th component. <span id="eq-em-p-variate-mixture-parameters"><span class="math display">
\begin{aligned}
v_{i,k}^{(t+1)} &amp;= \frac{\omega_k^{(t)} |\Sigma_k^{(t)}|^{-1/2} \exp\left\{-\frac{1}{2}(x_i - \mu_k^{(t)})^T [\Sigma_k^{(t)}]^{-1}(x_i - \mu_k^{(t)})\right\}}{\sum_{l=1}^K \omega_l^{(t)} |\Sigma_l^{(t)}|^{-1/2} \exp\left\{-\frac{1}{2}(x_i - \mu_l^{(t)})^T [\Sigma_l^{(t)}]^{-1}(x_i - \mu_l^{(t)})\right\}} \\
\omega_k^{(t+1)} &amp;= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)}}{\sum_{i=1}^n \sum_{l=1}^K v_{i,l}^{(t+1)}} \\
\mu_k^{(t+1)} &amp;= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} x_i}{\sum_{i=1}^n v_{i,k}^{(t+1)}} \\
\Sigma_k^{(t+1)} &amp;= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)}) (x_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^n v_{i,k}^{(t+1)}}
\end{aligned}
\tag{43}</span></span></p>
<p>where <span class="math inline">v_{i,k}^{(t+1)}</span> is the posterior probability of the <span class="math inline">k</span>-th component given the observed data <span class="math inline">x_i</span> and the current estimates of the parameters <span class="math inline">\omega^{(t)}</span>, <span class="math inline">\mu^{(t)}</span>, and <span class="math inline">\Sigma^{(t)}</span>.</p>
</section>
</section>
<section id="mcmc" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="mcmc"><span class="header-section-number">4</span> 3. MCMC</h2>
<p><span id="eq-mcmc-mixture"><span class="math display">
f(x) = \sum_{k=1}^K \omega_k g_k(x | \theta_k)
\tag{44}</span></span></p>
<p>where <span class="math inline">g_k(x \mid \theta_k)</span> is the PDF/PMF of the <span class="math inline">k</span>-th component distribution evaluated at <span class="math inline">x</span> with parameter <span class="math inline">\theta_k</span>.</p>
<p><span id="eq-dirichlet-prior"><span class="math display">
\mathbb{P}r(\omega) \;=\;
\frac{\Gamma \bigl ( \sum_{k=1}^K a_k \bigr ) }{\prod_{k=1}^K \Gamma(a_k)}
\prod_{k=1}^K \omega_k^{\,a_k-1},
\quad
\sum_{k=1}^K \omega_k = 1
\tag{45}</span></span></p>
<section id="markov-chain-monte-carlo-algorithms-for-mixture-mod-" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="markov-chain-monte-carlo-algorithms-for-mixture-mod-"><span class="header-section-number">4.1</span> 3.1 Markov chain monte carlo algorithms for mixture mod-</h3>
<p>els</p>
<p><span id="eq-pxc-given-theta-omega"><span class="math display">
\mathbb{P}r(x, c \mid \theta, \omega) = \mathbb{P}r(x \mid c, \theta)\ \mathbb{P}r(c \mid \omega)
\tag{46}</span></span></p>
<p>where</p>
<p><span id="eq-px-given-c-theta"><span class="math display">
\mathbb{P}r(x \mid c, \theta) = \prod_{i=1}^n g_{c_i}(x_i \mid \theta_{c_i})
\tag{47}</span></span></p>
<p>and</p>
<p><span id="eq-pc-given-omega"><span class="math display">
\mathbb{P}r(c \mid \omega) = \prod_{i=1}^n \prod_{k=1}^K
\omega_k ^{\mathbb{1}(c_i = k)} =
\prod_{k=1}^K \omega_k^{\sum_{i=1}^n \mathbb{1}(c_i = k)}
\tag{48}</span></span></p>
<p>where <span class="math inline">1(c_i = k)</span> is an indicator function that is 1 if <span class="math inline">c_i = k</span> and 0 otherwise.</p>
<ol start="21" type="1">
<li>Joint posterior (with latent labels)</li>
</ol>
<p><span id="eq-joint-posterior"><span class="math display">
\mathbb{P}r(c,\theta,\omega \mid x)
\;\propto\;
\Bigl(\prod_{i=1}^n g_{c_i}(x_i\mid \theta_{c_i})\Bigr)
\Bigl(\prod_{k=1}^K \omega_k^{\sum_{i=1}^n1(c_i=k)}\Bigr)
\,p(\omega)\,p(\theta)
\tag{49}</span></span></p>
<p>Each of the full conditional distributions can be derived from this joint posterior by retaining the terms that involve the parameter of interest, and recognizing the product of the selected terms as the kernel of a known family of distributions.</p>
<ol start="22" type="1">
<li>Full conditional for <span class="math inline">\omega</span></li>
</ol>
<p><span id="eq-full-cond-omega"><span class="math display">
\mathbb{P}r(\omega \mid c,\theta,x)
\;\propto\;
\mathbb{P}r(c \mid \omega) \mathbb{P}r(\omega)
\;=\;
\prod_{k=1}^K
\omega_k^{\,a_k + \sum_{i=1}^n1(c_i=k)\;-\;1}
\tag{50}</span></span></p>
<p>This clearly corresponds to the kernel of another Dirichlet distribution with updated parameters <span class="math display">
a_k^* = a_k + m_k, \qquad k = 1, \ldots, K,
</span></p>
<p>where <span class="math inline">m_k = \sum_{i=1}^n 1(c_i = k)</span> is the number of observations in component <span class="math inline">k</span>.</p>
<p>Full conditional for each component <span class="math inline">c_i</span></p>
<p><span id="eq-fullcond-ci-1"><span class="math display">
\mathbb{P}r(c_i \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x) \propto \mathbb{P}r(x_i \mid c_i,\theta_k) \mathbb{P}r(c_i \mid \omega)
\tag{51}</span></span></p>
<p>hence</p>
<p><span id="eq-fullcond-ci-2"><span class="math display">
\mathbb{P}r(c_i = k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)
\;=\;
\frac{\omega_k\,g_k(x_i\mid \theta_k)}
     {\sum_{l=1}^K \omega_l\,g_l(x_i\mid \theta_l)}
\tag{52}</span></span></p>
<p>Note the similarity with the formula for the expected weights <span class="math inline">v_{i,k}</span> in the EM algorithm.)</p>
<p>Full conditional for component parameters <span class="math inline">\theta_k</span></p>
<p><span id="eq-fullcond-theta_k"><span class="math display">
\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)
\;\propto\;
\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)
\prod_{i:c_i=k} \mathbb{P}r(x_i\mid \theta_k)
\tag{53}</span></span></p>
<p>In the most common case in which the priors for <span class="math inline">\theta_k</span> are independent this is simply:</p>
<p><span id="eq-fullcond-theta_k-indep"><span class="math display">
\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)
\;\propto\;
\mathbb{P}r(\theta_k) \prod_{i:c_i=k} \mathbb{P}r(x_i\mid \theta_k)
\tag{54}</span></span></p>
</section>
<section id="the-mcmc-algorithm-for-a-location-mixture-of-two-gaussian-distributions" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="the-mcmc-algorithm-for-a-location-mixture-of-two-gaussian-distributions"><span class="header-section-number">4.2</span> 3.2 The MCMC algorithm for a location mixture of two gaussian distributions</h3>
<p><span id="eq-mcmc-location-mixture"><span class="math display">
f(x \mid \omega, \mu_1, \mu_2, \sigma) = \omega \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{ -\frac{(x - \mu_1)^2}{2\sigma^2} \right\} + (1-\omega) \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{ -\frac{(x - \mu_2)^2}{2\sigma^2} \right\}
\tag{55}</span></span></p>
<ol start="25" type="1">
<li>Gaussian prior for means <span class="math inline">\mu\_k</span></li>
</ol>
<p><span id="eq-prior-muk"><span class="math display">
\mathbb{P}r(\mu_k)
=\frac{1}{\sqrt{2\pi\tau^2}}
\exp\!\Bigl\{-\tfrac{(\mu_k-\eta)^2}{2\tau^2}\Bigr\}
\tag{56}</span></span></p>
<ol start="26" type="1">
<li>Inverse-Gamma prior for variance <span class="math inline">\sigma^2</span>:</li>
</ol>
<p>with shape parameter <span class="math inline">a</span> and scale parameter <span class="math inline">b</span> for <span class="math inline">\sigma</span></p>
<p><span id="eq-prior-sigma2"><span class="math display">
\mathbb{P}r(\sigma^2)
=\frac{1}{b^a\Gamma(a)}
(\sigma^2)^{-\,d-1}
\exp\!\Bigl\{-\tfrac{q}{\sigma^2}\Bigr\}
\tag{57}</span></span></p>
$$
<span class="math display">\begin{aligned}
\mathbb{P}r(\mu_k \mid c, \mu_1, \ldots, \mu_{k-1}, \mu_{k+1}, \ldots, \mu_K, \omega, x)
&amp; \;\propto\;
\exp\Biggl\{
-\frac{(\mu_k - \eta)^2}{2\tau^2}
\Biggr\} \prod_{i:c_i=k}
\exp\Biggl\{
-\frac{(x_i - \mu_k)^2}{2\sigma^2}
\Biggr\}
\\ &amp; \;\propto\;
\exp\Biggl\{
-\frac{1}{2}
\Biggl[ m_k \frac{1}{\sigma^2} - 2 \frac{\mu_k \sum_{i:c_i=k} x_i}{\sigma^2} + \frac{\mu_k}{\tau^2} - 2 \frac{\mu_k\eta}{\tau^2} \Biggr] \Biggr\}
\\ &amp; \;\propto\;
\exp\Biggl\{
-\frac{1}{2}
\Biggl[ \frac{m_k}{\sigma^2} + \frac{1}{\tau^2} \Biggr] \Biggl[ \mu_k - \frac{\sigma^2 \sum_{i:c_i=k} x_i + \frac{\eta}{\tau^2} }{\mu_k + \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2}} \Biggr] \Biggr\}

\end{aligned}</span>
<p>$$ {#eq-fullcond-mu_k}</p>
<p>which is just the kernel of a normal distribution with updated mean</p>
<p><span id="eq-eta-k-star"><span class="math display">
\eta_k^* = \frac{\frac{1}{\sigma^2}\sum_{i:c_i=k} x_i + \frac{\eta}{\tau^2}}{ \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2}u^2}
\tag{58}</span></span></p>
<p>and updated standard deviation</p>
<p><span id="eq-post-muk"><span class="math display">
\tau_k^*=\Bigl[ \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2} \Bigr]^{-1/2}
\tag{59}</span></span></p>
<p>finaly</p>
<p><span class="math display">
\begin{aligned}
\mathbb{P}r(\sigma^2 | c, \mu, \omega, x)
&amp;\propto
\Biggl(\frac{1}{\sigma^2}\Biggr)^{d+1}
\exp\Biggl\{
− q
\sigma^2
\Biggr\}
\Biggl(\frac{1}{\sigma^2}\Biggr)^{n/2}
\exp\Biggl\{
− \frac{1}{2\sigma^2}
\sum_{i=1}^{n}(x_i − \mu_{c_i})^2
\Biggr\}
\\&amp;=
\Biggl(\frac{1}{\sigma^2}\Biggr)^{n/2+d+1}
\exp\Biggl\{
− \frac{1}{\sigma^2}
\Biggl[ \frac{1}{2} \sum_{i=1}^{n}(x_i − \mu_{c_i})^2
+ q
\Biggr]
\Biggr\}
\end{aligned}
</span></p>
<p>which is the kernel of another inverse Gamma distribution with shape d∗ =n/2 + d and rate parameter</p>
<p><span id="eq-post-sigma2"><span class="math display">
q^*=\tfrac12\sum_{i=1}^n(x_i-\mu_{c_i})^2 + q
\tag{60}</span></span></p>
</section>
<section id="general-location-and-scale-mixtures-of-p-variate-gaus-" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="general-location-and-scale-mixtures-of-p-variate-gaus-"><span class="header-section-number">4.3</span> 3.3 general location and scale mixtures of p-variate gaus-</h3>
<p>sian distributions</p>
<ol start="29" type="1">
<li>General <span class="math inline">q</span>-variate Gaussian mixture</li>
</ol>
<p><span id="eq-post-sigma2"><span class="math display">
\sigma^2 \;\sim\;\mathrm{InvGamma}\bigl(d^*,\,q^*\bigr),
\quad
d^*=\tfrac{n}{2}+d,
\quad
q^*=\tfrac12\sum_{i=1}^n(x_i-\mu_{c_i})^2 + q
\tag{61}</span></span></p>
<p><span id="eq-multivariate-mixture"><span class="math display">
f(x)
=\sum_{k=1}^K
\omega_k\,
\frac{1}{(2\pi)^{q/2}\,\lvert\Sigma_k\rvert^{1/2}}\,
\exp\!\Bigl\{-\tfrac12(x-\mu_k)^\top\Sigma_k^{-1}(x-\mu_k)\Bigr\}
\tag{62}</span></span></p>
<ol start="30" type="1">
<li>Posterior for multivariate <span class="math inline">\mu\_k</span></li>
</ol>
<p><span id="eq-post-multivariate-mu"><span class="math display">
\mu_k \;\sim\; N\bigl(b_k^*,\,B_k^*\bigr),
\quad
B_k^*=\bigl(B^{-1}+m_k\,\Sigma_k^{-1}\bigr)^{-1},
\quad
b_k^*=B_k^*\bigl(B^{-1}b + \Sigma_k^{-1}\sum_{i:c_i=k}x_i\bigr)
\tag{63}</span></span></p>
<ol start="31" type="1">
<li>Posterior for multivariate <span class="math inline">\Sigma\_k</span></li>
</ol>
<p><span id="eq-post-multivariate-sigma"><span class="math display">
\Sigma_k \;\sim\;\mathrm{InvWishart}\bigl(\nu^*,\,S^*\bigr),
\quad
\nu^*=\nu + m_k,
\quad
S^*=S + \sum_{i:c_i=k}(x_i-\mu_k)(x_i-\mu_k)^\top
\tag{64}</span></span></p>
</section>
</section>
<section id="applications-of-mixture-models" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="applications-of-mixture-models"><span class="header-section-number">5</span> 4. Applications of Mixture Models</h2>
<section id="density-estimation" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="density-estimation"><span class="header-section-number">5.1</span> 4.1 density estimation</h3>
<ol start="32" type="1">
<li>Kernel density estimator (general form)</li>
</ol>
<p><span id="eq-kde-general"><span class="math display">
\tilde f(x)
=\frac{1}{n}\sum_{i=1}^n\frac{1}{h}\,
g \Bigl(\tfrac{\|x-x_i \| }{h}\Bigr)
\tag{65}</span></span></p>
<ol start="33" type="1">
<li>Gaussian kernel density estimator</li>
</ol>
<p><span id="eq-kde-gaussian"><span class="math display">
\tilde f(x)
=\sum_{i=1}^n\frac{1}{n}\,
\tfrac{1}{\sqrt{2\pi\,}h}\,
\exp\!\Bigl\{-\tfrac{1}{2}\left(\dfrac{x-x_i}{h}\right)^2\Bigr\}
\tag{66}</span></span></p>
<p>In order to understand the relationship between kernel density estimation and mixture models it is useful to contrast (8) with the density estimate <span class="math display">
\hat f(x) = \sum_{k=1}^K \hat{\omega}_k \frac{1}{\sqrt{2\pi} \hat{\sigma}} \exp\Biggl\{-\frac{1}{2\hat{\sigma}^2}(x - \hat{\mu}_k)^2\Biggr\}
</span></p>
<p>obtained by plugging-in the maximum likelihood estimates of the parameters, <span class="math inline">\hat{\omega}_1, . . . , \hat{\omega}_K, \hat{\mu}_1, . . . , \hat{\mu}_K</span> and <span class="math inline">\hat{\sigma}</span> of a location mixture of K univariate Gaussian distributions.</p>
</section>
<section id="clustering-unsupervised-classification" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="clustering-unsupervised-classification"><span class="header-section-number">5.2</span> 4.2 clustering (unsupervised classification</h3>
<p><span id="eq-clustering"><span class="math display">
f(x) = \sum_{k=1}^K \frac{1}{K} \left(\frac{1}{\sqrt{2\pi\sigma}} \right)^p \exp\left\{-\frac{1}{2\sigma^2} (x − \mu_k)^T (x − \mu_k)\right\}
\tag{67}</span></span></p>
</section>
<section id="supervised-classification" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="supervised-classification"><span class="header-section-number">5.3</span> 4.3 (supervised) classification</h3>
</section>
</section>
<section id="practical-considerations" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="practical-considerations"><span class="header-section-number">6</span> 5. Practical Considerations</h2>
<section id="ensuring-numerical-stability-when-computing-class-probabilities" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="ensuring-numerical-stability-when-computing-class-probabilities"><span class="header-section-number">6.1</span> 5.1 Ensuring numerical stability when computing class probabilities</h3>
<p><span id="eq-zk-sum"><span class="math display">
\begin{aligned}
\frac{z_k}{\sum_{l=1}^K z_l} &amp;= \frac{\exp\{ \log z_k \}}{\sum_{l=1}^K \exp\left\{ \log z_l \right\}} &amp;&amp;  \text{(exp and log are inverse
functions)}\\
&amp;= \frac{\exp\{ -b \}\exp\{ \log z_k \}}{\exp\{ -b \}\sum_{l=1}^K \exp\left\{ \log z_l \right\}} &amp;&amp; \text{(multiply and divide by
the same quantity)} \ e^{-b} \\
&amp;= \frac{\exp\left\{ \log z_k - b \right\}}{\sum_{l=1}^K \exp\left\{ \log z_l - b \right\}} \\
\end{aligned}
\tag{68}</span></span></p>
<p>Although (11) is valid for any b, it should be clear that some values will work better than others for the purpose of avoiding a 0/0 calculation. In particular, we are interested in choosing a value b that makes at least one of the terms in the denominator different from zero after exponentiation. One such choice is <span class="math inline">b=\max_{l=1,\ldots,K} \log z_l</span>, which gives us</p>
<p><span class="math display">
\sum_{l=1}^K \exp\left\{ \log z_l - \max_{l=1,\ldots,K} \log z_l \right\} = 1 + \sum_{l:l \neq l^*} \exp\left\{ \log z_l - \max_{l=1,\ldots,K} \log z_l \right\}
</span></p>
<p>One key advantage of this choice is that all the terms in the sum are less or equal than one, which ensures that we do not overflow when computing <span class="math inline">\exp\left\{ \log z_l - \max_{l=1,\ldots,K} \log z_l \right\}</span>.</p>
</section>
<section id="numerical-consequences-of-multimodality" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="numerical-consequences-of-multimodality"><span class="header-section-number">6.2</span> 5.2 numerical consequences of multimodality</h3>
<p>no math in this section</p>
</section>
<section id="selecting-the-number-components" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="selecting-the-number-components"><span class="header-section-number">6.3</span> 5.3 selecting the number components</h3>
<p><span class="math display">
\text{BIC} = -2 \ell(\hat{\theta}) + p \log(n)
</span></p>
<p>where <span class="math inline">\ell(\hat{\theta})</span> is the maximized log-likelihood, <span class="math inline">p</span> is number of free parameters, <span class="math inline">n</span> is sample size.</p>
</section>
<section id="fully-bayesian-inference-on-the-number-of-components" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="fully-bayesian-inference-on-the-number-of-components"><span class="header-section-number">6.4</span> 5.4 fully Bayesian inference on the number of components</h3>
<p><span class="math display">
f(x) = \sum_{k=1}^K \omega_k g_k(x | \theta_k) + \sum_{k=K+1}^K 0g_k(x | \theta_k)
</span></p>
<p><span class="math display">
(\omega_1, \ldots, \omega_K) \sim Dirichlet( \frac{1}{K}, \ldots , \frac{1}{K} )
</span></p>
<p>where K is the number of components in the mixture model.</p>
<p><span id="eq-expected-K"><span class="math display">
\mathbb{E}(K^*)=\sum_{i=1}^m \frac{\alpha}{\alpha + i - 1} \approx \frac{\alpha \log(n + \alpha -1) }{\alpha}
\tag{69}</span></span></p>
<p>where <span class="math inline">\alpha</span> is the concentration parameter of the Dirichlet process prior, <span class="math inline">n</span> is the number of observations, and <span class="math inline">K^*</span> is the number of components in the mixture model.</p>
</section>
<section id="fully-bayesian-inference-on-the-partition-structure" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="fully-bayesian-inference-on-the-partition-structure"><span class="header-section-number">6.5</span> 5.5 fully Bayesian inference on the partition structure</h3>
<p>Extracting formulas from <strong>Section 4.2 (“Clustering (unsupervised classification)”) to the end</strong> of <em>mixturemodels (2).pdf</em>:</p>
<hr>
</section>
</section>
<section id="section-4.2-clustering-unsupervised-classification-formulas" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="section-4.2-clustering-unsupervised-classification-formulas"><span class="header-section-number">7</span> Section 4.2: Clustering (Unsupervised Classification) — Formulas</h2>
<section id="hard-assignment-mode-assignment-for-em" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="hard-assignment-mode-assignment-for-em"><span class="header-section-number">7.1</span> 1. Hard assignment (mode assignment for EM):</h3>
<p><span class="math display">
\hat{c}_i = \arg\max_k \hat{v}_{i,k}
</span></p>
<p>where <span class="math inline">\hat{v}_{i,k}</span> are the final iteration weights.</p>
<hr>
</section>
<section id="posterior-probability-of-cluster-assignment-bayesian-approach" class="level3" data-number="7.2">
<h3 data-number="7.2" class="anchored" data-anchor-id="posterior-probability-of-cluster-assignment-bayesian-approach"><span class="header-section-number">7.2</span> 2. Posterior Probability of Cluster Assignment (Bayesian approach):</h3>
<p><span class="math display">
\mathbb{P}r(c_i = k \mid x, \text{rest}) = v_{i,k}
</span></p>
<p>where <span class="math inline">v_{i,k}</span> is as defined in the EM/MCMC steps (probability that observation <span class="math inline">i</span> comes from component <span class="math inline">k</span>).</p>
<hr>
</section>
</section>
<section id="section-4.3-supervised-classification" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="section-4.3-supervised-classification"><span class="header-section-number">8</span> Section 4.3: (Supervised) Classification</h2>
<section id="posterior-probability-of-class-membership-ldaqdamixture-discriminant-analysis" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="posterior-probability-of-class-membership-ldaqdamixture-discriminant-analysis"><span class="header-section-number">8.1</span> 3. Posterior probability of class membership (LDA/QDA/Mixture discriminant analysis):</h3>
<p><span class="math display">
\mathbb{P}r(Y = k \mid X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}
</span></p>
<p>where <span class="math inline">\pi_k</span> is prior class probability, <span class="math inline">f_k(x)</span> is the class-conditional density for class <span class="math inline">k</span>.</p>
<hr>
</section>
<section id="for-a-mixture-model" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="for-a-mixture-model"><span class="header-section-number">8.2</span> 4. For a mixture model:</h3>
<p><span class="math display">
f_k(x) = \sum_{j=1}^{M_k} w_{k,j} g_{k,j}(x)
</span></p>
<p>where each class-conditional density can itself be a mixture (with weights <span class="math inline">w_{k,j}</span> and kernels <span class="math inline">g_{k,j}(x)</span>).</p>
<hr>
</section>
<section id="posterior-probability-for-classification-under-mixture-model" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="posterior-probability-for-classification-under-mixture-model"><span class="header-section-number">8.3</span> 5. Posterior probability for classification under mixture model:</h3>
<p><span class="math display">
\mathbb{P}r(Y = k \mid X = x) = \frac{\pi_k \sum_{j=1}^{M_k} w_{k,j} g_{k,j}(x)}{\sum_{l=1}^K \pi_l \sum_{j=1}^{M_l} w_{l,j} g_{l,j}(x)}
</span></p>
<hr>
</section>
</section>
<section id="section-5-practical-considerations" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="section-5-practical-considerations"><span class="header-section-number">9</span> Section 5: Practical Considerations</h2>
<section id="log-sum-exp-trick-for-numerical-stability" class="level3" data-number="9.1">
<h3 data-number="9.1" class="anchored" data-anchor-id="log-sum-exp-trick-for-numerical-stability"><span class="header-section-number">9.1</span> 6. Log-sum-exp trick (for numerical stability):</h3>
<p><span class="math display">
\log \left( \sum_{k=1}^K e^{a_k} \right) = a_{k^*} + \log \left( \sum_{k=1}^K e^{a_k - a_{k^*}} \right)
</span></p>
<p>where <span class="math inline">a_{k^*} = \max\{a_1, \ldots, a_K\}</span>.</p>
<hr>
</section>
<section id="bayesian-information-criterion-bic" class="level3" data-number="9.2">
<h3 data-number="9.2" class="anchored" data-anchor-id="bayesian-information-criterion-bic"><span class="header-section-number">9.2</span> 7. Bayesian Information Criterion (BIC):</h3>
<p><span class="math display">
\text{BIC} = -2 \ell(\hat{\theta}) + p \log(n)
</span></p>
<p>where <span class="math inline">\ell(\hat{\theta})</span> is the maximized log-likelihood, <span class="math inline">p</span> is number of free parameters, <span class="math inline">n</span> is sample size.</p>
<hr>
</section>
<section id="dirichlet-process-partition-probability" class="level3" data-number="9.3">
<h3 data-number="9.3" class="anchored" data-anchor-id="dirichlet-process-partition-probability"><span class="header-section-number">9.3</span> 8. Dirichlet process partition probability:</h3>
<p><span class="math display">
\mathbb{P}r(c_1, \ldots, c_n) = \frac{\alpha^K \prod_{k=1}^K (n_k - 1)!}{\prod_{j=1}^n (\alpha + j - 1)}
</span></p>
<p>where <span class="math inline">K</span> is number of clusters, <span class="math inline">n_k</span> is number of points in cluster <span class="math inline">k</span>, <span class="math inline">\alpha</span> is the concentration parameter.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title :</span><span class="co"> "Mixture Models"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">## 1. Basic Concepts:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1.1 Definition of a finite mixture model</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Mixture Model (CDF):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>F(x) = \sum_{k=1}^K \omega_k G_k(x)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mixture-cdf}</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>where $G_k(x)$ is the CDF of the $k$-th component distribution and $\omega_k$ is the weight for the $k$-th component.</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Mixture Model (PDF/PMF):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>f(x) = \sum_{k=1}^K \omega_k g_k(x)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mixture-pdf}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>where $g_k(x)$ is the PDF/PMF of the $k$-th component distribution and $\omega_k$ is the weight for the $k$-th component.</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Example: Exponential Mixture CDF:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>F(x) = \omega_1 \left(1 - \exp\left<span class="sc">\{</span>\frac{x}{\theta_1}\right<span class="sc">\}</span>\right)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="ss">     + </span>\omega_2 \left(1 - \exp\left<span class="sc">\{</span>\frac{x}{\theta_2}\right<span class="sc">\}</span>\right)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="ss">     + </span>\omega_3 \left(1 - \exp\left<span class="sc">\{</span>\frac{x}{\theta_3}\right<span class="sc">\}</span>\right)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>$$ {#eq-exponential-mixture-cdf}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Example: Exponential Mixture PDF:</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>f(x) = \frac{\omega_1}{\theta_1} \exp\left<span class="sc">\{</span>\frac{x}{\theta_1}\right<span class="sc">\}</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">    + </span>\frac{\omega_2}{\theta_2} \exp\left<span class="sc">\{</span>\frac{x}{\theta_2}\right<span class="sc">\}</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="ss">    + </span>\frac{\omega_3}{\theta_3} \exp\left<span class="sc">\{</span>\frac{x}{\theta_3}\right<span class="sc">\}</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>$$ {#eq-exponential-mixture-pdf}</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Gamma Mixture PDF:</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>f(x) =</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    \omega \frac{x^{\nu_1-1}}{\Gamma(\nu_1)\lambda_1^{\nu_1}}\ \exp \left<span class="sc">\{</span>\frac{x}{\lambda_1}\right<span class="sc">\}</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">    + </span>(1-\omega) \frac{x^{\nu_2-1}}{\Gamma(\nu_2)\lambda_2^{\nu_2}}\ \exp \left<span class="sc">\{</span>\frac{x}{\lambda_2}\right<span class="sc">\}</span> &amp; x &gt; 0 <span class="sc">\\</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    0 &amp; \text{otherwise}</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gamma-mixture-pdf}</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Mean and Variance of a Mixture:</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>\mathbb{E}_F(X) = \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}<span class="co">[</span><span class="ot">X</span><span class="co">]</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mixture-mean}</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>\operatorname{Var}_F(X) &amp; = \mathbb{E}_F(X^2) - <span class="sc">\{</span>\mathbb{E}_F(X)<span class="sc">\}</span>^2 <span class="sc">\\</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{k=1}^K \omega_k \left<span class="sc">\{</span> \mathbb{E}_{G_k}(X^2) \right\} - \left\{ \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}(X) \right<span class="sc">\}</span>^2 <span class="sc">\\</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{k=1}^K \omega_k \left<span class="sc">\{</span> \operatorname{Var}_{G_k}(X) + [\mathbb{E}_{G_k}(X)]^2 \right\} - \left\{ \sum_{k=1}^K \omega_k \mathbb{E}_{G_k}(X) \right<span class="sc">\}</span>^2</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mixture-variance}</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Special Case (Component means zero):</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>\operatorname{Var}_F(X) = \sum_{k=1}^K \omega_k \operatorname{Var}_{G_k}(X)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mixture-variance-zero-mean}</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1.2 Why finite mixture models?</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>Finite mixtures of distributions within a single family provide a lot of flexibility. For example, a mixture of Gaussian distributions can have a bimodal density.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Example: Bimodal Mixture:</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>f(x) = 0.6 \frac{1}{\sqrt{2\pi}} \exp\left<span class="sc">\{</span> -\frac{1}{2}x^2 \right<span class="sc">\}</span> </span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="ss">     + </span>0.4 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left<span class="sc">\{</span> -\frac{1}{2}\frac{(x-5)^2}{4} \right<span class="sc">\}</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>$$ {#eq-bimodal-mixture}</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>Example: Skewed Unimodal Mixture:</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>f(x) = 0.55 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left<span class="sc">\{</span> -\frac{1}{2} \frac{x^2}{2} \right<span class="sc">\}</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="ss">    + </span>0.45 \frac{1}{\sqrt{2\pi} 2} \exp\left<span class="sc">\{</span> -\frac{1}{2}\left(\frac{x-3}{2}\right)^2 \right<span class="sc">\}</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>$$ {#eq-skewed-unimodal-mixture}</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>Example: Symmetric Heavy-tailed Mixture:</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>f(x) = 0.4 \frac{1}{\sqrt{2\pi}\sqrt{2}} \exp\left<span class="sc">\{</span> -\frac{1}{2} \frac{x^2}{2} \right<span class="sc">\}</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="ss">    + </span>0.4 \frac{1}{\sqrt{2\pi} 4} \exp\left<span class="sc">\{</span> -\frac{1}{2} \frac{x^2}{16} \right<span class="sc">\}</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="ss">    + </span>0.2 \frac{1}{\sqrt{2\pi} \sqrt{20}} \exp\left<span class="sc">\{</span> -\frac{1}{2} \frac{x^2}{20} \right<span class="sc">\}</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>$$ {#eq-symmetric-heavy-tailed-mixture}</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>Zero-inflated Negative Binomial PMF:</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(x) = </span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>    \omega_1 + (1-\omega_1)\theta^r &amp; x=0 <span class="sc">\\</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>    (1-\omega_1) \binom{x+r-1}{x} \theta^r (1-\theta)^x &amp; x&gt;1</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>$$ {#eq-zero-inflated-negative-binomial-pmf}</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="ss">12. </span>Regular Negative Binomial PMF:</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>p^*(x) = \binom{x+r-1}{x} \theta^r (1-\theta)^x</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>$$ {#eq-regular-negative-binomial-pmf}</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="ss">13. </span>Zero-inflated Log-Gaussian PDF:</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>f(x) = </span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>    \omega_1 \delta_0(x) + (1-\omega_1)\frac{1}{\sqrt{2\pi}\sigma x}\exp\left<span class="sc">\{</span> -\frac{(\ln x - \mu)^2}{2\sigma^2} \right<span class="sc">\}</span> &amp; x &gt; 0 <span class="sc">\\</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>    0 &amp; \text{otherwise}</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>$$ {#eq-zero-inflated-log-gaussian-pdf}</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>where $\delta_0(x)$ is the Dirac delta function at $x=0$.</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1.3 hierarchical representation of finite mixtures</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a><span class="ss">14. </span>Mixture Model (Hierarchical):</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>X \mid c \sim G_c, \quad \mathbb{P}r(c = k) = \omega_k</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hierarchical-mixture}</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>where $G_c$ is the distribution of the $k$-th component and $\omega_k$ is the weight for the $k$-th component.</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1.4 The likelihood function for mixture models</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a><span class="ss">15. </span>Observed-data Likelihood for a mixture Model</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>L_O(\theta, \omega; x) \propto \mathbb{P}r(x \mid \theta, \omega) = \prod_{i=1}^n \sum_{k=1}^K \omega_k g_k(x_i \mid \theta_k)</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>$$ {#eq-observed-data-likelihood}</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>where $g_k(x_i \mid \theta_k)$ is the PDF/PMF of the $k$-th component distribution evaluated at $x_i$ with parameter $\theta_k$.</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="ss">16. </span>Mixture Model (Likelihood, complete-data):</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>L(\theta, \omega; x, c) = \mathbb{P}r(x, c \mid \theta, \omega) = \prod_{i=1}^n \prod_{k=1}^K <span class="co">[</span><span class="ot">\omega_k g_k(x_i \mid \theta_k)</span><span class="co">]</span>^{1(c_i = k)}</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>$$ {#eq-complete-data-likelihood}</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>where $1(c_i = k)$ is an indicator function that is 1 if $c_i = k$ and 0 otherwise.</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="ss">17. </span>Alternative complete-data likelihood decomposition:</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(x, c \mid \theta, \omega) = \mathbb{P}r(x \mid c, \theta) \mathbb{P}r(c \mid \omega)</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>$$ {#eq-complete-data-likelihood-decomposition}</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(x \mid c, \theta) = \prod_{i=1}^n g_{c_i}(x_i \mid \theta_{c_i})</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>$$ {#eq-complete-data-likelihood-x}</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c \mid \omega) = \prod_{k=1}^K \omega_k^{\sum_{i=1}^n 1(c_i = k)}</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>$$ {#eq-complete-data-likelihood-c}</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>where $1(c_i = k)$ is an indicator function that is 1 if $c_i = k$ and 0 otherwise.</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1.5 parameter identifiability</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>Label switching</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>TODO : missing formula</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>f(x) = ...</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>$$ {#eq-parameter-identifiability-mix1}</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>TODO : missing formula</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>f(x) = ...</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>$$ {#eq-parameter-identifiability-mix2}</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>Number of components</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>f(x) = ...</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>$$ {#eq-parameter-identifiability-mix3}</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2. Maximum Likelihood Estimation For Mixture Models:</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="ss">18. </span>Maximum Likelihood Estimator (MLE) for Mixture:</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>(\hat{\omega}, \hat{\theta}) = \arg\max_{\omega, \theta} \prod_{i=1}^n \sum_{k=1}^K \omega_k g_k(x_i \mid \theta_k)</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>$$ {#eq-max-observed-data-likelihood}</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>where $\hat{\omega}$ and $\hat{\theta}$ are the MLEs for the weights and parameters of the mixture components, respectively.</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2.1 EM Algorithm for Mixture Models:</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a><span class="ss">19. </span>EM algorithm Set Q-function in E step:</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>E step:</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>Q(\omega, \theta \mid \omega^{(t)}, \theta^{(t)}, x) = \mathbb{E}_{c \mid \omega^{(t)}, \theta^{(t)}, x} <span class="co">[</span><span class="ot">\log \mathbb{P}r(x, c \mid \omega, \theta)</span><span class="co">]</span></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>$$ {#eq-em-Q-function}</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>where $Q(\omega, \theta \mid \omega^{(t)}, \theta^{(t)}, x)$ is the expected complete-data log-likelihood given the current estimates of the parameters $\omega^{(t)}$ and $\theta^{(t)}$ and the observed data $x$.</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a><span class="ss">19. </span>EM algorithm Set parameters in M step:</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)}) = \arg\max_{\omega, \theta} Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, y)   </span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>$$ {#eq-em-m-step}</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>where $(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)})$ are the updated estimates of the parameters after the M step.</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- missing em stuff here --&gt;</span></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c_i = k \mid \omega, \theta, x) = \frac{\omega_k g_k(x_i \mid \theta_k)}{\sum_{l=1}^K \omega_l g_l(x_i \mid \theta_l)} = v_{i,k}(\omega, \theta)</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>$$ {#eq-conditional-independence-of-components}</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>where $v_{i,k}(\omega, \theta)$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\omega$ and $\theta$.</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>and $(\hat{\omega}^{(t+1)}, \hat{\theta}^{(t+1)})$ are the updated estimates of the parameters after the M step.</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>Also, remember that:</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(x, c | \theta, \omega) =</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>\prod_{i=1}^n</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>\prod_{k=1}^K</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">\omega_k g_k(x_i | \theta_k)</span><span class="co">]</span>^{\mathbb{1}(c_i=k)}</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>$$ {#eq-em-m-step}</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>The value of $v_{i,k}(\omega, \theta)$ can be interpreted as the probability that observation $i$ was generated from component $k$ if we assume that the true parameters of</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>the mixture model are $\omega$ and $\theta$.</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- missing em stuff here --&gt;</span></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c_i = k \mid \omega, \theta, x) = \frac{\omega_k g_k(x_i \mid \theta_k)}{\sum_{l=1}^K \omega_l g_l(x_i \mid \theta_l)} = v_{i,k}(\omega, \theta)</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>$$ {#eq-conditional-independence-of-components}</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>where $v_{i,k}(\omega, \theta)$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\omega$ and $\theta$.</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>which implies that </span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>\log \mathbb{P}r(x_i,c \mid \theta, \omega) = \sum_{n=1}^N \sum_{k=1}^K \mathbb{1}(c_i=k) <span class="co">[</span><span class="ot">\log(\omega_k) + \log(g_k(x_i \mid \theta_k))</span><span class="co">]</span></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>$$ {#eq-log-likelihood}</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>where $\mathbb{1}(c_i=k)$ is an indicator function that is 1 if $c_i = k$ and 0 otherwise.</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>Hence </span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x) = \sum_{i=1}^n \sum_{k=1}^K v_{i,k} \mathbb{E}_{c \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x} \left [\mathbb{1}_{(c_i=k)} \log(\omega_k) + \log(g_k(x_i \mid \theta_k))\right]</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>$$ {#eq-em-Q-function}</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>where $v_{i,k}(\omega^{(t)}, \theta^{(t)})$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\omega^{(t)}$ and $\theta^{(t)}$.</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>and therefore</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>Q(\omega, \theta \mid \hat{\omega}^{(t)}, \hat{\theta}^{(t)}, x) = \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)}\left(\hat{\omega}^{(t)}, \hat{\theta}^{(t)}\right) <span class="co">[</span><span class="ot">\log(\omega_k) + \log(g_k(x_i \mid \theta_k))</span><span class="co">]</span></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>$$ {#eq-em-Q-function}</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>where $v_{i,k}(\hat{\omega}^{(t)}, \hat{\theta}^{(t)})$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\hat{\omega}^{(t)}$ and $\hat{\theta}^{(t)}$.</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>(Remember that the term that is constant with respect to c can come out of the expectation, and that the expected value of an indicator function is just the probability of the event inside the indicator). </span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>Hence, roughly speaking, we can see that the Q function is in this case equivalent to a weighted average of the log likelihoods associated with each of the components in the mixture.</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.2 The EM algorithm for a Location Mixture of Two Gaussians</span></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>f(x\mid \omega, \mu_1,\mu_2, \sigma) = \omega \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left<span class="sc">\{</span> -\frac{(x - \mu_1)^2}{2\sigma^2} \right<span class="sc">\}</span> + (1-\omega) \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left<span class="sc">\{</span> -\frac{(x - \mu_2)^2}{2\sigma^2} \right<span class="sc">\}</span></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gaussian-location-mixture}</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>where $\omega_1$ and $\omega_2$ are the weights of the two components, $\mu_1$ and $\mu_2$ are the means of the two components, and $\sigma^2$ is the common variance.</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>The expected weights are:</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>v^{(t+1)}_{i,1} =v^{(t+1)}_{i,1} \left (\hat{\omega}^{(t)},\hat{\mu}^{(t)},\hat{\sigma}^{(t)}\right)=\frac{\hat{\omega}^{(t)}_1 \frac{1}{\sqrt{2\pi}\hat{\sigma}^{(t)}} \exp\left\{ -\frac{(x_i - \hat{\mu}^{(t)}_1)^2}{2\hat{\sigma}^{(t)}} \right\}}{\sum_{k=1}^K \hat{\omega}^{(t)}_k \frac{1}{\sqrt{2\pi}\hat{\sigma}^{(t)}} \exp\left<span class="sc">\{</span> -\frac{(x_i - \hat{\mu}^{(t)}_k)^2}{2\hat{\sigma}^{(t)}} \right<span class="sc">\}</span>}</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>$$ {#eq-location-mixture-weights}</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>v^{(t+1)}_{i,2} = 1 - v^{(t+1)}_{i,1}</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>Therefore, the Q function is</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>Q(\omega, \mu_1, \mu_2, \sigma \mid \hat{\omega}^{(t)}, \hat{\mu}^{(t)}_1,\hat{\mu}^{(t)}_2, \hat{\sigma}^{(t)}, x) = \sum_{i=1}^n &amp; v_{i,1}^{(t+1)} \left <span class="co">[</span><span class="ot">\log(\omega) - \frac{1}{2} \log(2\pi) - \log(\sigma) - \frac{(x_i - \mu_1)^2}{2\sigma^2}\right </span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>&amp; v_{i,2}^{(t+1)} \left <span class="co">[</span><span class="ot">\log(1-\omega) - \frac{1}{2} \log(2\pi) - \log(\sigma) - \frac{(x_i - \mu_2)^2}{2\sigma^2}\right</span><span class="co">]</span> </span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Q-function}</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>where $v_{i,1}^{(t+1)}$ and $v_{i,2}^{(t+1)}$ are the posterior probabilities of the first and second components given the observed data $x_i$ and the current estimates of the parameters $\hat{\omega}^{(t)}$, $\hat{\mu}^{(t)}_1$, $\hat{\mu}^{(t)}_2$, and $\hat{\sigma}^{(t)}$.</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>The Q function is a function of the parameters $\omega$, $\mu_1$, $\mu_2$, and $\sigma$ and is used to update the estimates of these parameters in the M step of the EM algorithm.</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>to maximize Q we compute its partial derivatives with respect to $\omega$, $\mu_k$, and $\sigma$ and set them equal to zero.</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>The partial derivative with respect to $\omega$ is:</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>\frac{\partial Q}{\partial \omega} = \sum_{i=1}^n \left <span class="co">[</span><span class="ot">\frac{v_{i,1}^{(t+1)}}{\omega} - \frac{v_{i,2}^{(t+1)}}{1-\omega} \right </span><span class="co">]</span> = 0</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>$$ {#eq-partial-derivative-of-Q-wrt-omega}</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>\frac{\partial Q}{\partial \mu_k} = \sum_{i=1}^n v_{i,k}^{(t+1)} \frac{1}{\sigma^2} (x_i - \mu_k) = 0</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>$$ {#eq-partial-derivative-of-Q-wrt-mu}</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>\frac{\partial Q}{\partial \sigma} = \sum_{i=1}^n \left <span class="co">[</span><span class="ot">\frac{v_{i,1}^{(t+1)}}{\sigma} - \frac{(x_i - \mu_1)^2}{\sigma^3} + \frac{v_{i,2}^{(t+1)}}{\sigma} - \frac{(x_i - \mu_2)^2}{\sigma^3} \right </span><span class="co">]</span> = 0</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>$$ {#eq-partial-derivative-of-Q-wrt-sigma}</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>By setting (@eq-partial-derivative-of-Q-wrt-omega) equal to zero we get:</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>&amp;\left <span class="sc">\{</span> \sum_{i=1}^n v_{i,2}^{(t+1)} \right <span class="sc">\}</span>  \omega^{(t+1)} &amp; = &amp;</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>\left <span class="sc">\{</span> \sum_{i=1}^n v_{i,1}^{(t+1)} \right <span class="sc">\}</span> \left (1 - \omega^{(t+1)}\right) </span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \implies &amp; \left <span class="sc">\{</span> \sum_{i=1}^n v_{i,1}^{(t+1)} \right <span class="sc">\}</span> &amp; = &amp;</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>\left <span class="sc">\{</span> \sum_{i=1}^n v_{i,1}^{(t+1)} + \sum_{i=1}^n v_{i,2}^{(t+1)} \right <span class="sc">\}</span> \omega^{(t+1)}</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \implies &amp; \omega^{(t+1)} &amp; = &amp; \frac{\sum_{i=1}^n v_{i,1}^{(t+1)}}{\sum_{i=1}^n v_{i,1}^{(t+1)} + v_{i,2}^{(t+1)}} </span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;  &amp;= &amp; \frac{1}{n} \sum_{i=1}^n v_{i,1}^{(t+1)}</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>$$ {#eq-omega-update}</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>where $\omega^{(t+1)}$ is the updated estimate of the weight for the first component after the M step.</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>where we used the fact that $\sum_{i=1}^n v_{i,1}^{(t+1)} + \sum_{i=1}^n v_{i,2}^{(t+1)} = n$.</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>This means that the new estimate of $\omega$ is the average of the posterior probabilities of the first component over all observations.</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>by setting (@eq-partial-derivative-of-Q-wrt-mu) equal to zero we get:</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a>            &amp;&amp; 0 &amp;&amp; = &amp; \sum_{i=1}^n v_{i,k}^{(t+1)} (x_i - \mu_k) </span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \implies &amp;&amp; \sum_{i=1}^n v_{i,k}^{(t+1)} x_i &amp;&amp; = &amp; \sum_{i=1}^n v_{i,k}^{(t+1)} \mu_k</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \implies &amp;&amp; \mu_k^{(t+1)} &amp;&amp; = &amp; \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} x_i}{\sum_{i=1}^n v_{i,k}^{(t+1)}}</span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a>$$ {#eq-em-solving-for-mu}</span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>where we used the fact that $\sum_{i=1}^n v_{i,k}^{(t+1)} = n$.</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>Note that the partial estimator for μk is a weighted average of the xis, with the weight associated with observation i being proportional to the probability that such observation was generated by component k. Again, if the components are well separated and values of vi,k are all close to either 0 or 1, this weighted average is roughly the average of the observations coming from component k</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>Finally, making the same argument for the partial derivative with respect to σ, we get:</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>&amp; </span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>    \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} &amp;=&amp; \left ( \frac {1}{\sigma^{(t+1)}} \right ) ^ 2  \sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)})^2</span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \implies &amp;</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a> \sigma^{(t+1)} &amp;=&amp; \sqrt{\frac{\sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)})^2}{\sum_{i=1}^n \sum_{k=1}^K v_{i,k}^{(t+1)}}}</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a>$$ {#eq-em-solving-for-sigma}</span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2.3 general location and scale mixtures of p-variate gaussian distributions</span></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>f(x) =</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>\sum_{k=1}^K</span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>\omega_k</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>\left( \frac{1}{(2\pi)} \right) ^\frac{q}{2}</span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>\Sigma_k<span class="pp">|</span>-\frac{1}{2} \exp</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>(x - \theta_k)^T \Sigma_k^{-1}(x - \theta_k)</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>\right<span class="sc">\}</span></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a>$$ {#eq-general-location-scale-mixture}</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a>where $\theta_k$ is the mean of the $k$-th component, $\Sigma_k$ is the covariance matrix of the $k$-th component, and $\omega_k$ is the weight for the $k$-th component.</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a>v_{i,k}^{(t+1)} &amp;= \frac{\omega_k^{(t)} |\Sigma_k^{(t)}|^{-1/2} \exp\left<span class="sc">\{</span>-\frac{1}{2}(x_i - \mu_k^{(t)})^T <span class="co">[</span><span class="ot">\Sigma_k^{(t)}</span><span class="co">]</span>^{-1}(x_i - \mu_k^{(t)})\right<span class="sc">\}</span>}{\sum_{l=1}^K \omega_l^{(t)} |\Sigma_l^{(t)}|^{-1/2} \exp\left<span class="sc">\{</span>-\frac{1}{2}(x_i - \mu_l^{(t)})^T <span class="co">[</span><span class="ot">\Sigma_l^{(t)}</span><span class="co">]</span>^{-1}(x_i - \mu_l^{(t)})\right<span class="sc">\}</span>} <span class="sc">\\</span></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a>\omega_k^{(t+1)} &amp;= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)}}{\sum_{i=1}^n \sum_{l=1}^K v_{i,l}^{(t+1)}} <span class="sc">\\</span></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>\mu_k^{(t+1)} &amp;= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} x_i}{\sum_{i=1}^n v_{i,k}^{(t+1)}} <span class="sc">\\</span></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>\Sigma_k^{(t+1)} &amp;= \frac{\sum_{i=1}^n v_{i,k}^{(t+1)} (x_i - \mu_k^{(t+1)}) (x_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^n v_{i,k}^{(t+1)}}</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a>$$ {#eq-em-p-variate-mixture-parameters}</span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a>where $v_{i,k}^{(t+1)}$ is the posterior probability of the $k$-th component given the observed data $x_i$ and the current estimates of the parameters $\omega^{(t)}$, $\mu^{(t)}$, and $\Sigma^{(t)}$.</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3. MCMC</span></span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a>f(x) = \sum_{k=1}^K \omega_k g_k(x | \theta_k)</span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-mixture}</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>where $g_k(x \mid \theta_k)$ is the PDF/PMF of the $k$-th component distribution evaluated at $x$ with parameter $\theta_k$.</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\omega) \;=\;</span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>\frac{\Gamma \bigl ( \sum_{k=1}^K a_k \bigr ) }{\prod_{k=1}^K \Gamma(a_k)}</span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a>\prod_{k=1}^K \omega_k^{\,a_k-1},</span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>\sum_{k=1}^K \omega_k = 1</span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>$$ {#eq-dirichlet-prior}</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3.1 Markov chain monte carlo algorithms for mixture mod-</span></span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>els</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(x, c \mid \theta, \omega) = \mathbb{P}r(x \mid c, \theta)\ \mathbb{P}r(c \mid \omega)</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>$$ {#eq-pxc-given-theta-omega}</span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>where </span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(x \mid c, \theta) = \prod_{i=1}^n g_{c_i}(x_i \mid \theta_{c_i})</span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a>$$ {#eq-px-given-c-theta}</span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c \mid \omega) = \prod_{i=1}^n \prod_{k=1}^K </span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>\omega_k ^{\mathbb{1}(c_i = k)} =</span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>\prod_{k=1}^K \omega_k^{\sum_{i=1}^n \mathbb{1}(c_i = k)} </span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a>$$ {#eq-pc-given-omega}</span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a>where $1(c_i = k)$ is an indicator function that is 1 if $c_i = k$ and 0 otherwise.</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a><span class="ss">21. </span>Joint posterior (with latent labels)</span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c,\theta,\omega \mid x)</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a>\;\propto\;</span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>\Bigl(\prod_{i=1}^n g_{c_i}(x_i\mid \theta_{c_i})\Bigr)</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a>\Bigl(\prod_{k=1}^K \omega_k^{\sum_{i=1}^n1(c_i=k)}\Bigr)</span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>\,p(\omega)\,p(\theta)</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a>$$ {#eq-joint-posterior}</span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a>Each of the full conditional distributions can be derived from this joint posterior by retaining the terms that involve the parameter of interest, and recognizing the product of the selected terms as the kernel of a known</span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>family of distributions.</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a><span class="ss">22. </span>Full conditional for $\omega$</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\omega \mid c,\theta,x)</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a>\;\propto\;</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c \mid \omega) \mathbb{P}r(\omega)</span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a>\;=\;</span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a>\prod_{k=1}^K</span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a>\omega_k^{\,a_k + \sum_{i=1}^n1(c_i=k)\;-\;1}</span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a>$$ {#eq-full-cond-omega}</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a>This clearly corresponds to the kernel of another Dirichlet distribution</span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a>with updated parameters</span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a>a_k^* = a_k + m_k, \qquad k = 1, \ldots, K,</span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a>where $m_k = \sum_{i=1}^n 1(c_i = k)$ is the number of observations in component $k$.</span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a>Full conditional for each component $c_i$</span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c_i \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x) \propto \mathbb{P}r(x_i \mid c_i,\theta_k) \mathbb{P}r(c_i \mid \omega)</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a>$$ {#eq-fullcond-ci-1}</span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a>hence</span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c_i = k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)</span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a>\;=\;</span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a>\frac{\omega_k\,g_k(x_i\mid \theta_k)}</span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a>     {\sum_{l=1}^K \omega_l\,g_l(x_i\mid \theta_l)}</span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>$$ {#eq-fullcond-ci-2}</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>Note the similarity with the formula for the expected weights $v_{i,k}$ in the</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a>EM algorithm.) </span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a>Full conditional for component parameters $\theta_k$</span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)</span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a>\;\propto\;</span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x)</span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a>\prod_{i:c_i=k} \mathbb{P}r(x_i\mid \theta_k)</span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a>$$ {#eq-fullcond-theta_k}</span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a>In the most common case in which the priors for $\theta_k$ are independent this is simply:</span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta_k \mid c,\theta_1,\ldots,\theta_{k-1},\omega,x) </span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a>\;\propto\;</span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta_k) \prod_{i:c_i=k} \mathbb{P}r(x_i\mid \theta_k)</span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a>$$ {#eq-fullcond-theta_k-indep}</span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3.2 The MCMC algorithm for a location mixture of two gaussian distributions</span></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>f(x \mid \omega, \mu_1, \mu_2, \sigma) = \omega \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left<span class="sc">\{</span> -\frac{(x - \mu_1)^2}{2\sigma^2} \right<span class="sc">\}</span> + (1-\omega) \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left<span class="sc">\{</span> -\frac{(x - \mu_2)^2}{2\sigma^2} \right<span class="sc">\}</span></span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-location-mixture}</span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a><span class="ss">25. </span>Gaussian prior for means $\mu<span class="sc">\_</span>k$</span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\mu_k)</span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a>=\frac{1}{\sqrt{2\pi\tau^2}}</span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a>\exp<span class="sc">\!</span>\Bigl<span class="sc">\{</span>-\tfrac{(\mu_k-\eta)^2}{2\tau^2}\Bigr<span class="sc">\}</span></span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a>$$ {#eq-prior-muk}</span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a><span class="ss">26. </span>Inverse-Gamma prior for variance $\sigma^2$:</span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a>with shape parameter $a$ and scale parameter $b$ for $\sigma$</span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\sigma^2)</span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a>=\frac{1}{b^a\Gamma(a)}</span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a>(\sigma^2)^{-\,d-1}</span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a>\exp<span class="sc">\!</span>\Bigl<span class="sc">\{</span>-\tfrac{q}{\sigma^2}\Bigr<span class="sc">\}</span></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a>$$ {#eq-prior-sigma2}</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\mu_k \mid c, \mu_1, \ldots, \mu_{k-1}, \mu_{k+1}, \ldots, \mu_K, \omega, x) </span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a>&amp; \;\propto\;</span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a>\exp\Biggl<span class="sc">\{</span></span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a>-\frac{(\mu_k - \eta)^2}{2\tau^2}</span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a>\Biggr<span class="sc">\}</span> \prod_{i:c_i=k}</span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a>\exp\Biggl<span class="sc">\{</span></span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a>-\frac{(x_i - \mu_k)^2}{2\sigma^2}</span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a>\Biggr<span class="sc">\}</span></span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp; \;\propto\;</span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a>\exp\Biggl<span class="sc">\{</span></span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a>-\frac{1}{2}</span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a>\Biggl<span class="co">[</span><span class="ot"> m_k \frac{1}{\sigma^2} - 2 \frac{\mu_k \sum_{i:c_i=k} x_i}{\sigma^2} + \frac{\mu_k}{\tau^2} - 2 \frac{\mu_k\eta}{\tau^2} \Biggr</span><span class="co">]</span> \Biggr<span class="sc">\}</span></span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp; \;\propto\;</span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a>\exp\Biggl<span class="sc">\{</span></span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a>-\frac{1}{2}</span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a>\Biggl<span class="co">[</span><span class="ot"> \frac{m_k}{\sigma^2} + \frac{1}{\tau^2} \Biggr</span><span class="co">]</span> \Biggl<span class="co">[</span><span class="ot"> \mu_k - \frac{\sigma^2 \sum_{i:c_i=k} x_i + \frac{\eta}{\tau^2} }{\mu_k + \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2}} \Biggr</span><span class="co">]</span> \Biggr<span class="sc">\}</span></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a>$$ {#eq-fullcond-mu_k}</span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a>which is just the kernel of a normal distribution with updated mean</span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a>\eta_k^* = \frac{\frac{1}{\sigma^2}\sum_{i:c_i=k} x_i + \frac{\eta}{\tau^2}}{ \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2}u^2}</span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a>$$ {#eq-eta-k-star}</span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a>and updated standard deviation</span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a>\tau_k^*=\Bigl<span class="co">[</span><span class="ot"> \frac{m_k}{\sigma^2}+ \frac{1}{\tau^2} \Bigr</span><span class="co">]</span>^{-1/2}</span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a>$$ {#eq-post-muk}</span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a>finaly </span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\sigma^2 | c, \mu, \omega, x) </span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a>&amp;\propto </span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a>\Biggl(\frac{1}{\sigma^2}\Biggr)^{d+1}</span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a>\exp\Biggl<span class="sc">\{</span></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a>− q</span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a>\sigma^2</span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a>\Biggr<span class="sc">\}</span></span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a>\Biggl(\frac{1}{\sigma^2}\Biggr)^{n/2}</span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a>\exp\Biggl<span class="sc">\{</span></span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a>− \frac{1}{2\sigma^2}</span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^{n}(x_i − \mu_{c_i})^2</span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a>\Biggr<span class="sc">\}</span></span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>&amp;=</span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a>\Biggl(\frac{1}{\sigma^2}\Biggr)^{n/2+d+1}</span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a>\exp\Biggl<span class="sc">\{</span></span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a>− \frac{1}{\sigma^2}</span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a>\Biggl[ \frac{1}{2} \sum_{i=1}^{n}(x_i − \mu_{c_i})^2</span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>q</span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a>\Biggr]</span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a>\Biggr<span class="sc">\}</span></span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a>which is the kernel of another inverse Gamma distribution with shape d∗ =n/2 + d and rate parameter</span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a>q^*=\tfrac12\sum_{i=1}^n(x_i-\mu_{c_i})^2 + q</span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a>$$ {#eq-post-sigma2}</span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3.3 general location and scale mixtures of p-variate gaus-</span></span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a>sian distributions</span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a><span class="ss">29. </span>General $q$-variate Gaussian mixture</span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a>\sigma^2 \;\sim\;\mathrm{InvGamma}\bigl(d^*,\,q^*\bigr),</span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a>d^*=\tfrac{n}{2}+d,</span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a>q^*=\tfrac12\sum_{i=1}^n(x_i-\mu_{c_i})^2 + q</span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a>$$ {#eq-post-sigma2}</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a>f(x)</span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a>=\sum_{k=1}^K</span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a>\omega_k\,</span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a>\frac{1}{(2\pi)^{q/2}\,\lvert\Sigma_k\rvert^{1/2}}\,</span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a>\exp<span class="sc">\!</span>\Bigl<span class="sc">\{</span>-\tfrac12(x-\mu_k)^\top\Sigma_k^{-1}(x-\mu_k)\Bigr<span class="sc">\}</span></span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a>$$ {#eq-multivariate-mixture}</span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a><span class="ss">30. </span>Posterior for multivariate $\mu<span class="sc">\_</span>k$</span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a>\mu_k \;\sim\; N\bigl(b_k^*,\,B_k^*\bigr),</span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a>B_k^*=\bigl(B^{-1}+m_k\,\Sigma_k^{-1}\bigr)^{-1},</span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a>b_k^*=B_k^*\bigl(B^{-1}b + \Sigma_k^{-1}\sum_{i:c_i=k}x_i\bigr)</span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a>$$ {#eq-post-multivariate-mu}</span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a><span class="ss">31. </span>Posterior for multivariate $\Sigma<span class="sc">\_</span>k$</span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a>\Sigma_k \;\sim\;\mathrm{InvWishart}\bigl(\nu^*,\,S^*\bigr),</span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>\nu^*=\nu + m_k,</span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a>\quad</span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>S^*=S + \sum_{i:c_i=k}(x_i-\mu_k)(x_i-\mu_k)^\top</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a>$$ {#eq-post-multivariate-sigma}</span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a><span class="fu">## 4. Applications of Mixture Models</span></span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4.1 density estimation</span></span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a><span class="ss">32. </span>Kernel density estimator (general form)</span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a>\tilde f(x)</span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a>=\frac{1}{n}\sum_{i=1}^n\frac{1}{h}\,</span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a>g \Bigl(\tfrac{<span class="sc">\|</span>x-x_i <span class="sc">\|</span> }{h}\Bigr)</span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a>$$ {#eq-kde-general}</span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a><span class="ss">33. </span>Gaussian kernel density estimator</span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a>\tilde f(x)</span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a>=\sum_{i=1}^n\frac{1}{n}\,</span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a>\tfrac{1}{\sqrt{2\pi\,}h}\,</span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a>\exp<span class="sc">\!</span>\Bigl<span class="sc">\{</span>-\tfrac{1}{2}\left(\dfrac{x-x_i}{h}\right)^2\Bigr<span class="sc">\}</span></span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a>$$ {#eq-kde-gaussian}</span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a>In order to understand the relationship between kernel density estimation</span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a>and mixture models it is useful to contrast (8) with the density estimate</span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a>\hat f(x) = \sum_{k=1}^K \hat{\omega}_k \frac{1}{\sqrt{2\pi} \hat{\sigma}} \exp\Biggl<span class="sc">\{</span>-\frac{1}{2\hat{\sigma}^2}(x - \hat{\mu}_k)^2\Biggr<span class="sc">\}</span></span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a>obtained by plugging-in the maximum likelihood estimates of the parameters, $\hat{\omega}_1, . . . , \hat{\omega}_K, \hat{\mu}_1, . . . , \hat{\mu}_K$ and $\hat{\sigma}$ of a location mixture of K univariate Gaussian distributions.</span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4.2 clustering (unsupervised classification</span></span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a>f(x) = \sum_{k=1}^K \frac{1}{K} \left(\frac{1}{\sqrt{2\pi\sigma}} \right)^p \exp\left<span class="sc">\{</span>-\frac{1}{2\sigma^2} (x − \mu_k)^T (x − \mu_k)\right<span class="sc">\}</span></span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a>$$ {#eq-clustering}</span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4.3 (supervised) classification</span></span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5. Practical Considerations</span></span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5.1 Ensuring numerical stability when computing class probabilities</span></span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a>\frac{z_k}{\sum_{l=1}^K z_l} &amp;= \frac{\exp<span class="sc">\{</span> \log z_k <span class="sc">\}</span>}{\sum_{l=1}^K \exp\left<span class="sc">\{</span> \log z_l \right<span class="sc">\}</span>} &amp;&amp;  \text{(exp and log are inverse</span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a>functions)}<span class="sc">\\</span></span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\exp<span class="sc">\{</span> -b <span class="sc">\}</span>\exp<span class="sc">\{</span> \log z_k <span class="sc">\}</span>}{\exp<span class="sc">\{</span> -b <span class="sc">\}</span>\sum_{l=1}^K \exp\left<span class="sc">\{</span> \log z_l \right<span class="sc">\}</span>} &amp;&amp; \text{(multiply and divide by</span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a>the same quantity)} \ e^{-b} <span class="sc">\\</span></span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\exp\left<span class="sc">\{</span> \log z_k - b \right<span class="sc">\}</span>}{\sum_{l=1}^K \exp\left<span class="sc">\{</span> \log z_l - b \right<span class="sc">\}</span>} <span class="sc">\\</span></span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a>$$ {#eq-zk-sum}</span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a>Although (11) is valid for any b, it should be clear that some values will</span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a>work better than others for the purpose of avoiding a 0/0 calculation. In</span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a>particular, we are interested in choosing a value b that makes at least one of</span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a>the terms in the denominator different from zero after exponentiation. One</span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a>such choice is $b=\max_{l=1,\ldots,K} \log z_l$, which gives us</span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a>\sum_{l=1}^K \exp\left<span class="sc">\{</span> \log z_l - \max_{l=1,\ldots,K} \log z_l \right<span class="sc">\}</span> = 1 + \sum_{l:l \neq l^*} \exp\left<span class="sc">\{</span> \log z_l - \max_{l=1,\ldots,K} \log z_l \right<span class="sc">\}</span></span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-722"><a href="#cb1-722" aria-hidden="true" tabindex="-1"></a>One key advantage of this choice is that all the terms in the sum are less or equal than one, which ensures that we do not overflow</span>
<span id="cb1-723"><a href="#cb1-723" aria-hidden="true" tabindex="-1"></a>when computing  $\exp\left<span class="sc">\{</span> \log z_l - \max_{l=1,\ldots,K} \log z_l \right<span class="sc">\}</span>$.</span>
<span id="cb1-724"><a href="#cb1-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-725"><a href="#cb1-725" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5.2 numerical consequences of multimodality</span></span>
<span id="cb1-726"><a href="#cb1-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-727"><a href="#cb1-727" aria-hidden="true" tabindex="-1"></a>no math in this section</span>
<span id="cb1-728"><a href="#cb1-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-729"><a href="#cb1-729" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5.3 selecting the number components</span></span>
<span id="cb1-730"><a href="#cb1-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-731"><a href="#cb1-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-732"><a href="#cb1-732" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-733"><a href="#cb1-733" aria-hidden="true" tabindex="-1"></a>\text{BIC} = -2 \ell(\hat{\theta}) + p \log(n)</span>
<span id="cb1-734"><a href="#cb1-734" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-735"><a href="#cb1-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-736"><a href="#cb1-736" aria-hidden="true" tabindex="-1"></a>where $\ell(\hat{\theta})$ is the maximized log-likelihood, $p$ is number of free parameters, $n$ is sample size.</span>
<span id="cb1-737"><a href="#cb1-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-738"><a href="#cb1-738" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5.4 fully Bayesian inference on the number of components</span></span>
<span id="cb1-739"><a href="#cb1-739" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-740"><a href="#cb1-740" aria-hidden="true" tabindex="-1"></a>f(x) = \sum_{k=1}^K \omega_k g_k(x | \theta_k) + \sum_{k=K+1}^K 0g_k(x | \theta_k)</span>
<span id="cb1-741"><a href="#cb1-741" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-742"><a href="#cb1-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-743"><a href="#cb1-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-744"><a href="#cb1-744" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-745"><a href="#cb1-745" aria-hidden="true" tabindex="-1"></a>(\omega_1, \ldots, \omega_K) \sim Dirichlet( \frac{1}{K}, \ldots , \frac{1}{K} )</span>
<span id="cb1-746"><a href="#cb1-746" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-747"><a href="#cb1-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-748"><a href="#cb1-748" aria-hidden="true" tabindex="-1"></a>where K is the number of components in the mixture model.</span>
<span id="cb1-749"><a href="#cb1-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-750"><a href="#cb1-750" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-751"><a href="#cb1-751" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(K^*)=\sum_{i=1}^m \frac{\alpha}{\alpha + i - 1} \approx \frac{\alpha \log(n + \alpha -1) }{\alpha}</span>
<span id="cb1-752"><a href="#cb1-752" aria-hidden="true" tabindex="-1"></a>$$ {#eq-expected-K}</span>
<span id="cb1-753"><a href="#cb1-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-754"><a href="#cb1-754" aria-hidden="true" tabindex="-1"></a>where $\alpha$ is the concentration parameter of the Dirichlet process prior, $n$ is the number of observations, and $K^*$ is the number of components in the mixture model.</span>
<span id="cb1-755"><a href="#cb1-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-756"><a href="#cb1-756" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5.5 fully Bayesian inference on the partition structure</span></span>
<span id="cb1-757"><a href="#cb1-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-758"><a href="#cb1-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-759"><a href="#cb1-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-760"><a href="#cb1-760" aria-hidden="true" tabindex="-1"></a>Extracting formulas from **Section 4.2 ("Clustering (unsupervised classification)") to the end** of *mixturemodels (2).pdf*:</span>
<span id="cb1-761"><a href="#cb1-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-762"><a href="#cb1-762" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-763"><a href="#cb1-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-764"><a href="#cb1-764" aria-hidden="true" tabindex="-1"></a><span class="fu">## Section 4.2: Clustering (Unsupervised Classification) — Formulas</span></span>
<span id="cb1-765"><a href="#cb1-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-766"><a href="#cb1-766" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Hard assignment (mode assignment for EM):</span></span>
<span id="cb1-767"><a href="#cb1-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-768"><a href="#cb1-768" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-769"><a href="#cb1-769" aria-hidden="true" tabindex="-1"></a>\hat{c}_i = \arg\max_k \hat{v}_{i,k}</span>
<span id="cb1-770"><a href="#cb1-770" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-771"><a href="#cb1-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-772"><a href="#cb1-772" aria-hidden="true" tabindex="-1"></a>where $\hat{v}_{i,k}$ are the final iteration weights.</span>
<span id="cb1-773"><a href="#cb1-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-774"><a href="#cb1-774" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-775"><a href="#cb1-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-776"><a href="#cb1-776" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Posterior Probability of Cluster Assignment (Bayesian approach):</span></span>
<span id="cb1-777"><a href="#cb1-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-778"><a href="#cb1-778" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-779"><a href="#cb1-779" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c_i = k \mid x, \text{rest}) = v_{i,k}</span>
<span id="cb1-780"><a href="#cb1-780" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-781"><a href="#cb1-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-782"><a href="#cb1-782" aria-hidden="true" tabindex="-1"></a>where $v_{i,k}$ is as defined in the EM/MCMC steps (probability that observation $i$ comes from component $k$).</span>
<span id="cb1-783"><a href="#cb1-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-784"><a href="#cb1-784" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-785"><a href="#cb1-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-786"><a href="#cb1-786" aria-hidden="true" tabindex="-1"></a><span class="fu">## Section 4.3: (Supervised) Classification</span></span>
<span id="cb1-787"><a href="#cb1-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-788"><a href="#cb1-788" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Posterior probability of class membership (LDA/QDA/Mixture discriminant analysis):</span></span>
<span id="cb1-789"><a href="#cb1-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-790"><a href="#cb1-790" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-791"><a href="#cb1-791" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(Y = k \mid X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)}</span>
<span id="cb1-792"><a href="#cb1-792" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-793"><a href="#cb1-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-794"><a href="#cb1-794" aria-hidden="true" tabindex="-1"></a>where $\pi_k$ is prior class probability, $f_k(x)$ is the class-conditional density for class $k$.</span>
<span id="cb1-795"><a href="#cb1-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-796"><a href="#cb1-796" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-797"><a href="#cb1-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-798"><a href="#cb1-798" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4. For a mixture model:</span></span>
<span id="cb1-799"><a href="#cb1-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-800"><a href="#cb1-800" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-801"><a href="#cb1-801" aria-hidden="true" tabindex="-1"></a>f_k(x) = \sum_{j=1}^{M_k} w_{k,j} g_{k,j}(x)</span>
<span id="cb1-802"><a href="#cb1-802" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-803"><a href="#cb1-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-804"><a href="#cb1-804" aria-hidden="true" tabindex="-1"></a>where each class-conditional density can itself be a mixture (with weights $w_{k,j}$ and kernels $g_{k,j}(x)$).</span>
<span id="cb1-805"><a href="#cb1-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-806"><a href="#cb1-806" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-807"><a href="#cb1-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-808"><a href="#cb1-808" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5. Posterior probability for classification under mixture model:</span></span>
<span id="cb1-809"><a href="#cb1-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-810"><a href="#cb1-810" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-811"><a href="#cb1-811" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(Y = k \mid X = x) = \frac{\pi_k \sum_{j=1}^{M_k} w_{k,j} g_{k,j}(x)}{\sum_{l=1}^K \pi_l \sum_{j=1}^{M_l} w_{l,j} g_{l,j}(x)}</span>
<span id="cb1-812"><a href="#cb1-812" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-813"><a href="#cb1-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-814"><a href="#cb1-814" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-815"><a href="#cb1-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-816"><a href="#cb1-816" aria-hidden="true" tabindex="-1"></a><span class="fu">## Section 5: Practical Considerations</span></span>
<span id="cb1-817"><a href="#cb1-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-818"><a href="#cb1-818" aria-hidden="true" tabindex="-1"></a><span class="fu">### 6. Log-sum-exp trick (for numerical stability):</span></span>
<span id="cb1-819"><a href="#cb1-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-820"><a href="#cb1-820" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-821"><a href="#cb1-821" aria-hidden="true" tabindex="-1"></a>\log \left( \sum_{k=1}^K e^{a_k} \right) = a_{k^*} + \log \left( \sum_{k=1}^K e^{a_k - a_{k^*}} \right)</span>
<span id="cb1-822"><a href="#cb1-822" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-823"><a href="#cb1-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-824"><a href="#cb1-824" aria-hidden="true" tabindex="-1"></a>where $a_{k^*} = \max<span class="sc">\{</span>a_1, \ldots, a_K<span class="sc">\}</span>$.</span>
<span id="cb1-825"><a href="#cb1-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-826"><a href="#cb1-826" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-827"><a href="#cb1-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-828"><a href="#cb1-828" aria-hidden="true" tabindex="-1"></a><span class="fu">### 7. Bayesian Information Criterion (BIC):</span></span>
<span id="cb1-829"><a href="#cb1-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-830"><a href="#cb1-830" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-831"><a href="#cb1-831" aria-hidden="true" tabindex="-1"></a>\text{BIC} = -2 \ell(\hat{\theta}) + p \log(n)</span>
<span id="cb1-832"><a href="#cb1-832" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-833"><a href="#cb1-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-834"><a href="#cb1-834" aria-hidden="true" tabindex="-1"></a>where $\ell(\hat{\theta})$ is the maximized log-likelihood, $p$ is number of free parameters, $n$ is sample size.</span>
<span id="cb1-835"><a href="#cb1-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-836"><a href="#cb1-836" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-837"><a href="#cb1-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-838"><a href="#cb1-838" aria-hidden="true" tabindex="-1"></a><span class="fu">### 8. Dirichlet process partition probability:</span></span>
<span id="cb1-839"><a href="#cb1-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-840"><a href="#cb1-840" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-841"><a href="#cb1-841" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(c_1, \ldots, c_n) = \frac{\alpha^K \prod_{k=1}^K (n_k - 1)!}{\prod_{j=1}^n (\alpha + j - 1)}</span>
<span id="cb1-842"><a href="#cb1-842" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-843"><a href="#cb1-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-844"><a href="#cb1-844" aria-hidden="true" tabindex="-1"></a>where $K$ is number of clusters, $n_k$ is number of points in cluster $k$, $\alpha$ is the concentration parameter.</span>
<span id="cb1-845"><a href="#cb1-845" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>