<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="keywords" content="Mixture Models, Classification, Naïve Bayes Classifier, Notes">

<title>75&nbsp; Classification - M4L7 – Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C3-L07-Ex1.html" rel="next">
<link href="./C3-L06.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-cec409635dab7d42c7b562035797153a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-cec409635dab7d42c7b562035797153a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(images/banner_deep.jpg);
background-size: cover;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C3-L00.html">Mixture Models</a></li><li class="breadcrumb-item"><a href="./C3-L07.html"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C3-L00.html">Mixture Models</a></li><li class="breadcrumb-item"><a href="./C3-L07.html"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></a></li></ol></nav>
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Bayesian Statistics: Mixture Models - Applications</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Bayesian Statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Mixture Models, Classification, Naïve Bayes Classifier, Notes</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C1-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Concept to Data Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Paradigms of probability - M1L1HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law - M1L2HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probability and Bayes’ Theorem - M1L2HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Distributions - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Random Variables - M1L3HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Homework on Distributions - M1L3HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Frequentist Inference - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Frequentist MLE - M2L3HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Bayesian Inference - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference - M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Priors - M3L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities - M3L6HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Homework on Priors - M2L7HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Poisson Data - M3L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Homework on Poisson Data - M3L8HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Beta Bernoulli - M3L8HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Homework on Exponential Data - M4L9HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Normally distributed Data - M4L10</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Homework on Normal Data - M4L10HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Non-Informative Priors - M4L11</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Homework Alternative Priors - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Brief Review of Regression - M4L12</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW2</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C2-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Techniques and Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm - M2L4HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Gibbs sampling - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Homework Gibbs-Sampling algorithm - M2L22HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Assessing Convergence - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Homework on M-H algorithm M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Linear regression - M3L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Homework on Linear Regression Model Part 1 - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Homework on Deviance information criterion - M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">ANOVA - M3L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Homework on ANOVA - M3L8HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Homework on Multiple Factor ANOVA - M3L8HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Logistic regression - M3L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression - M3L9HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Poisson regression - M4L10</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Homework on Poisson regression - M4L10HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Hierarchical modeling - M4L11</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Capstone Project - M4L12</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models - M4L12HW1</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C3-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mixture Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Definition of Mixture Models - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Likelihood functions for Mixture Models - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Homework The Likelihood function - M1L2HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Homework Identifiability - M1L2HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Homework The likelihood function M1L2HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Homework on simulating from a Poisson Mixture Model - M1L2HW4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model - M1L2HW5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Homework Sim mixture of exponential distributions - M1L2HW6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture models - M2L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">MCMC for Mixture Models - M4L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Density Estimation - M4L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Computational Considerations - M5L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Determining the number of components - M5L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Homework on Bayesian Information Criteria (BIC) - M5L09HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Homework on BIC for zero-inflated mixtures - M5L09HW4</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time Series Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Stationarity, The ACF and the PCF M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">The AR(1) process: definitions and properties - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">The AR(1): MLE and Bayesian inference - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">The AR(p) process - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">Bayesian Inference in the AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Quiz: Spectral representation of the AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1 - M3L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 1 M3L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Seasonal NDLMs M4L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 2 - M4L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Practice Graded Assignment: NDLM data analysis - M4L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C5-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Capstone Project</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Homework - Practice Quiz for Week 1 – M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Homework - first-step-for-the-project – M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Model Selection Criteria - M2L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Homework - Determine the order of your data – M2L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Homework - Calculate DIC for single AR model – M2L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L03.1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">106</span>&nbsp; <span class="chapter-title">Bayesian location mixture of AR(p) models - M3L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L03.2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">107</span>&nbsp; <span class="chapter-title">Determine the number of mixture components</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L03.3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">108</span>&nbsp; <span class="chapter-title">Location and scale mixture of AR model</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C6-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Non-Parametric Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C6-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">109</span>&nbsp; <span class="chapter-title">Gaussian Processes for Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C6-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">110</span>&nbsp; <span class="chapter-title">Dirichlet Process</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C6-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">111</span>&nbsp; <span class="chapter-title">Dirichlet Process</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Appendix: Bayes by backprop</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Appendix: Yule-Walker Equations &amp; Durbin-Levinson Recursion</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Moore-Penrose Inversion &amp; Cholesky Decomposition</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Appendix: Inequalities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Appendix: Wold’s theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-classification-mixture-models" id="toc-sec-classification-mixture-models" class="nav-link active" data-scroll-target="#sec-classification-mixture-models"><span class="header-section-number">75.1</span> Classification with Mixture Models <span class="emoji" data-emoji="movie_camera">🎥</span></a></li>
  <li><a href="#sec-mixture-models-naïve-bayes" id="toc-sec-mixture-models-naïve-bayes" class="nav-link" data-scroll-target="#sec-mixture-models-naïve-bayes"><span class="header-section-number">75.2</span> Mixture Models and Naïve Bayes classifiers <span class="emoji" data-emoji="movie_camera">🎥</span></a></li>
  <li><a href="#naïve-bayes-classifiers-sec-naïve-bayes-classifiers" id="toc-naïve-bayes-classifiers-sec-naïve-bayes-classifiers" class="nav-link" data-scroll-target="#naïve-bayes-classifiers-sec-naïve-bayes-classifiers"><span class="header-section-number">75.3</span> Naïve Bayes classifiers {sec-naïve-bayes-classifiers}</a>
  <ul class="collapse">
  <li><a href="#sec-classification-LDA-EM-Alg" id="toc-sec-classification-LDA-EM-Alg" class="nav-link" data-scroll-target="#sec-classification-LDA-EM-Alg"><span class="header-section-number">75.3.1</span> LDA and the EM algorithm <span class="emoji" data-emoji="movie_camera">🎥</span></a></li>
  </ul></li>
  <li><a href="#sec-classification-linear-quadratic-discriminant-analysis" id="toc-sec-classification-linear-quadratic-discriminant-analysis" class="nav-link" data-scroll-target="#sec-classification-linear-quadratic-discriminant-analysis"><span class="header-section-number">75.4</span> Linear and quadratic discriminant analysis in the context of Mixture Models <span class="emoji" data-emoji="movie_camera">🎥</span></a>
  <ul class="collapse">
  <li><a href="#classification-example" id="toc-classification-example" class="nav-link" data-scroll-target="#classification-example"><span class="header-section-number">75.4.1</span> Classification example</a></li>
  </ul></li>
  <li><a href="#sec-em-algorithm-classification-sample" id="toc-sec-em-algorithm-classification-sample" class="nav-link" data-scroll-target="#sec-em-algorithm-classification-sample"><span class="header-section-number">75.5</span> Sample EM algorithm for classification problems <span class="emoji" data-emoji="spiral_notepad">🗒️</span> <span class="math inline">\mathcal{R}</span></a></li>
  <li><a href="#semi-supervised-quadratic-discriminant-analysis" id="toc-semi-supervised-quadratic-discriminant-analysis" class="nav-link" data-scroll-target="#semi-supervised-quadratic-discriminant-analysis"><span class="header-section-number">76</span> Semi-supervised, quadratic discriminant analysis</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<section id="sec-classification-mixture-models" class="level2" data-number="75.1">
<h2 data-number="75.1" class="anchored" data-anchor-id="sec-classification-mixture-models"><span class="header-section-number">75.1</span> Classification with Mixture Models <span class="emoji" data-emoji="movie_camera">🎥</span></h2>
<p><mark><strong>Classification</strong> is <em>a supervised learning problem where we want to predict the class of a new observation based on its features</em>.</mark></p>
<p>According to the instructor the main difference from clustering is that in classification we have a training set. I would think the main difference is that we have labels for some of the data, while in clustering we do not have labels at all.</p>
<p>The fact that we have labels and a training set means we should know how many classes we have and we can use these labels to train a model and use it to predict the class of a new observation.</p>
<p> The instructor mentions <em>Support Vector Machines</em> (SVM), <em>logistic regression</em> and <em>linear discriminant analysis</em> (LDA) as familiar examples of <em>classification methods</em>. These and a number of others are covered in <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>. We will focus on <em>Naïve Bayes classifiers</em> as it is the most similar to mixture models and the EM algorithm which we have seen earlier</p>
</section>
<section id="sec-mixture-models-naïve-bayes" class="level2 page-columns page-full" data-number="75.2">
<h2 data-number="75.2" class="anchored" data-anchor-id="sec-mixture-models-naïve-bayes"><span class="header-section-number">75.2</span> Mixture Models and Naïve Bayes classifiers <span class="emoji" data-emoji="movie_camera">🎥</span></h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-04-classification-overview.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="K-means clustering"><img src="images/c3l4-ss-04-classification-overview.png" class="img-fluid figure-img" style="width:53mm" alt="K-means clustering"></a></p>
<figcaption>K-means clustering</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-05-classification-naive-bayes.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="K-means clustering"><img src="images/c3l4-ss-05-classification-naive-bayes.png" class="img-fluid figure-img" style="width:53mm" alt="K-means clustering"></a></p>
<figcaption>K-means clustering</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-06-classification-mixtures.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Mixture Models for Clustering"><img src="images/c3l4-ss-06-classification-mixtures.png" class="img-fluid figure-img" style="width:53mm" alt="Mixture Models for Clustering"></a></p>
<figcaption>Mixture Models for Clustering</figcaption>
</figure>
</div></div>

</section>
<section id="naïve-bayes-classifiers-sec-naïve-bayes-classifiers" class="level2 page-columns page-full" data-number="75.3">
<h2 data-number="75.3" class="anchored" data-anchor-id="naïve-bayes-classifiers-sec-naïve-bayes-classifiers"><span class="header-section-number">75.3</span> Naïve Bayes classifiers {sec-naïve-bayes-classifiers}</h2>
<p> The idea of Naïve Bayes classifiers is that we want to know what is the probability that observation i belongs to class k and we can obtain this using Bayes’ theorem by computing the prior probability that an observation is in that class. This is just the frequency of the class multiplied by the density of that class and divided by the sum over the classes of the same expression.</p>
<p><span id="eq-bayes-classifier"><span class="math display">
\mathbb{P}r(x_i \in \text{class}_k) = \frac{w_k \cdot g_k(x_i \mid \theta_k)}{\sum_{j=1}^K w_j \cdot g_j(x_i\mid \theta_j)}
\tag{75.1}</span></span></p>
<p>where <span class="math inline">w_k</span> is the prior probability of class k, <span class="math inline">g_k(x_i \mid \theta_k)</span> is the density of class k, and <span class="math inline">\theta_k</span> is the parameter of class k.</p>
<p>with</p>
<p><span id="eq-bayes-classifier-assign"><span class="math display">
\tilde{c}_i = \arg \max_k \mathbb{P}r(x_i \in \text{class}_k)\ for \; i=n+1,\ldots,n+m
\tag{75.2}</span></span></p>
<p>The naïve Bayes classifier assumes that the features are conditionally independent given the class. This means that the density of class k can be written as the product of the densities of each feature given the class: <span id="eq-naïve-bayes-density"><span class="math display">
g_k(x_i\mid \theta_k) = \prod_{l=1}^p g_{kl}(x_{il}\mid\theta_{kl})
\tag{75.3}</span></span></p>
<p>where <span class="math inline">g_{kl}(x_{il} \mid \theta_{kl})</span> is the density of feature <span class="math inline">l</span> given class <span class="math inline">k</span> and <span class="math inline">\theta_{kl}</span> is the parameter of feature <span class="math inline">l</span> given class <span class="math inline">k</span>. This means that we can estimate the density of each feature separately and then multiply them together to get the density of the class.</p>
<p>This is <em>a very strong assumption</em> and is not true in general. However, it works well in practice and is often used in text classification problems where the features are the words in the text.</p>
<p>The <strong>Naïve Bayes classifier</strong> is a special case of the mixture model where the components are the classes and the densities are the product of the densities of each feature given the class. This means that we can use the EM algorithm to estimate the parameters of the model in the same way as we did for the mixture model. The only difference is that we need to estimate the densities of each feature separately and then multiply them together to get the density of the class.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- TODO: summarize this transcript -->
<!-- 
_C3-L07-T01.qmd
https://www.coursera.org/learn/mixture-models/lecture/5pvz2/mixture-models-and-naïve-bayes-classifiers 
-->
<p>The last class of problems for which mixture models are very useful is classification problems. If you come from the machine learning literature, you will call this supervised classification to contrast, again, unsupervised classification that I called clustering before. The goal in supervised classification is to start with a training set and use the information in a training set to determine the classes or the labels of a second group of observations that you call the test set. So you start with a training set that contains known labels classes. You also have a test set that has unknown labels, and you want to use this information to make predictions about the test set labels. For example, you may want to decide whether a person suffers from a disease or not based on a set of medical tests, maybe P medical tests, and you have gone out and measured those tests in a number of individuals. So you know those individuals whether they are sick or they are not sick. Based on that training set that is labeled where you know what the real quality of the individuals is, then you go out and you are going to pick just a random person that comes into your medical appointment, and based on the results of the test, now you want to decide if that individual suffers from the disease or not. So the presence of the training set is really what distinguishes clustering problems from classification problems. In clustering problems, we don’t have a training set. We don’t have anything that gives us a hint about how the classes look like. We’re trying to do the process of dividing the observations into groups in some sense blindly. That’s why it’s sometimes called unsupervised classification because you can think that the training set provides supervision in how you do the classification. In typical supervised classification problems on the other hand, you do have that training set. You do have that group of labeled observations that can help you make decisions about how the new groups will look like. So in some sense, supervised classification is a simpler problem than unsupervised classification because of the presence of the training set. Now, there are a number of classification procedures out there. This is a fairly common problem in the literature. You may be familiar with things like support vector machines or logistic regression for classification. I want to discuss today the similarities between using mixture models for classification and some techniques such as linear discriminant analysis, and in particular with Naïve Bayes classifiers. The idea of Naïve Bayes classifiers is very simple. So if you want to know what is the probability that observation i belongs to class k, you can typically obtain that by just using Bayes’ theorem by computing the prior probability that an observation is in that class. That is typically just the frequency of the class multiplied by the density of that class and divided by the sum over the classes of the same expression. Now, again, this should be very familiar. This quantity here is essentially what we used both in the EM algorithm to compute the [inaudible] case and in the MCMC algorithm if you are fitting a mixture model from a Bayesian perspective to sample the class labels C sub x. So in other words, it’s clear just from writing the expression from Naïve Bayes that there should be a very close relationship between doing Naïve Bayes and doing mixture models. In fact, you can cast Naïve Bayes classifiers as just as a special case of mixture models. Let’s discuss Naïve Bayes classifiers where we use Gaussian kernels for the classification. Let’s enter this a little bit of notation. So remember that we have both a test set and a training set. So let’s call X_1 up to X_n my training set, and let’s call X_n plus 1 up to X_n plus m the test set. In other words, we have n observations in the training set, we have m observations in the test set and we just group the observations together so that the first n in the sample are the training and last m are the test. In addition to this, because the training set is labeled, we’re going to have C_1 up to C_n are known, but C_1 or C_n plus 1 up to C_m plus n are unknown and we want to protect them. Let’s write a Naïve Bayes classifier that uses Gaussian kernels, and we’re going to use the more general Gaussian kernels that we can. So in that case, the probability that observation i belongs to class k, it’s going to be equal to Omega_k 1 over the square root 2 Pi to the p.&nbsp;Remember that we’re working with P variate normal. So we can have P features for each individual, determinant of Sigma_k to the minus one 1/2 X of minus one 1/2 X_i minus Mu k transpose sigma sub k inverse X_i minus Mu k, divided by the sum over the components of exactly the same expression. This has to be l, minus Mu sub l transpose sigma l inverse X_i minus Mu l. So this is just Bayes theorem as we have written multiple times in this course. So what you do is, you need this expression only for the training set because for the test set you already know what class you are in. So what you typically do is a two-step process in which you get Mu k hat and Sigma hat sub k are estimated from the training set. You could do different things, but it’s very fairly common to just fit a multivariate Gaussian to each one of the components. So your Cs, your labels divide your training set into groups. For each one of those groups, you fit one different normal and that gives you Sigma and Mu. Similarly, for Omega k, you want to get an estimate for Omega k, and the natural thing to do is to just use the frequency, the fraction of the observations in the training set that belong to each one of the classes. Once you have those, then you classify new observations as by letting C_i be equal to the org max of that probability. Where the probabilities are computed by plugging in these maximum likelihood estimators in this formula up here. As I said, this is done for n plus 1 all the way to n plus m. So you don’t need to do this for the training set, the training set you know the labels and you use those labels to compute the MLEs that get plugged into this. Now, with additional observations in those MLEs, you can decide what are the classes for them. So this is what a naïve Bayes classifier based on Gaussian distributions for each one of the classes would look like. Now, this is exactly the same as the EM algorithm that we have discussed in the past for mixture models, if we make a couple of assumptions or if we incorporate a couple of assumptions into the algorithm. So let’s write down that next. We can recast the algorithms that we just saw for naïve Bayes classifier based on Gaussian kernels in the context of the EM algorithm that we have been discussing for mixtures. That is very easy, we’re going to think, again, about an E-step and an M-step, and we’re going to add an additional post-processing step, if you will. In our E-step, if you remember, what we did in the past was to compute the indicators for the variables. So that is our variables V_i,k that corresponds to the weights that are associated with each one of the components. What we’re going to do in this case is we’re going to define the V_i,k in a very simple fashion rather than doing it using Bayes theorem. Because we actually know what observations or what components are generating each of the observations in the training set, we can call V_i,k just one or zero if C_i is equal to k and zero otherwise, for all the observations that go from one to n.&nbsp;In other words, this is for the training set. Once we have defined our E-step in this way, we’re going to have an M-step where we compute Mu sub k and Omega sub k. To put it in the same way that we did with the EM algorithm, this is going to have a very particular shape. It’s going to have the sum from one to n of V_i,k X_i divided by the sum from one to n of V_i,k. In a similar expression for my matrix Sigma, Sigma is going to be Sigma sub k, it’s going to be one over the sum of the V_i,k from one to n, sum from one to n of V_i,k X_i minus Mu k, X_i minus Mu k transpose. These are expressions that we have seen in the past when filling mixtures of multivariate Gaussians to data. This is just a fancy way, so casting it in terms of the E-step and the M-step, it’s just a fancy way to say, I know what my assignments are, for sure, because this is a training set. So this is just computing the average of the observations that are in category K because, in this case, these are either zeros or ones. Similarly, here, this is just the variance covariance matrix of the observations that are in component K, but it’s written in a fancy way using this V_i,k as indicators. Then, we have a post-processing. It’s in the post-processing step where the test set comes into play. So for now, we have only used the training set for our calculations. In the post-processing step, what we do is we allocate C_i based on the arc max over K of the posterior distribution of the class allocations. So that is probability that X_i belongs to class K. So this is just another way to write the algorithms as we had before, that is very simple in the context of [inaudible]. So why did I go through the trouble of expressing this in this complicated manner when I had a very simple description before? Well, because now you can try to generalize this from this supervised setting where you completely break apart the estimation of the parameters that only uses the training set and the classification that only uses the test set. You can actually try to combine information from both, and it should be clear that if you have training sets that are just very small compared to the test set, the estimates that you get for Mu and Sigma will be very bad because they will be based on very few observations, very few data points. So if you could somehow use some of the information that you are recovering by doing the classification to help you estimate what Mu and Sigma are, they’ll probably give you more robust, stronger algorithm. How to do that should be relatively straightforward once you think about it in this context. For the observations to the training set, we have the value of the V_i,k, but we could add an estimate of the value of the V_i,k for the observations in the test set to this calculation. We already know how to do that. So we’re going to turn the algorithm iterative now. So these guys are always going to be defined in this way because I know the C’s, but these guys are refined at every iteration of the algorithm. I’ll just make this essentially equal to the probability that X_i belongs to class K given the current parameters of the model, so given the current Omegas, the current Mus, and the current Sigmas. Then, I can extend my sums to m down here and down here. Now, what I’m doing is, for the observations that I know what class they are in, these weights are either zeros or ones. For the ones that I don’t know but I’m trying to classify, they will be some number between zero and one, and I’m just going to do a weighted average so you can think about this, again, as a weighted average of the information that I know for sure, and the information that I’m recovering about Mu and Sigma from the classification. So again, this now becomes an iterative algorithm, so I need to think about t plus 1, t plus 1, t plus 1, t plus 1, t plus 1, t plus 1, and t plus 1. So I have turned what was just a two-step algorithm that doesn’t require any iteration, I turned it into an iterative algorithm that uses the whole sample to estimate the parameters of the classes. This is sometimes called a semi-supervised; I don’t necessarily like the term very much. But this is sometimes called a semi-supervised algorithm, in the sense that it’s not completely supervised because of the addition of this information and the fact that now, the sums go up to m. But it’s also not fully unsupervised because I’m using the information, I’m using this piece up here that has information where I know their true labels. Once the algorithm converges, I’m still going to do the post-processing step that is to go from this V_i,k’s that I computed here for the test set to generate what are the labels for those observations.</p>
</div>
</div>
</div>
<section id="sec-classification-LDA-EM-Alg" class="level3 page-columns page-full" data-number="75.3.1">
<h3 data-number="75.3.1" class="anchored" data-anchor-id="sec-classification-LDA-EM-Alg"><span class="header-section-number">75.3.1</span> LDA and the EM algorithm <span class="emoji" data-emoji="movie_camera">🎥</span></h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-08-lda.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="LDA"><img src="images/c3l4-ss-08-lda.png" class="img-fluid figure-img" style="width:53mm" alt="LDA"></a></p>
<figcaption>LDA</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-09-lda.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="LDA"><img src="images/c3l4-ss-09-lda.png" class="img-fluid figure-img" style="width:53mm" alt="LDA"></a></p>
<figcaption>LDA</figcaption>
</figure>
</div></div>
<p>It is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis and also to quadratic discriminant analysis.</p>
<p>To illustrate that connection, we start with a very simple mixture model.</p>
<p>So let’s start with a mixture model of the form:</p>
<p><span id="eq-mixture-model-simple"><span class="math display">
f(x) = \sum_{k=1}^2 \omega_k \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{\text{det}(\Sigma)}} e^{-\frac{1}{2}(x - \mu_k)^T \Sigma^{-1} (x - \mu_k)}.
\tag{75.4}</span></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- TODO: summarize this transcript -->
<!-- 
_C3-L07-T02.qmd
https://www.coursera.org/learn/mixture-models/lecture/OVNXq/linear-and-quadratic-discriminant-analysis-in-the-context-of-mixture-models 
-->
<p>It is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis. And also, by the way, quadratic discriminant analysis. But let’s start with linear discriminant analysis.</p>
<p>And to illustrate that connection, let’s start with a very simple mixture model.</p>
<p>So let’s start with a mixture model of the form, <span class="math inline">f(x) =</span> the sum from 1 to 2. So I’m going to be working only with two components of omega k, 1 over the square root 2pi to the p determinant of sigma to the -1 half, x- 1 half, x, mu sub k, transpose sigma inverse, x- mu sub k. So this is two-component mixture with locations bearing with a component, but the same variance-covariance matrix for the two components that I have in the mixture.</p>
<p>And let’s think about how the procedure would look like if we were to do Naïve Bayes classification using this mixture. If I follow the unsupervised example that I have discussed before, the probability that I put observation i in class, 1, say, I only have two classes.</p>
<p>So as you see again, consider one of them and the other one is just 1- the numbers that I get here. It’s going to be equal.</p>
<p>And I’m going to expand this in all its glory. It’s going to be a little bit long. So it’s going to be omega 1, 1 over the square root 2pi to the p determinant of sigma to the- 1 half, x of- 1 half, x- mu k transpose sigma inverse x- mu k. And in the denominator, we’re going to have the same expression first. And then we’re going to have omega 2, that is just 1- omega 1 but 2pi to the p determinant of sigma to the- 1 half x- 1 half x- mu 2, sigma inverse x- mu 2. Okay, and we know that the probability that xi belongs to class 1 is exactly the same expression but replacing mu, 1 which is what should be up here, replacing mu1 with mu2.</p>
<p>So, in the post processing step, we are going to assign C sub i = 1 if and only if the probability that xi belongs to class 1 is greater than the probability that xi belongs to class 2. And because the two expressions are the same in the denominator, the only thing that changes is the numerator, then this happens if and only if omega 1, 1 over the square root 2pi to the p determinant sigma to the- 1 half x- 1 half, X- mu1 transpose sigma inverse x- mu1, Is greater than omega 2, 1 over the square root 2pi to the p determinant of sigma to the- 1 half x of- 1 half x- mu2, sigma inverse x- mu2. So probability of class 1 greater than probability of class 2 only if this quantity is greater than the same thing but evaluated for the second component in the mixture. So let’s do a little bit of algebra and let’s try to simplify this expression a little bit and we will see that that simplification leads to a very nice expression that matches exactly what you get out of linear discriminant analysis. So now we want to simplify this expression that corresponds to the situation where we’re going to label an observation coming from class 1, and we want to make it much more compact. So a few things that we can observe. So one of them is we have 1 over square root 2pi to the p on both sides, so we can cancel that. The other thing that we observe is that we have the determinant of the variance-covariance matrix on both sides. And because we’re assuming that the two components have the same variance- covariance matrix, we can again just simplify both terms on either side. And the next thing that I’m going to do is I’m going to move all the omegas to one side and bring all the terms with the exponentials to the other side. If I do that, I’m going to end up on the left hand side with the exponent of- 1 half, X- mu1 transpose sigma inverse x- mu1. And then this term came to the other side in the denominator, but that just means that when it goes into the exponential, I need to change all to reverse signs. So it’s going to be- x- mu2 transpose sigma inverse x- mu2. So that’s the expression once you move this to the denominator and combine the two exponentials. And this needs to be greater than omega 2 divided by omega 1. Now, some further simplifications. I can take the logarithm on both sides and I can multiply by -2 on both sides, and I end up with an expression that looks like x- mu 1 transpose sigma inverse x- mu1- x- mu 2 transpose sigma inverse x- mu 2 has to be less than, because I’m going to end up multiplying by a -2. So less than -2 log of omega 2 divided by omega 1. So now we have this difference of two quadratic forms needs to be less than a certain constant that depends on what are my prior weights for each one of the two components. Now, to finish simplifying this, we need to expand these two squares, which is pretty straightforward. So first we’re going to have x sigma inverse x transpose sigma inverse x. This is just a square. So it’s going to be 2 times x transpose sigma inverse mu1. And finally, <span class="math inline">\mu_1</span> transpose sigma inverse <span class="math inline">\mu_1</span>. And then we need to subtract a similar expression but using mu2 for it turns. So it’s going to be x transpose sigma inverse x. It’s going to be +, in this case, 2x transpose sigma inverse mu2. And finally, again,- mu2 transpose sigma inverse mu2, and all of these needs to be less than -2 log of omega 2, Divided by omega 1. So you can see that the expressions are relatively straightforward.</p>
<p>And one of the things that is very nice, and it’s a consequence of having the same variance-covariance matrix for each one of the components, is that now this quadratic term of the data is going to cancel out. And so, we can just basically learn together a couple of terms. So we can write, 2 times, X transpose sigma inverse multiplied by mu2- mu1. So I’m taking this term and combining it with this term. So, the term here and the term here.</p>
<p>And then I’m going to say that this has to be less than -2 times log of omega 2 divided by omega 1, and I’m going to move this two terms to the right. So,+ mu2 transpose sigma inverse mu2- mu1 transpose sigma inverse mu1. So this is actually quite a bit of simplification and it’s a very interesting one. Because you can think about this, Thing on the right hand side, just call this T for threshold. So this is your sum threshold and that threshold is basically computed based on the training data. So if I know the classes of some observations, I can get what the means for each one of the classes are, I can estimate the common sigma, and I can estimate the relative frequencies. And with that, I can obtain a stress score from the training set. And I can think about this matrix product as sum vector a. The form of this simplified expression is very interesting. You can see that the right-hand side, all this expression in the box, it’s just a threshold that can be easily computed from the training set. We can estimate the weight and we can estimate the mean and the covariance of the two components. And then, this product of the variance-covariance or the inverse of the variance-covariance matrix times the difference of the means corresponds to a vector a that can also be computed from the training set. So essentially, the decision of whether we classify an observation in class 1 or class 2 is going to depend on whether a linear combination, and that’s what x transpose times a is, is just a linear combination of the values of x. So whether this linear combination of the values of x is greater than a given threshold or not. In other words, what we’re doing, In a setting where we only have two variables, for example, x1 and x2, the linear combination of the entries is just a line on the plane. So this product just corresponds to a line. And by deciding whether we are above the line or below the line, we’re just saying that one of the regions corresponds to class, 2, and the other region corresponds to class 1. So this is the reason why the procedure is called linear discriminant analysis because it uses a straight line to decide whether observations should be classified in class 1 and class 2. Now, there are some more interesting things that you can do.</p>
<p>For example, you don’t have to assume that the sigmas are the same, you could assume that the <span class="math inline">\sigma_i</span> are different. If you were to do that, then you’d be in a situation that is analogous to this one with the main difference being that now these terms here wouldn’t necessarily simplify.</p>
<p>But then, you can rearrange terms in such a way that now, you’re going to have a quadratic form of <span class="math inline">x</span> being less than a certain threshold. And in that case, you’re <strong>separating hyperplane</strong>.</p>
<p><mark>Instead of being a hyperplane or line, it’s going to be a quadratic form. And that is the reason why when you’re doing Naïve Bayes and you’re working with kernels that are Gaussian and have different variance-covariance matrices, you call the procedure <strong>quadratic discriminant analysis.</strong></mark> Because it uses a quadratic form, a parabola or something like that to separate the two classes that you’re working with.</p>
<p>The nice thing about thinking about this classification procedures in the context of mixture models is again, thinking about ways in which you can generalize and address the shortcomings of the procedure. It’s clear that the main issue with classification procedures based on Gaussians is that data in the real world sometimes doesn’t look like multivariate Gaussian distributions.</p>
<p>One possible extension is to instead of considering the density, this ps here to be a single Gaussian, you can kind of use mixtures a second time and borrow some ideas from when we did density estimation. And say well, I’m going to have a mixture and each component of that mixture is in turn a second mixture that may have a few components. And that may allow for the shape of the clusters to be much more general, and that’s what we call <strong>mixture discriminant analysis</strong>.</p>
<p> As before, if you instead of doing the Algorithm and the simple maximum likelihood estimation that I described before, you instead use Bayesian estimators for your process, then you will have Bayesian equivalent of linear discriminant analysis and quadratic discriminant analysis. So it is very useful to think about your statistical methods in the context of mixture models for the purpose of both generalizing and understanding the shortcomings of what you’re doing.</p>
</div>
</div>
</div>
</section>
</section>
<section id="sec-classification-linear-quadratic-discriminant-analysis" class="level2" data-number="75.4">
<h2 data-number="75.4" class="anchored" data-anchor-id="sec-classification-linear-quadratic-discriminant-analysis"><span class="header-section-number">75.4</span> Linear and quadratic discriminant analysis in the context of Mixture Models <span class="emoji" data-emoji="movie_camera">🎥</span></h2>
<section id="classification-example" class="level3" data-number="75.4.1">
<h3 data-number="75.4.1" class="anchored" data-anchor-id="classification-example"><span class="header-section-number">75.4.1</span> Classification example</h3>
<p> </p>
<p>This video walks through the code in <a href="#sec-em-algorithm-classification-sample" class="quarto-xref"><span>Section 75.5</span></a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- TODO: summarize this transcript -->
<!-- 
_C3-L07-T03.qmd
https://www.coursera.org/learn/mixture-models/lecture/OVNXq/linear-and-quadratic-discriminant-analysis-in-the-context-of-mixture-models 
-->
<p>I’m going to illustrate now to use of mixture models for classification using the wind dataset. Unlike the previous datasets that we work with, this one is not included in R by default. So the two files that you need wind training and wind tests are available on the website, make sure that you download them and that you have them in the right directory for R to read them. And in this case, I made sure that I change the directory where I’m looking at before I start working with this, and that I put my files in there.</p>
<p>Okay, so the wind dataset is an interesting one, it’s a series of measurements for different varieties of wine. They come from three different cultivars, and for each particular variety of wine. They did a chemical analysis and measure 13 different variables that have to do with different chemical components present in the world. So we have a label set where we know which samples from which of the three cultivars. And now we want to use the information that we clean out of that to classify to decide a series of new wines to assign them to the cultivar that we think they come from. We actually do know the truth for the test set, so we will actually first do the predictions we’ll act as if we don’t know what the cultivar of the test set is. And then we will compare the predictions that we’re making against the truth, as a way to tell how well the algorithm is to it, okay. So the first thing that we need to do is load our dataset as I said, you need to make sure that the two files are in the directory where you’re working. So make sure of that, remember that we called n the sample size of the training set and m the training size the size of the test set. So I’m just calling the variables that way, and I’m going to use mixture of normals mixture of multivariate normals by location and scale. So I’m going to use a method that is essentially equivalent to doing quadratic discriminant analysis. And, I want to run the Algorithm that I discussed on the board, but in a situation which we assume that we’re going to work with semi-supervised learning. In other words, I went around the Version of the algorithm in which we’re going to use all the observation both in the training and the test set, to learn the parameters of the classes. So it’s going to be an iterative algorithm. So we know in advance as we have three classes because we have three cultivars. B in this case is going to be 13 because there are 13 features that were measured on each wine. So if you come down here, you can see that B 13, we can try to do a graph of the data. In this case the graph is not going to be terribly readable because there are so many variables, but it may still provide a little bit of intuition. So the variables that are measured things like alcohol, the ash, the alkalinity, the level of magnesium, the hue that has to do with how dark the wine is, proline. So you can see here where the variables are there are measured, and even though the graph is not very readable at least you can see that the classes do not fully overlap. So we do have some hope that we may be able to do classification in the problem. That’s pretty much the main thing that you can say out of this graph here, okay. So, as I said before mixture of models with different components, different variances and different means for each component its normal component in the mixture. Same type of standard initialization that we have done before. And we’re going to do the E and the M step here, remember that for the observations in the training set. We know the class, so the value of B are either 0 or 1, and because we do the calculation first in the log scale, then we do either 0 or minus infinity. So 0 corresponds to probability of 1 and minus infinity corresponds to a probability of 0 in the log scale. And then for the observations in the test set, we have just a regular way in which we compute the probability that the observation comes from each class. And once we have done this then we subtract, we do as we have always done subtract maximums and then re-standardize. So this is how the ES step gets adapted in the case of semisupervised classification. And then the structure of the m-step is exactly the same structure of the regular Algorithm. So we compute means and variances for each one of the components as weighted averages of the different quantities. We check conversions in the standard way, in which we have been checking convergence. And finally once everything is done, we will get a classification, so let’s run it for this dataset. It runs actually quite quickly, we have only 12 iterations and we have converged. Now what the Algorithm gives gave us is just the B values, that is the probability that an observation comes from a given class. Now, we typically are going to want to convert those peas into Cs and as we saw on the board, that is done by just selecting the class that has the highest probability.</p>
<p>So if we do that for our training set in this case, and if you look at the indexes here, they run from n + 1 to n + m, which means that we’re looking at test set. If we just get what is the maximum we can see that the first block of observations is assigned to component two. Most of this block is assigned to component two except for this guy here, and then the the remaining block of observation is assigned to components three. So now how does that compare with the truth? So we can actually go into winder test, and the first column of that file contains the true labels, and we can say that it matches actually pretty well. So the ones all match, the twos match except for one guy, the one we had kind of identified before, and the threes all match together. And we can actually if you just want to have a summary of how many errors you make. You can do a little comparison like this, and you can find that there is only a single error in the classification that the algorithm does.</p>
<p>Now let’s compare that with just using quadratic discriminant analysis and linear discriminant analysis. The way they are implemented in R, so QDA and LDA are the two functions that you will need, they are part of the mass package. So, We first feed the QDA model and then we that fitted model to predict the classes. And now if we see what the regular QDA does is it’s going to give me this long list of probabilities for the test set. And we can turn those into labels and in particular we can see how many errors we’re making in the prediction. And you can see that we make a single mistake, which is actually not the mistake that we had made before. So if we just look at this one here and we compare it against the, Classification that our algorithm did, and we compared it against the truth.</p>
<p>We see that our algorithm makes a mistake in this observation and QDA does not, and instead the error is somewhere else in this sample. It’s basically here, so you can see that the QDA classifies this as two, when the reality is that it’s a three. So our results are not identical to QDA even though our method is asymptotically going to be equivalent to QDA but they don’t give us exactly the same result, but they give us very similar accuracy. Interestingly if you run LDA and you try to look at how many errors you have in that case, you will see that LDA in this case has no errors, even though it’s a simpler more restrictive classification procedure. So this can happen, so it’s a relatively large sample, so a single a difference in a single error is not a very large difference. So, hopefully this illustrates how classification or how measurements can be used for classification in a real life setting.</p>
</div>
</div>
</div>
</section>
</section>
<section id="sec-em-algorithm-classification-sample" class="level2" data-number="75.5">
<h2 data-number="75.5" class="anchored" data-anchor-id="sec-em-algorithm-classification-sample"><span class="header-section-number">75.5</span> Sample EM algorithm for classification problems <span class="emoji" data-emoji="spiral_notepad">🗒️</span> <span class="math inline">\mathcal{R}</span></h2>
<p>In the following code sample we will illustrate how to use the EM algorithm for classification problems.</p>
<p>Using mixture models for classification in the wine dataset Compare linear and quadratic discriminant analysis and a (semi-supervised) location and scale mixture model with K normals Comparing only against the EM algorithm</p>
</section>
<section id="semi-supervised-quadratic-discriminant-analysis" class="level1" data-number="76">
<h1 data-number="76"><span class="header-section-number">76</span> Semi-supervised, quadratic discriminant analysis</h1>
<div class="cell">
<div class="sourceCode cell-code" id="annotated-cell-1"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1">1</button><span id="annotated-cell-1-1" class="code-annotation-target"><a href="#annotated-cell-1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="annotated-cell-1-2"><a href="#annotated-cell-1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2">2</button><span id="annotated-cell-1-3" class="code-annotation-target"><a href="#annotated-cell-1-3" aria-hidden="true" tabindex="-1"></a>wine.training <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">"data/wine_training.txt"</span>, <span class="at">sep=</span><span class="st">","</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4" aria-hidden="true" tabindex="-1"></a>wine.test <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">"data/wine_test.txt"</span>, <span class="at">sep=</span><span class="st">","</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up global variables</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="3">3</button><span id="annotated-cell-1-7" class="code-annotation-target"><a href="#annotated-cell-1-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(wine.training)[<span class="dv">1</span>]</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="4">4</button><span id="annotated-cell-1-8" class="code-annotation-target"><a href="#annotated-cell-1-8" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> <span class="fu">dim</span>(wine.test)[<span class="dv">1</span>]</span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">as.matrix</span>(wine.training[,<span class="sc">-</span><span class="dv">1</span>]), </span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="5">5</button><span id="annotated-cell-1-10" class="code-annotation-target"><a href="#annotated-cell-1-10" aria-hidden="true" tabindex="-1"></a>          <span class="fu">as.matrix</span>(wine.test[,<span class="sc">-</span><span class="dv">1</span>]))</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="6">6</button><span id="annotated-cell-1-11" class="code-annotation-target"><a href="#annotated-cell-1-11" aria-hidden="true" tabindex="-1"></a>p       <span class="ot">=</span> <span class="fu">dim</span>(x)[<span class="dv">2</span>]</span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12" aria-hidden="true" tabindex="-1"></a>KK      <span class="ot">=</span> <span class="dv">3</span></span>
<span id="annotated-cell-1-13"><a href="#annotated-cell-1-13" aria-hidden="true" tabindex="-1"></a>epsilon <span class="ot">=</span> <span class="fl">0.00001</span></span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="1,2" data-code-annotation="1">Load necessary libraries</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="3,4" data-code-annotation="2">Load the wine dataset</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="7" data-code-annotation="3">Size of the training set</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="8" data-code-annotation="4">Size of the test set</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="10" data-code-annotation="5">The dataset stacking: training set + test set</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="11" data-code-annotation="6">Number of features in the dataset</span>
</dd>
</dl>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="annotated-cell-2"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-2-1"><a href="#annotated-cell-2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="annotated-cell-2-2"><a href="#annotated-cell-2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>)<span class="sc">+</span><span class="fl">0.1</span>)</span>
<span id="annotated-cell-2-3"><a href="#annotated-cell-2-3" aria-hidden="true" tabindex="-1"></a>colscale <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"black"</span>,<span class="st">"red"</span>,<span class="st">"blue"</span>)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-2" data-target-annotation="7">7</button><span id="annotated-cell-2-4" class="code-annotation-target"><a href="#annotated-cell-2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">col=</span>colscale[wine.training[,<span class="dv">1</span>]], <span class="at">pch=</span>wine.training[,<span class="dv">1</span>])</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-2" data-target-annotation="7">7</dt>
<dd>
<span data-code-cell="annotated-cell-2" data-code-lines="4" data-code-annotation="7">Pair plot of the training set of the wine dataset</span>
</dd>
</dl>
</div>
<div class="cell-output-display">
<div id="fig-pair-plot-wine-training-EDA" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pair-plot-wine-training-EDA-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C3-L07_files/figure-html/fig-pair-plot-wine-training-EDA-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;76.1: Pair plot of the training set of the wine dataset"><img src="C3-L07_files/figure-html/fig-pair-plot-wine-training-EDA-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pair-plot-wine-training-EDA-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;76.1: Pair plot of the training set of the wine dataset
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="annotated-cell-3"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><span id="annotated-cell-3-1"><a href="#annotated-cell-3-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Initialization step</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="8">8</button><span id="annotated-cell-3-2" class="code-annotation-target"><a href="#annotated-cell-3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">63252</span>)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="9">9</button><span id="annotated-cell-3-3" class="code-annotation-target"><a href="#annotated-cell-3-3" aria-hidden="true" tabindex="-1"></a>w   <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>,KK)<span class="sc">/</span>KK</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="10">10</button><span id="annotated-cell-3-4" class="code-annotation-target"><a href="#annotated-cell-3-4" aria-hidden="true" tabindex="-1"></a>mu  <span class="ot">=</span> <span class="fu">rmvnorm</span>(KK, <span class="fu">apply</span>(x,<span class="dv">2</span>,mean), <span class="fu">var</span>(x))</span>
<span id="annotated-cell-3-5"><a href="#annotated-cell-3-5" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="11">11</button><span id="annotated-cell-3-6" class="code-annotation-target"><a href="#annotated-cell-3-6" aria-hidden="true" tabindex="-1"></a>Sigma      <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim=</span><span class="fu">c</span>(KK,p,p))</span>
<span id="annotated-cell-3-7"><a href="#annotated-cell-3-7" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">1</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK  </span>
<span id="annotated-cell-3-8"><a href="#annotated-cell-3-8" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">2</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK</span>
<span id="annotated-cell-3-9"><a href="#annotated-cell-3-9" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">3</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK</span>
<span id="annotated-cell-3-10"><a href="#annotated-cell-3-10" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="12">12</button><span id="annotated-cell-3-11" class="code-annotation-target"><a href="#annotated-cell-3-11" aria-hidden="true" tabindex="-1"></a>sw     <span class="ot">=</span> <span class="cn">FALSE</span></span>
<span id="annotated-cell-3-12"><a href="#annotated-cell-3-12" aria-hidden="true" tabindex="-1"></a>KL     <span class="ot">=</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="annotated-cell-3-13"><a href="#annotated-cell-3-13" aria-hidden="true" tabindex="-1"></a>KL.out <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="annotated-cell-3-14"><a href="#annotated-cell-3-14" aria-hidden="true" tabindex="-1"></a>s      <span class="ot">=</span> <span class="dv">0</span></span>
<span id="annotated-cell-3-15"><a href="#annotated-cell-3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-3-16"><a href="#annotated-cell-3-16" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span>(<span class="sc">!</span>sw){</span>
<span id="annotated-cell-3-17"><a href="#annotated-cell-3-17" aria-hidden="true" tabindex="-1"></a>  <span class="do">## E step</span></span>
<span id="annotated-cell-3-18"><a href="#annotated-cell-3-18" aria-hidden="true" tabindex="-1"></a>  v <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim=</span><span class="fu">c</span>(n<span class="sc">+</span>m,KK))</span>
<span id="annotated-cell-3-19"><a href="#annotated-cell-3-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){  </span>
<span id="annotated-cell-3-20"><a href="#annotated-cell-3-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the log of the weights</span></span>
<span id="annotated-cell-3-21"><a href="#annotated-cell-3-21" aria-hidden="true" tabindex="-1"></a>    v[<span class="dv">1</span><span class="sc">:</span>n,k] <span class="ot">=</span> <span class="fu">ifelse</span>(wine.training[,<span class="dv">1</span>]<span class="sc">==</span>k,<span class="dv">0</span>,<span class="sc">-</span><span class="cn">Inf</span>)  <span class="co"># Training set</span></span>
<span id="annotated-cell-3-22"><a href="#annotated-cell-3-22" aria-hidden="true" tabindex="-1"></a>    v[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m),k] <span class="ot">=</span> <span class="fu">log</span>(w[k]) <span class="sc">+</span> mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(x[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m),], mu[k,], Sigma[k,,],<span class="at">log=</span><span class="cn">TRUE</span>)  <span class="co"># Test set</span></span>
<span id="annotated-cell-3-23"><a href="#annotated-cell-3-23" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="annotated-cell-3-24"><a href="#annotated-cell-3-24" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="annotated-cell-3-25"><a href="#annotated-cell-3-25" aria-hidden="true" tabindex="-1"></a>    v[i,] <span class="ot">=</span> <span class="fu">exp</span>(v[i,] <span class="sc">-</span> <span class="fu">max</span>(v[i,]))<span class="sc">/</span></span>
<span id="annotated-cell-3-26"><a href="#annotated-cell-3-26" aria-hidden="true" tabindex="-1"></a>            <span class="fu">sum</span>(<span class="fu">exp</span>(v[i,] <span class="sc">-</span> <span class="fu">max</span>(v[i,])))  </span>
<span id="annotated-cell-3-27"><a href="#annotated-cell-3-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Go from logs to actual weights in a numerically stable manner</span></span>
<span id="annotated-cell-3-28"><a href="#annotated-cell-3-28" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="annotated-cell-3-29"><a href="#annotated-cell-3-29" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="annotated-cell-3-30"><a href="#annotated-cell-3-30" aria-hidden="true" tabindex="-1"></a>  <span class="do">## M step</span></span>
<span id="annotated-cell-3-31"><a href="#annotated-cell-3-31" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">=</span> <span class="fu">apply</span>(v,<span class="dv">2</span>,mean)</span>
<span id="annotated-cell-3-32"><a href="#annotated-cell-3-32" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span>KK, <span class="at">ncol=</span>p)</span>
<span id="annotated-cell-3-33"><a href="#annotated-cell-3-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="annotated-cell-3-34"><a href="#annotated-cell-3-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="annotated-cell-3-35"><a href="#annotated-cell-3-35" aria-hidden="true" tabindex="-1"></a>      mu[k,]    <span class="ot">=</span> mu[k,] <span class="sc">+</span> v[i,k]<span class="sc">*</span>x[i,]</span>
<span id="annotated-cell-3-36"><a href="#annotated-cell-3-36" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="annotated-cell-3-37"><a href="#annotated-cell-3-37" aria-hidden="true" tabindex="-1"></a>    mu[k,] <span class="ot">=</span> mu[k,]<span class="sc">/</span><span class="fu">sum</span>(v[,k])</span>
<span id="annotated-cell-3-38"><a href="#annotated-cell-3-38" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="annotated-cell-3-39"><a href="#annotated-cell-3-39" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>,<span class="at">dim=</span><span class="fu">c</span>(KK,p,p))</span>
<span id="annotated-cell-3-40"><a href="#annotated-cell-3-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="annotated-cell-3-41"><a href="#annotated-cell-3-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="annotated-cell-3-42"><a href="#annotated-cell-3-42" aria-hidden="true" tabindex="-1"></a>      Sigma[k,,] <span class="ot">=</span> Sigma[k,,] <span class="sc">+</span> v[i,k]<span class="sc">*</span>(x[i,] <span class="sc">-</span> </span>
<span id="annotated-cell-3-43"><a href="#annotated-cell-3-43" aria-hidden="true" tabindex="-1"></a>                   mu[k,])<span class="sc">%*%</span><span class="fu">t</span>(x[i,] <span class="sc">-</span> mu[k,])</span>
<span id="annotated-cell-3-44"><a href="#annotated-cell-3-44" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="annotated-cell-3-45"><a href="#annotated-cell-3-45" aria-hidden="true" tabindex="-1"></a>    Sigma[k,,] <span class="ot">=</span> Sigma[k,,]<span class="sc">/</span><span class="fu">sum</span>(v[,k])</span>
<span id="annotated-cell-3-46"><a href="#annotated-cell-3-46" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="annotated-cell-3-47"><a href="#annotated-cell-3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-3-48"><a href="#annotated-cell-3-48" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Convergence Check step</span></span>
<span id="annotated-cell-3-49"><a href="#annotated-cell-3-49" aria-hidden="true" tabindex="-1"></a>  KLn <span class="ot">=</span> <span class="dv">0</span></span>
<span id="annotated-cell-3-50"><a href="#annotated-cell-3-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="annotated-cell-3-51"><a href="#annotated-cell-3-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="annotated-cell-3-52"><a href="#annotated-cell-3-52" aria-hidden="true" tabindex="-1"></a>      KLn <span class="ot">=</span> KLn <span class="sc">+</span> v[i,k]<span class="sc">*</span>(<span class="fu">log</span>(w[k]) <span class="sc">+</span> </span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="13">13</button><span id="annotated-cell-3-53" class="code-annotation-target"><a href="#annotated-cell-3-53" aria-hidden="true" tabindex="-1"></a>            mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(x[i,],mu[k,],Sigma[k,,],<span class="at">log=</span><span class="cn">TRUE</span>))</span>
<span id="annotated-cell-3-54"><a href="#annotated-cell-3-54" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="annotated-cell-3-55"><a href="#annotated-cell-3-55" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="annotated-cell-3-56"><a href="#annotated-cell-3-56" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">abs</span>(KLn<span class="sc">-</span>KL)<span class="sc">/</span><span class="fu">abs</span>(KLn)<span class="sc">&lt;</span>epsilon){</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="14">14</button><span id="annotated-cell-3-57" class="code-annotation-target"><a href="#annotated-cell-3-57" aria-hidden="true" tabindex="-1"></a>    sw<span class="ot">=</span><span class="cn">TRUE</span></span>
<span id="annotated-cell-3-58"><a href="#annotated-cell-3-58" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="annotated-cell-3-59"><a href="#annotated-cell-3-59" aria-hidden="true" tabindex="-1"></a>  KL <span class="ot">=</span> KLn</span>
<span id="annotated-cell-3-60"><a href="#annotated-cell-3-60" aria-hidden="true" tabindex="-1"></a>  KL.out <span class="ot">=</span> <span class="fu">c</span>(KL.out, KL)</span>
<span id="annotated-cell-3-61"><a href="#annotated-cell-3-61" aria-hidden="true" tabindex="-1"></a>  s <span class="ot">=</span> s <span class="sc">+</span> <span class="dv">1</span></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-3" data-target-annotation="15">15</button><span id="annotated-cell-3-62" class="code-annotation-target"><a href="#annotated-cell-3-62" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste</span>(s, KLn))</span>
<span id="annotated-cell-3-63"><a href="#annotated-cell-3-63" aria-hidden="true" tabindex="-1"></a>}</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-3" data-target-annotation="8">8</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="2" data-code-annotation="8">Set the random seed for reproducibility</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="9">9</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="3" data-code-annotation="9">Initialize set equal weights to each component</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="10">10</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="4" data-code-annotation="10">Cluster centers randomly spread over the support of the data</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="11">11</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="6" data-code-annotation="11">Initial variances are assumed to be the same</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="12">12</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="11" data-code-annotation="12">Init Convergence flag to False</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="13">13</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="53" data-code-annotation="13">Compute the Kullback-Leibler divergence</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="14">14</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="57" data-code-annotation="14">If converged, set the flag to True and exit the outer loop</span>
</dd>
<dt data-target-cell="annotated-cell-3" data-target-annotation="15">15</dt>
<dd>
<span data-code-cell="annotated-cell-3" data-code-lines="62" data-code-annotation="15">Report iteration and Kullback-Leibler divergence</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "1 -3146.58419305226"
[1] "2 -2942.48222029706"
[1] "3 -2873.76499310479"
[1] "4 -2852.76768638231"
[1] "5 -2796.247735428"
[1] "6 -2791.29098585679"
[1] "7 -2791.23059641487"
[1] "8 -2791.14094416728"
[1] "9 -2791.05612416221"
[1] "10 -2790.99254414223"
[1] "11 -2790.95228067601"
[1] "12 -2790.92945838389"</code></pre>
</div>
</div>
<p>Next we perform predictions and evaluate the classification.</p>
<div class="cell">
<div class="sourceCode cell-code" id="annotated-cell-4"><pre class="sourceCode r code-annotation-code code-with-copy code-annotated"><code class="sourceCode r"><button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="16">16</button><span id="annotated-cell-4-1" class="code-annotation-target"><a href="#annotated-cell-4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)]</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="17">17</button><span id="annotated-cell-4-2" class="code-annotation-target"><a href="#annotated-cell-4-2" aria-hidden="true" tabindex="-1"></a>wine.test[,<span class="dv">1</span>]</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="18">18</button><span id="annotated-cell-4-3" class="code-annotation-target"><a href="#annotated-cell-4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="sc">==</span> wine.test[,<span class="dv">1</span>]</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-4" data-target-annotation="19">19</button><span id="annotated-cell-4-4" class="code-annotation-target"><a href="#annotated-cell-4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(<span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="sc">==</span> wine.test[,<span class="dv">1</span>]))</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-4" data-target-annotation="16">16</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="1" data-code-annotation="16">Predicted labels for the test set</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="17">17</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="2" data-code-annotation="17">True labels for the test set</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="18">18</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="3" data-code-annotation="18">Comparison of predicted labels with true labels</span>
</dd>
<dt data-target-cell="annotated-cell-4" data-target-annotation="19">19</dt>
<dd>
<span data-code-cell="annotated-cell-4" data-code-lines="4" data-code-annotation="19">Number of errors in the classification - One error</span>
</dd>
</dl>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3
[39] 3 3 3 3 3 3 3 3
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3
[39] 3 3 3 3 3 3 3 3
 [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
[25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[1] 1</code></pre>
</div>
</div>
<p>Using the <code>qda</code> and <code>lda</code> functions in <code>R</code></p>
<p>First we preform quadratic discriminant analysis using the <code>qda</code> function in R, which is part of the MASS package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># qda</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>modqda <span class="ot">=</span> <span class="fu">qda</span>(<span class="at">grouping=</span>wine.training[,<span class="dv">1</span>], <span class="at">x=</span>wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">method=</span><span class="st">"mle"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>ccpredqda <span class="ot">=</span> <span class="fu">predict</span>(modqda,<span class="at">newdata=</span>wine.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(ccpredqda<span class="sc">$</span>class <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># One error</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
</div>
<p>Next we perform linear discriminant analysis using the <code>lda</code> function in R, which is also part of the MASS package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>modlda <span class="ot">=</span> <span class="fu">lda</span>(<span class="at">grouping=</span>wine.training[,<span class="dv">1</span>], <span class="at">x=</span>wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">method=</span><span class="st">"mle"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>ccpredlda <span class="ot">=</span> <span class="fu">predict</span>(modlda,<span class="at">newdata=</span>wine.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(ccpredlda<span class="sc">$</span>class <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># No errors!!!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-james2013introduction" class="csl-entry" role="listitem">
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. Springer Texts in Statistics. Springer New York. <a href="https://books.google.co.il/books?id=qcI_AAAAQBAJ">https://books.google.co.il/books?id=qcI_AAAAQBAJ</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C3-L06.html" class="pagination-link" aria-label="Clustering - M4L6">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C3-L07-Ex1.html" class="pagination-link" aria-label="Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1">
        <span class="nav-page-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title :</span><span class="co"> 'Classification - M4L7'</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle :</span><span class="co"> 'Bayesian Statistics: Mixture Models - Applications'</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">  - Bayesian Statistics</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - Mixture Models</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - Classification</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - Naïve Bayes Classifier</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - Notes</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## Classification with Mixture Models  :movie_camera:  {#sec-classification-mixture-models</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">**Classification** is *a supervised learning problem where we want to predict the class of a new observation based on its features*.</span><span class="co">]</span>{.mark}</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>According to the instructor the main difference from clustering is that in classification we have a training set. I would think the main difference is that we have labels for some of the data, while in clustering we do not have labels at all.</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>The fact that we have labels and a training set means we should know how many classes we have and we can use these labels to train a model and use it to predict the class of a new observation.</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>\index{Naïve Bayes classifier}</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>The instructor mentions *Support Vector Machines* (SVM), *logistic regression* and *linear discriminant analysis* (LDA) as familiar examples of *classification methods*. These and a number of others are covered in [@james2013introduction]. We will focus on *Naïve Bayes classifiers* as it is the most similar to mixture models and the EM algorithm which we have seen earlier</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mixture Models and Naïve Bayes classifiers  :movie_camera: {#sec-mixture-models-naïve-bayes}</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="al">![K-means clustering](images/c3l4-ss-04-classification-overview.png)</span>{.column-margin width="53mm"}</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="al">![K-means clustering](images/c3l4-ss-05-classification-naive-bayes.png)</span>{.column-margin width="53mm"}</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="al">![Mixture Models for Clustering](images/c3l4-ss-06-classification-mixtures.png)</span>{.column-margin width="53mm"}</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="fu">## Naïve Bayes classifiers {sec-naïve-bayes-classifiers}</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>\index{Naïve Bayes classifier}</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>The idea of Naïve Bayes classifiers is that we want to know what is the probability that observation i belongs to class k and we can obtain this using Bayes' theorem by computing the prior probability that an observation is in that class. This is just the frequency of the class multiplied by the density of that class and divided by the sum over the classes of the same expression.</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(x_i \in \text{class}_k) = \frac{w_k \cdot g_k(x_i \mid \theta_k)}{\sum_{j=1}^K w_j \cdot g_j(x_i\mid \theta_j)}</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>$$ {#eq-bayes-classifier}</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>where $w_k$ is the prior probability of class k, $g_k(x_i \mid \theta_k)$ is the density of class k, and $\theta_k$ is the parameter of class k.</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>with </span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>\tilde{c}_i = \arg \max_k \mathbb{P}r(x_i \in \text{class}_k)\ for \; i=n+1,\ldots,n+m</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>$$ {#eq-bayes-classifier-assign}</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>The naïve Bayes classifier assumes that the features are conditionally independent given the class. This means that the density of class k can be written as the product of the densities of each feature given the class:</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>g_k(x_i\mid \theta_k) = \prod_{l=1}^p g_{kl}(x_{il}\mid\theta_{kl})</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>$$ {#eq-naïve-bayes-density}</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>where $g_{kl}(x_{il} \mid \theta_{kl})$ is the density of feature $l$ given class $k$ and $\theta_{kl}$ is the parameter of feature $l$ given class $k$. This means that we can estimate the density of each feature separately and then multiply them together to get the density of the class.</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>This is *a very strong assumption* and is not true in general. However, it works well in practice and is often used in text classification problems where the features are the words in the text.</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>The **Naïve Bayes classifier** is a special case of the mixture model where the components are the classes and the densities are the product of the densities of each feature given the class. This means that we can use the EM algorithm to estimate the parameters of the model in the same way as we did for the mixture model. The only difference is that we need to estimate the densities of each feature separately and then multiply them together to get the density of the class.</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"} </span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: summarize this transcript --&gt;</span></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>{{&lt; include transcripts/_C3-L07-T01.qmd &gt;}}</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a><span class="fu">### LDA and the EM algorithm  :movie_camera:  {#sec-classification-LDA-EM-Alg}</span></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a><span class="al">![LDA](images/c3l4-ss-08-lda.png)</span>{.column-margin width="53mm"}</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a><span class="al">![LDA](images/c3l4-ss-09-lda.png)</span>{.column-margin width="53mm"}</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>It is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis and also  to quadratic discriminant analysis.</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>To illustrate that connection, we start with a very simple mixture model. </span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>So let's start with a mixture model of the form:</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>f(x) = \sum_{k=1}^2 \omega_k \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{\text{det}(\Sigma)}} e^{-\frac{1}{2}(x - \mu_k)^T \Sigma^{-1} (x - \mu_k)}.</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mixture-model-simple}</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: summarize this transcript --&gt;</span></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>{{&lt; include transcripts/_C3-L07-T02.qmd &gt;}}</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a><span class="fu">## Linear and quadratic discriminant analysis in the context of Mixture Models  :movie_camera:  {#sec-classification-linear-quadratic-discriminant-analysis}</span></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a><span class="fu">### Classification example</span></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>\index{mixture!classification} \index{mixture!LDA} \index{mixture!QDA}</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>This video walks through the code in @sec-em-algorithm-classification-sample.</span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: summarize this transcript --&gt;</span></span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>{{&lt; include transcripts/_C3-L07-T03.qmd &gt;}}</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sample EM algorithm for classification problems  :spiral_notepad:  $\mathcal{R}${#sec-em-algorithm-classification-sample}</span></span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>In the following code sample we will illustrate how to use the EM algorithm for classification problems.</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>Using mixture models for classification in the wine dataset</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>Compare linear and quadratic discriminant analysis and a </span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>(semi-supervised) location and scale mixture model with K normals</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>Comparing only against the EM algorithm</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a><span class="fu"># Semi-supervised, quadratic discriminant analysis </span></span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lbl-em-algorithm-classification-initialization</span></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)    <span class="co">#&lt;1&gt;</span></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm) <span class="co">#&lt;1&gt;</span></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>wine.training <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">"data/wine_training.txt"</span>, <span class="at">sep=</span><span class="st">","</span>, <span class="at">header=</span><span class="cn">TRUE</span>)     <span class="co">#&lt;2&gt;</span></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>wine.test <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">"data/wine_test.txt"</span>, <span class="at">sep=</span><span class="st">","</span>, <span class="at">header=</span><span class="cn">TRUE</span>) <span class="co">#&lt;2&gt;</span></span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up global variables</span></span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(wine.training)[<span class="dv">1</span>]  <span class="co"># &lt;3&gt;</span></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> <span class="fu">dim</span>(wine.test)[<span class="dv">1</span>]      <span class="co"># &lt;4&gt;</span></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">as.matrix</span>(wine.training[,<span class="sc">-</span><span class="dv">1</span>]), </span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>          <span class="fu">as.matrix</span>(wine.test[,<span class="sc">-</span><span class="dv">1</span>]))   <span class="co"># &lt;5&gt;</span></span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a>p       <span class="ot">=</span> <span class="fu">dim</span>(x)[<span class="dv">2</span>]                    <span class="co"># &lt;6&gt;</span></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>KK      <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>epsilon <span class="ot">=</span> <span class="fl">0.00001</span></span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Load necessary libraries</span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Load the wine dataset</span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Size of the training set</span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Size of the test set</span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>The dataset stacking: training set + test set</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Number of features in the dataset</span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pair-plot-wine-training-EDA</span></span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.cap: "Pair plot of the training set of the wine dataset"</span></span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>)<span class="sc">+</span><span class="fl">0.1</span>)</span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a>colscale <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"black"</span>,<span class="st">"red"</span>,<span class="st">"blue"</span>)</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">col=</span>colscale[wine.training[,<span class="dv">1</span>]], <span class="at">pch=</span>wine.training[,<span class="dv">1</span>]) <span class="co">#&lt;7&gt;</span></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Pair plot of the training set of the wine dataset</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lbl-em-algorithm-classification-EM-algorithm</span></span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a><span class="do">## Initialization step</span></span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">63252</span>) <span class="co"># &lt;8&gt;</span></span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>w   <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>,KK)<span class="sc">/</span>KK  <span class="co"># &lt;9&gt;</span></span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>mu  <span class="ot">=</span> <span class="fu">rmvnorm</span>(KK, <span class="fu">apply</span>(x,<span class="dv">2</span>,mean), <span class="fu">var</span>(x)) <span class="co"># &lt;10&gt;</span></span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a>Sigma      <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim=</span><span class="fu">c</span>(KK,p,p))  <span class="co"># &lt;11&gt;</span></span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">1</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK  </span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">2</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK</span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">3</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK</span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>sw     <span class="ot">=</span> <span class="cn">FALSE</span> <span class="co"># &lt;12&gt;</span></span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>KL     <span class="ot">=</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a>KL.out <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>s      <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span>(<span class="sc">!</span>sw){</span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>  <span class="do">## E step</span></span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>  v <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim=</span><span class="fu">c</span>(n<span class="sc">+</span>m,KK))</span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){  </span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the log of the weights</span></span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a>    v[<span class="dv">1</span><span class="sc">:</span>n,k] <span class="ot">=</span> <span class="fu">ifelse</span>(wine.training[,<span class="dv">1</span>]<span class="sc">==</span>k,<span class="dv">0</span>,<span class="sc">-</span><span class="cn">Inf</span>)  <span class="co"># Training set</span></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a>    v[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m),k] <span class="ot">=</span> <span class="fu">log</span>(w[k]) <span class="sc">+</span> mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(x[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m),], mu[k,], Sigma[k,,],<span class="at">log=</span><span class="cn">TRUE</span>)  <span class="co"># Test set</span></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a>    v[i,] <span class="ot">=</span> <span class="fu">exp</span>(v[i,] <span class="sc">-</span> <span class="fu">max</span>(v[i,]))<span class="sc">/</span></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a>            <span class="fu">sum</span>(<span class="fu">exp</span>(v[i,] <span class="sc">-</span> <span class="fu">max</span>(v[i,])))  </span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Go from logs to actual weights in a numerically stable manner</span></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>  <span class="do">## M step</span></span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">=</span> <span class="fu">apply</span>(v,<span class="dv">2</span>,mean)</span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span>KK, <span class="at">ncol=</span>p)</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a>      mu[k,]    <span class="ot">=</span> mu[k,] <span class="sc">+</span> v[i,k]<span class="sc">*</span>x[i,]</span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a>    mu[k,] <span class="ot">=</span> mu[k,]<span class="sc">/</span><span class="fu">sum</span>(v[,k])</span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>,<span class="at">dim=</span><span class="fu">c</span>(KK,p,p))</span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a>      Sigma[k,,] <span class="ot">=</span> Sigma[k,,] <span class="sc">+</span> v[i,k]<span class="sc">*</span>(x[i,] <span class="sc">-</span> </span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a>                   mu[k,])<span class="sc">%*%</span><span class="fu">t</span>(x[i,] <span class="sc">-</span> mu[k,])</span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a>    Sigma[k,,] <span class="ot">=</span> Sigma[k,,]<span class="sc">/</span><span class="fu">sum</span>(v[,k])</span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Convergence Check step</span></span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a>  KLn <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a>      KLn <span class="ot">=</span> KLn <span class="sc">+</span> v[i,k]<span class="sc">*</span>(<span class="fu">log</span>(w[k]) <span class="sc">+</span> </span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a>            mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(x[i,],mu[k,],Sigma[k,,],<span class="at">log=</span><span class="cn">TRUE</span>)) <span class="co"># &lt;13&gt;</span></span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">abs</span>(KLn<span class="sc">-</span>KL)<span class="sc">/</span><span class="fu">abs</span>(KLn)<span class="sc">&lt;</span>epsilon){</span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a>    sw<span class="ot">=</span><span class="cn">TRUE</span> <span class="co"># &lt;14&gt;</span></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a>  KL <span class="ot">=</span> KLn</span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a>  KL.out <span class="ot">=</span> <span class="fu">c</span>(KL.out, KL)</span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a>  s <span class="ot">=</span> s <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste</span>(s, KLn)) <span class="co"># &lt;15&gt;</span></span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Set the random seed for reproducibility</span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>Initialize set equal weights to each component</span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>Cluster centers randomly spread over the support of the data</span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>Initial variances are assumed to be the same</span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a><span class="ss">12. </span>Init Convergence flag to False</span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a><span class="ss">13. </span>Compute the Kullback-Leibler divergence</span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a><span class="ss">14. </span>If converged, set the flag to True and exit the outer loop</span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a><span class="ss">15. </span>Report iteration and Kullback-Leibler divergence</span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a>Next we perform predictions and evaluate the classification.</span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lbl-em-classification-prediction</span></span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a><span class="co">#| </span></span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="co"># &lt;16&gt;</span></span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a>wine.test[,<span class="dv">1</span>] <span class="co"># &lt;17&gt;</span></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="sc">==</span> wine.test[,<span class="dv">1</span>] <span class="co"># &lt;18&gt;</span></span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(<span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># &lt;19&gt; </span></span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a><span class="ss">16. </span>Predicted labels for the test set</span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a><span class="ss">17. </span>True labels for the test set</span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a><span class="ss">18. </span>Comparison of predicted labels with true labels</span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a><span class="ss">19. </span>Number of errors in the classification - One error</span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a>Using the <span class="in">`qda`</span> and <span class="in">`lda`</span> functions in <span class="in">`R`</span></span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a>First we preform quadratic discriminant analysis using the <span class="in">`qda`</span> function in R, which is part of the MASS package.</span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lbl-em-classification-qda</span></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a><span class="co"># qda</span></span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a>modqda <span class="ot">=</span> <span class="fu">qda</span>(<span class="at">grouping=</span>wine.training[,<span class="dv">1</span>], <span class="at">x=</span>wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">method=</span><span class="st">"mle"</span>)</span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a>ccpredqda <span class="ot">=</span> <span class="fu">predict</span>(modqda,<span class="at">newdata=</span>wine.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(ccpredqda<span class="sc">$</span>class <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># One error</span></span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a>Next we perform linear discriminant analysis using the <span class="in">`lda`</span> function in R, which is also part of the MASS package.</span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lbl-em-classification-lda</span></span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a>modlda <span class="ot">=</span> <span class="fu">lda</span>(<span class="at">grouping=</span>wine.training[,<span class="dv">1</span>], <span class="at">x=</span>wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">method=</span><span class="st">"mle"</span>)</span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a>ccpredlda <span class="ot">=</span> <span class="fu">predict</span>(modlda,<span class="at">newdata=</span>wine.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(ccpredlda<span class="sc">$</span>class <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># No errors!!!</span></span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Oren Bochman</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>