<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="keywords" content="Mixture Models, Classification, notes">

<title>76&nbsp; Classification - M4L7 â€“ Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C3-L07-Ex1.html" rel="next">
<link href="./C3-L06.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(images/banner_deep.jpg);
background-size: cover;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C3-L07.html"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Bayesian Statistics: Mixture Models - Applications</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Bayesian Statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Mixture Models, Classification, notes</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Paradigms of probability - M1L1HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesâ€™ Theorem - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayesâ€™ Law - M1L2HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Probability and Bayesâ€™ Theorem - M1L2HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Distributions - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Random Variables - M1L3HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Homework on Distributions - M1L3HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Frequentist Inference - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Frequentist MLE - M2L3HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bayesian Inference - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference - M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Priors - M3L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities - M3L6HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Homework on Priors - M2L7HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Poisson Data - M3L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Homework on Poisson Data - M3L8HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Beta Bernoulli - M3L8HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Homework on Exponential Data - M4L9HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Normally distributed Data - M4L10</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Homework on Normal Data - M4L10HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Non-Informative Priors - M4L11</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Homework Alternative Priors - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Brief Review of Regression - M4L12</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm - M2L4HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Gibbs sampling - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Homework Gibbs-Sampling algorithm - M2L22HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Assessing Convergence - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Homework on M-H algorithm M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Linear regression - M3L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Homework on Linear Regression Model Part 1 - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Homework on Deviance information criterion - M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">ANOVA - M3L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Homework on ANOVA - M3L8HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Homework on Multiple Factor ANOVA - M3L8HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Logistic regression - M3L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression - M3L9HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Poisson regression - M4L10</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Homework on Poisson regression - M4L10HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Hierarchical modeling - M4L11</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Capstone Project - M4L12</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models - M4L12HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Definitions of Mixture Models - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Likelihood functions for Mixture Models - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Homework The Likelihood function - M1L2HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Homework Identifiability - M1L2HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Homework The likelihood function M1L2HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Homework on simulating from a Poisson Mixture Model - M1L2HW4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model - M1L2HW5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Homework Sim mixture of exponential distributions - M1L2HW6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">M2L3 - The EM algorithm for Mixture models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">MCMC for Mixture Models - M4L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Density Estimation - M4L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Computational Considerations - M5L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Determining the number of components - M5L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Homework on Bayesian Information Criteria (BIC) - M5L09HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Homework on BIC for zero-inflated mixtures - M5L09HW4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Week 0: Introductions to time series analysis and the AR(1) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Stationarity, The ACF and the PCF M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">The AR(1) process: definitions and properties - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(1) process:Maximum likelihood estimation and Bayesian inference M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">The AR(p) process - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Bayesian Inference in the AR(p) - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1 - M3L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 1 M3L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Seasonal NDLMs M4L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 2 - M4L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Time Series Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">106</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">107</span>&nbsp; <span class="chapter-title">Bayes by backprop</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">108</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">109</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#mixture-models-and-naive-bayes-classifiers" id="toc-mixture-models-and-naive-bayes-classifiers" class="nav-link active" data-scroll-target="#mixture-models-and-naive-bayes-classifiers"><span class="header-section-number">76.0.1</span> Mixture Models and naive Bayes classifiers</a></li>
  <li><a href="#lda-and-the-em-algorithm" id="toc-lda-and-the-em-algorithm" class="nav-link" data-scroll-target="#lda-and-the-em-algorithm"><span class="header-section-number">76.0.2</span> LDA and the EM algorithm</a></li>
  <li><a href="#linear-and-quadratic-discriminant-analysis-in-the-context-of-mixture-models" id="toc-linear-and-quadratic-discriminant-analysis-in-the-context-of-mixture-models" class="nav-link" data-scroll-target="#linear-and-quadratic-discriminant-analysis-in-the-context-of-mixture-models"><span class="header-section-number">76.0.3</span> Linear and quadratic discriminant analysis in the context of Mixture Models</a></li>
  <li><a href="#classification-example" id="toc-classification-example" class="nav-link" data-scroll-target="#classification-example"><span class="header-section-number">76.0.4</span> Classification example</a></li>
  <li><a href="#sample-em-algorithm-for-classification-problems" id="toc-sample-em-algorithm-for-classification-problems" class="nav-link" data-scroll-target="#sample-em-algorithm-for-classification-problems"><span class="header-section-number">76.0.5</span> Sample EM algorithm for classification problems</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<p>Classification is a supervised learning problem where we want to predict the class of a new observation based on its features.</p>
<p>According to the instructor the main difference from clustering is that in classification we have a training set. I would think the main difference is that we have labels for some of the data, while in clustering we do not have labels at all.</p>
<p>The fact that we have labels and a training set means we should know how many classes we have and we can use these labels to train a model and use it to predict the class of a new observation.</p>
<p>The instructor mentions Support Vector Machines (SVM), logistic regression and linear discriminant analysis (LDA) as familiar examples of classification methods. These and a number of others are covered in <span class="citation" data-cites="james2013introduction">(<a href="references.html#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>. We will focus on Naive Bayes classifiers as it is the most similar to mixture models and the EM algorithm which we have seen earlier</p>
<section id="mixture-models-and-naive-bayes-classifiers" class="level3 page-columns page-full" data-number="76.0.1">
<h3 data-number="76.0.1" class="anchored" data-anchor-id="mixture-models-and-naive-bayes-classifiers"><span class="header-section-number">76.0.1</span> Mixture Models and naive Bayes classifiers</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-04-classification-overview.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="K-means clustering"><img src="images/c3l4-ss-04-classification-overview.png" class="img-fluid figure-img" style="width:53mm" alt="K-means clustering"></a></p>
<figcaption>K-means clustering</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-05-classification-naive-bayes.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="K-means clustering"><img src="images/c3l4-ss-05-classification-naive-bayes.png" class="img-fluid figure-img" style="width:53mm" alt="K-means clustering"></a></p>
<figcaption>K-means clustering</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-06-classification-mixtures.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Mixture Models for Clustering"><img src="images/c3l4-ss-06-classification-mixtures.png" class="img-fluid figure-img" style="width:53mm" alt="Mixture Models for Clustering"></a></p>
<figcaption>Mixture Models for Clustering</figcaption>
</figure>
</div></div>

<section id="naive-bayes-classifiers" class="level4" data-number="76.0.1.1">
<h4 data-number="76.0.1.1" class="anchored" data-anchor-id="naive-bayes-classifiers"><span class="header-section-number">76.0.1.1</span> Naive Bayes classifiers</h4>
<p>The idea of Naive Bayes classifiers is that we want to know what is the probability that observation i belongs to class k and we can obtain this using Bayesâ€™ theorem by computing the prior probability that an observation is in that class. This is just the frequency of the class multiplied by the density of that class and divided by the sum over the classes of the same expression.</p>
<p><span id="eq-bayes-classifier"><span class="math display">
\mathbb{P}r(x_i \in \text{class}_k) = \frac{w_k \cdot g_k(x_i|\theta_k)}{\sum_{j=1}^K w_j \cdot g_j(x_i|\theta_j)}
\tag{76.1}</span></span></p>
<p>where <span class="math inline">w_k</span> is the prior probability of class k, <span class="math inline">g_k(x_i|\theta_k)</span> is the density of class k, and <span class="math inline">\theta_k</span> is the parameter of class k.</p>
<p>with</p>
<p><span class="math display">
\tilde{c}_i = \arg \max_k \mathbb{P}r(x_i \in \text{class}_k)\ for \; i=n+1,\ldots,n+m
</span></p>
<p>The naive Bayes classifier assumes that the features are conditionally independent given the class. This means that the density of class k can be written as the product of the densities of each feature given the class: <span class="math display">
g_k(x_i|\theta_k) = \prod_{l=1}^p g_{kl}(x_{il}|\theta_{kl})
</span></p>
<p>where <span class="math inline">g_{kl}(x_{il}|\theta_{kl})</span> is the density of feature l given class k and <span class="math inline">\theta_{kl}</span> is the parameter of feature l given class k. This means that we can estimate the density of each feature separately and then multiply them together to get the density of the class.</p>
<p>This is a very strong assumption and is not true in general. However, it works well in practice and is often used in text classification problems where the features are the words in the text.</p>
<p>The naive Bayes classifier is a special case of the mixture model where the components are the classes and the densities are the product of the densities of each feature given the class. This means that we can use the EM algorithm to estimate the parameters of the model in the same way as we did for the mixture model. The only difference is that we need to estimate the densities of each feature separately and then multiply them together to get the density of the class.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The last class of problems for which mixture models are very useful is classification problems. If you come from the machine learning literature, you will call this supervised classification to contrast, again, unsupervised classification that I called clustering before. The goal in supervised classification is to start with a training set and use the information in a training set to determine the classes or the labels of a second group of observations that you call the test set. So you start with a training set that contains known labels classes. You also have a test set that has unknown labels, and you want to use this information to make predictions about the test set labels. For example, you may want to decide whether a person suffers from a disease or not based on a set of medical tests, maybe P medical tests, and you have gone out and measured those tests in a number of individuals. So you know those individuals whether they are sick or they are not sick. Based on that training set that is labeled where you know what the real quality of the individuals is, then you go out and you are going to pick just a random person that comes into your medical appointment, and based on the results of the test, now you want to decide if that individual suffers from the disease or not. So the presence of the training set is really what distinguishes clustering problems from classification problems. In clustering problems, we donâ€™t have a training set. We donâ€™t have anything that gives us a hint about how the classes look like. Weâ€™re trying to do the process of dividing the observations into groups in some sense blindly. Thatâ€™s why itâ€™s sometimes called unsupervised classification because you can think that the training set provides supervision in how you do the classification. In typical supervised classification problems on the other hand, you do have that training set. You do have that group of labeled observations that can help you make decisions about how the new groups will look like. So in some sense, supervised classification is a simpler problem than unsupervised classification because of the presence of the training set. Now, there are a number of classification procedures out there. This is a fairly common problem in the literature. You may be familiar with things like support vector machines or logistic regression for classification. I want to discuss today the similarities between using mixture models for classification and some techniques such as linear discriminant analysis, and in particular with Naive Bayes classifiers. The idea of Naive Bayes classifiers is very simple. So if you want to know what is the probability that observation i belongs to class k, you can typically obtain that by just using Bayesâ€™ theorem by computing the prior probability that an observation is in that class. That is typically just the frequency of the class multiplied by the density of that class and divided by the sum over the classes of the same expression. Now, again, this should be very familiar. This quantity here is essentially what we used both in the EM algorithm to compute the [inaudible] case and in the MCMC algorithm if you are fitting a mixture model from a Bayesian perspective to sample the class labels C sub x. So in other words, itâ€™s clear just from writing the expression from Naive Bayes that there should be a very close relationship between doing Naive Bayes and doing mixture models. In fact, you can cast Naive Bayes classifiers as just as a special case of mixture models. Letâ€™s discuss Naive Bayes classifiers where we use Gaussian kernels for the classification. Letâ€™s enter this a little bit of notation. So remember that we have both a test set and a training set. So letâ€™s call X_1 up to X_n my training set, and letâ€™s call X_n plus 1 up to X_n plus m the test set. In other words, we have n observations in the training set, we have m observations in the test set and we just group the observations together so that the first n in the sample are the training and last m are the test. In addition to this, because the training set is labeled, weâ€™re going to have C_1 up to C_n are known, but C_1 or C_n plus 1 up to C_m plus n are unknown and we want to protect them. Letâ€™s write a Naive Bayes classifier that uses Gaussian kernels, and weâ€™re going to use the more general Gaussian kernels that we can. So in that case, the probability that observation i belongs to class k, itâ€™s going to be equal to Omega_k 1 over the square root 2 Pi to the p.&nbsp;Remember that weâ€™re working with P variate normal. So we can have P features for each individual, determinant of Sigma_k to the minus one 1/2 X of minus one 1/2 X_i minus Mu k transpose sigma sub k inverse X_i minus Mu k, divided by the sum over the components of exactly the same expression. This has to be l, minus Mu sub l transpose sigma l inverse X_i minus Mu l. So this is just Bayes theorem as we have written multiple times in this course. So what you do is, you need this expression only for the training set because for the test set you already know what class you are in. So what you typically do is a two-step process in which you get Mu k hat and Sigma hat sub k are estimated from the training set. You could do different things, but itâ€™s very fairly common to just fit a multivariate Gaussian to each one of the components. So your Cs, your labels divide your training set into groups. For each one of those groups, you fit one different normal and that gives you Sigma and Mu. Similarly, for Omega k, you want to get an estimate for Omega k, and the natural thing to do is to just use the frequency, the fraction of the observations in the training set that belong to each one of the classes. Once you have those, then you classify new observations as by letting C_i be equal to the org max of that probability. Where the probabilities are computed by plugging in these maximum likelihood estimators in this formula up here. As I said, this is done for n plus 1 all the way to n plus m. So you donâ€™t need to do this for the training set, the training set you know the labels and you use those labels to compute the MLEs that get plugged into this. Now, with additional observations in those MLEs, you can decide what are the classes for them. So this is what a naive Bayes classifier based on Gaussian distributions for each one of the classes would look like. Now, this is exactly the same as the EM algorithm that we have discussed in the past for mixture models, if we make a couple of assumptions or if we incorporate a couple of assumptions into the algorithm. So letâ€™s write down that next. We can recast the algorithms that we just saw for naive Bayes classifier based on Gaussian kernels in the context of the EM algorithm that we have been discussing for mixtures. That is very easy, weâ€™re going to think, again, about an E-step and an M-step, and weâ€™re going to add an additional post-processing step, if you will. In our E-step, if you remember, what we did in the past was to compute the indicators for the variables. So that is our variables V_i,k that corresponds to the weights that are associated with each one of the components. What weâ€™re going to do in this case is weâ€™re going to define the V_i,k in a very simple fashion rather than doing it using Bayes theorem. Because we actually know what observations or what components are generating each of the observations in the training set, we can call V_i,k just one or zero if C_i is equal to k and zero otherwise, for all the observations that go from one to n.&nbsp;In other words, this is for the training set. Once we have defined our E-step in this way, weâ€™re going to have an M-step where we compute Mu sub k and Omega sub k. To put it in the same way that we did with the EM algorithm, this is going to have a very particular shape. Itâ€™s going to have the sum from one to n of V_i,k X_i divided by the sum from one to n of V_i,k. In a similar expression for my matrix Sigma, Sigma is going to be Sigma sub k, itâ€™s going to be one over the sum of the V_i,k from one to n, sum from one to n of V_i,k X_i minus Mu k, X_i minus Mu k transpose. These are expressions that we have seen in the past when filling mixtures of multivariate Gaussians to data. This is just a fancy way, so casting it in terms of the E-step and the M-step, itâ€™s just a fancy way to say, I know what my assignments are, for sure, because this is a training set. So this is just computing the average of the observations that are in category K because, in this case, these are either zeros or ones. Similarly, here, this is just the variance covariance matrix of the observations that are in component K, but itâ€™s written in a fancy way using this V_i,k as indicators. Then, we have a post-processing. Itâ€™s in the post-processing step where the test set comes into play. So for now, we have only used the training set for our calculations. In the post-processing step, what we do is we allocate C_i based on the arc max over K of the posterior distribution of the class allocations. So that is probability that X_i belongs to class K. So this is just another way to write the algorithms as we had before, that is very simple in the context of [inaudible]. So why did I go through the trouble of expressing this in this complicated manner when I had a very simple description before? Well, because now you can try to generalize this from this supervised setting where you completely break apart the estimation of the parameters that only uses the training set and the classification that only uses the test set. You can actually try to combine information from both, and it should be clear that if you have training sets that are just very small compared to the test set, the estimates that you get for Mu and Sigma will be very bad because they will be based on very few observations, very few data points. So if you could somehow use some of the information that you are recovering by doing the classification to help you estimate what Mu and Sigma are, theyâ€™ll probably give you more robust, stronger algorithm. How to do that should be relatively straightforward once you think about it in this context. For the observations to the training set, we have the value of the V_i,k, but we could add an estimate of the value of the V_i,k for the observations in the test set to this calculation. We already know how to do that. So weâ€™re going to turn the algorithm iterative now. So these guys are always going to be defined in this way because I know the Câ€™s, but these guys are refined at every iteration of the algorithm. Iâ€™ll just make this essentially equal to the probability that X_i belongs to class K given the current parameters of the model, so given the current Omegas, the current Mus, and the current Sigmas. Then, I can extend my sums to m down here and down here. Now, what Iâ€™m doing is, for the observations that I know what class they are in, these weights are either zeros or ones. For the ones that I donâ€™t know but Iâ€™m trying to classify, they will be some number between zero and one, and Iâ€™m just going to do a weighted average so you can think about this, again, as a weighted average of the information that I know for sure, and the information that Iâ€™m recovering about Mu and Sigma from the classification. So again, this now becomes an iterative algorithm, so I need to think about t plus 1, t plus 1, t plus 1, t plus 1, t plus 1, t plus 1, and t plus 1. So I have turned what was just a two-step algorithm that doesnâ€™t require any iteration, I turned it into an iterative algorithm that uses the whole sample to estimate the parameters of the classes. This is sometimes called a semi-supervised; I donâ€™t necessarily like the term very much. But this is sometimes called a semi-supervised algorithm, in the sense that itâ€™s not completely supervised because of the addition of this information and the fact that now, the sums go up to m. But itâ€™s also not fully unsupervised because Iâ€™m using the information, Iâ€™m using this piece up here that has information where I know their true labels. Once the algorithm converges, Iâ€™m still going to do the post-processing step that is to go from this V_i,kâ€™s that I computed here for the test set to generate what are the labels for those observations.</p>
</div>
</div>
</div>
</section>
</section>
<section id="lda-and-the-em-algorithm" class="level3 page-columns page-full" data-number="76.0.2">
<h3 data-number="76.0.2" class="anchored" data-anchor-id="lda-and-the-em-algorithm"><span class="header-section-number">76.0.2</span> LDA and the EM algorithm</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-08-lda.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="LDA"><img src="images/c3l4-ss-08-lda.png" class="img-fluid figure-img" style="width:53mm" alt="LDA"></a></p>
<figcaption>LDA</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c3l4-ss-09-lda.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="LDA"><img src="images/c3l4-ss-09-lda.png" class="img-fluid figure-img" style="width:53mm" alt="LDA"></a></p>
<figcaption>LDA</figcaption>
</figure>
</div></div>
<p>It is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis and also to quadratic discriminant analysis.</p>
<p>To illustrate that connection, we start with a very simple mixture model.</p>
<p>So letâ€™s start with a mixture model of the form,</p>
<p><span class="math display">
f(x) = \sum_{k=1}^2 \omega_k \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{\text{det}(\Sigma)}} e^{-\frac{1}{2}(x - \mu_k)^T \Sigma^{-1} (x - \mu_k)}.
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>It is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis. And also, by the way, quadratic discriminant analysis. But letâ€™s start with linear discriminant analysis.</p>
<p>And to illustrate that connection, letâ€™s start with a very simple mixture model.</p>
<p>So letâ€™s start with a mixture model of the form, <span class="math inline">f(x) =</span> the sum from 1 to 2. So Iâ€™m going to be working only with two components of omega k, 1 over the square root 2pi to the p determinant of sigma to the -1 half, x- 1 half, x, mu sub k, transpose sigma inverse, x- mu sub k. So this is two-component mixture with locations bearing with a component, but the same variance-covariance matrix for the two components that I have in the mixture.</p>
<p>And letâ€™s think about how the procedure would look like if we were to do Naive Bayes classification using this mixture. If I follow the unsupervised example that I have discussed before, the probability that I put observation i in class, 1, say, I only have two classes.</p>
<p>So as you see again, consider one of them and the other one is just 1- the numbers that I get here. Itâ€™s going to be equal.</p>
<p>And Iâ€™m going to expand this in all its glory. Itâ€™s going to be a little bit long. So itâ€™s going to be omega 1, 1 over the square root 2pi to the p determinant of sigma to the- 1 half, x of- 1 half, x- mu k transpose sigma inverse x- mu k. And in the denominator, weâ€™re going to have the same expression first. And then weâ€™re going to have omega 2, that is just 1- omega 1 but 2pi to the p determinant of sigma to the- 1 half x- 1 half x- mu 2, sigma inverse x- mu 2. Okay, and we know that the probability that xi belongs to class 1 is exactly the same expression but replacing mu, 1 which is what should be up here, replacing mu1 with mu2.</p>
<p>So, in the post processing step, we are going to assign C sub i = 1 if and only if the probability that xi belongs to class 1 is greater than the probability that xi belongs to class 2. And because the two expressions are the same in the denominator, the only thing that changes is the numerator, then this happens if and only if omega 1, 1 over the square root 2pi to the p determinant sigma to the- 1 half x- 1 half, X- mu1 transpose sigma inverse x- mu1, Is greater than omega 2, 1 over the square root 2pi to the p determinant of sigma to the- 1 half x of- 1 half x- mu2, sigma inverse x- mu2. So probability of class 1 greater than probability of class 2 only if this quantity is greater than the same thing but evaluated for the second component in the mixture. So letâ€™s do a little bit of algebra and letâ€™s try to simplify this expression a little bit and we will see that that simplification leads to a very nice expression that matches exactly what you get out of linear discriminant analysis. So now we want to simplify this expression that corresponds to the situation where weâ€™re going to label an observation coming from class 1, and we want to make it much more compact. So a few things that we can observe. So one of them is we have 1 over square root 2pi to the p on both sides, so we can cancel that. The other thing that we observe is that we have the determinant of the variance-covariance matrix on both sides. And because weâ€™re assuming that the two components have the same variance- covariance matrix, we can again just simplify both terms on either side. And the next thing that Iâ€™m going to do is Iâ€™m going to move all the omegas to one side and bring all the terms with the exponentials to the other side. If I do that, Iâ€™m going to end up on the left hand side with the exponent of- 1 half, X- mu1 transpose sigma inverse x- mu1. And then this term came to the other side in the denominator, but that just means that when it goes into the exponential, I need to change all to reverse signs. So itâ€™s going to be- x- mu2 transpose sigma inverse x- mu2. So thatâ€™s the expression once you move this to the denominator and combine the two exponentials. And this needs to be greater than omega 2 divided by omega 1. Now, some further simplifications. I can take the logarithm on both sides and I can multiply by -2 on both sides, and I end up with an expression that looks like x- mu 1 transpose sigma inverse x- mu1- x- mu 2 transpose sigma inverse x- mu 2 has to be less than, because Iâ€™m going to end up multiplying by a -2. So less than -2 log of omega 2 divided by omega 1. So now we have this difference of two quadratic forms needs to be less than a certain constant that depends on what are my prior weights for each one of the two components. Now, to finish simplifying this, we need to expand these two squares, which is pretty straightforward. So first weâ€™re going to have x sigma inverse x transpose sigma inverse x. This is just a square. So itâ€™s going to be 2 times x transpose sigma inverse mu1. And finally, <span class="math inline">\mu_1</span> transpose sigma inverse <span class="math inline">\mu_1</span>. And then we need to subtract a similar expression but using mu2 for it turns. So itâ€™s going to be x transpose sigma inverse x. Itâ€™s going to be +, in this case, 2x transpose sigma inverse mu2. And finally, again,- mu2 transpose sigma inverse mu2, and all of these needs to be less than -2 log of omega 2, Divided by omega 1. So you can see that the expressions are relatively straightforward.</p>
<p>And one of the things that is very nice, and itâ€™s a consequence of having the same variance-covariance matrix for each one of the components, is that now this quadratic term of the data is going to cancel out. And so, we can just basically learn together a couple of terms. So we can write, 2 times, X transpose sigma inverse multiplied by mu2- mu1. So Iâ€™m taking this term and combining it with this term. So, the term here and the term here.</p>
<p>And then Iâ€™m going to say that this has to be less than -2 times log of omega 2 divided by omega 1, and Iâ€™m going to move this two terms to the right. So,+ mu2 transpose sigma inverse mu2- mu1 transpose sigma inverse mu1. So this is actually quite a bit of simplification and itâ€™s a very interesting one. Because you can think about this, Thing on the right hand side, just call this T for threshold. So this is your sum threshold and that threshold is basically computed based on the training data. So if I know the classes of some observations, I can get what the means for each one of the classes are, I can estimate the common sigma, and I can estimate the relative frequencies. And with that, I can obtain a stress score from the training set. And I can think about this matrix product as sum vector a. The form of this simplified expression is very interesting. You can see that the right-hand side, all this expression in the box, itâ€™s just a threshold that can be easily computed from the training set. We can estimate the weight and we can estimate the mean and the covariance of the two components. And then, this product of the variance-covariance or the inverse of the variance-covariance matrix times the difference of the means corresponds to a vector a that can also be computed from the training set. So essentially, the decision of whether we classify an observation in class 1 or class 2 is going to depend on whether a linear combination, and thatâ€™s what x transpose times a is, is just a linear combination of the values of x. So whether this linear combination of the values of x is greater than a given threshold or not. In other words, what weâ€™re doing, In a setting where we only have two variables, for example, x1 and x2, the linear combination of the entries is just a line on the plane. So this product just corresponds to a line. And by deciding whether we are above the line or below the line, weâ€™re just saying that one of the regions corresponds to class, 2, and the other region corresponds to class 1. So this is the reason why the procedure is called linear discriminant analysis because it uses a straight line to decide whether observations should be classified in class 1 and class 2. Now, there are some more interesting things that you can do. For example, you donâ€™t have to assume that the sigmas are the same, you could assume that the sigmas are different. If you were to do that, then youâ€™d be in a situation that is analogous to this one with the main difference being that now these terms here wouldnâ€™t necessarily simplify. But then, you can rearrange terms in such a way that now, youâ€™re going to have a quadratic form of x being less than a certain threshold. And in that case, youâ€™re separating hyperplane. Instead of being a hyperplane or line, itâ€™s going to be a quadratic form. And that is the reason why when youâ€™re doing Naive Bayes and youâ€™re working with kernels that are Gaussian and have different variance-covariance matrices, you call the procedure quadratic discriminant analysis. Because it uses a quadratic form, a parabola or something like that to separate the two classes that youâ€™re working with. The nice thing about thinking about this classification procedures in the context of mixture models is again, thinking about ways in which you can generalize and address the shortcomings of the procedure. Itâ€™s clear that the main issue with classification procedures based on Gaussians is that data in the real world sometimes doesnâ€™t look like multivariate Gaussian distributions. So one possible extension is to instead of considering the density, this ps here to be a single Gaussian, you can kind of use mixtures a second time and borrow some ideas from when we did density estimation. And say well, Iâ€™m going to have a mixture and each component of that mixture is in turn a second mixture that may have a few components. And that may allow for the shape of the clusters to be much more general, and thatâ€™s what we call mixture discriminant analysis. As before, if you instead of doing the Algorithm and the simple maximum likelihood estimation that I described before, you instead use Bayesian estimators for your process, then you will have Bayesian equivalent of linear discriminant analysis and quadratic discriminant analysis. So it is very useful to think about your statistical methods in the context of mixture models for the purpose of both generalizing and understanding the shortcomings of what youâ€™re doing.</p>
</div>
</div>
</div>
</section>
<section id="linear-and-quadratic-discriminant-analysis-in-the-context-of-mixture-models" class="level3" data-number="76.0.3">
<h3 data-number="76.0.3" class="anchored" data-anchor-id="linear-and-quadratic-discriminant-analysis-in-the-context-of-mixture-models"><span class="header-section-number">76.0.3</span> Linear and quadratic discriminant analysis in the context of Mixture Models</h3>
</section>
<section id="classification-example" class="level3" data-number="76.0.4">
<h3 data-number="76.0.4" class="anchored" data-anchor-id="classification-example"><span class="header-section-number">76.0.4</span> Classification example</h3>
<p>This video discusses the code in the next section.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Iâ€™m going to illustrate now to use of mixture models for classification using the wind dataset. Unlike the previous datasets that we work with, this one is not included in R by default. So the two files that you need wind training and wind tests are available on the website, make sure that you download them and that you have them in the right directory for R to read them. And in this case, I made sure that I change the directory where Iâ€™m looking at before I start working with this, and that I put my files in there.</p>
<p>Okay, so the wind dataset is an interesting one, itâ€™s a series of measurements for different varieties of wine. They come from three different cultivars, and for each particular variety of wine. They did a chemical analysis and measure 13 different variables that have to do with different chemical components present in the world. So we have a label set where we know which samples from which of the three cultivars. And now we want to use the information that we clean out of that to classify to decide a series of new wines to assign them to the cultivar that we think they come from. We actually do know the truth for the test set, so we will actually first do the predictions weâ€™ll act as if we donâ€™t know what the cultivar of the test set is. And then we will compare the predictions that weâ€™re making against the truth, as a way to tell how well the algorithm is to it, okay. So the first thing that we need to do is load our dataset as I said, you need to make sure that the two files are in the directory where youâ€™re working. So make sure of that, remember that we called n the sample size of the training set and m the training size the size of the test set. So Iâ€™m just calling the variables that way, and Iâ€™m going to use mixture of normals mixture of multivariate normals by location and scale. So Iâ€™m going to use a method that is essentially equivalent to doing quadratic discriminant analysis. And, I want to run the Algorithm that I discussed on the board, but in a situation which we assume that weâ€™re going to work with semi-supervised learning. In other words, I went around the Version of the algorithm in which weâ€™re going to use all the observation both in the training and the test set, to learn the parameters of the classes. So itâ€™s going to be an iterative algorithm. So we know in advance as we have three classes because we have three cultivars. B in this case is going to be 13 because there are 13 features that were measured on each wine. So if you come down here, you can see that B 13, we can try to do a graph of the data. In this case the graph is not going to be terribly readable because there are so many variables, but it may still provide a little bit of intuition. So the variables that are measured things like alcohol, the ash, the alkalinity, the level of magnesium, the hue that has to do with how dark the wine is, proline. So you can see here where the variables are there are measured, and even though the graph is not very readable at least you can see that the classes do not fully overlap. So we do have some hope that we may be able to do classification in the problem. Thatâ€™s pretty much the main thing that you can say out of this graph here, okay. So, as I said before mixture of models with different components, different variances and different means for each component its normal component in the mixture. Same type of standard initialization that we have done before. And weâ€™re going to do the E and the M step here, remember that for the observations in the training set. We know the class, so the value of B are either 0 or 1, and because we do the calculation first in the log scale, then we do either 0 or minus infinity. So 0 corresponds to probability of 1 and minus infinity corresponds to a probability of 0 in the log scale. And then for the observations in the test set, we have just a regular way in which we compute the probability that the observation comes from each class. And once we have done this then we subtract, we do as we have always done subtract maximums and then re-standardize. So this is how the ES step gets adapted in the case of semisupervised classification. And then the structure of the m-step is exactly the same structure of the regular Algorithm. So we compute means and variances for each one of the components as weighted averages of the different quantities. We check conversions in the standard way, in which we have been checking convergence. And finally once everything is done, we will get a classification, so letâ€™s run it for this dataset. It runs actually quite quickly, we have only 12 iterations and we have converged. Now what the Algorithm gives gave us is just the B values, that is the probability that an observation comes from a given class. Now, we typically are going to want to convert those peas into Cs and as we saw on the board, that is done by just selecting the class that has the highest probability.</p>
<p>So if we do that for our training set in this case, and if you look at the indexes here, they run from n + 1 to n + m, which means that weâ€™re looking at test set. If we just get what is the maximum we can see that the first block of observations is assigned to component two. Most of this block is assigned to component two except for this guy here, and then the the remaining block of observation is assigned to components three. So now how does that compare with the truth? So we can actually go into winder test, and the first column of that file contains the true labels, and we can say that it matches actually pretty well. So the ones all match, the twos match except for one guy, the one we had kind of identified before, and the threes all match together. And we can actually if you just want to have a summary of how many errors you make. You can do a little comparison like this, and you can find that there is only a single error in the classification that the algorithm does.</p>
<p>Now letâ€™s compare that with just using quadratic discriminant analysis and linear discriminant analysis. The way they are implemented in R, so QDA and LDA are the two functions that you will need, they are part of the mass package. So, We first feed the QDA model and then we that fitted model to predict the classes. And now if we see what the regular QDA does is itâ€™s going to give me this long list of probabilities for the test set. And we can turn those into labels and in particular we can see how many errors weâ€™re making in the prediction. And you can see that we make a single mistake, which is actually not the mistake that we had made before. So if we just look at this one here and we compare it against the, Classification that our algorithm did, and we compared it against the truth.</p>
<p>We see that our algorithm makes a mistake in this observation and QDA does not, and instead the error is somewhere else in this sample. Itâ€™s basically here, so you can see that the QDA classifies this as two, when the reality is that itâ€™s a three. So our results are not identical to QDA even though our method is asymptotically going to be equivalent to QDA but they donâ€™t give us exactly the same result, but they give us very similar accuracy. Interestingly if you run LDA and you try to look at how many errors you have in that case, you will see that LDA in this case has no errors, even though itâ€™s a simpler more restrictive classification procedure. So this can happen, so itâ€™s a relatively large sample, so a single a difference in a single error is not a very large difference. So, hopefully this illustrates how classification or how measurements can be used for classification in a real life setting.</p>
</div>
</div>
</div>
</section>
<section id="sample-em-algorithm-for-classification-problems" class="level3" data-number="76.0.5">
<h3 data-number="76.0.5" class="anchored" data-anchor-id="sample-em-algorithm-for-classification-problems"><span class="header-section-number">76.0.5</span> Sample EM algorithm for classification problems</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Using mixture models for classification in the wine dataset</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Compare linear and quadratic discriminant analysis and a </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   (semi-supervised) location and scale mixture model with K normals</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Comparing only against the EM algorithm</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Semi-supervised, quadratic discriminant analysis </span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="do">### Loading data and setting up global variables</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>wine.training <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">"data/wine_training.txt"</span>, <span class="at">sep=</span><span class="st">","</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>wine.test <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">"data/wine_test.txt"</span>, <span class="at">sep=</span><span class="st">","</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(wine.training)[<span class="dv">1</span>]  <span class="co"># Size of the training set</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> <span class="fu">dim</span>(wine.test)[<span class="dv">1</span>]      <span class="co"># Size of the test set</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">as.matrix</span>(wine.training[,<span class="sc">-</span><span class="dv">1</span>]), <span class="fu">as.matrix</span>(wine.test[,<span class="sc">-</span><span class="dv">1</span>]))   <span class="co"># Create dataset of observations, first n belong to the training set, and the rest belong to the test set</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>p       <span class="ot">=</span> <span class="fu">dim</span>(x)[<span class="dv">2</span>]              <span class="co"># Number of features</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>KK      <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>epsilon <span class="ot">=</span> <span class="fl">0.00001</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>)<span class="sc">+</span><span class="fl">0.1</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>colscale <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"black"</span>,<span class="st">"red"</span>,<span class="st">"blue"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">col=</span>colscale[wine.training[,<span class="dv">1</span>]], <span class="at">pch=</span>wine.training[,<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C3-L07_files/figure-html/lbl-em-algorithm-classification-sample-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="C3-L07_files/figure-html/lbl-em-algorithm-classification-sample-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the parameters of the algorithm</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">63252</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>w   <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>,KK)<span class="sc">/</span>KK  <span class="co">#Assign equal weight to each component to start with</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>mu  <span class="ot">=</span> <span class="fu">rmvnorm</span>(KK, <span class="fu">apply</span>(x,<span class="dv">2</span>,mean), <span class="fu">var</span>(x))   <span class="co">#Cluster centers randomly spread over the support of the data</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>Sigma      <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim=</span><span class="fu">c</span>(KK,p,p))  <span class="co">#Initial variances are assumed to be the same</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">1</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK  </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">2</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">3</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>sw     <span class="ot">=</span> <span class="cn">FALSE</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>KL     <span class="ot">=</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>KL.out <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>s      <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span>(<span class="sc">!</span>sw){</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="do">## E step</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  v <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim=</span><span class="fu">c</span>(n<span class="sc">+</span>m,KK))</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){  <span class="co">#Compute the log of the weights</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    v[<span class="dv">1</span><span class="sc">:</span>n,k] <span class="ot">=</span> <span class="fu">ifelse</span>(wine.training[,<span class="dv">1</span>]<span class="sc">==</span>k,<span class="dv">0</span>,<span class="sc">-</span><span class="cn">Inf</span>)  <span class="co"># Training set</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    v[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m),k] <span class="ot">=</span> <span class="fu">log</span>(w[k]) <span class="sc">+</span> mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(x[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m),], mu[k,], Sigma[k,,],<span class="at">log=</span><span class="cn">TRUE</span>)  <span class="co"># Test set</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    v[i,] <span class="ot">=</span> <span class="fu">exp</span>(v[i,] <span class="sc">-</span> <span class="fu">max</span>(v[i,]))<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">exp</span>(v[i,] <span class="sc">-</span> <span class="fu">max</span>(v[i,])))  <span class="co">#Go from logs to actual weights in a numerically stable manner</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>  <span class="do">## M step</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">=</span> <span class="fu">apply</span>(v,<span class="dv">2</span>,mean)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span>KK, <span class="at">ncol=</span>p)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>      mu[k,]    <span class="ot">=</span> mu[k,] <span class="sc">+</span> v[i,k]<span class="sc">*</span>x[i,]</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    mu[k,] <span class="ot">=</span> mu[k,]<span class="sc">/</span><span class="fu">sum</span>(v[,k])</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>,<span class="at">dim=</span><span class="fu">c</span>(KK,p,p))</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>      Sigma[k,,] <span class="ot">=</span> Sigma[k,,] <span class="sc">+</span> v[i,k]<span class="sc">*</span>(x[i,] <span class="sc">-</span> mu[k,])<span class="sc">%*%</span><span class="fu">t</span>(x[i,] <span class="sc">-</span> mu[k,])</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    Sigma[k,,] <span class="ot">=</span> Sigma[k,,]<span class="sc">/</span><span class="fu">sum</span>(v[,k])</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>  <span class="do">##Check convergence</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>  KLn <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>      KLn <span class="ot">=</span> KLn <span class="sc">+</span> v[i,k]<span class="sc">*</span>(<span class="fu">log</span>(w[k]) <span class="sc">+</span> mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(x[i,],mu[k,],Sigma[k,,],<span class="at">log=</span><span class="cn">TRUE</span>))</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">abs</span>(KLn<span class="sc">-</span>KL)<span class="sc">/</span><span class="fu">abs</span>(KLn)<span class="sc">&lt;</span>epsilon){</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    sw<span class="ot">=</span><span class="cn">TRUE</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>  KL <span class="ot">=</span> KLn</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>  KL.out <span class="ot">=</span> <span class="fu">c</span>(KL.out, KL)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>  s <span class="ot">=</span> s <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste</span>(s, KLn))</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "1 -3146.58419305226"
[1] "2 -2942.48222029706"
[1] "3 -2873.76499310479"
[1] "4 -2852.76768638231"
[1] "5 -2796.247735428"
[1] "6 -2791.29098585679"
[1] "7 -2791.23059641487"
[1] "8 -2791.14094416728"
[1] "9 -2791.05612416221"
[1] "10 -2790.99254414223"
[1] "11 -2790.95228067601"
[1] "12 -2790.92945838389"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Predicted labels</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3
[39] 3 3 3 3 3 3 3 3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">## True labels</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>wine.test[,<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3
[39] 3 3 3 3 3 3 3 3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Comparison</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="sc">==</span> wine.test[,<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
[25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(<span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># One error</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the qda and lda functions in R</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># qda</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>modqda <span class="ot">=</span> <span class="fu">qda</span>(<span class="at">grouping=</span>wine.training[,<span class="dv">1</span>], <span class="at">x=</span>wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">method=</span><span class="st">"mle"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>ccpredqda <span class="ot">=</span> <span class="fu">predict</span>(modqda,<span class="at">newdata=</span>wine.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(ccpredqda<span class="sc">$</span>class <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># One error</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># lda</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>modlda <span class="ot">=</span> <span class="fu">lda</span>(<span class="at">grouping=</span>wine.training[,<span class="dv">1</span>], <span class="at">x=</span>wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">method=</span><span class="st">"mle"</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>ccpredlda <span class="ot">=</span> <span class="fu">predict</span>(modlda,<span class="at">newdata=</span>wine.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(ccpredlda<span class="sc">$</span>class <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># No errors!!!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0</code></pre>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-james2013introduction" class="csl-entry" role="listitem">
James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. Springer Texts in Statistics. Springer New York. <a href="https://books.google.co.il/books?id=qcI_AAAAQBAJ">https://books.google.co.il/books?id=qcI_AAAAQBAJ</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/âŒ˜/g, 'âŒƒ');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C3-L06.html" class="pagination-link" aria-label="Clustering - M4L6">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C3-L07-Ex1.html" class="pagination-link" aria-label="Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1">
        <span class="nav-page-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb16" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="an">title :</span><span class="co"> 'Classification - M4L7'</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle :</span><span class="co"> 'Bayesian Statistics: Mixture Models - Applications'</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">  - Bayesian Statistics</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - Mixture Models</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - Classification</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - notes</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>Classification is a supervised learning problem where we want to predict the class of a new observation based on its features.</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>According to the instructor the main difference from clustering is that in classification we have a training set. I would think the main difference is that we have labels for some of the data, while in clustering we do not have labels at all.</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>The fact that we have labels and a training set means we should know how many classes we have and we can use these labels to train a model and use it to predict the class of a new observation.</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>The instructor mentions Support Vector Machines (SVM), logistic regression and linear discriminant analysis (LDA) as familiar examples of classification methods. These and a number of others are covered in <span class="co">[</span><span class="ot">@james2013introduction</span><span class="co">]</span>. We will focus on Naive Bayes classifiers as it is the most similar to mixture models and the EM algorithm which we have seen earlier</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mixture Models and naive Bayes classifiers</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="al">![K-means clustering](images/c3l4-ss-04-classification-overview.png)</span>{.column-margin width="53mm"}</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="al">![K-means clustering](images/c3l4-ss-05-classification-naive-bayes.png)</span>{.column-margin width="53mm"}</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="al">![Mixture Models for Clustering](images/c3l4-ss-06-classification-mixtures.png)</span>{.column-margin width="53mm"}</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Naive Bayes classifiers</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>The idea of Naive Bayes classifiers is that we want to know what is the probability that observation i belongs to class k and we can obtain this using Bayes' theorem by computing the prior probability that an observation is in that class. This is just the frequency of the class multiplied by the density of that class and divided by the sum over the classes of the same expression.</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(x_i \in \text{class}_k) = \frac{w_k \cdot g_k(x_i|\theta_k)}{\sum_{j=1}^K w_j \cdot g_j(x_i|\theta_j)}</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>$$ {#eq-bayes-classifier}</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>where $w_k$ is the prior probability of class k, $g_k(x_i|\theta_k)$ is the density of class k, and $\theta_k$ is the parameter of class k.</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>with </span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>\tilde{c}_i = \arg \max_k \mathbb{P}r(x_i \in \text{class}_k)\ for \; i=n+1,\ldots,n+m</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>The naive Bayes classifier assumes that the features are conditionally independent given the class. This means that the density of class k can be written as the product of the densities of each feature given the class:</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>g_k(x_i|\theta_k) = \prod_{l=1}^p g_{kl}(x_{il}|\theta_{kl})</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>where $g_{kl}(x_{il}|\theta_{kl})$ is the density of feature l given class k and $\theta_{kl}$ is the parameter of feature l given class k. This means that we can estimate the density of each feature separately and then multiply them together to get the density of the class.</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>This is a very strong assumption and is not true in general. However, it works well in practice and is often used in text classification problems where the features are the words in the text.</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>The naive Bayes classifier is a special case of the mixture model where the components are the classes and the densities are the product of the densities of each feature given the class. This means that we can use the EM algorithm to estimate the parameters of the model in the same way as we did for the mixture model. The only difference is that we need to estimate the densities of each feature separately and then multiply them together to get the density of the class.</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"} </span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>The last class of problems for which mixture models are very useful is classification problems.  If you come from the machine learning literature, you will call this supervised classification to contrast, again, unsupervised classification that I called clustering before. The goal in supervised classification is to start with a training set and use the information in a training set to determine the classes or the labels of a second group of observations that you call the test set. So you start with a training set that contains known labels classes. You also have a test set that has unknown labels, and you want to use this information to make predictions about the test set labels. For example, you may want to decide whether a person suffers from a disease or not based on a set of medical tests, maybe P medical tests, and you have gone out and measured those tests in a number of individuals. So you know those individuals whether they are sick or they are not sick. Based on that training set that is labeled where you know what the real quality</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>of the individuals is, then you go out and</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>you are going to pick just a random person that comes into your medical appointment, and based on the</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>results of the test, now you want to decide</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>if that individual suffers from the disease or not. So the presence of the</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>training set is really what distinguishes</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>clustering problems from classification problems. In clustering problems, we</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>don't have a training set. We don't have anything</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>that gives us a hint about how the</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>classes look like. We're trying to do the process of dividing the observations into groups in some sense blindly. That's why it's sometimes called unsupervised classification</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>because you can think that the training set provides supervision in how you</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>do the classification. In typical supervised classification</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>problems on the other hand, you do have that training set. You do have that group of labeled observations</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>that can help you make decisions about how the new groups</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>will look like. So in some sense, supervised classification</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>is a simpler problem than unsupervised</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>classification because of the presence of</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>the training set. Now, there are a number of classification</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>procedures out there. This is a fairly common</span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>problem in the literature. You may be familiar with things like support vector machines or logistic regression</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>for classification. I want to discuss today the similarities between</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>using mixture models for classification and</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>some techniques such as linear discriminant analysis, and in particular with Naive Bayes classifiers. The idea of Naive Bayes</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>classifiers is very simple. So if you want to know what</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>is the probability that observation i belongs to class k, you can typically obtain</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>that by just using Bayes' theorem by computing the prior probability that an observation is in that class. That is typically</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>just the frequency of the class multiplied by the density of that class and divided by the sum over the classes</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>of the same expression. Now, again, this should</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>be very familiar. This quantity here is</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>essentially what we used both in the EM</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>algorithm to compute the <span class="co">[</span><span class="ot">inaudible</span><span class="co">]</span> case and in the MCMC algorithm if you are fitting a mixture model from a Bayesian perspective to sample the class labels C sub x. So in other words, it's clear just from writing the expression</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>from Naive Bayes that there should be a very</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>close relationship between doing Naive Bayes</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>and doing mixture models. In fact, you can cast Naive Bayes</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>classifiers as just as a special case of mixture models. Let's discuss Naive</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>Bayes classifiers where we use Gaussian kernels</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>for the classification. Let's enter this a</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>little bit of notation. So remember that we have both a test set and</span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>a training set. So let's call X_1 up to</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>X_n my training set, and let's call X_n plus 1 up</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>to X_n plus m the test set. In other words, we have n observations in</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>the training set, we have m observations in the test set and we just</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>group the observations together so that the</span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>first n in the sample are the training and</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>last m are the test. In addition to this, because the training</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a>set is labeled, we're going to have C_1</span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>up to C_n are known, but C_1 or C_n plus</span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>1 up to C_m plus n are unknown and we</span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a>want to protect them. Let's write a Naive</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a>Bayes classifier that uses Gaussian kernels, and we're going to use the more general Gaussian</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>kernels that we can. So in that case, the probability</span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a>that observation i belongs to class k, it's going to be</span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a>equal to Omega_k 1 over the square root 2 Pi to the p. Remember that we're working</span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a>with P variate normal. So we can have P features</span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a>for each individual, determinant of Sigma_k to the minus one 1/2</span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a>X of minus one 1/2 X_i minus Mu k transpose sigma sub k</span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a>inverse X_i minus Mu k, divided by the sum over the components of</span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>exactly the same expression. This has to be l, minus Mu sub l transpose</span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>sigma l inverse X_i minus Mu l. So this is just Bayes theorem as we have written multiple</span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a>times in this course. So what you do is, you need this expression</span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a>only for the training set because for the test</span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a>set you already know what class you are in. So what you typically do is</span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a>a two-step process in which you get Mu k hat and Sigma hat sub k are estimated from the training set. You could do different things, but it's very fairly</span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a>common to just fit a multivariate Gaussian to each one of the components. So your Cs, your labels divide your</span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a>training set into groups. For each one of those groups, you fit one different</span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a>normal and that gives you Sigma and Mu. Similarly, for Omega k, you want to get an</span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a>estimate for Omega k, and the natural thing to do</span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a>is to just use the frequency, the fraction of the observations in the training set that belong to each one</span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a>of the classes. Once you have those, then you classify</span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a>new observations as by letting C_i be equal to the org max</span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a>of that probability. Where the probabilities</span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>are computed by plugging in these maximum</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a>likelihood estimators in this formula up here. As I said, this is done</span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a>for n plus 1 all the way to n plus m. So you don't need to do this</span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>for the training set, the training set</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a>you know the labels and you use those labels to compute the MLEs that</span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a>get plugged into this. Now, with additional</span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>observations in those MLEs, you can decide what are</span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a>the classes for them. So this is what a naive</span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a>Bayes classifier based on Gaussian distributions for each one of the classes</span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a>would look like. Now, this is exactly the same as the EM algorithm that we have discussed in the</span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a>past for mixture models, if we make a couple of assumptions or if we incorporate a couple of assumptions</span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a>into the algorithm. So let's write down that next. We can recast the</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a>algorithms that we just saw for naive Bayes</span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a>classifier based on Gaussian kernels</span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>in the context of the EM algorithm that we have been discussing</span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>for mixtures. That is very easy, we're going to think, again, about an E-step and an M-step, and we're going to add an additional post-processing</span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>step, if you will. In our E-step, if you remember, what we did in the</span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>past was to compute the indicators for the variables. So that is our variables V_i,k that corresponds to the weights that are associated with each</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a>one of the components. What we're going to do</span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a>in this case is we're going to define the V_i,k in a very simple fashion</span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a>rather than doing it using Bayes theorem. Because we actually know what observations</span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a>or what components are generating each of the observations in the training set, we can call V_i,k</span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a>just one or zero if C_i is equal to k</span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a>and zero otherwise, for all the observations</span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a>that go from one to n. In other words, this is for the training set. Once we have defined</span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a>our E-step in this way, we're going to have an</span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a>M-step where we compute Mu sub k and Omega sub k. To put it in the same way that we</span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a>did with the EM algorithm, this is going to have a</span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a>very particular shape. It's going to have the sum</span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a>from one to n of V_i,k X_i divided by the sum from one to n of V_i,k. In a similar expression</span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a>for my matrix Sigma, Sigma is going to be Sigma sub k, it's going to be</span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a>one over the sum of the V_i,k from one to n, sum from one to n of V_i,k X_i minus Mu k, X_i minus Mu k transpose. These are expressions that we</span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a>have seen in the past when filling mixtures of</span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a>multivariate Gaussians to data. This is just a fancy way, so casting it in terms of</span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a>the E-step and the M-step, it's just a fancy way to say, I know what my assignments are, for sure, because this</span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a>is a training set. So this is just computing the average of the</span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a>observations that are in category K because, in this case, these are</span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a>either zeros or ones. Similarly, here, this is just the variance</span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a>covariance matrix of the observations that</span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a>are in component K, but it's written in</span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a>a fancy way using this V_i,k as indicators. Then, we have a post-processing. It's in the</span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a>post-processing step where the test set comes into play. So for now, we have only used the training set for</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a>our calculations. In the post-processing step, what we do is we</span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a>allocate C_i based on the arc max over K of the posterior distribution</span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a>of the class allocations. So that is probability that X_i belongs to class K. So this is just another way to write the algorithms</span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a>as we had before, that is very simple in the</span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a>context of <span class="co">[</span><span class="ot">inaudible</span><span class="co">]</span>. So why did I go through the</span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a>trouble of expressing this in this complicated manner when I had a very simple</span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a>description before? Well, because now you can try to generalize this from</span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a>this supervised setting where you</span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a>completely break apart the estimation</span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a>of the parameters that only uses the training set and the classification that</span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a>only uses the test set. You can actually try to</span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a>combine information from both, and it should be</span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a>clear that if you have training sets that are just very small</span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a>compared to the test set, the estimates that you get</span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a>for Mu and Sigma will be very bad because they will be based on very few observations,</span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a>very few data points. So if you could somehow use some of the information</span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a>that you are recovering by doing the</span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a>classification to help you estimate what</span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a>Mu and Sigma are, they'll probably give you more robust, stronger algorithm. How to do that should be</span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a>relatively straightforward once you think about</span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a>it in this context. For the observations</span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a>to the training set, we have the value of the V_i,k, but we could add an estimate of the</span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a>value of the V_i,k for the observations in the test</span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a>set to this calculation. We already know how to do that. So we're going to turn the</span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a>algorithm iterative now. So these guys are always going to be defined in this way because</span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a>I know the C's, but these guys are refined at every iteration</span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a>of the algorithm. I'll just make this essentially</span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a>equal to the probability that X_i belongs to class K given the current</span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a>parameters of the model, so given the current Omegas, the current Mus, and</span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a>the current Sigmas. Then, I can extend my sums to</span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a>m down here and down here. Now, what I'm doing is, for the observations that I</span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a>know what class they are in, these weights are</span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a>either zeros or ones. For the ones that I don't know but I'm trying to classify, they will be some number</span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a>between zero and one, and I'm just going to do a weighted average so you</span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a>can think about this, again, as a weighted average of the information that</span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a>I know for sure, and the information that</span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a>I'm recovering about Mu and Sigma from the</span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a>classification. So again, this now becomes</span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a>an iterative algorithm, so I need to think about</span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a>t plus 1, t plus 1, t plus 1, t plus 1, t plus 1, t plus 1, and t plus 1. So I have turned what was just a two-step algorithm that doesn't require</span>
<span id="cb16-217"><a href="#cb16-217" aria-hidden="true" tabindex="-1"></a>any iteration, I turned it into an iterative</span>
<span id="cb16-218"><a href="#cb16-218" aria-hidden="true" tabindex="-1"></a>algorithm that uses the whole sample to estimate the parameters</span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a>of the classes. This is sometimes called</span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a>a semi-supervised; I don't necessarily like</span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a>the term very much. But this is sometimes called</span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a>a semi-supervised algorithm, in the sense that it's</span>
<span id="cb16-223"><a href="#cb16-223" aria-hidden="true" tabindex="-1"></a>not completely supervised because of the addition of this information</span>
<span id="cb16-224"><a href="#cb16-224" aria-hidden="true" tabindex="-1"></a>and the fact that now, the sums go up to m.</span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a>But it's also not fully unsupervised because</span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a>I'm using the information, I'm using this piece</span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a>up here that has information where I</span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a>know their true labels. Once the algorithm converges, I'm still going to do the post-processing</span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a>step that is to go from this V_i,k's that I computed here for the test set to generate what are the</span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a>labels for those observations.</span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a><span class="fu">### LDA and the EM algorithm</span></span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-237"><a href="#cb16-237" aria-hidden="true" tabindex="-1"></a><span class="al">![LDA](images/c3l4-ss-08-lda.png)</span>{.column-margin width="53mm"}</span>
<span id="cb16-238"><a href="#cb16-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-239"><a href="#cb16-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a><span class="al">![LDA](images/c3l4-ss-09-lda.png)</span>{.column-margin width="53mm"}</span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a>It is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis and also  to quadratic discriminant analysis.</span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a>To illustrate that connection, we start with a very simple mixture model. </span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a>So let's start with a mixture model of the form, </span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-249"><a href="#cb16-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-250"><a href="#cb16-250" aria-hidden="true" tabindex="-1"></a>f(x) = \sum_{k=1}^2 \omega_k \frac{1}{\sqrt{2\pi}} \frac{1}{\sqrt{\text{det}(\Sigma)}} e^{-\frac{1}{2}(x - \mu_k)^T \Sigma^{-1} (x - \mu_k)}.</span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a>It is important to understand the connection between using mixture models for classification and other procedures that are commonly used out there for classification. One example of those procedures that has a strong connection is linear discriminant analysis. </span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a>And also, by the way, quadratic discriminant analysis.  But let's start with linear discriminant analysis. </span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a>And to illustrate that connection, let's start with a very simple mixture model. </span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>So let's start with a mixture model of the form, $f(x) =$ the sum from 1 to 2. </span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a>So I'm going to be working only with two components of omega k, 1 over the square root 2pi to the p determinant of sigma to the -1 half, x- 1 half, x, mu sub k, transpose sigma inverse, x- mu sub k. </span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a>So this is two-component mixture with locations bearing with a component, but the same variance-covariance matrix for the two components that I have in the mixture. </span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a>And let's think about how the procedure would look like if we were to do Naive Bayes classification using this mixture. </span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>If I follow the unsupervised example that I have discussed before, the probability that I put observation i in class, 1, say, I only have two classes. </span>
<span id="cb16-268"><a href="#cb16-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-269"><a href="#cb16-269" aria-hidden="true" tabindex="-1"></a>So as you see again, consider one of them and the other one is just 1- the numbers that I get here. </span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a>It's going to be equal. </span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a>And I'm going to expand this in all its glory. </span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a>It's going to be a little bit long. So it's going to be omega 1, 1 over the square root 2pi to the p determinant of sigma to the- 1 half, x of- 1 half, x- mu k transpose sigma inverse x- mu k. </span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a>And in the denominator, we're going to have the same expression first. </span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a>And then we're going to have omega 2, that is just 1- omega 1 but 2pi to the p determinant of sigma to the- 1 half x- 1 half x- mu 2, sigma inverse x- mu 2. </span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>Okay, and we know that the probability that xi belongs to class 1 is exactly the same expression but replacing mu, 1 which is what should be up here, replacing mu1 with mu2. </span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a>So, in the post processing step, we are going to assign C sub i = 1 if and only if the probability that xi belongs to class 1 is greater than the probability that xi belongs to class 2. </span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a>And because the two expressions are the same in the denominator, the only thing that</span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a>changes is the numerator, then this happens if and only if omega 1, 1 over the square root 2pi to the p determinant sigma to the- 1 half x- 1 half, X- mu1 transpose sigma inverse x- mu1, Is greater than omega 2, 1 over the square root 2pi to the p determinant of sigma to the- 1 half x of- 1 half x- mu2, sigma inverse x- mu2. </span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a>So probability of class 1 greater than probability of class 2 only if this quantity is greater than the same thing but evaluated for the second component in the mixture. </span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a>So let's do a little bit of algebra and</span>
<span id="cb16-283"><a href="#cb16-283" aria-hidden="true" tabindex="-1"></a>let's try to simplify this expression a little bit and we will see that that simplification leads to a very nice expression that matches exactly what you get out of linear discriminant analysis. </span>
<span id="cb16-284"><a href="#cb16-284" aria-hidden="true" tabindex="-1"></a>So now we want to simplify this expression that corresponds to the situation where we're going to label an observation coming from class 1, and we want to make it much more compact. </span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a>So a few things that we can observe. So one of them is we have 1 over square root 2pi to the p on both sides, so we can cancel that. </span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a>The other thing that we observe is that we have the determinant of the variance-covariance matrix on both sides. </span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a>And because we're assuming that the two components have the same variance- covariance matrix, we can again just simplify both terms on either side. </span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a>And the next thing that I'm going to do is I'm going to move all the omegas to one side and bring all the terms with the exponentials to the other side. </span>
<span id="cb16-289"><a href="#cb16-289" aria-hidden="true" tabindex="-1"></a>If I do that, I'm going to end up on the left hand side with the exponent of- 1 half, X- mu1 transpose sigma inverse x- mu1. </span>
<span id="cb16-290"><a href="#cb16-290" aria-hidden="true" tabindex="-1"></a>And then this term came to the other side in the denominator, but that just means that when it goes into the exponential, I need to change all to reverse signs. So it's going to be- x- mu2 transpose sigma inverse x- mu2. </span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a>So that's the expression once you move this to the denominator and combine the two exponentials. </span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a>And this needs to be greater than omega 2 divided by omega 1. Now, some further simplifications. I can take the logarithm on both sides and I can multiply by -2 on both sides, and I end up with an expression that looks like x- mu 1 transpose sigma inverse x- mu1- x- mu 2 transpose sigma inverse x- mu 2 has to be less than, because I'm going to end up multiplying by a -2. </span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a>So less than -2 log of omega 2 divided by omega 1. </span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a>So now we have this difference of two quadratic forms needs to be less than a certain constant that depends on what are my prior weights for each one of the two components. </span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a>Now, to finish simplifying this, we need to expand these two squares, which is pretty straightforward. </span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a>So first we're going to have x sigma inverse x transpose sigma inverse x. </span>
<span id="cb16-297"><a href="#cb16-297" aria-hidden="true" tabindex="-1"></a>This is just a square. So it's going to be 2 times x transpose sigma inverse mu1. </span>
<span id="cb16-298"><a href="#cb16-298" aria-hidden="true" tabindex="-1"></a>And finally, $\mu_1$ transpose sigma inverse $\mu_1$. </span>
<span id="cb16-299"><a href="#cb16-299" aria-hidden="true" tabindex="-1"></a>And then we need to subtract a similar expression but using mu2 for it turns. </span>
<span id="cb16-300"><a href="#cb16-300" aria-hidden="true" tabindex="-1"></a>So it's going to be x transpose sigma inverse x. </span>
<span id="cb16-301"><a href="#cb16-301" aria-hidden="true" tabindex="-1"></a>It's going to be +, in this case, 2x transpose sigma inverse mu2. </span>
<span id="cb16-302"><a href="#cb16-302" aria-hidden="true" tabindex="-1"></a>And finally, again,- mu2 transpose sigma inverse mu2, and all of these needs to be less than -2 log of omega 2, Divided by omega 1. </span>
<span id="cb16-303"><a href="#cb16-303" aria-hidden="true" tabindex="-1"></a>So you can see that the expressions are relatively straightforward. </span>
<span id="cb16-304"><a href="#cb16-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-305"><a href="#cb16-305" aria-hidden="true" tabindex="-1"></a>And one of the things that is very nice, and it's a consequence of having the same variance-covariance matrix for each one of the components, is that now this quadratic term of the data is going to cancel out. </span>
<span id="cb16-306"><a href="#cb16-306" aria-hidden="true" tabindex="-1"></a>And so, we can just basically learn together a couple of terms. So we can write, 2 times, X transpose sigma inverse multiplied by mu2- mu1. </span>
<span id="cb16-307"><a href="#cb16-307" aria-hidden="true" tabindex="-1"></a>So I'm taking this term and combining it with this term. So, the term here and the term here. </span>
<span id="cb16-308"><a href="#cb16-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-309"><a href="#cb16-309" aria-hidden="true" tabindex="-1"></a>And then I'm going to say that this has to be less than -2 times log of omega 2 divided by omega 1, and I'm going to move this two terms to the right. </span>
<span id="cb16-310"><a href="#cb16-310" aria-hidden="true" tabindex="-1"></a>So,+ mu2 transpose sigma inverse mu2- mu1 transpose sigma inverse mu1. </span>
<span id="cb16-311"><a href="#cb16-311" aria-hidden="true" tabindex="-1"></a>So this is actually quite a bit of simplification and it's a very interesting one. </span>
<span id="cb16-312"><a href="#cb16-312" aria-hidden="true" tabindex="-1"></a>Because you can think about this, Thing on the right hand side, just call this T for threshold. </span>
<span id="cb16-313"><a href="#cb16-313" aria-hidden="true" tabindex="-1"></a>So this is your sum threshold and that threshold is basically computed based on the training data. </span>
<span id="cb16-314"><a href="#cb16-314" aria-hidden="true" tabindex="-1"></a>So if I know the classes of some observations, I can get what the means for each one of the classes are, I can estimate the common sigma, and I can estimate the relative frequencies. </span>
<span id="cb16-315"><a href="#cb16-315" aria-hidden="true" tabindex="-1"></a>And with that, I can obtain a stress score from the training set. </span>
<span id="cb16-316"><a href="#cb16-316" aria-hidden="true" tabindex="-1"></a>And I can think about this matrix product as sum vector a. </span>
<span id="cb16-317"><a href="#cb16-317" aria-hidden="true" tabindex="-1"></a>The form of this simplified expression is very interesting. </span>
<span id="cb16-318"><a href="#cb16-318" aria-hidden="true" tabindex="-1"></a>You can see that the right-hand side, all this expression in the box, it's just a threshold that can be easily computed from the training set. </span>
<span id="cb16-319"><a href="#cb16-319" aria-hidden="true" tabindex="-1"></a>We can estimate the weight and we can estimate the mean and the covariance of the two components. </span>
<span id="cb16-320"><a href="#cb16-320" aria-hidden="true" tabindex="-1"></a>And then, this product of the variance-covariance or the inverse of the variance-covariance matrix times the difference of the means corresponds to a vector a that can also be computed from the training set. </span>
<span id="cb16-321"><a href="#cb16-321" aria-hidden="true" tabindex="-1"></a>So essentially, the decision of whether we classify an observation in class 1 or class 2 is going to depend on whether a linear combination, and that's what x transpose times a is, is just a linear combination of the values of x. </span>
<span id="cb16-322"><a href="#cb16-322" aria-hidden="true" tabindex="-1"></a>So whether this linear combination of the values of x is greater than a given threshold or not. </span>
<span id="cb16-323"><a href="#cb16-323" aria-hidden="true" tabindex="-1"></a>In other words, what we're doing, In a setting where we only have two variables, for example, x1 and x2, the linear combination of</span>
<span id="cb16-324"><a href="#cb16-324" aria-hidden="true" tabindex="-1"></a>the entries is just a line on the plane. </span>
<span id="cb16-325"><a href="#cb16-325" aria-hidden="true" tabindex="-1"></a>So this product just corresponds to a line. </span>
<span id="cb16-326"><a href="#cb16-326" aria-hidden="true" tabindex="-1"></a>And by deciding whether we are above</span>
<span id="cb16-327"><a href="#cb16-327" aria-hidden="true" tabindex="-1"></a>the line or below the line, we're just saying that one of the regions corresponds to class, 2, and</span>
<span id="cb16-328"><a href="#cb16-328" aria-hidden="true" tabindex="-1"></a>the other region corresponds to class 1. </span>
<span id="cb16-329"><a href="#cb16-329" aria-hidden="true" tabindex="-1"></a>So this is the reason why the procedure is called linear discriminant analysis because it uses a straight line to decide whether observations should be classified in class 1 and class 2. </span>
<span id="cb16-330"><a href="#cb16-330" aria-hidden="true" tabindex="-1"></a>Now, there are some more interesting things that you can do. </span>
<span id="cb16-331"><a href="#cb16-331" aria-hidden="true" tabindex="-1"></a>For example, you don't have to assume that the sigmas are the same, you could assume that the sigmas are different. </span>
<span id="cb16-332"><a href="#cb16-332" aria-hidden="true" tabindex="-1"></a>If you were to do that, then you'd be in a situation that is analogous to this one with the main difference being that now these terms here wouldn't necessarily simplify. </span>
<span id="cb16-333"><a href="#cb16-333" aria-hidden="true" tabindex="-1"></a>But then, you can rearrange terms in such a way that now, you're going to have a quadratic form of x being less than a certain threshold. And in that case, you're separating hyperplane. </span>
<span id="cb16-334"><a href="#cb16-334" aria-hidden="true" tabindex="-1"></a>Instead of being a hyperplane or line, it's going to be a quadratic form. </span>
<span id="cb16-335"><a href="#cb16-335" aria-hidden="true" tabindex="-1"></a>And that is the reason why when you're doing Naive Bayes and you're working with kernels that are Gaussian and have different variance-covariance matrices, you call the procedure quadratic discriminant analysis. </span>
<span id="cb16-336"><a href="#cb16-336" aria-hidden="true" tabindex="-1"></a>Because it uses a quadratic form, a parabola or something like that to separate the two classes that you're working with. </span>
<span id="cb16-337"><a href="#cb16-337" aria-hidden="true" tabindex="-1"></a>The nice thing about thinking about this classification procedures in the context of mixture models is again, thinking about ways in which you can generalize and address the shortcomings of the procedure. </span>
<span id="cb16-338"><a href="#cb16-338" aria-hidden="true" tabindex="-1"></a>It's clear that the main issue with classification procedures based on Gaussians is that data in the real world sometimes doesn't look like multivariate Gaussian distributions. </span>
<span id="cb16-339"><a href="#cb16-339" aria-hidden="true" tabindex="-1"></a>So one possible extension is to instead of considering the density, this ps here to be a single Gaussian, you can kind of use mixtures a second time and borrow some ideas from when we did density estimation. </span>
<span id="cb16-340"><a href="#cb16-340" aria-hidden="true" tabindex="-1"></a>And say well, I'm going to have a mixture and each component of that mixture is in turn a second mixture that may have a few components. </span>
<span id="cb16-341"><a href="#cb16-341" aria-hidden="true" tabindex="-1"></a>And that may allow for the shape of the clusters to be much more general, and that's what we call mixture discriminant analysis. </span>
<span id="cb16-342"><a href="#cb16-342" aria-hidden="true" tabindex="-1"></a>As before, if you instead of doing the Algorithm and the simple maximum likelihood estimation that I described before, you instead use Bayesian estimators for your process, then you will have Bayesian equivalent of linear discriminant analysis and quadratic discriminant analysis. </span>
<span id="cb16-343"><a href="#cb16-343" aria-hidden="true" tabindex="-1"></a>So it is very useful to think about your statistical methods in the context of mixture models for the purpose of both generalizing and understanding the shortcomings of what you're doing.</span>
<span id="cb16-344"><a href="#cb16-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-345"><a href="#cb16-345" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-346"><a href="#cb16-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-347"><a href="#cb16-347" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear and quadratic discriminant analysis in the context of Mixture Models</span></span>
<span id="cb16-348"><a href="#cb16-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-349"><a href="#cb16-349" aria-hidden="true" tabindex="-1"></a><span class="fu">### Classification example</span></span>
<span id="cb16-350"><a href="#cb16-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-351"><a href="#cb16-351" aria-hidden="true" tabindex="-1"></a>This video discusses the code in the next section.</span>
<span id="cb16-352"><a href="#cb16-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-353"><a href="#cb16-353" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb16-354"><a href="#cb16-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-355"><a href="#cb16-355" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb16-356"><a href="#cb16-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-357"><a href="#cb16-357" aria-hidden="true" tabindex="-1"></a>I'm going to illustrate now to use of mixture models for classification using the wind dataset. </span>
<span id="cb16-358"><a href="#cb16-358" aria-hidden="true" tabindex="-1"></a>Unlike the previous datasets that we work with, this one is not included in R by default. </span>
<span id="cb16-359"><a href="#cb16-359" aria-hidden="true" tabindex="-1"></a>So the two files that you need wind training and wind tests are available on the website, make sure that you download them and that you have them in the right directory for R to read them. </span>
<span id="cb16-360"><a href="#cb16-360" aria-hidden="true" tabindex="-1"></a>And in this case, I made sure that I change the directory where I'm looking at before I start working with this, and that I put my files in there. </span>
<span id="cb16-361"><a href="#cb16-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-362"><a href="#cb16-362" aria-hidden="true" tabindex="-1"></a>Okay, so the wind dataset is an interesting one, it's a series of measurements for different varieties of wine. </span>
<span id="cb16-363"><a href="#cb16-363" aria-hidden="true" tabindex="-1"></a>They come from three different cultivars, and for each particular variety of wine. </span>
<span id="cb16-364"><a href="#cb16-364" aria-hidden="true" tabindex="-1"></a>They did a chemical analysis and measure 13 different variables that have to do with different chemical components present in the world. </span>
<span id="cb16-365"><a href="#cb16-365" aria-hidden="true" tabindex="-1"></a>So we have a label set where we know which samples from which of the three cultivars. </span>
<span id="cb16-366"><a href="#cb16-366" aria-hidden="true" tabindex="-1"></a>And now we want to use the information that we clean out of that to classify to decide a series of new wines to assign them to the cultivar</span>
<span id="cb16-367"><a href="#cb16-367" aria-hidden="true" tabindex="-1"></a>that we think they come from. </span>
<span id="cb16-368"><a href="#cb16-368" aria-hidden="true" tabindex="-1"></a>We actually do know the truth for the test set, so we will actually first do the predictions we'll act as if we don't know what the cultivar of the test set is. And then we will compare the predictions that we're making against the truth, as a way to tell how well the algorithm is to it, okay. </span>
<span id="cb16-369"><a href="#cb16-369" aria-hidden="true" tabindex="-1"></a>So the first thing that we need to do is load our dataset as I said, you need to make sure that the two files are in the directory where you're working. </span>
<span id="cb16-370"><a href="#cb16-370" aria-hidden="true" tabindex="-1"></a>So make sure of that, remember that we called n the sample size of the training set and m the training size the size of the test set. </span>
<span id="cb16-371"><a href="#cb16-371" aria-hidden="true" tabindex="-1"></a>So I'm just calling the variables that way, and I'm going to use mixture of normals mixture of multivariate normals by location and scale. So I'm going to use a method that is essentially equivalent to doing quadratic discriminant analysis. </span>
<span id="cb16-372"><a href="#cb16-372" aria-hidden="true" tabindex="-1"></a>And, I want to run the Algorithm that I discussed on the board, but in a situation which we assume that we're going to work with semi-supervised learning. </span>
<span id="cb16-373"><a href="#cb16-373" aria-hidden="true" tabindex="-1"></a>In other words, I went around the Version of the algorithm in which we're going to use all the observation both in the training and the test set, to learn the parameters of the classes. </span>
<span id="cb16-374"><a href="#cb16-374" aria-hidden="true" tabindex="-1"></a>So it's going to be an iterative algorithm. </span>
<span id="cb16-375"><a href="#cb16-375" aria-hidden="true" tabindex="-1"></a>So we know in advance as we have three classes because we have three cultivars. </span>
<span id="cb16-376"><a href="#cb16-376" aria-hidden="true" tabindex="-1"></a>B in this case is going to be 13 because there are 13 features that were measured on each wine. </span>
<span id="cb16-377"><a href="#cb16-377" aria-hidden="true" tabindex="-1"></a>So if you come down here, you can see that B 13, we can try to do a graph of the data. </span>
<span id="cb16-378"><a href="#cb16-378" aria-hidden="true" tabindex="-1"></a>In this case the graph is not going to be terribly readable because there are so many variables, but it may still provide a little bit of intuition. </span>
<span id="cb16-379"><a href="#cb16-379" aria-hidden="true" tabindex="-1"></a>So the variables that are measured things like alcohol, the ash, the alkalinity, the level of magnesium, the hue that has to do with</span>
<span id="cb16-380"><a href="#cb16-380" aria-hidden="true" tabindex="-1"></a>how dark the wine is, proline. </span>
<span id="cb16-381"><a href="#cb16-381" aria-hidden="true" tabindex="-1"></a>So you can see here where the variables are there are measured, and even though the graph is not very readable at least you can see that the classes do not fully overlap. </span>
<span id="cb16-382"><a href="#cb16-382" aria-hidden="true" tabindex="-1"></a>So we do have some hope that we may be able to do classification in the problem. </span>
<span id="cb16-383"><a href="#cb16-383" aria-hidden="true" tabindex="-1"></a>That's pretty much the main thing that you can say out of this graph here, okay. </span>
<span id="cb16-384"><a href="#cb16-384" aria-hidden="true" tabindex="-1"></a>So, as I said before mixture of models with different components, different variances and different means for each component its normal</span>
<span id="cb16-385"><a href="#cb16-385" aria-hidden="true" tabindex="-1"></a>component in the mixture. Same type of standard initialization that we have done before. </span>
<span id="cb16-386"><a href="#cb16-386" aria-hidden="true" tabindex="-1"></a>And we're going to do the E and the M step here, remember that for the observations in the training set. </span>
<span id="cb16-387"><a href="#cb16-387" aria-hidden="true" tabindex="-1"></a>We know the class, so the value of B are either 0 or 1, and because we do the calculation first in the log scale, then we do either 0 or minus infinity. </span>
<span id="cb16-388"><a href="#cb16-388" aria-hidden="true" tabindex="-1"></a>So 0 corresponds to probability of 1 and minus infinity corresponds to a probability of 0 in the log scale. </span>
<span id="cb16-389"><a href="#cb16-389" aria-hidden="true" tabindex="-1"></a>And then for the observations in the test set, we have just a regular way in which we compute the probability that the observation comes from each class. </span>
<span id="cb16-390"><a href="#cb16-390" aria-hidden="true" tabindex="-1"></a>And once we have done this then we subtract, we do as we have always done subtract maximums and then re-standardize. </span>
<span id="cb16-391"><a href="#cb16-391" aria-hidden="true" tabindex="-1"></a>So this is how the ES step gets adapted in the case of semisupervised classification. </span>
<span id="cb16-392"><a href="#cb16-392" aria-hidden="true" tabindex="-1"></a>And then the structure of the m-step is exactly the same structure of the regular Algorithm. </span>
<span id="cb16-393"><a href="#cb16-393" aria-hidden="true" tabindex="-1"></a>So we compute means and variances for each one of the components as weighted averages of the different quantities. </span>
<span id="cb16-394"><a href="#cb16-394" aria-hidden="true" tabindex="-1"></a>We check conversions in the standard way, in which we have been checking convergence. </span>
<span id="cb16-395"><a href="#cb16-395" aria-hidden="true" tabindex="-1"></a>And finally once everything is done, we will get a classification, so let's run it for this dataset. </span>
<span id="cb16-396"><a href="#cb16-396" aria-hidden="true" tabindex="-1"></a>It runs actually quite quickly, we have only 12 iterations and we have converged. </span>
<span id="cb16-397"><a href="#cb16-397" aria-hidden="true" tabindex="-1"></a>Now what the Algorithm gives gave us is just the B values, that is the probability that an observation comes from a given class. </span>
<span id="cb16-398"><a href="#cb16-398" aria-hidden="true" tabindex="-1"></a>Now, we typically are going to want to convert those peas into Cs and as we saw on the board, that is done by just selecting the class that has the highest probability. </span>
<span id="cb16-399"><a href="#cb16-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-400"><a href="#cb16-400" aria-hidden="true" tabindex="-1"></a>So if we do that for our training set in this case, and if you look at the indexes here, they run from n + 1 to n + m, which means that we're looking at test set. </span>
<span id="cb16-401"><a href="#cb16-401" aria-hidden="true" tabindex="-1"></a>If we just get what is the maximum we can see that the first block of observations is assigned to component two.</span>
<span id="cb16-402"><a href="#cb16-402" aria-hidden="true" tabindex="-1"></a>Most of this block is assigned to component two except for this guy here, and then the the remaining block of observation is assigned to components three. </span>
<span id="cb16-403"><a href="#cb16-403" aria-hidden="true" tabindex="-1"></a>So now how does that compare with the truth? So we can actually go into winder test, and the first column of that file contains the true labels, and we can say that it matches actually pretty well. </span>
<span id="cb16-404"><a href="#cb16-404" aria-hidden="true" tabindex="-1"></a>So the ones all match, the twos match except for one guy, the one we had kind of identified before, and the threes all match together. </span>
<span id="cb16-405"><a href="#cb16-405" aria-hidden="true" tabindex="-1"></a>And we can actually if you just want to have a summary of how many errors you make. </span>
<span id="cb16-406"><a href="#cb16-406" aria-hidden="true" tabindex="-1"></a>You can do a little comparison like this, and you can find that there is only a single error in the classification that the algorithm does. </span>
<span id="cb16-407"><a href="#cb16-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-408"><a href="#cb16-408" aria-hidden="true" tabindex="-1"></a>Now let's compare that with just using quadratic discriminant analysis and linear discriminant analysis. </span>
<span id="cb16-409"><a href="#cb16-409" aria-hidden="true" tabindex="-1"></a>The way they are implemented in R, so QDA and LDA are the two functions that you will need, they are part of the mass package. </span>
<span id="cb16-410"><a href="#cb16-410" aria-hidden="true" tabindex="-1"></a>So, We first feed the QDA model and then we that fitted model to predict the classes. </span>
<span id="cb16-411"><a href="#cb16-411" aria-hidden="true" tabindex="-1"></a>And now if we see what the regular QDA does is it's going to give me this long list of probabilities for the test set. </span>
<span id="cb16-412"><a href="#cb16-412" aria-hidden="true" tabindex="-1"></a>And we can turn those into labels and in particular we can see how many errors we're making in the prediction. </span>
<span id="cb16-413"><a href="#cb16-413" aria-hidden="true" tabindex="-1"></a>And you can see that we make a single mistake, which is actually not the mistake that we had made before. </span>
<span id="cb16-414"><a href="#cb16-414" aria-hidden="true" tabindex="-1"></a>So if we just look at this one here and we compare it against the, Classification that our algorithm did, and we compared it against the truth. </span>
<span id="cb16-415"><a href="#cb16-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-416"><a href="#cb16-416" aria-hidden="true" tabindex="-1"></a>We see that our algorithm makes a mistake in this observation and QDA does not, and instead the error is somewhere else in this sample. </span>
<span id="cb16-417"><a href="#cb16-417" aria-hidden="true" tabindex="-1"></a>It's basically here, so you can see that the QDA classifies this as two, when the reality is that it's a three. </span>
<span id="cb16-418"><a href="#cb16-418" aria-hidden="true" tabindex="-1"></a>So our results are not identical to QDA even though our method is asymptotically going to be equivalent to QDA but they</span>
<span id="cb16-419"><a href="#cb16-419" aria-hidden="true" tabindex="-1"></a>don't give us exactly the same result, but they give us very similar accuracy. </span>
<span id="cb16-420"><a href="#cb16-420" aria-hidden="true" tabindex="-1"></a>Interestingly if you run LDA and you try to look at how many errors you have in that case, you will see that LDA in this case has no errors, even though it's a simpler more restrictive classification procedure. </span>
<span id="cb16-421"><a href="#cb16-421" aria-hidden="true" tabindex="-1"></a>So this can happen, so it's a relatively large sample, so a single a difference in a single error is not a very large difference. </span>
<span id="cb16-422"><a href="#cb16-422" aria-hidden="true" tabindex="-1"></a>So, hopefully this illustrates how classification or how measurements can be used for classification in a real life setting.</span>
<span id="cb16-423"><a href="#cb16-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-424"><a href="#cb16-424" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-425"><a href="#cb16-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-426"><a href="#cb16-426" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sample EM algorithm for classification problems</span></span>
<span id="cb16-427"><a href="#cb16-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-430"><a href="#cb16-430" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb16-431"><a href="#cb16-431" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lbl-em-algorithm-classification-sample</span></span>
<span id="cb16-432"><a href="#cb16-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-433"><a href="#cb16-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-434"><a href="#cb16-434" aria-hidden="true" tabindex="-1"></a><span class="do">## Using mixture models for classification in the wine dataset</span></span>
<span id="cb16-435"><a href="#cb16-435" aria-hidden="true" tabindex="-1"></a><span class="do">## Compare linear and quadratic discriminant analysis and a </span></span>
<span id="cb16-436"><a href="#cb16-436" aria-hidden="true" tabindex="-1"></a><span class="do">##   (semi-supervised) location and scale mixture model with K normals</span></span>
<span id="cb16-437"><a href="#cb16-437" aria-hidden="true" tabindex="-1"></a><span class="do">## Comparing only against the EM algorithm</span></span>
<span id="cb16-438"><a href="#cb16-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-439"><a href="#cb16-439" aria-hidden="true" tabindex="-1"></a><span class="co"># Semi-supervised, quadratic discriminant analysis </span></span>
<span id="cb16-440"><a href="#cb16-440" aria-hidden="true" tabindex="-1"></a><span class="do">### Loading data and setting up global variables</span></span>
<span id="cb16-441"><a href="#cb16-441" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb16-442"><a href="#cb16-442" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb16-443"><a href="#cb16-443" aria-hidden="true" tabindex="-1"></a>wine.training <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">"data/wine_training.txt"</span>, <span class="at">sep=</span><span class="st">","</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb16-444"><a href="#cb16-444" aria-hidden="true" tabindex="-1"></a>wine.test <span class="ot">=</span> <span class="fu">read.table</span>(<span class="st">"data/wine_test.txt"</span>, <span class="at">sep=</span><span class="st">","</span>, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb16-445"><a href="#cb16-445" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">dim</span>(wine.training)[<span class="dv">1</span>]  <span class="co"># Size of the training set</span></span>
<span id="cb16-446"><a href="#cb16-446" aria-hidden="true" tabindex="-1"></a>m <span class="ot">=</span> <span class="fu">dim</span>(wine.test)[<span class="dv">1</span>]      <span class="co"># Size of the test set</span></span>
<span id="cb16-447"><a href="#cb16-447" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">as.matrix</span>(wine.training[,<span class="sc">-</span><span class="dv">1</span>]), <span class="fu">as.matrix</span>(wine.test[,<span class="sc">-</span><span class="dv">1</span>]))   <span class="co"># Create dataset of observations, first n belong to the training set, and the rest belong to the test set</span></span>
<span id="cb16-448"><a href="#cb16-448" aria-hidden="true" tabindex="-1"></a>p       <span class="ot">=</span> <span class="fu">dim</span>(x)[<span class="dv">2</span>]              <span class="co"># Number of features</span></span>
<span id="cb16-449"><a href="#cb16-449" aria-hidden="true" tabindex="-1"></a>KK      <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb16-450"><a href="#cb16-450" aria-hidden="true" tabindex="-1"></a>epsilon <span class="ot">=</span> <span class="fl">0.00001</span></span>
<span id="cb16-451"><a href="#cb16-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-452"><a href="#cb16-452" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb16-453"><a href="#cb16-453" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>)<span class="sc">+</span><span class="fl">0.1</span>)</span>
<span id="cb16-454"><a href="#cb16-454" aria-hidden="true" tabindex="-1"></a>colscale <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"black"</span>,<span class="st">"red"</span>,<span class="st">"blue"</span>)</span>
<span id="cb16-455"><a href="#cb16-455" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">col=</span>colscale[wine.training[,<span class="dv">1</span>]], <span class="at">pch=</span>wine.training[,<span class="dv">1</span>])</span>
<span id="cb16-456"><a href="#cb16-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-457"><a href="#cb16-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-458"><a href="#cb16-458" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the parameters of the algorithm</span></span>
<span id="cb16-459"><a href="#cb16-459" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">63252</span>)</span>
<span id="cb16-460"><a href="#cb16-460" aria-hidden="true" tabindex="-1"></a>w   <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span>,KK)<span class="sc">/</span>KK  <span class="co">#Assign equal weight to each component to start with</span></span>
<span id="cb16-461"><a href="#cb16-461" aria-hidden="true" tabindex="-1"></a>mu  <span class="ot">=</span> <span class="fu">rmvnorm</span>(KK, <span class="fu">apply</span>(x,<span class="dv">2</span>,mean), <span class="fu">var</span>(x))   <span class="co">#Cluster centers randomly spread over the support of the data</span></span>
<span id="cb16-462"><a href="#cb16-462" aria-hidden="true" tabindex="-1"></a>Sigma      <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim=</span><span class="fu">c</span>(KK,p,p))  <span class="co">#Initial variances are assumed to be the same</span></span>
<span id="cb16-463"><a href="#cb16-463" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">1</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK  </span>
<span id="cb16-464"><a href="#cb16-464" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">2</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK</span>
<span id="cb16-465"><a href="#cb16-465" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">3</span>,,] <span class="ot">=</span> <span class="fu">var</span>(x)<span class="sc">/</span>KK</span>
<span id="cb16-466"><a href="#cb16-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-467"><a href="#cb16-467" aria-hidden="true" tabindex="-1"></a>sw     <span class="ot">=</span> <span class="cn">FALSE</span></span>
<span id="cb16-468"><a href="#cb16-468" aria-hidden="true" tabindex="-1"></a>KL     <span class="ot">=</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb16-469"><a href="#cb16-469" aria-hidden="true" tabindex="-1"></a>KL.out <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb16-470"><a href="#cb16-470" aria-hidden="true" tabindex="-1"></a>s      <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb16-471"><a href="#cb16-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-472"><a href="#cb16-472" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span>(<span class="sc">!</span>sw){</span>
<span id="cb16-473"><a href="#cb16-473" aria-hidden="true" tabindex="-1"></a>  <span class="do">## E step</span></span>
<span id="cb16-474"><a href="#cb16-474" aria-hidden="true" tabindex="-1"></a>  v <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim=</span><span class="fu">c</span>(n<span class="sc">+</span>m,KK))</span>
<span id="cb16-475"><a href="#cb16-475" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){  <span class="co">#Compute the log of the weights</span></span>
<span id="cb16-476"><a href="#cb16-476" aria-hidden="true" tabindex="-1"></a>    v[<span class="dv">1</span><span class="sc">:</span>n,k] <span class="ot">=</span> <span class="fu">ifelse</span>(wine.training[,<span class="dv">1</span>]<span class="sc">==</span>k,<span class="dv">0</span>,<span class="sc">-</span><span class="cn">Inf</span>)  <span class="co"># Training set</span></span>
<span id="cb16-477"><a href="#cb16-477" aria-hidden="true" tabindex="-1"></a>    v[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m),k] <span class="ot">=</span> <span class="fu">log</span>(w[k]) <span class="sc">+</span> mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(x[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m),], mu[k,], Sigma[k,,],<span class="at">log=</span><span class="cn">TRUE</span>)  <span class="co"># Test set</span></span>
<span id="cb16-478"><a href="#cb16-478" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-479"><a href="#cb16-479" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb16-480"><a href="#cb16-480" aria-hidden="true" tabindex="-1"></a>    v[i,] <span class="ot">=</span> <span class="fu">exp</span>(v[i,] <span class="sc">-</span> <span class="fu">max</span>(v[i,]))<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">exp</span>(v[i,] <span class="sc">-</span> <span class="fu">max</span>(v[i,])))  <span class="co">#Go from logs to actual weights in a numerically stable manner</span></span>
<span id="cb16-481"><a href="#cb16-481" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-482"><a href="#cb16-482" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-483"><a href="#cb16-483" aria-hidden="true" tabindex="-1"></a>  <span class="do">## M step</span></span>
<span id="cb16-484"><a href="#cb16-484" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">=</span> <span class="fu">apply</span>(v,<span class="dv">2</span>,mean)</span>
<span id="cb16-485"><a href="#cb16-485" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span>KK, <span class="at">ncol=</span>p)</span>
<span id="cb16-486"><a href="#cb16-486" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb16-487"><a href="#cb16-487" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb16-488"><a href="#cb16-488" aria-hidden="true" tabindex="-1"></a>      mu[k,]    <span class="ot">=</span> mu[k,] <span class="sc">+</span> v[i,k]<span class="sc">*</span>x[i,]</span>
<span id="cb16-489"><a href="#cb16-489" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-490"><a href="#cb16-490" aria-hidden="true" tabindex="-1"></a>    mu[k,] <span class="ot">=</span> mu[k,]<span class="sc">/</span><span class="fu">sum</span>(v[,k])</span>
<span id="cb16-491"><a href="#cb16-491" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-492"><a href="#cb16-492" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">array</span>(<span class="dv">0</span>,<span class="at">dim=</span><span class="fu">c</span>(KK,p,p))</span>
<span id="cb16-493"><a href="#cb16-493" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb16-494"><a href="#cb16-494" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb16-495"><a href="#cb16-495" aria-hidden="true" tabindex="-1"></a>      Sigma[k,,] <span class="ot">=</span> Sigma[k,,] <span class="sc">+</span> v[i,k]<span class="sc">*</span>(x[i,] <span class="sc">-</span> mu[k,])<span class="sc">%*%</span><span class="fu">t</span>(x[i,] <span class="sc">-</span> mu[k,])</span>
<span id="cb16-496"><a href="#cb16-496" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-497"><a href="#cb16-497" aria-hidden="true" tabindex="-1"></a>    Sigma[k,,] <span class="ot">=</span> Sigma[k,,]<span class="sc">/</span><span class="fu">sum</span>(v[,k])</span>
<span id="cb16-498"><a href="#cb16-498" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-499"><a href="#cb16-499" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-500"><a href="#cb16-500" aria-hidden="true" tabindex="-1"></a>  <span class="do">##Check convergence</span></span>
<span id="cb16-501"><a href="#cb16-501" aria-hidden="true" tabindex="-1"></a>  KLn <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb16-502"><a href="#cb16-502" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n<span class="sc">+</span>m)){</span>
<span id="cb16-503"><a href="#cb16-503" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>KK){</span>
<span id="cb16-504"><a href="#cb16-504" aria-hidden="true" tabindex="-1"></a>      KLn <span class="ot">=</span> KLn <span class="sc">+</span> v[i,k]<span class="sc">*</span>(<span class="fu">log</span>(w[k]) <span class="sc">+</span> mvtnorm<span class="sc">::</span><span class="fu">dmvnorm</span>(x[i,],mu[k,],Sigma[k,,],<span class="at">log=</span><span class="cn">TRUE</span>))</span>
<span id="cb16-505"><a href="#cb16-505" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-506"><a href="#cb16-506" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-507"><a href="#cb16-507" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">abs</span>(KLn<span class="sc">-</span>KL)<span class="sc">/</span><span class="fu">abs</span>(KLn)<span class="sc">&lt;</span>epsilon){</span>
<span id="cb16-508"><a href="#cb16-508" aria-hidden="true" tabindex="-1"></a>    sw<span class="ot">=</span><span class="cn">TRUE</span></span>
<span id="cb16-509"><a href="#cb16-509" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-510"><a href="#cb16-510" aria-hidden="true" tabindex="-1"></a>  KL <span class="ot">=</span> KLn</span>
<span id="cb16-511"><a href="#cb16-511" aria-hidden="true" tabindex="-1"></a>  KL.out <span class="ot">=</span> <span class="fu">c</span>(KL.out, KL)</span>
<span id="cb16-512"><a href="#cb16-512" aria-hidden="true" tabindex="-1"></a>  s <span class="ot">=</span> s <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb16-513"><a href="#cb16-513" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">paste</span>(s, KLn))</span>
<span id="cb16-514"><a href="#cb16-514" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-515"><a href="#cb16-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-516"><a href="#cb16-516" aria-hidden="true" tabindex="-1"></a><span class="do">## Predicted labels</span></span>
<span id="cb16-517"><a href="#cb16-517" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)]</span>
<span id="cb16-518"><a href="#cb16-518" aria-hidden="true" tabindex="-1"></a><span class="do">## True labels</span></span>
<span id="cb16-519"><a href="#cb16-519" aria-hidden="true" tabindex="-1"></a>wine.test[,<span class="dv">1</span>]</span>
<span id="cb16-520"><a href="#cb16-520" aria-hidden="true" tabindex="-1"></a><span class="do">## Comparison</span></span>
<span id="cb16-521"><a href="#cb16-521" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="sc">==</span> wine.test[,<span class="dv">1</span>]</span>
<span id="cb16-522"><a href="#cb16-522" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(<span class="fu">apply</span>(v, <span class="dv">1</span>, which.max)[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(n<span class="sc">+</span>m)] <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># One error</span></span>
<span id="cb16-523"><a href="#cb16-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-524"><a href="#cb16-524" aria-hidden="true" tabindex="-1"></a><span class="co"># Using the qda and lda functions in R</span></span>
<span id="cb16-525"><a href="#cb16-525" aria-hidden="true" tabindex="-1"></a><span class="co"># qda</span></span>
<span id="cb16-526"><a href="#cb16-526" aria-hidden="true" tabindex="-1"></a>modqda <span class="ot">=</span> <span class="fu">qda</span>(<span class="at">grouping=</span>wine.training[,<span class="dv">1</span>], <span class="at">x=</span>wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">method=</span><span class="st">"mle"</span>)</span>
<span id="cb16-527"><a href="#cb16-527" aria-hidden="true" tabindex="-1"></a>ccpredqda <span class="ot">=</span> <span class="fu">predict</span>(modqda,<span class="at">newdata=</span>wine.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb16-528"><a href="#cb16-528" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(ccpredqda<span class="sc">$</span>class <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># One error</span></span>
<span id="cb16-529"><a href="#cb16-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-530"><a href="#cb16-530" aria-hidden="true" tabindex="-1"></a><span class="co"># lda</span></span>
<span id="cb16-531"><a href="#cb16-531" aria-hidden="true" tabindex="-1"></a>modlda <span class="ot">=</span> <span class="fu">lda</span>(<span class="at">grouping=</span>wine.training[,<span class="dv">1</span>], <span class="at">x=</span>wine.training[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">method=</span><span class="st">"mle"</span>)</span>
<span id="cb16-532"><a href="#cb16-532" aria-hidden="true" tabindex="-1"></a>ccpredlda <span class="ot">=</span> <span class="fu">predict</span>(modlda,<span class="at">newdata=</span>wine.test[,<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb16-533"><a href="#cb16-533" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="sc">!</span>(ccpredlda<span class="sc">$</span>class <span class="sc">==</span> wine.test[,<span class="dv">1</span>])) <span class="co"># No errors!!!</span></span>
<span id="cb16-534"><a href="#cb16-534" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>