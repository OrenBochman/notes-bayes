<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="keywords" content="Markov Chain Monte Carlo, Metropolis-Hastings, MCMC, Gibbs Sampling">
<meta name="description" content="An introduction to the Metropolis-Hastings algorithm for sampling from complex probability distributions.">

<title>34&nbsp; Metropolis-Hastings - M2L4 – Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C2-L04-Ex1.html" rel="next">
<link href="./C2-L03.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(images/banner_deep.jpg);
background-size: cover;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C2-L04.html"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Bayesian Statistics: Techniques and Models</p>
                  <div>
        <div class="description">
          An introduction to the Metropolis-Hastings algorithm for sampling from complex probability distributions.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Monte Carlo Estimation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Markov Chain Monte Carlo, Metropolis-Hastings, MCMC, Gibbs Sampling</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Paradigms of probability - M1L1HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayes’ Theorem - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law - M1L2HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Probability and Bayes’ Theorem - M1L2HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Distributions - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Random Variables - M1L3HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Homework on Distributions - M1L3HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Frequentist Inference - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Frequentist MLE - M2L3HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bayesian Inference - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference - M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Priors - M3L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities - M3L6HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Homework on Priors - M2L7HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Poisson Data - M3L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Homework on Poisson Data - M3L8HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Beta Bernoulli - M3L8HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Homework on Exponential Data - M4L9HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Normally distributed Data - M4L10</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Homework on Normal Data - M4L10HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Non-Informative Priors - M4L11</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Homework Alternative Priors - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Brief Review of Regression - M4L12</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm - M2L4HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Gibbs sampling - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Homework Gibbs-Sampling algorithm - M2L22HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Assessing Convergence - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Homework on M-H algorithm M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Linear regression - M3L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Homework on Linear Regression Model Part 1 - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Homework on Deviance information criterion - M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">ANOVA - M3L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Homework on ANOVA - M3L8HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Homework on Multiple Factor ANOVA - M3L8HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Logistic regression - M3L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression - M3L9HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Poisson regression - M4L10</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Homework on Poisson regression - M4L10HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Hierarchical modeling - M4L11</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Capstone Project - M4L12</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models - M4L12HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Definitions of Mixture Models - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Likelihood functions for Mixture Models - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Homework The Likelihood function - M1L2HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Homework Identifiability - M1L2HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Homework The likelihood function M1L2HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Homework on simulating from a Poisson Mixture Model - M1L2HW4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model - M1L2HW5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Homework Sim mixture of exponential distributions - M1L2HW6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture models - M2L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">MCMC for Mixture Models - M4L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Density Estimation - M4L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Computational Considerations - M5L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Determining the number of components - M5L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Homework on Bayesian Information Criteria (BIC) - M5L09HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Homework on BIC for zero-inflated mixtures - M5L09HW4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Week 0: Introductions to time series analysis and the AR(1) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Stationarity, The ACF and the PCF M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">The AR(1) process: definitions and properties - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(1): MLE and Bayesian inference - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">The AR(p) process - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Bayesian Inference in the AR(p) - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Quiz: Spectral representation of the AR(p) - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p) - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1 - M3L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 1 M3L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Seasonal NDLMs M4L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 2 - M4L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Homework - Practice Quiz for Week 1 – M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Homework - first-step-for-the-project – M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Model Selection Criteria - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">106</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">107</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">108</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">109</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">110</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">111</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">112</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">113</span>&nbsp; <span class="chapter-title">Appendix: Bayes by backprop</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">114</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">115</span>&nbsp; <span class="chapter-title">Appendix: Yule-Walker Equations &amp; Durbin-Levinson Recursion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">116</span>&nbsp; <span class="chapter-title">Moore-Penrose Inversion &amp; Cholesky Decomposition</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">117</span>&nbsp; <span class="chapter-title">Appendix: Inequalities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">118</span>&nbsp; <span class="chapter-title">Appendix: Wold’s theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">119</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-m2l4-metropolis-hastings" id="toc-sec-m2l4-metropolis-hastings" class="nav-link active" data-scroll-target="#sec-m2l4-metropolis-hastings"><span class="header-section-number">34.1</span> Markov chain Monte Carlo (MCMC)</a></li>
  <li><a href="#sec-metropolis-hastings-alg" id="toc-sec-metropolis-hastings-alg" class="nav-link" data-scroll-target="#sec-metropolis-hastings-alg"><span class="header-section-number">34.2</span> The Metropolis-Hastings Algorithm 🎥</a></li>
  <li><a href="#proposal-distribution-q" id="toc-proposal-distribution-q" class="nav-link" data-scroll-target="#proposal-distribution-q"><span class="header-section-number">34.3</span> Proposal distribution <em>q</em></a></li>
  <li><a href="#acceptance-rate-α" id="toc-acceptance-rate-α" class="nav-link" data-scroll-target="#acceptance-rate-α"><span class="header-section-number">34.4</span> Acceptance rate α</a></li>
  <li><a href="#demonstration-of-a-discrete-case" id="toc-demonstration-of-a-discrete-case" class="nav-link" data-scroll-target="#demonstration-of-a-discrete-case"><span class="header-section-number">34.5</span> Demonstration of a Discrete case</a></li>
  <li><a href="#random-walk-with-normal-likelihood-t-prior" id="toc-random-walk-with-normal-likelihood-t-prior" class="nav-link" data-scroll-target="#random-walk-with-normal-likelihood-t-prior"><span class="header-section-number">34.6</span> Random walk with Normal likelihood, t prior</a></li>
  <li><a href="#introduction-to-jags" id="toc-introduction-to-jags" class="nav-link" data-scroll-target="#introduction-to-jags"><span class="header-section-number">35</span> Introduction to JAGS</a>
  <ul class="collapse">
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup"><span class="header-section-number">35.1</span> Setup</a>
  <ul class="collapse">
  <li><a href="#introduction-to-jags-1" id="toc-introduction-to-jags-1" class="nav-link" data-scroll-target="#introduction-to-jags-1"><span class="header-section-number">35.1.1</span> Introduction to <code>JAGS</code></a></li>
  <li><a href="#installation-and-setup" id="toc-installation-and-setup" class="nav-link" data-scroll-target="#installation-and-setup"><span class="header-section-number">35.1.2</span> Installation and setup</a></li>
  </ul></li>
  <li><a href="#modeling-in-jags" id="toc-modeling-in-jags" class="nav-link" data-scroll-target="#modeling-in-jags"><span class="header-section-number">35.2</span> Modeling in <code>JAGS</code></a>
  <ul class="collapse">
  <li><a href="#specify-the-model" id="toc-specify-the-model" class="nav-link" data-scroll-target="#specify-the-model"><span class="header-section-number">35.2.1</span> 1. Specify the model</a></li>
  <li><a href="#set-up-the-model" id="toc-set-up-the-model" class="nav-link" data-scroll-target="#set-up-the-model"><span class="header-section-number">35.2.2</span> 2. Set up the model</a></li>
  <li><a href="#run-the-mcmc-sampler" id="toc-run-the-mcmc-sampler" class="nav-link" data-scroll-target="#run-the-mcmc-sampler"><span class="header-section-number">35.2.3</span> 3. Run the MCMC sampler</a></li>
  <li><a href="#post-processing" id="toc-post-processing" class="nav-link" data-scroll-target="#post-processing"><span class="header-section-number">35.2.4</span> 4. Post-processing</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Understand the basics of MCMC including Metropolis-Hastings and Gibbs sampling.</li>
<li>Write a statistical model in JAGS and produce posterior samples by calling JAGS from R.</li>
<li>Assess MCMC output to determine if it is suitable for inference.</li>
</ul>
</div>
</div>
<section id="sec-m2l4-metropolis-hastings" class="level2" data-number="34.1">
<h2 data-number="34.1" class="anchored" data-anchor-id="sec-m2l4-metropolis-hastings"><span class="header-section-number">34.1</span> Markov chain Monte Carlo (MCMC)</h2>
<p> <strong>Metropolis-Hastings</strong> (M-H) is an algorithm that allows us to sample from a generic probability distribution (which we will call the target distribution), even if we do not know the normalizing constant. To do this, we construct and sample from a Markov chain whose stationary distribution is the target distribution. It consists of picking an arbitrary starting value and iteratively accepting or rejecting candidate samples drawn from another distribution, one that is easy to sample.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Why use M-H or MCMC?
</div>
</div>
<div class="callout-body-container callout-body">
<p>We will use M-H or other MCMC methods if there is no easy way to simulate independent draws from the target distribution. This can be due to non-conjugate priors, challenges in evaluating the normalizing constant or multiple explanatory variables.</p>
</div>
</div>
</section>
<section id="sec-metropolis-hastings-alg" class="level2 page-columns page-full" data-number="34.2">
<h2 data-number="34.2" class="anchored" data-anchor-id="sec-metropolis-hastings-alg"><span class="header-section-number">34.2</span> The Metropolis-Hastings Algorithm 🎥</h2>

<div class="no-row-height column-margin column-container"><div id="fig-metropolis-hastings-alg" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-metropolis-hastings-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l04-ss-01-Metropolis-Hastings.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;34.1: The Metropolis-Hastings Algorithm"><img src="images/c2l04-ss-01-Metropolis-Hastings.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-metropolis-hastings-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.1: The Metropolis-Hastings Algorithm
</figcaption>
</figure>
</div></div><p> Let’s say we wish to produce samples from a <em>target distribution</em> <span class="math inline">\mathbb{P}r(\theta) \propto g(\theta)</span>, where we don’t know the normalizing constant (since <span class="math inline">\int g(\theta)d\theta</span> is hard or impossible to compute), so we only have <span class="math inline">g(\theta)</span>, the <em>unnormalized joint probability</em> to work with. The Metropolis-Hastings algorithm proceeds as follows.</p>
<ol type="1">
<li>Select an <strong>initial value</strong> <span class="math inline">\theta_0</span>.</li>
<li>For <span class="math inline">i=1,\dots,m</span> repeat the following steps:
<ol type="a">
<li><strong>Draw a candidate sample</strong> <span class="math inline">\theta^∗</span> from a <strong>proposal distribution</strong>  <span class="math inline">q(\theta^* \mid \theta_{i−1})</span> .</li>
<li><strong>Compute the ratio</strong> <span class="math inline">\alpha = \frac{g(\theta^*) / q(\theta^* \mid \theta_{i-1}) }{g(\theta_{i-1}) / q(\theta_{i-1} \mid \theta^*)} = \frac{g(\theta^*)q(\theta_{i-1} \mid \theta^*)}{g(\theta_{i-1})q(\theta^* \mid \theta_{i-1})}</span></li>
<li><ul>
<li>If <span class="math inline">\alpha\ge 1</span>, then <strong>accept</strong> <span class="math inline">\theta^∗</span> and set <span class="math inline">\theta_i=\theta^∗</span>.</li>
<li>If <span class="math inline">0&lt;\alpha&lt;1</span>:
<ul>
<li><strong>accept</strong> <span class="math inline">\theta^∗</span> and set <span class="math inline">\theta_i=\theta^∗</span> with probability <span class="math inline">\alpha</span>,</li>
<li><strong>reject</strong> <span class="math inline">\theta^∗</span> and set <span class="math inline">\theta_i=\theta_{i−1}</span> with probability <span class="math inline">1−\alpha</span>.</li>
</ul></li>
</ul></li>
</ol></li>
</ol>
<div class="no-row-height column-margin column-container"><span class=""><strong>proposal distribution</strong> <span class="math inline">q</span></span></div><div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Correction to the proposal distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Steps 2.b and 2.c act as a <strong>correction</strong>  since the <em>proposal distribution</em> is not the <em>target distribution</em>. At each step in the chain, we draw a random candidate value of the parameter and decide whether to <em>“move”</em> the chain there or remain where we are. If the proposed move to the candidate is “advantageous,” <span class="math inline">(\alpha \ge 1)</span> we “move” there and if it is not <em>“advantageous,”</em> we still might move there, but only with probability <span class="math inline">\alpha</span>. Since our decision to “move” to the candidate only depends on where the chain currently is, this is a <em>Markov chain</em>.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><span class="callout-margin-content"><strong>correction</strong></span></div></section>
<section id="proposal-distribution-q" class="level2" data-number="34.3">
<h2 data-number="34.3" class="anchored" data-anchor-id="proposal-distribution-q"><span class="header-section-number">34.3</span> Proposal distribution <em>q</em></h2>
<p>One careful choice we must make is the candidate generating distribution <span class="math inline">q(\theta^∗\mid\theta_{i−1})</span>. It may or may not depend on the previous iteration’s value of <span class="math inline">\theta</span>.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Independent Metropolis-Hastings
</div>
</div>
<div class="callout-body-container callout-body">
<p>The simpler case is when the proposal distribution <span class="math inline">q</span> does not depend on the previous value. We then write it as <span class="math inline">q(\theta^∗)</span>. This arises if it is always the same distribution. We call this case <strong>independent Metropolis-Hastings</strong>. If we use independent M-H, <span class="math inline">q(\theta)</span> <strong>should be as similar as possible to</strong> <span class="math inline">\mathbb{P}r(\theta)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Random-Walk Metropolis-Hastings
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the more general case, the proposal distribution takes the form <span class="math inline">q(\theta^∗\mid\theta_{i−1})</span> with dependence on the previous iteration, is <strong>Random-Walk Metropolis-Hastings</strong>. Here, the proposal distribution is centered on <span class="math inline">\theta_{i−1}</span>.</p>
<p>For instance, it might be a Normal distribution with mean <span class="math inline">\theta_{i−1}</span>. Because the Normal distribution is <em>symmetric</em>, this example comes with another advantage: <span class="math inline">q(\theta^* \mid \theta_{i−1})=q(\theta_{i−1}∣\theta^*)</span> causing it to cancel out when we calculate <span class="math inline">\alpha</span>.</p>
<p>Thus, in <strong>Random-Walk M-H</strong> where the candidate is drawn from a Normal with mean <span class="math inline">\theta_{i−1}</span> and constant variance, the acceptance ratio is simply <span class="math inline">\alpha=g(\theta^∗)/g(\theta_{i−1})</span>.</p>
</div>
</div>
</section>
<section id="acceptance-rate-α" class="level2" data-number="34.4">
<h2 data-number="34.4" class="anchored" data-anchor-id="acceptance-rate-α"><span class="header-section-number">34.4</span> Acceptance rate α</h2>
<p>Clearly, not all candidate draws are accepted, so our Markov chain sometimes “stays” where it is, possibly for many iterations. How often you want the chain to accept candidates depends on the type of algorithm you use. If you approximate <span class="math inline">\mathbb{P}r(\theta)</span> with <span class="math inline">q(\theta^∗)</span> and always draw candidates from that, accepting candidates often is good; it means <span class="math inline">q(\theta^∗)</span> is approximating <span class="math inline">\mathbb{P}r(\theta)</span> well. However, you still may want <span class="math inline">q</span> to have a larger variance than <span class="math inline">p</span> and see some rejection of candidates as an assurance that <span class="math inline">q</span> is covering the space well.</p>
<p>As we will see in coming examples, a high acceptance rate for the Random-Walk Metropolis-Hastings sampler is not a good thing. If the random walk is taking too small of steps, it will accept often but will take a very long time to fully explore the posterior. If the random walk is taking too large of steps, many of its proposals will have a low probability and the acceptance rate will be low, wasting many draws. Ideally, a random walk sampler should accept somewhere between 23% and 50% of the candidates proposed.</p>
<p>In the next segment, we will see a demonstration of this algorithm used in a discrete case, where we can show mathematically that the Markov chain converges to the target distribution. In the following segment, we will demonstrate coding a Random-Walk Metropolis-Hastings algorithm in R to solve one of the problems from the end of Lesson 2.</p>
</section>
<section id="demonstration-of-a-discrete-case" class="level2 page-columns page-full" data-number="34.5">
<h2 data-number="34.5" class="anchored" data-anchor-id="demonstration-of-a-discrete-case"><span class="header-section-number">34.5</span> Demonstration of a Discrete case</h2>

<div class="no-row-height column-margin column-container"><div id="fig-mcmc-coin-flip-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mcmc-coin-flip-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l04-ss-02-Monte-Carlo-Demonstration.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;34.2: MCMC Coin Flip Example"><img src="images/c2l04-ss-02-Monte-Carlo-Demonstration.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mcmc-coin-flip-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.2: MCMC Coin Flip Example
</figcaption>
</figure>
</div></div><p>The following segment is by Herbert Lee, a professor of statistics and applied mathematics at the University of California, Santa Cruz.</p>
<p>The following is a demonstration of using Markov chain Monte Carlo, used to estimate posterior probabilities in a simplified case, where we can actually work out the correct answer in closed form. We demonstrate that the Metropolis-Hastings algorithm is indeed working, and giving us the right answer.</p>
<p>If you recall from the previous course, the example where your brother or maybe your sister, has a loaded coin that you know will come up heads 70% of the time. But they come to you with some coin, you’re not sure if it’s the loaded coin or a fair coin, and they want to make a bet with you. And you have to figure out which coin this is.</p>
<p>Suppose you have a prior probability that it’s a 60% probability, that they’ll bring a loaded coin to you. They let you flip it five times, and you get two heads and three tails.</p>
<p>And then you need to figure out, what’s your posterior probability that this is a loaded coin.</p>
<p>Our unknown parameter <span class="math inline">\theta</span>, can either take the values <em>fair</em> or <em>loaded</em>.</p>
<p><span id="eq-mcmc-coin-flip-rv"><span class="math display">
\theta = \{\text{fair, loaded} \}
\tag{34.1}</span></span></p>
<p>Our <strong>prior</strong> for <span class="math inline">\theta</span> is the probability of theta equals loaded, is 0.6.</p>
<p><span id="eq-mcmc-coin-flip-prior"><span class="math display">
\mathbb{P}r(\theta=\text{loaded})=0.6 \qquad  \text{(prior)}
\tag{34.2}</span></span></p>
<p>Our likelihood will follow a Binomial distribution, depending upon the value of <span class="math inline">\theta</span>.</p>
<p><span id="eq-mcmc-coin-flip-likelihood"><span class="math display">
f(x\mid \theta) = {5 \choose x} \frac{1}{2}^5\mathbb{I}_{\theta=\text{fair}}+  {5 \choose x} (.7)^x(.3)^{5-x}\mathbb{I}_{\theta=\text{loaded}}  \qquad  \text{(likelihood)}
\tag{34.3}</span></span></p>
<p>Our posterior then, we can look at posterior for theta, given that we saw <span class="math inline">x=2</span> equals two heads, posterior is the likelihood times the prior, divided by a normalizing constant.</p>
<p><span id="eq-mcmc-coin-flip-posterior"><span class="math display">
  \begin{aligned}
    f(\theta \mid X=2) &amp;=
      \frac{ \frac{1}{2}^5(0.4)\mathbb{I}_{(\theta=\text{fair})} + (.7)^2(.3)^{3}(.6)\mathbb{I}_{(\theta=\text{loaded})}}
           { \frac{1}{2}^5(0.4) + (.7)^2(.3)^{3}(.6)}  
  \\&amp;=\frac{ 0.0125 \mathbb{I}_{(\theta=\text{fair})} + 0.00794 \mathbb{I}_{(\theta=\text{loaded})}}
           { 0.0125 + 0.00794}
  \\&amp;= 0.612 \mathbb{I}_{(\theta=\text{fair})} + 0.388 \mathbb{I}_{(\theta=\text{loaded})}
  \qquad  \text{(posterior) }
  \end{aligned}
\tag{34.4}</span></span></p>
<p>In this case, we can work out the binomial and our prior. And we see that we get these expressions at the end. We get posterior probability of <span class="math inline">\theta</span> is loaded given that we saw two heads, to be <span class="math inline">0.388</span>.</p>
<p><span id="eq-mcmc-coin-posterior-conditional-probability"><span class="math display">
\therefore \mathbb{P}r(\theta=\text{loaded}\mid X=2) = 0.388 \qquad  \text{(posterior conditional probability ) }
\tag{34.5}</span></span></p>
<p>This is all review from the previous course so far.</p>
<p>But suppose we had a more complicated problem, where we couldn’t work this all out in closed form? We’ll know the likelihood and the prior, but we may not be able to get this normalizing constant. Can we instead do this by simulation? And indeed, yes we can.</p>
<p>We can do this with Markov chain Monte Carlo. In particular, using the Metropolis-Hastings algorithm. What we’ll do is, we’ll set up a Markov chain whose <strong>equilibrium distribution</strong> has this <strong>posterior distribution</strong>. So we’ll consider a Markov chain with two states, theta equals fair and theta equals loaded. And we’ll allow the chain to move between those two states, with certain transition probabilities. We set this up using this using the Metropolis-Hastings algorithm.</p>

<div class="no-row-height column-margin column-container"><div id="fig-metropolis-hastings-alg" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-metropolis-hastings-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l04-ss-03-Monte-Carlo-Demonstration.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;34.3: The Metropolis-Hastings Algorithm"><img src="images/c2l04-ss-03-Monte-Carlo-Demonstration.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-metropolis-hastings-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.3: The Metropolis-Hastings Algorithm
</figcaption>
</figure>
</div></div><p>So under the Metropolis-Hastings algorithm, step one is we start at an arbitrary location. And in this case, we can</p>
<ol type="1">
<li>start at either <span class="math inline">\theta \ne \text{fair}</span>, or <span class="math inline">\theta \ne \text{loaded}</span>.</li>
</ol>
<p>It does not really matter where we start, we’ll be moving back and forth and we’re going to look at the long-term running average, the long-term simulations.</p>
<p>So the key is we’ll be simulating.</p>
<ol start="2" type="1">
<li>Run <span class="math inline">m</span> simulations and in each iteration, we’ll propose a candidate and either accept it or reject it.</li>
</ol>
<ol type="a">
<li>So the first part is we’re proposing a new candidate. We’ll call this candidate <span class="math inline">\theta^*</span>, and we’re going to propose it be the other state compared to where we are now. Where we are now is <span class="math inline">\theta_{i-1}</span>, and so we’ll propose to move to <span class="math inline">\theta^*</span>.
<ul>
<li>If our <em>current state</em> is <code>fair</code>, we’ll propose <span class="math inline">\theta^*=\text{loaded}</span>.</li>
<li>If our <em>current state</em> is <code>loaded</code>, we’ll propose <span class="math inline">\theta^*=\text{fair}</span>.</li>
</ul></li>
</ol>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class=""><strong>what’s our acceptance probability alpha?</strong></span></div></div>
<p>The general form for <span class="math inline">\alpha</span> is:</p>
<p><span id="eq-alpha"><span class="math display">
\begin {aligned}
\alpha &amp;= {
            { g(\theta^*)     / q(\theta^*     \mid  \theta_{i-1}) }
      \over {g(\theta_{i-1}) / q(\theta_{i-1} \mid  \theta^*)     }
      }
\\      &amp;= {
            { f(x=2 \mid \theta^*) f(\theta^*)     / 1 }
      \over { f(x=2 \mid \theta_{i-1})f(\theta_{i-1}) / 1    }
} \qquad \text {(sub. g,q)}
\end{aligned}
\tag{34.6}</span></span></p>
<p>In this case,</p>
<ul>
<li><p><span class="math inline">g()</span> is our un-normalized likelihood times prior</p></li>
<li><p><span class="math inline">q()</span>, the <em>proposal distribution</em>, is, in this case, since we always accept the opposite state deterministically i.e.&nbsp;<span class="math inline">\theta^*=\neg \theta{i_1}</span> with <span class="math inline">P=1</span></p></li>
<li><p>If <span class="math inline">\theta^* = \text{loaded} \implies \alpha = {0.00794 \over 0.0125}=0.635</span></p></li>
<li><p>If <span class="math inline">\theta^* = \text{fair} \implies \alpha = { 0.0125 \over 0.00794}=1.574</span></p></li>
</ul>

<div class="no-row-height column-margin column-container"><div id="fig-metropolis-hastings-alg" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-metropolis-hastings-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l04-ss-04-Monte-Carlo-Demonstration.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;34.4: The Metropolis-Hastings Algorithm"><img src="images/c2l04-ss-04-Monte-Carlo-Demonstration.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-metropolis-hastings-alg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.4: The Metropolis-Hastings Algorithm
</figcaption>
</figure>
</div></div><p>Given these probabilities, we then can do the acceptance or rejection step.</p>
<p><span class="math display">
\begin{cases}
\text{ accept } \theta^* \text { and set } \theta_i=\text{fair} &amp; \text{If } \theta^*=\text{fair,  } \alpha&gt;1
\\ \begin {cases}
   \text{ accept } \theta^* \text{  and set } \theta_i=\text{loaded} &amp;  \text{ With probability } 0.635
\\ \text{ reject } \theta^* \text{ and set } \theta_i=\text{fair}     &amp;  \text{ Otherwise }
\end{cases} &amp; \text{If } \theta^*=\text{loaded, } \alpha=.635
\end{cases}
</span></p>
<p>If the <span class="math inline">\theta^*=\text{loaded} \implies \alpha=0.635</span>. So we accept theta star with probability <span class="math inline">0.635</span>. And if we accept it. Set <span class="math inline">\theta_i=\text{loaded}</span> Otherwise, set <span class="math inline">\theta_i = \theta_{i- 1}</span>, if we do not accept, it stays in that same old fair state.</p>
<p>We can draw this out as a Markov chain with two states, Fair and ‘loaded’. If it’s in the ‘loaded’ state, it will move with probability one to the fair state. If it’s in the fair state, it will move with a probability of <span class="math inline">0.635</span> to the ‘loaded’ state. And with a probability of <span class="math inline">0.365</span> it will stay in the fair state.</p>

<div class="no-row-height column-margin column-container"><div id="fig-mcmc-coins-states" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mcmc-coins-states-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l04-states.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;34.5: state diagram"><img src="images/c2l04-states.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mcmc-coins-states-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.5: state diagram
</figcaption>
</figure>
</div></div><p>And so here’s a little diagram for this Markov chain with two states. In which case it will move back and forth with certain probabilities.</p>
<p>Thus, if we wanted to find our <strong>posterior probability</strong> , <span class="math inline">f(\theta=\text{loaded} \mid x=2)</span>. We can simulate from this Markov chain using these transition probabilities. And observe the fraction of time that it spends in the state theta equals ‘loaded’. And this gives us a good estimate of the posterior probability that it’s the ‘loaded’ coin. In this particular case, we can also show that this gives us the theoretical right answer.</p>
<p>If you’ve seen a little bit of the theory of Markov chains. We can say that a Markov chain with transition probability capital <span class="math inline">P</span>, has stationary distribution <span class="math inline">\Pi</span>.</p>
<p><span id="eq-mcmc-coins-stationarity"><span class="math display">
\pi P = \pi \qquad \text{(def. stationary distribution)}
\tag{34.7}</span></span></p>
<p>Here we have a transition probability matrix <span class="math inline">P</span>, where we can think about ‘fair’ and ‘loaded’. Moving from the ‘fair’ state, remaining in the ‘fair’ state happens with a probability of <span class="math inline">0.365</span> and it moves from ‘fair’ to ‘loaded’, with a probability of <span class="math inline">0.635</span>. If it’s in the ‘loaded’ state, we’ll move to the ‘fair’ state with probability one, and it will stay in the ‘loaded’ state with probability 0.</p>
<p><span class="math display">
P=
\begin{bmatrix}
   0.365 &amp; 0.635 \\
   1     &amp; 0
\end{bmatrix}
</span></p>
<p>In this case, we want our stationary distribution to be the posterior probabilities.</p>
<p><span class="math display">
\Pi=
\begin{bmatrix}
    0.612 &amp; 0.388 \\
\end{bmatrix}
</span></p>
<p>Which you can recall are 0.<span class="math inline">612</span> of being ‘fair’ and 0.<span class="math inline">388</span> of being ‘loaded’. And so indeed, if you do just the minimal amount of matrix algebra, you can see that 0.612, 0.388 Multiplied by this matrix, 0.365, 0.635, 1, 0, does indeed give you 0.612 and 0.388, at least to within rounding error.</p>
<p><span id="eq-mcmc-coins-proof"><span class="math display">
\begin{aligned}
  \Pi P &amp;=
  \begin{bmatrix}
  0.612 &amp; 0.388
  \end{bmatrix}
  \begin{bmatrix}
  0.365 &amp; 0.635 \\
      1 &amp; 0
  \end{bmatrix}   \\
  &amp;= \begin{bmatrix}
  0.612 &amp; 0.388
  \end{bmatrix}
  \\&amp;= \Pi
\end{aligned}
\tag{34.8}</span></span></p>
<p>Thus in this case we can see, that we do get the correct stationary distribution for the Markov chain using the Metropolis–Hastings algorithm. And that when we simulate it, we do get correct estimates then of the posterior probabilities.</p>
<p>This is a nice simple example where we can work out the posterior probabilities in closed form. We don’t need to run Markov chain Monte Carlo. But this method is very powerful because all we need is to be able to evaluate the likelihood and the prior, we don’t need to evaluate the full posterior and get that normalizing constant. And so this applies to a much broader range of more complicated problems. Where we can use Markov chain Monte Carlo to simulate, to be able to get these probabilities. We’ll make good use of this in the rest of this course.</p>
</section>
<section id="random-walk-with-normal-likelihood-t-prior" class="level2" data-number="34.6">
<h2 data-number="34.6" class="anchored" data-anchor-id="random-walk-with-normal-likelihood-t-prior"><span class="header-section-number">34.6</span> Random walk with Normal likelihood, t prior</h2>
<p>Recall the model from the last segment of Lesson 2 where the data are the percent change in total personnel from last year to this year for n=10 companies. We used a normal likelihood with known variance and t distribution for the prior on the unknown mean. Suppose the values are <span class="math inline">y=(1.2,1.4,−0.5,0.3,0.9,2.3,1.0,0.1,1.3,1.9)</span>. Because this model is not conjugate, the posterior distribution is not in a standard form that we can easily sample. To obtain posterior samples, we will set up a Markov chain whose stationary distribution is this posterior distribution.</p>
<p>Recall that the posterior distribution is</p>
<p><span class="math display">
\mathbb{P}r(\mu \mid y_1, \ldots, y_n) \propto \frac{\exp[ n ( \bar{y} \mu - \mu^2/2)]}{1 + \mu^2}
</span></p>
<p>The posterior distribution on the left is our target distribution and the expression on the right is our <span class="math inline">g(\mu)</span>.</p>
<p>The first thing we can do in R is write a function to evaluate <span class="math inline">g(\mu)</span>. Because posterior distributions include likelihoods (the product of many numbers that are potentially small), <span class="math inline">g(\mu)</span> might evaluate to such a small number that to the computer, it is effectively zero. This will cause a problem when we evaluate the acceptance ratio <span class="math inline">\alpha</span>. To avoid this problem, we can work on the log scale, which will be more numerically stable. Thus, we will write a function to evaluate</p>
<p><span class="math display">
\log(g(\mu)) = n ( \bar{y} \mu - \mu^2/2) - \log(1 + \mu^2)
</span></p>
<p>This function will require three arguments, <span class="math inline">\mu</span>, <span class="math inline">\bar{y}</span>, and <span class="math inline">n</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>lg <span class="ot">=</span> <span class="cf">function</span>(mu, n, ybar) {</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  mu2 <span class="ot">=</span> mu<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  n <span class="sc">*</span> (ybar <span class="sc">*</span> mu <span class="sc">-</span> mu2 <span class="sc">/</span> <span class="fl">2.0</span>) <span class="sc">-</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">+</span> mu2)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let’s write a function to execute the <strong>Random-Walk Metropolis-Hastings</strong> sampler with <em>Normal</em> proposals.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>mh <span class="ot">=</span> <span class="cf">function</span>(n, ybar, n_iter, mu_init, cand_sd) {</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Random-Walk Metropolis-Hastings algorithm</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Step 1, initialize</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  mu_out <span class="ot">=</span> <span class="fu">numeric</span>(n_iter)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  accpt <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  mu_now <span class="ot">=</span> mu_init</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  lg_now <span class="ot">=</span> <span class="fu">lg</span>(<span class="at">mu=</span>mu_now, <span class="at">n=</span>n, <span class="at">ybar=</span>ybar)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Step 2, iterate</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iter) {</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="do">## step 2a</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    mu_cand <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="at">n=</span><span class="dv">1</span>, <span class="at">mean=</span>mu_now, <span class="at">sd=</span>cand_sd) <span class="co"># draw a candidate</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="do">## Step 2b</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    lg_cand <span class="ot">=</span> <span class="fu">lg</span>(<span class="at">mu=</span>mu_cand, <span class="at">n=</span>n, <span class="at">ybar=</span>ybar) <span class="co"># evaluate log of g with the candidate</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    lalpha <span class="ot">=</span> lg_cand <span class="sc">-</span> lg_now <span class="co"># log of acceptance ratio</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    alpha <span class="ot">=</span> <span class="fu">exp</span>(lalpha)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="do">## step 2c</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    u <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">1</span>) <span class="co"># draw a uniform variable which will be less than alpha with probability min(1, alpha)</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (u <span class="sc">&lt;</span> alpha) { <span class="co"># then accept the candidate</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>      mu_now <span class="ot">=</span> mu_cand</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>      accpt <span class="ot">=</span> accpt <span class="sc">+</span> <span class="dv">1</span> <span class="co"># to keep track of acceptance</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>      lg_now <span class="ot">=</span> lg_cand</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="do">## collect results</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    mu_out[i] <span class="ot">=</span> mu_now <span class="co"># save this iteration's value of mu</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>  <span class="do">## return a list of output</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">mu=</span>mu_out, <span class="at">accpt=</span>accpt<span class="sc">/</span>n_iter)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s set up the problem.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1.2</span>, <span class="fl">1.4</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.9</span>, <span class="fl">2.3</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span>, <span class="fl">1.3</span>, <span class="fl">1.9</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="fu">mean</span>(y)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(y)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(y, <span class="at">freq=</span><span class="cn">FALSE</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.0</span>, <span class="fl">3.0</span>)) <span class="co"># histogram of the data</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dt</span>(<span class="at">x=</span>x, <span class="at">df=</span><span class="dv">1</span>), <span class="at">lty=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>) <span class="co"># prior for mu</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(y, <span class="fu">rep</span>(<span class="dv">0</span>,n), <span class="at">pch=</span><span class="dv">1</span>) <span class="co"># individual data points</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(ybar, <span class="dv">0</span>, <span class="at">pch=</span><span class="dv">19</span>) <span class="co"># sample mean</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C2-L04_files/figure-html/C2-L04-3-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="C2-L04_files/figure-html/C2-L04-3-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Finally, we’re ready to run the sampler! Let’s use <span class="math inline">m=1000</span> iterations and proposal standard deviation (which controls the proposal step size) <span class="math inline">3.0</span>, and initial value at the prior median <span class="math inline">0</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">43</span>) <span class="co"># set the random seed for reproducibility</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">mh</span>(<span class="at">n=</span>n, <span class="at">ybar=</span>ybar, <span class="at">n_iter=</span><span class="fl">1e3</span>, <span class="at">mu_init=</span><span class="fl">0.0</span>, <span class="at">cand_sd=</span><span class="fl">3.0</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(post)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>List of 2
 $ mu   : num [1:1000] -0.113 1.507 1.507 1.507 1.507 ...
 $ accpt: num 0.122</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"coda"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(<span class="fu">as.mcmc</span>(post<span class="sc">$</span>mu))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C2-L04_files/figure-html/C2-L04-5-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="C2-L04_files/figure-html/C2-L04-5-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>This last plot is called a trace plot. It shows the history of the chain and provides basic feedback about whether the chain has reached its stationary distribution.</p>
<p>It appears our proposal step size was too large (acceptance rate below 23%). Let’s try another.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">mh</span>(<span class="at">n=</span>n, <span class="at">ybar=</span>ybar, <span class="at">n_iter=</span><span class="fl">1e3</span>, <span class="at">mu_init=</span><span class="fl">0.0</span>, <span class="at">cand_sd=</span><span class="fl">0.05</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>post<span class="sc">$</span>accpt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.946</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(<span class="fu">as.mcmc</span>(post<span class="sc">$</span>mu))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C2-L04_files/figure-html/C2-L04-7-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="C2-L04_files/figure-html/C2-L04-7-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Oops, the acceptance rate is too high (above 50%). Let’s try something in between.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">mh</span>(<span class="at">n=</span>n, <span class="at">ybar=</span>ybar, <span class="at">n_iter=</span><span class="fl">1e3</span>, <span class="at">mu_init=</span><span class="fl">0.0</span>, <span class="at">cand_sd=</span><span class="fl">0.9</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>post<span class="sc">$</span>accpt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.38</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(<span class="fu">as.mcmc</span>(post<span class="sc">$</span>mu))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C2-L04_files/figure-html/C2-L04-9-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="C2-L04_files/figure-html/C2-L04-9-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Which looks good. Just for fun, let’s see what happens if we initialize the chain at some far-off value.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">mh</span>(<span class="at">n=</span>n, <span class="at">ybar=</span>ybar, <span class="at">n_iter=</span><span class="fl">1e3</span>, <span class="at">mu_init=</span><span class="fl">30.0</span>, <span class="at">cand_sd=</span><span class="fl">0.9</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>post<span class="sc">$</span>accpt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.387</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(<span class="fu">as.mcmc</span>(post<span class="sc">$</span>mu))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C2-L04_files/figure-html/C2-L04-11-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="C2-L04_files/figure-html/C2-L04-11-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>It took awhile to find the stationary distribution, but it looks like we succeeded! If we discard the first 100 or so values, it appears like the rest of the samples come from the stationary distribution, our posterior distribution! Let’s plot the posterior density against the prior to see how the data updated our belief about <span class="math inline">\mu</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>post<span class="sc">$</span>mu_keep <span class="ot">=</span> post<span class="sc">$</span>mu[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>)] <span class="co"># discard the first 200 samples</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(post<span class="sc">$</span>mu_keep, <span class="at">adjust=</span><span class="fl">2.0</span>), <span class="at">main=</span><span class="st">""</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.0</span>, <span class="fl">3.0</span>), <span class="at">xlab=</span><span class="fu">expression</span>(mu)) <span class="co"># plot density estimate of the posterior</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dt</span>(<span class="at">x=</span>x, <span class="at">df=</span><span class="dv">1</span>), <span class="at">lty=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>) <span class="co"># prior for mu</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(ybar, <span class="dv">0</span>, <span class="at">pch=</span><span class="dv">19</span>) <span class="co"># sample mean</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fl">0.017</span><span class="sc">*</span><span class="fu">exp</span>(<span class="fu">lg</span>(<span class="at">mu=</span>x, <span class="at">n=</span>n, <span class="at">ybar=</span>ybar)), <span class="at">from=</span><span class="sc">-</span><span class="fl">1.0</span>, <span class="at">to=</span><span class="fl">3.0</span>, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"blue"</span>) <span class="co"># approximation to the true posterior in blue</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C2-L04_files/figure-html/C2-L04-12-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="C2-L04_files/figure-html/C2-L04-12-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>These results are encouraging, but they are preliminary. We still need to investigate more formally whether our Markov chain has converged to the stationary distribution. We will explore this in a future lesson.</p>
<p>Obtaining posterior samples using the Metropolis-Hastings algorithm can be time-consuming and require some fine-tuning, as we’ve just seen. The good news is that we can rely on software to do most of the work for us. In the next couple of videos, we’ll introduce a program that will make posterior sampling easy.</p>
</section>
<section id="introduction-to-jags" class="level1" data-number="35">
<h1 data-number="35"><span class="header-section-number">35</span> Introduction to JAGS</h1>
<section id="setup" class="level2" data-number="35.1">
<h2 data-number="35.1" class="anchored" data-anchor-id="setup"><span class="header-section-number">35.1</span> Setup</h2>
<section id="introduction-to-jags-1" class="level3" data-number="35.1.1">
<h3 data-number="35.1.1" class="anchored" data-anchor-id="introduction-to-jags-1"><span class="header-section-number">35.1.1</span> Introduction to <code>JAGS</code></h3>
<p>There are several software packages available that will handle the details of MCMC for us. See the supplementary material for a brief overview of options.</p>
<p>The package we will use in this course is <code>JAGS</code> (Just Another Gibbs Sampler) by <a href="https://www.aminer.cn/profile/martyn-plummer/53f44c8edabfaefedbb2c089?source=zz1">Martyn Plummer</a>. The program is free, and runs on Mac OS, Windows, and Linux. Better yet, the program can be run using <code>R</code> with the <code>rjags</code> and <code>R2jags</code> packages.</p>
<p>In <code>JAGS</code>, we can specify models and run MCMC samplers in just a few lines of code; <code>JAGS</code> does the rest for us, so we can focus more on the statistical modeling aspect and less on the implementation. It makes powerful Bayesian machinery available to us as we can fit a wide variety of statistical models with relative ease.</p>
</section>
<section id="installation-and-setup" class="level3" data-number="35.1.2">
<h3 data-number="35.1.2" class="anchored" data-anchor-id="installation-and-setup"><span class="header-section-number">35.1.2</span> Installation and setup</h3>
<p>The starting place for <code>JAGS</code> users is <a href="http://mcmc-jags.sourceforge.net">mcmc-jags.sourceforge.net</a>. At this site, you can find news about the features of the latest release of <code>JAGS</code>, links to program documentation, as well as instructions for installation.</p>
<p>The documentation is particularly important. It is available under the <a href="https://sourceforge.net/projects/mcmc-jags/files/">files page</a> link in the Manuals folder.</p>
<p>Also under the <a href="https://sourceforge.net/projects/mcmc-jags/files/">files page</a>, you will find the <code>JAGS</code> folder where you can download and install the latest version of <code>JAGS</code>. Select the version and operating system, and follow the instructions for download and installation.</p>
<p>Once <code>JAGS</code> is installed, we can immediately run it from <code>R</code> using the <code>rjags</code> package. The next segment will show how this is done.</p>
</section>
</section>
<section id="modeling-in-jags" class="level2" data-number="35.2">
<h2 data-number="35.2" class="anchored" data-anchor-id="modeling-in-jags"><span class="header-section-number">35.2</span> Modeling in <code>JAGS</code></h2>
<p>There are four steps to implementing a model in <code>JAGS</code> through <code>R</code>:</p>
<ol type="1">
<li>Specify the model.</li>
<li>Set up the model.</li>
<li>Run the MCMC sampler.</li>
<li>Post-processing.</li>
</ol>
<p>We will demonstrate these steps with our running example with the data are the percent change in total personnel from last year to this year for <span class="math inline">n=10</span> companies. We used a normal likelihood with known variance and t distribution for the prior on the unknown mean.</p>
<section id="specify-the-model" class="level3" data-number="35.2.1">
<h3 data-number="35.2.1" class="anchored" data-anchor-id="specify-the-model"><span class="header-section-number">35.2.1</span> 1. Specify the model</h3>
<p>In this step, we give <code>JAGS</code> the hierarchical structure of the model, assigning distributions to the data (the likelihood) and parameters (priors). The syntax for this step is very similar to <code>R</code>, but there are some key differences.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"rjags"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Linked to JAGS 4.3.2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded modules: basemod,bugs</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>mod_string <span class="ot">=</span> <span class="st">" model {</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="st">  for (i in 1:n) {</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="st">    y[i] ~ dnorm(mu, 1.0/sig2)</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="st">  mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="st">  sig2 = 1.0</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="st">} "</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>One of the primary differences between the syntax of <code>JAGS</code> and <code>R</code> is how the distributions are parameterized. Note that the normal distribution uses the mean and precision (instead of variance). When specifying distributions in <code>JAGS</code>, it is always a good idea to check the <code>JAGS</code> user manual <a href="https://sourceforge.net/projects/mcmc-jags/files/Manuals">here</a> in the chapter on Distributions.</p>
</section>
<section id="set-up-the-model" class="level3" data-number="35.2.2">
<h3 data-number="35.2.2" class="anchored" data-anchor-id="set-up-the-model"><span class="header-section-number">35.2.2</span> 2. Set up the model</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">50</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1.2</span>, <span class="fl">1.4</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.9</span>, <span class="fl">2.3</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span>, <span class="fl">1.3</span>, <span class="fl">1.9</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(y)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>data_jags <span class="ot">=</span> <span class="fu">list</span>(<span class="at">y=</span>y, <span class="at">n=</span>n)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"mu"</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>inits <span class="ot">=</span> <span class="cf">function</span>() {</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>  inits <span class="ot">=</span> <span class="fu">list</span>(<span class="st">"mu"</span><span class="ot">=</span><span class="fl">0.0</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>} <span class="co"># optional (and fixed)</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">=</span> <span class="fu">jags.model</span>(<span class="fu">textConnection</span>(mod_string), <span class="at">data=</span>data_jags, <span class="at">inits=</span>inits)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 10
   Unobserved stochastic nodes: 1
   Total graph size: 15

Initializing model</code></pre>
</div>
</div>
<p>There are multiple ways to specify initial values here. They can be explicitly set, as we did here, or they can be random, i.e., <code>list("mu"=rnorm(1))</code>. Also, we can omit the initial values, and <code>JAGS</code> will provide them.</p>
</section>
<section id="run-the-mcmc-sampler" class="level3" data-number="35.2.3">
<h3 data-number="35.2.3" class="anchored" data-anchor-id="run-the-mcmc-sampler"><span class="header-section-number">35.2.3</span> 3. Run the MCMC sampler</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">update</span>(mod, <span class="dv">500</span>) <span class="co"># burn-in</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>mod_sim <span class="ot">=</span> <span class="fu">coda.samples</span>(<span class="at">model=</span>mod, <span class="at">variable.names=</span>params, <span class="at">n.iter=</span><span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will discuss more options to the <code>coda.samples</code> function in coming examples.</p>
</section>
<section id="post-processing" class="level3" data-number="35.2.4">
<h3 data-number="35.2.4" class="anchored" data-anchor-id="post-processing"><span class="header-section-number">35.2.4</span> 4. Post-processing</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_sim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Iterations = 1501:2500
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 1000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean             SD       Naive SE Time-series SE 
      0.888690       0.303505       0.009598       0.012809 

2. Quantiles for each variable:

  2.5%    25%    50%    75%  97.5% 
0.2667 0.6920 0.8962 1.0872 1.4837 </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"coda"</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod_sim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C2-L04_files/figure-html/C2-L04-17-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="C2-L04_files/figure-html/C2-L04-17-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>We will discuss post processing further, including convergence diagnostics, in a coming lesson.</p>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C2-L03.html" class="pagination-link" aria-label="Monte Carlo estimation - M1L3">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C2-L04-Ex1.html" class="pagination-link" aria-label="Homework on the Metropolis-Hastings algorithm - M2L4HW1">
        <span class="nav-page-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm - M2L4HW1</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb27" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Metropolis-Hastings - M2L4"</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Bayesian Statistics: Techniques and Models"</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "An introduction to the Metropolis-Hastings algorithm for sampling from complex probability distributions."</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - Monte Carlo Estimation</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - Markov Chain Monte Carlo</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - Metropolis-Hastings</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - MCMC</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - Gibbs Sampling</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand the basics of MCMC including Metropolis-Hastings and Gibbs sampling.</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Write a statistical model in JAGS and produce posterior samples by calling JAGS from R.</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Assess MCMC output to determine if it is suitable for inference.</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## Markov chain Monte Carlo (MCMC)  {#sec-m2l4-metropolis-hastings}</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>\index{Metropolis-Hastings} </span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>**Metropolis-Hastings** (M-H) is an algorithm that allows us to sample from a generic probability distribution (which we will call the target distribution), even if we do not know the normalizing constant. To do this, we construct and sample from a Markov chain whose stationary distribution is the target distribution. It consists of picking an arbitrary starting value and iteratively accepting or rejecting candidate samples drawn from another distribution, one that is easy to sample.</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why use M-H or MCMC? {.unnumbered}</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>We will use M-H or other MCMC methods if there is no easy way to simulate independent draws from the target distribution. This can be due to non-conjugate priors, challenges in evaluating the normalizing constant or multiple explanatory variables.</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Metropolis-Hastings Algorithm 🎥 {#sec-metropolis-hastings-alg}</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a><span class="al">![The Metropolis-Hastings Algorithm](images/c2l04-ss-01-Metropolis-Hastings.png)</span>{#fig-metropolis-hastings-alg .column-margin group="slides" width="53mm"}</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>\index{target distribution}</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>Let's say we wish to produce samples from a *target distribution* $\mathbb{P}r(\theta) \propto g(\theta)$, where we don't know the normalizing constant (since $\int g(\theta)d\theta$ is hard or impossible to compute), so we only have $g(\theta)$, the *unnormalized joint probability* to work with. The Metropolis-Hastings algorithm proceeds as follows.</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Select an **initial value** $\theta_0$.</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>For $i=1,\dots,m$ repeat the following steps:</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>    a.  **Draw a candidate sample** $\theta^∗$ from a **proposal distribution** [**proposal distribution** $q$]{.column-margin} $q(\theta^* \mid \theta_{i−1})$ .</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>    b.  **Compute the ratio** $\alpha = \frac{g(\theta^*) / q(\theta^* \mid \theta_{i-1}) }{g(\theta_{i-1}) / q(\theta_{i-1} \mid \theta^*)} = \frac{g(\theta^*)q(\theta_{i-1} \mid \theta^*)}{g(\theta_{i-1})q(\theta^* \mid \theta_{i-1})}$</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>    c.  </span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a><span class="ss">        -   </span>If $\alpha\ge 1$, then **accept** $\theta^∗$ and set $\theta_i=\theta^∗$.</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a><span class="ss">        -   </span>If $0&lt;\alpha&lt;1$:</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a><span class="ss">            -   </span>**accept** $\theta^∗$ and set $\theta_i=\theta^∗$ with probability $\alpha$,</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a><span class="ss">            -   </span>**reject** $\theta^∗$ and set $\theta_i=\theta_{i−1}$ with probability $1−\alpha$.</span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a><span class="fu">## Correction to the proposal distribution</span></span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>Steps 2.b and 2.c act as a **correction** [**correction**]{.column-margin} since the *proposal distribution* is not the *target distribution*. </span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>At each step in the chain, we draw a random candidate value of the parameter and decide whether to *"move"* the chain there or remain where we are. </span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a>If the proposed move to the candidate is "advantageous," $(\alpha \ge 1)$ we "move" there and if it is not *"advantageous,"* we still might move there, but only with probability $\alpha$. </span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>Since our decision to "move" to the candidate only depends on where the chain currently is, this is a *Markov chain*.</span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proposal distribution *q*</span></span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>One careful choice we must make is the candidate generating distribution $q(\theta^∗\mid\theta_{i−1})$. </span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>It may or may not depend on the previous iteration's value of $\theta$.</span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a><span class="fu">## Independent Metropolis-Hastings</span></span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a>The simpler case is when the proposal distribution $q$ does not depend on the previous value. We then write it as $q(\theta^∗)$. </span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a>This arises if it is always the same distribution. We call this case **independent Metropolis-Hastings**. </span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>If we use independent M-H, $q(\theta)$ **should be as similar as possible to** $\mathbb{P}r(\theta)$.</span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb27-75"><a href="#cb27-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-76"><a href="#cb27-76" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb27-77"><a href="#cb27-77" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random-Walk Metropolis-Hastings</span></span>
<span id="cb27-78"><a href="#cb27-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-79"><a href="#cb27-79" aria-hidden="true" tabindex="-1"></a>In the more general case, the proposal distribution takes the form $q(\theta^∗\mid\theta_{i−1})$ with dependence on the previous iteration, is **Random-Walk Metropolis-Hastings**. Here, the proposal distribution is centered on $\theta_{i−1}$.</span>
<span id="cb27-80"><a href="#cb27-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-81"><a href="#cb27-81" aria-hidden="true" tabindex="-1"></a>For instance, it might be a Normal distribution with mean $\theta_{i−1}$. Because the Normal distribution is *symmetric*, this example comes with another advantage: $q(\theta^* \mid \theta_{i−1})=q(\theta_{i−1}∣\theta^*)$ causing it to cancel out when we calculate $\alpha$.</span>
<span id="cb27-82"><a href="#cb27-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-83"><a href="#cb27-83" aria-hidden="true" tabindex="-1"></a>Thus, in **Random-Walk M-H** where the candidate is drawn from a Normal with mean $\theta_{i−1}$ and constant variance, the acceptance ratio is simply $\alpha=g(\theta^∗)/g(\theta_{i−1})$.</span>
<span id="cb27-84"><a href="#cb27-84" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb27-85"><a href="#cb27-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-86"><a href="#cb27-86" aria-hidden="true" tabindex="-1"></a><span class="fu">## Acceptance rate α</span></span>
<span id="cb27-87"><a href="#cb27-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-88"><a href="#cb27-88" aria-hidden="true" tabindex="-1"></a>Clearly, not all candidate draws are accepted, so our Markov chain sometimes "stays" where it is, possibly for many iterations. How often you want the chain to accept candidates depends on the type of algorithm you use. If you approximate $\mathbb{P}r(\theta)$ with $q(\theta^∗)$ and always draw candidates from that, accepting candidates often is good; it means $q(\theta^∗)$ is approximating $\mathbb{P}r(\theta)$ well. However, you still may want $q$ to have a larger variance than $p$ and see some rejection of candidates as an assurance that $q$ is covering the space well.</span>
<span id="cb27-89"><a href="#cb27-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-90"><a href="#cb27-90" aria-hidden="true" tabindex="-1"></a>As we will see in coming examples, a high acceptance rate for the Random-Walk Metropolis-Hastings sampler is not a good thing. If the random walk is taking too small of steps, it will accept often but will take a very long time to fully explore the posterior. If the random walk is taking too large of steps, many of its proposals will have a low probability and the acceptance rate will be low, wasting many draws. Ideally, a random walk sampler should accept somewhere between 23% and 50% of the candidates proposed.</span>
<span id="cb27-91"><a href="#cb27-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-92"><a href="#cb27-92" aria-hidden="true" tabindex="-1"></a>In the next segment, we will see a demonstration of this algorithm used in a discrete case, where we can show mathematically that the Markov chain converges to the target distribution. In the following segment, we will demonstrate coding a Random-Walk Metropolis-Hastings algorithm in R to solve one of the problems from the end of Lesson 2.</span>
<span id="cb27-93"><a href="#cb27-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-94"><a href="#cb27-94" aria-hidden="true" tabindex="-1"></a><span class="fu">## Demonstration of a Discrete case</span></span>
<span id="cb27-95"><a href="#cb27-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-96"><a href="#cb27-96" aria-hidden="true" tabindex="-1"></a><span class="al">![MCMC Coin Flip Example](images/c2l04-ss-02-Monte-Carlo-Demonstration.png)</span>{#fig-mcmc-coin-flip-example .column-margin group="slides" width="53mm"}</span>
<span id="cb27-97"><a href="#cb27-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-98"><a href="#cb27-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-99"><a href="#cb27-99" aria-hidden="true" tabindex="-1"></a>The following segment is by Herbert Lee, a professor of statistics and applied mathematics at the University of California, Santa Cruz.</span>
<span id="cb27-100"><a href="#cb27-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-101"><a href="#cb27-101" aria-hidden="true" tabindex="-1"></a>The following is a demonstration of using Markov chain Monte Carlo, used to estimate posterior probabilities in a simplified case, where we can actually work out the correct answer in closed form. We demonstrate that the Metropolis-Hastings algorithm is indeed working, and giving us the right answer.</span>
<span id="cb27-102"><a href="#cb27-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-103"><a href="#cb27-103" aria-hidden="true" tabindex="-1"></a>If you recall from the previous course, the example where your brother or maybe your sister, has a loaded coin that you know will come up heads 70% of the time. But they come to you with some coin, you're not sure if it's the loaded coin or a fair coin, and they want to make a bet with you. And you have to figure out which coin this is.</span>
<span id="cb27-104"><a href="#cb27-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-105"><a href="#cb27-105" aria-hidden="true" tabindex="-1"></a>Suppose you have a prior probability that it's a 60% probability, that they'll bring a loaded coin to you. They let you flip it five times, and you get two heads and three tails.</span>
<span id="cb27-106"><a href="#cb27-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-107"><a href="#cb27-107" aria-hidden="true" tabindex="-1"></a>And then you need to figure out, what's your posterior probability that this is a loaded coin.</span>
<span id="cb27-108"><a href="#cb27-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-109"><a href="#cb27-109" aria-hidden="true" tabindex="-1"></a>Our unknown parameter $\theta$, can either take the values *fair* or *loaded*.</span>
<span id="cb27-110"><a href="#cb27-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-111"><a href="#cb27-111" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-112"><a href="#cb27-112" aria-hidden="true" tabindex="-1"></a>\theta = <span class="sc">\{</span>\text{fair, loaded} <span class="sc">\}</span></span>
<span id="cb27-113"><a href="#cb27-113" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-coin-flip-rv}</span>
<span id="cb27-114"><a href="#cb27-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-115"><a href="#cb27-115" aria-hidden="true" tabindex="-1"></a>Our **prior** for $\theta$ is the probability of theta equals loaded, is 0.6.</span>
<span id="cb27-116"><a href="#cb27-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-117"><a href="#cb27-117" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-118"><a href="#cb27-118" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta=\text{loaded})=0.6 \qquad  \text{(prior)}</span>
<span id="cb27-119"><a href="#cb27-119" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-coin-flip-prior}</span>
<span id="cb27-120"><a href="#cb27-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-121"><a href="#cb27-121" aria-hidden="true" tabindex="-1"></a>Our likelihood will follow a Binomial distribution, depending upon the value of $\theta$.</span>
<span id="cb27-122"><a href="#cb27-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-123"><a href="#cb27-123" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-124"><a href="#cb27-124" aria-hidden="true" tabindex="-1"></a>f(x\mid \theta) = {5 \choose x} \frac{1}{2}^5\mathbb{I}_{\theta=\text{fair}}+  {5 \choose x} (.7)^x(.3)^{5-x}\mathbb{I}_{\theta=\text{loaded}}  \qquad  \text{(likelihood)}</span>
<span id="cb27-125"><a href="#cb27-125" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-coin-flip-likelihood}</span>
<span id="cb27-126"><a href="#cb27-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-127"><a href="#cb27-127" aria-hidden="true" tabindex="-1"></a>Our posterior then, we can look at posterior for theta, given that we saw $x=2$ equals two heads, posterior is the likelihood times the prior, divided by a normalizing constant.</span>
<span id="cb27-128"><a href="#cb27-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-129"><a href="#cb27-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-130"><a href="#cb27-130" aria-hidden="true" tabindex="-1"></a>  \begin{aligned}</span>
<span id="cb27-131"><a href="#cb27-131" aria-hidden="true" tabindex="-1"></a>    f(\theta \mid X=2) &amp;= </span>
<span id="cb27-132"><a href="#cb27-132" aria-hidden="true" tabindex="-1"></a>      \frac{ \frac{1}{2}^5(0.4)\mathbb{I}_{(\theta=\text{fair})} + (.7)^2(.3)^{3}(.6)\mathbb{I}_{(\theta=\text{loaded})}}</span>
<span id="cb27-133"><a href="#cb27-133" aria-hidden="true" tabindex="-1"></a>           { \frac{1}{2}^5(0.4) + (.7)^2(.3)^{3}(.6)}  </span>
<span id="cb27-134"><a href="#cb27-134" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span>&amp;=\frac{ 0.0125 \mathbb{I}_{(\theta=\text{fair})} + 0.00794 \mathbb{I}_{(\theta=\text{loaded})}}</span>
<span id="cb27-135"><a href="#cb27-135" aria-hidden="true" tabindex="-1"></a>           { 0.0125 + 0.00794}</span>
<span id="cb27-136"><a href="#cb27-136" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span>&amp;= 0.612 \mathbb{I}_{(\theta=\text{fair})} + 0.388 \mathbb{I}_{(\theta=\text{loaded})}</span>
<span id="cb27-137"><a href="#cb27-137" aria-hidden="true" tabindex="-1"></a>  \qquad  \text{(posterior) }</span>
<span id="cb27-138"><a href="#cb27-138" aria-hidden="true" tabindex="-1"></a>  \end{aligned}</span>
<span id="cb27-139"><a href="#cb27-139" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-coin-flip-posterior}</span>
<span id="cb27-140"><a href="#cb27-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-141"><a href="#cb27-141" aria-hidden="true" tabindex="-1"></a>In this case, we can work out the binomial and our prior. And we see that we get these expressions at the end. We get posterior probability of $\theta$ is loaded given that we saw two heads, to be $0.388$.</span>
<span id="cb27-142"><a href="#cb27-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-143"><a href="#cb27-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-144"><a href="#cb27-144" aria-hidden="true" tabindex="-1"></a>\therefore \mathbb{P}r(\theta=\text{loaded}\mid X=2) = 0.388 \qquad  \text{(posterior conditional probability ) }</span>
<span id="cb27-145"><a href="#cb27-145" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-coin-posterior-conditional-probability}</span>
<span id="cb27-146"><a href="#cb27-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-147"><a href="#cb27-147" aria-hidden="true" tabindex="-1"></a>This is all review from the previous course so far.</span>
<span id="cb27-148"><a href="#cb27-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-149"><a href="#cb27-149" aria-hidden="true" tabindex="-1"></a>But suppose we had a more complicated problem, where we couldn't work this all out in closed form? We'll know the likelihood and the prior, but we may not be able to get this normalizing constant. Can we instead do this by simulation? And indeed, yes we can.</span>
<span id="cb27-150"><a href="#cb27-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-151"><a href="#cb27-151" aria-hidden="true" tabindex="-1"></a>We can do this with Markov chain Monte Carlo. In particular, using the Metropolis-Hastings algorithm. What we'll do is, we'll set up a Markov chain whose **equilibrium distribution** has this **posterior distribution**. So we'll consider a Markov chain with two states, theta equals fair and theta equals loaded. And we'll allow the chain to move between those two states, with certain transition probabilities. We set this up using this using the Metropolis-Hastings algorithm.</span>
<span id="cb27-152"><a href="#cb27-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-153"><a href="#cb27-153" aria-hidden="true" tabindex="-1"></a><span class="al">![The Metropolis-Hastings Algorithm](images/c2l04-ss-03-Monte-Carlo-Demonstration.png)</span>{#fig-metropolis-hastings-alg  .column-margin group="slides" width="53mm"}</span>
<span id="cb27-154"><a href="#cb27-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-155"><a href="#cb27-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-156"><a href="#cb27-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-157"><a href="#cb27-157" aria-hidden="true" tabindex="-1"></a>So under the Metropolis-Hastings algorithm, step one is we start at an arbitrary location. And in this case, we can</span>
<span id="cb27-158"><a href="#cb27-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-159"><a href="#cb27-159" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>start at either $\theta \ne \text{fair}$, or $\theta \ne \text{loaded}$.</span>
<span id="cb27-160"><a href="#cb27-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-161"><a href="#cb27-161" aria-hidden="true" tabindex="-1"></a>It does not really matter where we start, we'll be moving back and forth and we're going to look at the long-term running average, the long-term simulations.</span>
<span id="cb27-162"><a href="#cb27-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-163"><a href="#cb27-163" aria-hidden="true" tabindex="-1"></a>So the key is we'll be simulating.</span>
<span id="cb27-164"><a href="#cb27-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-165"><a href="#cb27-165" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Run $m$ simulations and in each iteration, we'll propose a candidate and either accept it or reject it.</span>
<span id="cb27-166"><a href="#cb27-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-167"><a href="#cb27-167" aria-hidden="true" tabindex="-1"></a>a.  So the first part is we're proposing a new candidate. We'll call this candidate $\theta^*$, and we're going to propose it be the other state compared to where we are now. Where we are now is $\theta_{i-1}$, and so we'll propose to move to $\theta^*$.</span>
<span id="cb27-168"><a href="#cb27-168" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>If our *current state* is `fair`, we'll propose $\theta^*=\text{loaded}$.</span>
<span id="cb27-169"><a href="#cb27-169" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>If our *current state* is `loaded`, we'll propose $\theta^*=\text{fair}$.</span>
<span id="cb27-170"><a href="#cb27-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-171"><a href="#cb27-171" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">**what's our acceptance probability alpha?**</span><span class="co">]</span>{.column-margin}</span>
<span id="cb27-172"><a href="#cb27-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-173"><a href="#cb27-173" aria-hidden="true" tabindex="-1"></a>The general form for $\alpha$ is:</span>
<span id="cb27-174"><a href="#cb27-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-175"><a href="#cb27-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-176"><a href="#cb27-176" aria-hidden="true" tabindex="-1"></a>\begin {aligned}</span>
<span id="cb27-177"><a href="#cb27-177" aria-hidden="true" tabindex="-1"></a>\alpha &amp;= {</span>
<span id="cb27-178"><a href="#cb27-178" aria-hidden="true" tabindex="-1"></a>            { g(\theta^*)     / q(\theta^*     \mid  \theta_{i-1}) }</span>
<span id="cb27-179"><a href="#cb27-179" aria-hidden="true" tabindex="-1"></a>      \over {g(\theta_{i-1}) / q(\theta_{i-1} \mid  \theta^*)     }</span>
<span id="cb27-180"><a href="#cb27-180" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb27-181"><a href="#cb27-181" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>      &amp;= {</span>
<span id="cb27-182"><a href="#cb27-182" aria-hidden="true" tabindex="-1"></a>            { f(x=2 \mid \theta^*) f(\theta^*)     / 1 }</span>
<span id="cb27-183"><a href="#cb27-183" aria-hidden="true" tabindex="-1"></a>      \over { f(x=2 \mid \theta_{i-1})f(\theta_{i-1}) / 1    }</span>
<span id="cb27-184"><a href="#cb27-184" aria-hidden="true" tabindex="-1"></a>} \qquad \text {(sub. g,q)}</span>
<span id="cb27-185"><a href="#cb27-185" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-186"><a href="#cb27-186" aria-hidden="true" tabindex="-1"></a>$$ {#eq-alpha}</span>
<span id="cb27-187"><a href="#cb27-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-188"><a href="#cb27-188" aria-hidden="true" tabindex="-1"></a>In this case,</span>
<span id="cb27-189"><a href="#cb27-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-190"><a href="#cb27-190" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$g()$ is our un-normalized likelihood times prior</span>
<span id="cb27-191"><a href="#cb27-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-192"><a href="#cb27-192" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$q()$, the *proposal distribution*, is, in this case, since we always accept the opposite state deterministically i.e. $\theta^*=\neg \theta{i_1}$ with $P=1$</span>
<span id="cb27-193"><a href="#cb27-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-194"><a href="#cb27-194" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>If $\theta^* = \text{loaded} \implies \alpha = {0.00794 \over 0.0125}=0.635$</span>
<span id="cb27-195"><a href="#cb27-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-196"><a href="#cb27-196" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>If $\theta^* = \text{fair} \implies \alpha = { 0.0125 \over 0.00794}=1.574$</span>
<span id="cb27-197"><a href="#cb27-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-198"><a href="#cb27-198" aria-hidden="true" tabindex="-1"></a><span class="al">![The Metropolis-Hastings Algorithm](images/c2l04-ss-04-Monte-Carlo-Demonstration.png)</span>{#fig-metropolis-hastings-alg  .column-margin group="slides" width="53mm"}</span>
<span id="cb27-199"><a href="#cb27-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-200"><a href="#cb27-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-201"><a href="#cb27-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-202"><a href="#cb27-202" aria-hidden="true" tabindex="-1"></a>Given these probabilities, we then can do the acceptance or rejection step.</span>
<span id="cb27-203"><a href="#cb27-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-204"><a href="#cb27-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-205"><a href="#cb27-205" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb27-206"><a href="#cb27-206" aria-hidden="true" tabindex="-1"></a>\text{ accept } \theta^* \text { and set } \theta_i=\text{fair} &amp; \text{If } \theta^*=\text{fair,  } \alpha&gt;1</span>
<span id="cb27-207"><a href="#cb27-207" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \begin {cases} </span>
<span id="cb27-208"><a href="#cb27-208" aria-hidden="true" tabindex="-1"></a>   \text{ accept } \theta^* \text{  and set } \theta_i=\text{loaded} &amp;  \text{ With probability } 0.635 </span>
<span id="cb27-209"><a href="#cb27-209" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \text{ reject } \theta^* \text{ and set } \theta_i=\text{fair}     &amp;  \text{ Otherwise }</span>
<span id="cb27-210"><a href="#cb27-210" aria-hidden="true" tabindex="-1"></a>\end{cases} &amp; \text{If } \theta^*=\text{loaded, } \alpha=.635</span>
<span id="cb27-211"><a href="#cb27-211" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb27-212"><a href="#cb27-212" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-213"><a href="#cb27-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-214"><a href="#cb27-214" aria-hidden="true" tabindex="-1"></a>If the $\theta^*=\text{loaded} \implies \alpha=0.635$. So we accept theta star with probability $0.635$. And if we accept it. Set $\theta_i=\text{loaded}$ Otherwise, set $\theta_i = \theta_{i- 1}$, if we do not accept, it stays in that same old fair state.</span>
<span id="cb27-215"><a href="#cb27-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-216"><a href="#cb27-216" aria-hidden="true" tabindex="-1"></a>We can draw this out as a Markov chain with two states, Fair and 'loaded'. If it's in the 'loaded' state, it will move with probability one to the fair state. If it's in the fair state, it will move with a probability of $0.635$ to the 'loaded' state. And with a probability of $0.365$ it will stay in the fair state.</span>
<span id="cb27-217"><a href="#cb27-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-218"><a href="#cb27-218" aria-hidden="true" tabindex="-1"></a><span class="al">![state diagram](images/c2l04-states.png)</span>{#fig-mcmc-coins-states .column-margin group="slides" width="53mm"}</span>
<span id="cb27-219"><a href="#cb27-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-220"><a href="#cb27-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-221"><a href="#cb27-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-222"><a href="#cb27-222" aria-hidden="true" tabindex="-1"></a>And so here's a little diagram for this Markov chain with two states. In which case it will move back and forth with certain probabilities.</span>
<span id="cb27-223"><a href="#cb27-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-224"><a href="#cb27-224" aria-hidden="true" tabindex="-1"></a>Thus, if we wanted to find our **posterior probability** , $f(\theta=\text{loaded} \mid x=2)$. We can simulate from this Markov chain using these transition probabilities. And observe the fraction of time that it spends in the state theta equals 'loaded'. And this gives us a good estimate of the posterior probability that it's the 'loaded' coin. In this particular case, we can also show that this gives us the theoretical right answer.</span>
<span id="cb27-225"><a href="#cb27-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-226"><a href="#cb27-226" aria-hidden="true" tabindex="-1"></a>If you've seen a little bit of the theory of Markov chains. We can say that a Markov chain with transition probability capital $P$, has stationary distribution $\Pi$.</span>
<span id="cb27-227"><a href="#cb27-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-228"><a href="#cb27-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-229"><a href="#cb27-229" aria-hidden="true" tabindex="-1"></a>\pi P = \pi \qquad \text{(def. stationary distribution)}</span>
<span id="cb27-230"><a href="#cb27-230" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-coins-stationarity}</span>
<span id="cb27-231"><a href="#cb27-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-232"><a href="#cb27-232" aria-hidden="true" tabindex="-1"></a>Here we have a transition probability matrix $P$, where we can think about 'fair' and 'loaded'. Moving from the 'fair' state, remaining in the 'fair' state happens with a probability of $0.365$ and it moves from 'fair' to 'loaded', with a probability of $0.635$. If it's in the 'loaded' state, we'll move to the 'fair' state with probability one, and it will stay in the 'loaded' state with probability 0.</span>
<span id="cb27-233"><a href="#cb27-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-234"><a href="#cb27-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-235"><a href="#cb27-235" aria-hidden="true" tabindex="-1"></a>P=</span>
<span id="cb27-236"><a href="#cb27-236" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb27-237"><a href="#cb27-237" aria-hidden="true" tabindex="-1"></a>   0.365 &amp; 0.635 <span class="sc">\\</span> </span>
<span id="cb27-238"><a href="#cb27-238" aria-hidden="true" tabindex="-1"></a>   1     &amp; 0</span>
<span id="cb27-239"><a href="#cb27-239" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} </span>
<span id="cb27-240"><a href="#cb27-240" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-241"><a href="#cb27-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-242"><a href="#cb27-242" aria-hidden="true" tabindex="-1"></a>In this case, we want our stationary distribution to be the posterior probabilities.</span>
<span id="cb27-243"><a href="#cb27-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-244"><a href="#cb27-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-245"><a href="#cb27-245" aria-hidden="true" tabindex="-1"></a>\Pi=</span>
<span id="cb27-246"><a href="#cb27-246" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb27-247"><a href="#cb27-247" aria-hidden="true" tabindex="-1"></a>    0.612 &amp; 0.388 <span class="sc">\\</span></span>
<span id="cb27-248"><a href="#cb27-248" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} </span>
<span id="cb27-249"><a href="#cb27-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-250"><a href="#cb27-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-251"><a href="#cb27-251" aria-hidden="true" tabindex="-1"></a>Which you can recall are 0.$612$ of being 'fair' and 0.$388$ of being 'loaded'. And so indeed, if you do just the minimal amount of matrix algebra, you can see that 0.612, 0.388 Multiplied by this matrix, 0.365, 0.635, 1, 0, does indeed give you 0.612 and 0.388, at least to within rounding error.</span>
<span id="cb27-252"><a href="#cb27-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-253"><a href="#cb27-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-254"><a href="#cb27-254" aria-hidden="true" tabindex="-1"></a>\begin{aligned} </span>
<span id="cb27-255"><a href="#cb27-255" aria-hidden="true" tabindex="-1"></a>  \Pi P &amp;=</span>
<span id="cb27-256"><a href="#cb27-256" aria-hidden="true" tabindex="-1"></a>  \begin{bmatrix} </span>
<span id="cb27-257"><a href="#cb27-257" aria-hidden="true" tabindex="-1"></a>  0.612 &amp; 0.388 </span>
<span id="cb27-258"><a href="#cb27-258" aria-hidden="true" tabindex="-1"></a>  \end{bmatrix}</span>
<span id="cb27-259"><a href="#cb27-259" aria-hidden="true" tabindex="-1"></a>  \begin{bmatrix} </span>
<span id="cb27-260"><a href="#cb27-260" aria-hidden="true" tabindex="-1"></a>  0.365 &amp; 0.635 <span class="sc">\\</span> </span>
<span id="cb27-261"><a href="#cb27-261" aria-hidden="true" tabindex="-1"></a>      1 &amp; 0 </span>
<span id="cb27-262"><a href="#cb27-262" aria-hidden="true" tabindex="-1"></a>  \end{bmatrix}   <span class="sc">\\</span></span>
<span id="cb27-263"><a href="#cb27-263" aria-hidden="true" tabindex="-1"></a>  &amp;= \begin{bmatrix}</span>
<span id="cb27-264"><a href="#cb27-264" aria-hidden="true" tabindex="-1"></a>  0.612 &amp; 0.388 </span>
<span id="cb27-265"><a href="#cb27-265" aria-hidden="true" tabindex="-1"></a>  \end{bmatrix} </span>
<span id="cb27-266"><a href="#cb27-266" aria-hidden="true" tabindex="-1"></a>  <span class="sc">\\</span>&amp;= \Pi</span>
<span id="cb27-267"><a href="#cb27-267" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-268"><a href="#cb27-268" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mcmc-coins-proof}</span>
<span id="cb27-269"><a href="#cb27-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-270"><a href="#cb27-270" aria-hidden="true" tabindex="-1"></a>Thus in this case we can see, that we do get the correct stationary distribution for the Markov chain using the Metropolis--Hastings algorithm. And that when we simulate it, we do get correct estimates then of the posterior probabilities.</span>
<span id="cb27-271"><a href="#cb27-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-272"><a href="#cb27-272" aria-hidden="true" tabindex="-1"></a>This is a nice simple example where we can work out the posterior probabilities in closed form. We don't need to run Markov chain Monte Carlo. But this method is very powerful because all we need is to be able to evaluate the likelihood and the prior, we don't need to evaluate the full posterior and get that normalizing constant. And so this applies to a much broader range of more complicated problems. Where we can use Markov chain Monte Carlo to simulate, to be able to get these probabilities. We'll make good use of this in the rest of this course.</span>
<span id="cb27-273"><a href="#cb27-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-274"><a href="#cb27-274" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random walk with Normal likelihood, t prior</span></span>
<span id="cb27-275"><a href="#cb27-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-276"><a href="#cb27-276" aria-hidden="true" tabindex="-1"></a>Recall the model from the last segment of Lesson 2 where the data are the percent change in total personnel from last year to this year for n=10 companies. We used a normal likelihood with known variance and t distribution for the prior on the unknown mean. Suppose the values are $y=(1.2,1.4,−0.5,0.3,0.9,2.3,1.0,0.1,1.3,1.9)$. Because this model is not conjugate, the posterior distribution is not in a standard form that we can easily sample. To obtain posterior samples, we will set up a Markov chain whose stationary distribution is this posterior distribution.</span>
<span id="cb27-277"><a href="#cb27-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-278"><a href="#cb27-278" aria-hidden="true" tabindex="-1"></a>Recall that the posterior distribution is</span>
<span id="cb27-279"><a href="#cb27-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-280"><a href="#cb27-280" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-281"><a href="#cb27-281" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\mu \mid y_1, \ldots, y_n) \propto \frac{\exp<span class="co">[</span><span class="ot"> n ( \bar{y} \mu - \mu^2/2)</span><span class="co">]</span>}{1 + \mu^2}</span>
<span id="cb27-282"><a href="#cb27-282" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb27-283"><a href="#cb27-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-284"><a href="#cb27-284" aria-hidden="true" tabindex="-1"></a>The posterior distribution on the left is our target distribution and the expression on the right is our $g(\mu)$.</span>
<span id="cb27-285"><a href="#cb27-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-286"><a href="#cb27-286" aria-hidden="true" tabindex="-1"></a>The first thing we can do in R is write a function to evaluate $g(\mu)$. Because posterior distributions include likelihoods (the product of many numbers that are potentially small), $g(\mu)$ might evaluate to such a small number that to the computer, it is effectively zero. This will cause a problem when we evaluate the acceptance ratio $\alpha$. To avoid this problem, we can work on the log scale, which will be more numerically stable. Thus, we will write a function to evaluate</span>
<span id="cb27-287"><a href="#cb27-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-288"><a href="#cb27-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-289"><a href="#cb27-289" aria-hidden="true" tabindex="-1"></a>\log(g(\mu)) = n ( \bar{y} \mu - \mu^2/2) - \log(1 + \mu^2)</span>
<span id="cb27-290"><a href="#cb27-290" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-291"><a href="#cb27-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-292"><a href="#cb27-292" aria-hidden="true" tabindex="-1"></a>This function will require three arguments, $\mu$, $\bar{y}$, and $n$.</span>
<span id="cb27-293"><a href="#cb27-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-296"><a href="#cb27-296" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-297"><a href="#cb27-297" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-1</span></span>
<span id="cb27-298"><a href="#cb27-298" aria-hidden="true" tabindex="-1"></a>lg <span class="ot">=</span> <span class="cf">function</span>(mu, n, ybar) {</span>
<span id="cb27-299"><a href="#cb27-299" aria-hidden="true" tabindex="-1"></a>  mu2 <span class="ot">=</span> mu<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb27-300"><a href="#cb27-300" aria-hidden="true" tabindex="-1"></a>  n <span class="sc">*</span> (ybar <span class="sc">*</span> mu <span class="sc">-</span> mu2 <span class="sc">/</span> <span class="fl">2.0</span>) <span class="sc">-</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">+</span> mu2)</span>
<span id="cb27-301"><a href="#cb27-301" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-302"><a href="#cb27-302" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-303"><a href="#cb27-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-304"><a href="#cb27-304" aria-hidden="true" tabindex="-1"></a>Next, let's write a function to execute the **Random-Walk Metropolis-Hastings** sampler with *Normal* proposals.</span>
<span id="cb27-305"><a href="#cb27-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-308"><a href="#cb27-308" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-309"><a href="#cb27-309" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-2</span></span>
<span id="cb27-310"><a href="#cb27-310" aria-hidden="true" tabindex="-1"></a>mh <span class="ot">=</span> <span class="cf">function</span>(n, ybar, n_iter, mu_init, cand_sd) {</span>
<span id="cb27-311"><a href="#cb27-311" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Random-Walk Metropolis-Hastings algorithm</span></span>
<span id="cb27-312"><a href="#cb27-312" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-313"><a href="#cb27-313" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Step 1, initialize</span></span>
<span id="cb27-314"><a href="#cb27-314" aria-hidden="true" tabindex="-1"></a>  mu_out <span class="ot">=</span> <span class="fu">numeric</span>(n_iter)</span>
<span id="cb27-315"><a href="#cb27-315" aria-hidden="true" tabindex="-1"></a>  accpt <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb27-316"><a href="#cb27-316" aria-hidden="true" tabindex="-1"></a>  mu_now <span class="ot">=</span> mu_init</span>
<span id="cb27-317"><a href="#cb27-317" aria-hidden="true" tabindex="-1"></a>  lg_now <span class="ot">=</span> <span class="fu">lg</span>(<span class="at">mu=</span>mu_now, <span class="at">n=</span>n, <span class="at">ybar=</span>ybar)</span>
<span id="cb27-318"><a href="#cb27-318" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-319"><a href="#cb27-319" aria-hidden="true" tabindex="-1"></a>  <span class="do">## Step 2, iterate</span></span>
<span id="cb27-320"><a href="#cb27-320" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iter) {</span>
<span id="cb27-321"><a href="#cb27-321" aria-hidden="true" tabindex="-1"></a>    <span class="do">## step 2a</span></span>
<span id="cb27-322"><a href="#cb27-322" aria-hidden="true" tabindex="-1"></a>    mu_cand <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="at">n=</span><span class="dv">1</span>, <span class="at">mean=</span>mu_now, <span class="at">sd=</span>cand_sd) <span class="co"># draw a candidate</span></span>
<span id="cb27-323"><a href="#cb27-323" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-324"><a href="#cb27-324" aria-hidden="true" tabindex="-1"></a>    <span class="do">## Step 2b</span></span>
<span id="cb27-325"><a href="#cb27-325" aria-hidden="true" tabindex="-1"></a>    lg_cand <span class="ot">=</span> <span class="fu">lg</span>(<span class="at">mu=</span>mu_cand, <span class="at">n=</span>n, <span class="at">ybar=</span>ybar) <span class="co"># evaluate log of g with the candidate</span></span>
<span id="cb27-326"><a href="#cb27-326" aria-hidden="true" tabindex="-1"></a>    lalpha <span class="ot">=</span> lg_cand <span class="sc">-</span> lg_now <span class="co"># log of acceptance ratio</span></span>
<span id="cb27-327"><a href="#cb27-327" aria-hidden="true" tabindex="-1"></a>    alpha <span class="ot">=</span> <span class="fu">exp</span>(lalpha)</span>
<span id="cb27-328"><a href="#cb27-328" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-329"><a href="#cb27-329" aria-hidden="true" tabindex="-1"></a>    <span class="do">## step 2c</span></span>
<span id="cb27-330"><a href="#cb27-330" aria-hidden="true" tabindex="-1"></a>    u <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">1</span>) <span class="co"># draw a uniform variable which will be less than alpha with probability min(1, alpha)</span></span>
<span id="cb27-331"><a href="#cb27-331" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (u <span class="sc">&lt;</span> alpha) { <span class="co"># then accept the candidate</span></span>
<span id="cb27-332"><a href="#cb27-332" aria-hidden="true" tabindex="-1"></a>      mu_now <span class="ot">=</span> mu_cand</span>
<span id="cb27-333"><a href="#cb27-333" aria-hidden="true" tabindex="-1"></a>      accpt <span class="ot">=</span> accpt <span class="sc">+</span> <span class="dv">1</span> <span class="co"># to keep track of acceptance</span></span>
<span id="cb27-334"><a href="#cb27-334" aria-hidden="true" tabindex="-1"></a>      lg_now <span class="ot">=</span> lg_cand</span>
<span id="cb27-335"><a href="#cb27-335" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb27-336"><a href="#cb27-336" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-337"><a href="#cb27-337" aria-hidden="true" tabindex="-1"></a>    <span class="do">## collect results</span></span>
<span id="cb27-338"><a href="#cb27-338" aria-hidden="true" tabindex="-1"></a>    mu_out[i] <span class="ot">=</span> mu_now <span class="co"># save this iteration's value of mu</span></span>
<span id="cb27-339"><a href="#cb27-339" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-340"><a href="#cb27-340" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb27-341"><a href="#cb27-341" aria-hidden="true" tabindex="-1"></a>  <span class="do">## return a list of output</span></span>
<span id="cb27-342"><a href="#cb27-342" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">mu=</span>mu_out, <span class="at">accpt=</span>accpt<span class="sc">/</span>n_iter)</span>
<span id="cb27-343"><a href="#cb27-343" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-344"><a href="#cb27-344" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-345"><a href="#cb27-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-346"><a href="#cb27-346" aria-hidden="true" tabindex="-1"></a>Now, let's set up the problem.</span>
<span id="cb27-347"><a href="#cb27-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-350"><a href="#cb27-350" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-351"><a href="#cb27-351" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-3</span></span>
<span id="cb27-352"><a href="#cb27-352" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1.2</span>, <span class="fl">1.4</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.9</span>, <span class="fl">2.3</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span>, <span class="fl">1.3</span>, <span class="fl">1.9</span>)</span>
<span id="cb27-353"><a href="#cb27-353" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="fu">mean</span>(y)</span>
<span id="cb27-354"><a href="#cb27-354" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(y)</span>
<span id="cb27-355"><a href="#cb27-355" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(y, <span class="at">freq=</span><span class="cn">FALSE</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.0</span>, <span class="fl">3.0</span>)) <span class="co"># histogram of the data</span></span>
<span id="cb27-356"><a href="#cb27-356" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dt</span>(<span class="at">x=</span>x, <span class="at">df=</span><span class="dv">1</span>), <span class="at">lty=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>) <span class="co"># prior for mu</span></span>
<span id="cb27-357"><a href="#cb27-357" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(y, <span class="fu">rep</span>(<span class="dv">0</span>,n), <span class="at">pch=</span><span class="dv">1</span>) <span class="co"># individual data points</span></span>
<span id="cb27-358"><a href="#cb27-358" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(ybar, <span class="dv">0</span>, <span class="at">pch=</span><span class="dv">19</span>) <span class="co"># sample mean</span></span>
<span id="cb27-359"><a href="#cb27-359" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-360"><a href="#cb27-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-361"><a href="#cb27-361" aria-hidden="true" tabindex="-1"></a>Finally, we're ready to run the sampler! Let's use $m=1000$ iterations and proposal standard deviation (which controls the proposal step size) $3.0$, and initial value at the prior median $0$.</span>
<span id="cb27-362"><a href="#cb27-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-365"><a href="#cb27-365" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-366"><a href="#cb27-366" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-4</span></span>
<span id="cb27-367"><a href="#cb27-367" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">43</span>) <span class="co"># set the random seed for reproducibility</span></span>
<span id="cb27-368"><a href="#cb27-368" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">mh</span>(<span class="at">n=</span>n, <span class="at">ybar=</span>ybar, <span class="at">n_iter=</span><span class="fl">1e3</span>, <span class="at">mu_init=</span><span class="fl">0.0</span>, <span class="at">cand_sd=</span><span class="fl">3.0</span>)</span>
<span id="cb27-369"><a href="#cb27-369" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(post)</span>
<span id="cb27-370"><a href="#cb27-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-371"><a href="#cb27-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-374"><a href="#cb27-374" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-375"><a href="#cb27-375" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-5</span></span>
<span id="cb27-376"><a href="#cb27-376" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"coda"</span>)</span>
<span id="cb27-377"><a href="#cb27-377" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(<span class="fu">as.mcmc</span>(post<span class="sc">$</span>mu))</span>
<span id="cb27-378"><a href="#cb27-378" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-379"><a href="#cb27-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-380"><a href="#cb27-380" aria-hidden="true" tabindex="-1"></a>This last plot is called a trace plot. It shows the history of the chain and provides basic feedback about whether the chain has reached its stationary distribution.</span>
<span id="cb27-381"><a href="#cb27-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-382"><a href="#cb27-382" aria-hidden="true" tabindex="-1"></a>It appears our proposal step size was too large (acceptance rate below 23%). Let's try another.</span>
<span id="cb27-383"><a href="#cb27-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-386"><a href="#cb27-386" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-387"><a href="#cb27-387" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-6</span></span>
<span id="cb27-388"><a href="#cb27-388" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">mh</span>(<span class="at">n=</span>n, <span class="at">ybar=</span>ybar, <span class="at">n_iter=</span><span class="fl">1e3</span>, <span class="at">mu_init=</span><span class="fl">0.0</span>, <span class="at">cand_sd=</span><span class="fl">0.05</span>)</span>
<span id="cb27-389"><a href="#cb27-389" aria-hidden="true" tabindex="-1"></a>post<span class="sc">$</span>accpt</span>
<span id="cb27-390"><a href="#cb27-390" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-391"><a href="#cb27-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-394"><a href="#cb27-394" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-395"><a href="#cb27-395" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-7</span></span>
<span id="cb27-396"><a href="#cb27-396" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(<span class="fu">as.mcmc</span>(post<span class="sc">$</span>mu))</span>
<span id="cb27-397"><a href="#cb27-397" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-398"><a href="#cb27-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-399"><a href="#cb27-399" aria-hidden="true" tabindex="-1"></a>Oops, the acceptance rate is too high (above 50%). Let's try something in between.</span>
<span id="cb27-400"><a href="#cb27-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-403"><a href="#cb27-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-404"><a href="#cb27-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-8</span></span>
<span id="cb27-405"><a href="#cb27-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-406"><a href="#cb27-406" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">mh</span>(<span class="at">n=</span>n, <span class="at">ybar=</span>ybar, <span class="at">n_iter=</span><span class="fl">1e3</span>, <span class="at">mu_init=</span><span class="fl">0.0</span>, <span class="at">cand_sd=</span><span class="fl">0.9</span>)</span>
<span id="cb27-407"><a href="#cb27-407" aria-hidden="true" tabindex="-1"></a>post<span class="sc">$</span>accpt</span>
<span id="cb27-408"><a href="#cb27-408" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-409"><a href="#cb27-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-412"><a href="#cb27-412" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-413"><a href="#cb27-413" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-9</span></span>
<span id="cb27-414"><a href="#cb27-414" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(<span class="fu">as.mcmc</span>(post<span class="sc">$</span>mu))</span>
<span id="cb27-415"><a href="#cb27-415" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-416"><a href="#cb27-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-417"><a href="#cb27-417" aria-hidden="true" tabindex="-1"></a>Which looks good. Just for fun, let's see what happens if we initialize the chain at some far-off value.</span>
<span id="cb27-418"><a href="#cb27-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-421"><a href="#cb27-421" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-422"><a href="#cb27-422" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-10</span></span>
<span id="cb27-423"><a href="#cb27-423" aria-hidden="true" tabindex="-1"></a>post <span class="ot">=</span> <span class="fu">mh</span>(<span class="at">n=</span>n, <span class="at">ybar=</span>ybar, <span class="at">n_iter=</span><span class="fl">1e3</span>, <span class="at">mu_init=</span><span class="fl">30.0</span>, <span class="at">cand_sd=</span><span class="fl">0.9</span>)</span>
<span id="cb27-424"><a href="#cb27-424" aria-hidden="true" tabindex="-1"></a>post<span class="sc">$</span>accpt</span>
<span id="cb27-425"><a href="#cb27-425" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-426"><a href="#cb27-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-429"><a href="#cb27-429" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-430"><a href="#cb27-430" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-11</span></span>
<span id="cb27-431"><a href="#cb27-431" aria-hidden="true" tabindex="-1"></a><span class="fu">traceplot</span>(<span class="fu">as.mcmc</span>(post<span class="sc">$</span>mu))</span>
<span id="cb27-432"><a href="#cb27-432" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-433"><a href="#cb27-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-434"><a href="#cb27-434" aria-hidden="true" tabindex="-1"></a>It took awhile to find the stationary distribution, but it looks like we succeeded! If we discard the first 100 or so values, it appears like the rest of the samples come from the stationary distribution, our posterior distribution! Let's plot the posterior density against the prior to see how the data updated our belief about $\mu$.</span>
<span id="cb27-435"><a href="#cb27-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-438"><a href="#cb27-438" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-439"><a href="#cb27-439" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-12</span></span>
<span id="cb27-440"><a href="#cb27-440" aria-hidden="true" tabindex="-1"></a>post<span class="sc">$</span>mu_keep <span class="ot">=</span> post<span class="sc">$</span>mu[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>)] <span class="co"># discard the first 200 samples</span></span>
<span id="cb27-441"><a href="#cb27-441" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(post<span class="sc">$</span>mu_keep, <span class="at">adjust=</span><span class="fl">2.0</span>), <span class="at">main=</span><span class="st">""</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.0</span>, <span class="fl">3.0</span>), <span class="at">xlab=</span><span class="fu">expression</span>(mu)) <span class="co"># plot density estimate of the posterior</span></span>
<span id="cb27-442"><a href="#cb27-442" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dt</span>(<span class="at">x=</span>x, <span class="at">df=</span><span class="dv">1</span>), <span class="at">lty=</span><span class="dv">2</span>, <span class="at">add=</span><span class="cn">TRUE</span>) <span class="co"># prior for mu</span></span>
<span id="cb27-443"><a href="#cb27-443" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(ybar, <span class="dv">0</span>, <span class="at">pch=</span><span class="dv">19</span>) <span class="co"># sample mean</span></span>
<span id="cb27-444"><a href="#cb27-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-445"><a href="#cb27-445" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fl">0.017</span><span class="sc">*</span><span class="fu">exp</span>(<span class="fu">lg</span>(<span class="at">mu=</span>x, <span class="at">n=</span>n, <span class="at">ybar=</span>ybar)), <span class="at">from=</span><span class="sc">-</span><span class="fl">1.0</span>, <span class="at">to=</span><span class="fl">3.0</span>, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">col=</span><span class="st">"blue"</span>) <span class="co"># approximation to the true posterior in blue</span></span>
<span id="cb27-446"><a href="#cb27-446" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-447"><a href="#cb27-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-448"><a href="#cb27-448" aria-hidden="true" tabindex="-1"></a>These results are encouraging, but they are preliminary. We still need to investigate more formally whether our Markov chain has converged to the stationary distribution. We will explore this in a future lesson.</span>
<span id="cb27-449"><a href="#cb27-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-450"><a href="#cb27-450" aria-hidden="true" tabindex="-1"></a>Obtaining posterior samples using the Metropolis-Hastings algorithm can be time-consuming and require some fine-tuning, as we've just seen. The good news is that we can rely on software to do most of the work for us. In the next couple of videos, we'll introduce a program that will make posterior sampling easy.</span>
<span id="cb27-451"><a href="#cb27-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-452"><a href="#cb27-452" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction to JAGS</span></span>
<span id="cb27-453"><a href="#cb27-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-454"><a href="#cb27-454" aria-hidden="true" tabindex="-1"></a><span class="fu">## Setup</span></span>
<span id="cb27-455"><a href="#cb27-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-456"><a href="#cb27-456" aria-hidden="true" tabindex="-1"></a><span class="fu">### Introduction to `JAGS`</span></span>
<span id="cb27-457"><a href="#cb27-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-458"><a href="#cb27-458" aria-hidden="true" tabindex="-1"></a>There are several software packages available that will handle the details of MCMC for us. See the supplementary material for a brief overview of options.</span>
<span id="cb27-459"><a href="#cb27-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-460"><a href="#cb27-460" aria-hidden="true" tabindex="-1"></a>The package we will use in this course is <span class="in">`JAGS`</span> (Just Another Gibbs Sampler) by <span class="co">[</span><span class="ot">Martyn Plummer</span><span class="co">](https://www.aminer.cn/profile/martyn-plummer/53f44c8edabfaefedbb2c089?source=zz1)</span>. The program is free, and runs on Mac OS, Windows, and Linux. Better yet, the program can be run using <span class="in">`R`</span> with the <span class="in">`rjags`</span> and <span class="in">`R2jags`</span> packages.</span>
<span id="cb27-461"><a href="#cb27-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-462"><a href="#cb27-462" aria-hidden="true" tabindex="-1"></a>In <span class="in">`JAGS`</span>, we can specify models and run MCMC samplers in just a few lines of code; <span class="in">`JAGS`</span> does the rest for us, so we can focus more on the statistical modeling aspect and less on the implementation. It makes powerful Bayesian machinery available to us as we can fit a wide variety of statistical models with relative ease.</span>
<span id="cb27-463"><a href="#cb27-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-464"><a href="#cb27-464" aria-hidden="true" tabindex="-1"></a><span class="fu">### Installation and setup</span></span>
<span id="cb27-465"><a href="#cb27-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-466"><a href="#cb27-466" aria-hidden="true" tabindex="-1"></a>The starting place for <span class="in">`JAGS`</span> users is <span class="co">[</span><span class="ot">mcmc-jags.sourceforge.net</span><span class="co">](http://mcmc-jags.sourceforge.net)</span>. At this site, you can find news about the features of the latest release of <span class="in">`JAGS`</span>, links to program documentation, as well as instructions for installation.</span>
<span id="cb27-467"><a href="#cb27-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-468"><a href="#cb27-468" aria-hidden="true" tabindex="-1"></a>The documentation is particularly important. It is available under the <span class="co">[</span><span class="ot">files page</span><span class="co">](https://sourceforge.net/projects/mcmc-jags/files/)</span> link in the Manuals folder.</span>
<span id="cb27-469"><a href="#cb27-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-470"><a href="#cb27-470" aria-hidden="true" tabindex="-1"></a>Also under the <span class="co">[</span><span class="ot">files page</span><span class="co">](https://sourceforge.net/projects/mcmc-jags/files/)</span>, you will find the <span class="in">`JAGS`</span> folder where you can download and install the latest version of <span class="in">`JAGS`</span>. Select the version and operating system, and follow the instructions for download and installation.</span>
<span id="cb27-471"><a href="#cb27-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-472"><a href="#cb27-472" aria-hidden="true" tabindex="-1"></a>Once <span class="in">`JAGS`</span> is installed, we can immediately run it from <span class="in">`R`</span> using the <span class="in">`rjags`</span> package. The next segment will show how this is done.</span>
<span id="cb27-473"><a href="#cb27-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-474"><a href="#cb27-474" aria-hidden="true" tabindex="-1"></a><span class="fu">## Modeling in `JAGS`</span></span>
<span id="cb27-475"><a href="#cb27-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-476"><a href="#cb27-476" aria-hidden="true" tabindex="-1"></a>There are four steps to implementing a model in <span class="in">`JAGS`</span> through <span class="in">`R`</span>:</span>
<span id="cb27-477"><a href="#cb27-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-478"><a href="#cb27-478" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Specify the model.</span>
<span id="cb27-479"><a href="#cb27-479" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Set up the model.</span>
<span id="cb27-480"><a href="#cb27-480" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Run the MCMC sampler.</span>
<span id="cb27-481"><a href="#cb27-481" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Post-processing.</span>
<span id="cb27-482"><a href="#cb27-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-483"><a href="#cb27-483" aria-hidden="true" tabindex="-1"></a>We will demonstrate these steps with our running example with the data are the percent change in total personnel from last year to this year for $n=10$ companies. We used a normal likelihood with known variance and t distribution for the prior on the unknown mean.</span>
<span id="cb27-484"><a href="#cb27-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-485"><a href="#cb27-485" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Specify the model</span></span>
<span id="cb27-486"><a href="#cb27-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-487"><a href="#cb27-487" aria-hidden="true" tabindex="-1"></a>In this step, we give <span class="in">`JAGS`</span> the hierarchical structure of the model, assigning distributions to the data (the likelihood) and parameters (priors). The syntax for this step is very similar to <span class="in">`R`</span>, but there are some key differences.</span>
<span id="cb27-488"><a href="#cb27-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-491"><a href="#cb27-491" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-492"><a href="#cb27-492" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-13</span></span>
<span id="cb27-493"><a href="#cb27-493" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"rjags"</span>)</span>
<span id="cb27-494"><a href="#cb27-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-495"><a href="#cb27-495" aria-hidden="true" tabindex="-1"></a>mod_string <span class="ot">=</span> <span class="st">" model {</span></span>
<span id="cb27-496"><a href="#cb27-496" aria-hidden="true" tabindex="-1"></a><span class="st">  for (i in 1:n) {</span></span>
<span id="cb27-497"><a href="#cb27-497" aria-hidden="true" tabindex="-1"></a><span class="st">    y[i] ~ dnorm(mu, 1.0/sig2)</span></span>
<span id="cb27-498"><a href="#cb27-498" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb27-499"><a href="#cb27-499" aria-hidden="true" tabindex="-1"></a><span class="st">  mu ~ dt(0.0, 1.0/1.0, 1.0) # location, inverse scale, degrees of freedom</span></span>
<span id="cb27-500"><a href="#cb27-500" aria-hidden="true" tabindex="-1"></a><span class="st">  sig2 = 1.0</span></span>
<span id="cb27-501"><a href="#cb27-501" aria-hidden="true" tabindex="-1"></a><span class="st">} "</span></span>
<span id="cb27-502"><a href="#cb27-502" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-503"><a href="#cb27-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-504"><a href="#cb27-504" aria-hidden="true" tabindex="-1"></a>One of the primary differences between the syntax of <span class="in">`JAGS`</span> and <span class="in">`R`</span> is how the distributions are parameterized. Note that the normal distribution uses the mean and precision (instead of variance). When specifying distributions in <span class="in">`JAGS`</span>, it is always a good idea to check the <span class="in">`JAGS`</span> user manual <span class="co">[</span><span class="ot">here</span><span class="co">](https://sourceforge.net/projects/mcmc-jags/files/Manuals)</span> in the chapter on Distributions.</span>
<span id="cb27-505"><a href="#cb27-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-506"><a href="#cb27-506" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Set up the model</span></span>
<span id="cb27-507"><a href="#cb27-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-510"><a href="#cb27-510" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-511"><a href="#cb27-511" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-14</span></span>
<span id="cb27-512"><a href="#cb27-512" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">50</span>)</span>
<span id="cb27-513"><a href="#cb27-513" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1.2</span>, <span class="fl">1.4</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.9</span>, <span class="fl">2.3</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span>, <span class="fl">1.3</span>, <span class="fl">1.9</span>)</span>
<span id="cb27-514"><a href="#cb27-514" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(y)</span>
<span id="cb27-515"><a href="#cb27-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-516"><a href="#cb27-516" aria-hidden="true" tabindex="-1"></a>data_jags <span class="ot">=</span> <span class="fu">list</span>(<span class="at">y=</span>y, <span class="at">n=</span>n)</span>
<span id="cb27-517"><a href="#cb27-517" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"mu"</span>)</span>
<span id="cb27-518"><a href="#cb27-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-519"><a href="#cb27-519" aria-hidden="true" tabindex="-1"></a>inits <span class="ot">=</span> <span class="cf">function</span>() {</span>
<span id="cb27-520"><a href="#cb27-520" aria-hidden="true" tabindex="-1"></a>  inits <span class="ot">=</span> <span class="fu">list</span>(<span class="st">"mu"</span><span class="ot">=</span><span class="fl">0.0</span>)</span>
<span id="cb27-521"><a href="#cb27-521" aria-hidden="true" tabindex="-1"></a>} <span class="co"># optional (and fixed)</span></span>
<span id="cb27-522"><a href="#cb27-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-523"><a href="#cb27-523" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">=</span> <span class="fu">jags.model</span>(<span class="fu">textConnection</span>(mod_string), <span class="at">data=</span>data_jags, <span class="at">inits=</span>inits)</span>
<span id="cb27-524"><a href="#cb27-524" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-525"><a href="#cb27-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-526"><a href="#cb27-526" aria-hidden="true" tabindex="-1"></a>There are multiple ways to specify initial values here. They can be explicitly set, as we did here, or they can be random, i.e., <span class="in">`list("mu"=rnorm(1))`</span>. Also, we can omit the initial values, and <span class="in">`JAGS`</span> will provide them.</span>
<span id="cb27-527"><a href="#cb27-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-528"><a href="#cb27-528" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Run the MCMC sampler</span></span>
<span id="cb27-529"><a href="#cb27-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-532"><a href="#cb27-532" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-533"><a href="#cb27-533" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-15</span></span>
<span id="cb27-534"><a href="#cb27-534" aria-hidden="true" tabindex="-1"></a><span class="fu">update</span>(mod, <span class="dv">500</span>) <span class="co"># burn-in</span></span>
<span id="cb27-535"><a href="#cb27-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-536"><a href="#cb27-536" aria-hidden="true" tabindex="-1"></a>mod_sim <span class="ot">=</span> <span class="fu">coda.samples</span>(<span class="at">model=</span>mod, <span class="at">variable.names=</span>params, <span class="at">n.iter=</span><span class="dv">1000</span>)</span>
<span id="cb27-537"><a href="#cb27-537" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-538"><a href="#cb27-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-539"><a href="#cb27-539" aria-hidden="true" tabindex="-1"></a>We will discuss more options to the <span class="in">`coda.samples`</span> function in coming examples.</span>
<span id="cb27-540"><a href="#cb27-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-541"><a href="#cb27-541" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4. Post-processing</span></span>
<span id="cb27-542"><a href="#cb27-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-545"><a href="#cb27-545" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-546"><a href="#cb27-546" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-16</span></span>
<span id="cb27-547"><a href="#cb27-547" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_sim)</span>
<span id="cb27-548"><a href="#cb27-548" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-549"><a href="#cb27-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-552"><a href="#cb27-552" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb27-553"><a href="#cb27-553" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: C2-L04-17</span></span>
<span id="cb27-554"><a href="#cb27-554" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"coda"</span>)</span>
<span id="cb27-555"><a href="#cb27-555" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod_sim)</span>
<span id="cb27-556"><a href="#cb27-556" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-557"><a href="#cb27-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-558"><a href="#cb27-558" aria-hidden="true" tabindex="-1"></a>We will discuss post processing further, including convergence diagnostics, in a coming lesson.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>