<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="dcterms.date" content="2024-11-03">
<meta name="keywords" content="notes, time series, autoregressive models, stationarity, MLE, AR(1) process, Yule-Walker equations, Durbin-Levinson recursion, R code">
<meta name="description" content="This lesson we will define the AR(1) process, Stationarity, ACF, PACF, differencing, smoothing">

<title>90&nbsp; The AR(1): MLE and Bayesian inference - M1L3 – Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C4-L04.html" rel="next">
<link href="./C4-L02.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(images/banner_deep.jpg);
background-size: cover;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C4-L03.html"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(1): MLE and Bayesian inference - M1L3</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(1): MLE and Bayesian inference - M1L3</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Time Series Analysis</p>
                  <div>
        <div class="description">
          This lesson we will define the AR(1) process, Stationarity, ACF, PACF, differencing, smoothing
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">bayesian statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 3, 2024</p>
      </div>
    </div>
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>notes, time series, autoregressive models, stationarity, MLE, AR(1) process, Yule-Walker equations, Durbin-Levinson recursion, R code</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Paradigms of probability - M1L1HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayes’ Theorem - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law - M1L2HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Probability and Bayes’ Theorem - M1L2HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Distributions - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Random Variables - M1L3HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Homework on Distributions - M1L3HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Frequentist Inference - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Frequentist MLE - M2L3HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bayesian Inference - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference - M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Priors - M3L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities - M3L6HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Homework on Priors - M2L7HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Poisson Data - M3L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Homework on Poisson Data - M3L8HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Beta Bernoulli - M3L8HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Homework on Exponential Data - M4L9HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Normally distributed Data - M4L10</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Homework on Normal Data - M4L10HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Non-Informative Priors - M4L11</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Homework Alternative Priors - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Brief Review of Regression - M4L12</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm - M2L4HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Gibbs sampling - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Homework Gibbs-Sampling algorithm - M2L22HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Assessing Convergence - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Homework on M-H algorithm M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Linear regression - M3L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Homework on Linear Regression Model Part 1 - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Homework on Deviance information criterion - M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">ANOVA - M3L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Homework on ANOVA - M3L8HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Homework on Multiple Factor ANOVA - M3L8HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Logistic regression - M3L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression - M3L9HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Poisson regression - M4L10</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Homework on Poisson regression - M4L10HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Hierarchical modeling - M4L11</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Capstone Project - M4L12</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models - M4L12HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Definitions of Mixture Models - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Likelihood functions for Mixture Models - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Homework The Likelihood function - M1L2HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Homework Identifiability - M1L2HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Homework The likelihood function M1L2HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Homework on simulating from a Poisson Mixture Model - M1L2HW4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model - M1L2HW5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Homework Sim mixture of exponential distributions - M1L2HW6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">M2L3 - The EM algorithm for Mixture models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">MCMC for Mixture Models - M4L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Density Estimation - M4L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Computational Considerations - M5L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Determining the number of components - M5L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Homework on Bayesian Information Criteria (BIC) - M5L09HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Homework on BIC for zero-inflated mixtures - M5L09HW4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Week 0: Introductions to time series analysis and the AR(1) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Stationarity, The ACF and the PCF M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">The AR(1) process: definitions and properties - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(1): MLE and Bayesian inference - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">The AR(p) process - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Bayesian Inference in the AR(p) - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1 - M3L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 1 M3L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Seasonal NDLMs M4L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 2 - M4L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Time Series Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">106</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">107</span>&nbsp; <span class="chapter-title">Bayes by backprop</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">108</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">109</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#review-of-maximum-likelihood-and-bayesian-inference-in-regression" id="toc-review-of-maximum-likelihood-and-bayesian-inference-in-regression" class="nav-link active" data-scroll-target="#review-of-maximum-likelihood-and-bayesian-inference-in-regression"><span class="header-section-number">90.1</span> Review of maximum likelihood and Bayesian inference in regression</a>
  <ul class="collapse">
  <li><a href="#regression-models-maximum-likelihood-estimation" id="toc-regression-models-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#regression-models-maximum-likelihood-estimation"><span class="header-section-number">90.1.1</span> Regression Models: Maximum Likelihood Estimation</a></li>
  <li><a href="#sec-bayes-regression" id="toc-sec-bayes-regression" class="nav-link" data-scroll-target="#sec-bayes-regression"><span class="header-section-number">90.1.2</span> Regression Models: Bayesian Inference</a></li>
  </ul></li>
  <li><a href="#sec-mle-ar1" id="toc-sec-mle-ar1" class="nav-link" data-scroll-target="#sec-mle-ar1"><span class="header-section-number">90.2</span> Maximum likelihood estimation in the AR(1) (video)</a>
  <ul class="collapse">
  <li><a href="#model-setup" id="toc-model-setup" class="nav-link" data-scroll-target="#model-setup"><span class="header-section-number">90.2.1</span> <strong>Model Setup</strong></a></li>
  <li><a href="#distributional-assumptions" id="toc-distributional-assumptions" class="nav-link" data-scroll-target="#distributional-assumptions"><span class="header-section-number">90.2.2</span> <strong>Distributional Assumptions</strong></a></li>
  <li><a href="#likelihood-approaches" id="toc-likelihood-approaches" class="nav-link" data-scroll-target="#likelihood-approaches"><span class="header-section-number">90.2.3</span> <strong>Likelihood Approaches</strong></a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">90.2.4</span> Conclusion</a></li>
  </ul></li>
  <li><a href="#r-code-mle-for-the-ar1-examples-reading" id="toc-r-code-mle-for-the-ar1-examples-reading" class="nav-link" data-scroll-target="#r-code-mle-for-the-ar1-examples-reading"><span class="header-section-number">90.3</span> R code: MLE for the AR(1), examples (reading)</a></li>
  <li><a href="#bayesian-inference-in-the-ar1" id="toc-bayesian-inference-in-the-ar1" class="nav-link" data-scroll-target="#bayesian-inference-in-the-ar1"><span class="header-section-number">90.4</span> Bayesian inference in the AR(1)</a></li>
  <li><a href="#bayesian-inference-in-the-ar1-conditional-likelihood-example-video" id="toc-bayesian-inference-in-the-ar1-conditional-likelihood-example-video" class="nav-link" data-scroll-target="#bayesian-inference-in-the-ar1-conditional-likelihood-example-video"><span class="header-section-number">90.5</span> Bayesian inference in the AR(1): Conditional likelihood example (video)</a></li>
  <li><a href="#r-code-ar1-bayesian-inference-conditional-likelihood-example-reading" id="toc-r-code-ar1-bayesian-inference-conditional-likelihood-example-reading" class="nav-link" data-scroll-target="#r-code-ar1-bayesian-inference-conditional-likelihood-example-reading"><span class="header-section-number">90.6</span> R Code: AR(1) Bayesian inference, conditional likelihood example (reading)</a></li>
  <li><a href="#quiz---mle-and-bayesian-inference-in-the-ar1" id="toc-quiz---mle-and-bayesian-inference-in-the-ar1" class="nav-link" data-scroll-target="#quiz---mle-and-bayesian-inference-in-the-ar1"><span class="header-section-number">90.7</span> Quiz - MLE and Bayesian inference in the AR(1)</a></li>
  <li><a href="#practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1" id="toc-practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1" class="nav-link" data-scroll-target="#practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1"><span class="header-section-number">90.8</span> Practice Graded Assignment: MLE and Bayesian inference in the AR(1)</a>
  <ul class="collapse">
  <li><a href="#bayesian-inference-in-the-ar1-full-likelihood-example-reading" id="toc-bayesian-inference-in-the-ar1-full-likelihood-example-reading" class="nav-link" data-scroll-target="#bayesian-inference-in-the-ar1-full-likelihood-example-reading"><span class="header-section-number">90.8.1</span> Bayesian Inference in the AR(1), : full likelihood example (reading)</a></li>
  <li><a href="#transformation-of-phi" id="toc-transformation-of-phi" class="nav-link" data-scroll-target="#transformation-of-phi"><span class="header-section-number">90.8.2</span> Transformation of <span class="math inline">\phi</span></a></li>
  <li><a href="#mcmc-algorithm-bayesian-inference-for-ar1-full-likelihood" id="toc-mcmc-algorithm-bayesian-inference-for-ar1-full-likelihood" class="nav-link" data-scroll-target="#mcmc-algorithm-bayesian-inference-for-ar1-full-likelihood"><span class="header-section-number">90.8.3</span> MCMC Algorithm: Bayesian Inference for AR(1), Full Likelihood</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Learning Objectives
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Perform maximum likelihood estimation for the full and conditional likelihood in an AR(1) <a href="#l2g3">#</a></label></li>
<li><label><input type="checkbox" checked="">Perform Bayesian inference for the <strong>AR(1)</strong> under the conditional likelihood and the reference prior <a href="#l2g4">#</a></label></li>
</ul>
</div>
</div>
</div>
<section id="review-of-maximum-likelihood-and-bayesian-inference-in-regression" class="level2" data-number="90.1">
<h2 data-number="90.1" class="anchored" data-anchor-id="review-of-maximum-likelihood-and-bayesian-inference-in-regression"><span class="header-section-number">90.1</span> Review of maximum likelihood and Bayesian inference in regression</h2>
<section id="regression-models-maximum-likelihood-estimation" class="level3" data-number="90.1.1">
<h3 data-number="90.1.1" class="anchored" data-anchor-id="regression-models-maximum-likelihood-estimation"><span class="header-section-number">90.1.1</span> Regression Models: Maximum Likelihood Estimation</h3>
<p> </p>
<p>Assume a regression model with the following structure: <span class="math display">
y_i = \beta_1x_{i,1} + \ldots + \beta_kx_{i,k} + \epsilon_i,
</span></p>
<p>for <span class="math inline">i = 1, \ldots, n</span> and <span class="math inline">\epsilon_i</span> independent random variables with <span class="math inline">\epsilon_i \sim \mathcal{N}(0, v) \quad \forall i</span>. This model can be written in matrix form as:</p>
<p><span class="math display">
y = \mathbf{X} \boldsymbol{\beta} + \boldsymbol\epsilon \qquad \boldsymbol\epsilon \sim \mathcal{N} (0, v\mathbf{I})
</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">y = (y_1, \ldots, y_n)′</span> is an n-dimensional vector of responses,</li>
<li><span class="math inline">\mathbf{X}</span> is an <span class="math inline">n × k</span> matrix containing the explanatory variables,</li>
<li><span class="math inline">\boldsymbol \beta = (\beta_1, \ldots, \beta_k)'</span> is the k-dimensional vector of regression coefficients,</li>
<li><span class="math inline">\boldsymbol \epsilon = (\epsilon_1, \ldots, \epsilon_n)'</span> is the n-dimensional vector of errors,</li>
<li><span class="math inline">\mathbf{I}</span> is an <span class="math inline">n \times n</span> identity matrix.</li>
</ul>
<p>If <span class="math inline">\mathbf{X}</span> is a <em>full rank</em> matrix with rank <span class="math inline">k</span> , the maximum likelihood estimator for <span class="math inline">\boldsymbol\beta</span>, denoted as <span class="math inline">\hat{\boldsymbol\beta}_{MLE}</span> is given by:</p>
<p><span id="eq-mle-estimator-beta"><span class="math display">
\hat{\boldsymbol{\beta}}_{MLE} = (\mathbf{X}'\mathbf{X})^{−1}\mathbf{X}'\mathbf{y},
\tag{90.1}</span></span></p>
<p>where <span class="math inline">(\mathbf{X}'\mathbf{X})^{−1}\mathbf{X}'</span> is the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose pseudoinverse</a> of the matrix <span class="math inline">\mathbf{X}</span>. This Moore-Penrose pseudoinverse of the matrix <span class="math inline">\mathbf{X}</span> is used to obtain the least squares solution to the linear regression problem.</p>
<p>and the MLE for <span class="math inline">v</span> is given by:</p>
<p><span id="eq-mle-estimator-v"><span class="math display">
\hat{v}_{MLE} = \frac{1}{n} (y − \mathbf{X} \hat{\boldsymbol{\beta}}_{MLE})′(y − \mathbf{X} \hat{\boldsymbol{\beta}}_{MLE})
\tag{90.2}</span></span></p>
<p><span class="math inline">\hat{v}_{MLE}</span> is not an unbiased estimator of <span class="math inline">v</span>, therefore, the following unbiased estimator of <span class="math inline">v</span> is typically used:</p>
<p><span id="eq-mle-unbiased-estimator-v"><span class="math display">
s^2 = \frac{1}{n-k}(y − \mathbf{X} \hat{\boldsymbol\beta}_{MLE} )′(y − \mathbf{X} \hat{\boldsymbol\beta}_{MLE} )
\tag{90.3}</span></span></p>
</section>
<section id="sec-bayes-regression" class="level3" data-number="90.1.2">
<h3 data-number="90.1.2" class="anchored" data-anchor-id="sec-bayes-regression"><span class="header-section-number">90.1.2</span> Regression Models: Bayesian Inference</h3>
<p>Assume once again we have a model with the structure in (1), which results in a likelihood of the form</p>
<p><span class="math display">
\mathbb{P}r(y \mid \boldsymbol{\beta} , v) = \frac{1}{(2\pi v)^{n/2}}\exp \left\{ -\frac{1}{2} (y − \mathbf{X} \boldsymbol{\beta})′(y − \mathbf{X} \boldsymbol{\beta}) \right\}
</span></p>
<p>If a prior of the form :</p>
<p><span id="eq-prior-regression"><span class="math display">
\mathbb{P}r(\boldsymbol{\beta}, v) \propto \frac{1}{v}
\tag{90.4}</span></span></p>
<p>is used, we obtain that the posterior distribution is given by:</p>
<p><span id="eq-posterior-regression"><span class="math display">
\mathbb{P}r(\boldsymbol{\beta},v \mid \mathbf{y}) \propto \frac{1}{v^{n/2+1}}\exp \left\{ -\frac{1}{2v} (\mathbf{y} − \mathbf{X} \boldsymbol{\beta})′(\mathbf{y} − \mathbf{X} \boldsymbol{\beta}) \right\}
\tag{90.5}</span></span></p>
<p>In addition it can be shown that</p>
<ul>
<li><span class="math inline">(\boldsymbol{\beta}\mid v, \mathbf{y}) \sim \mathcal{N} (\hat{\boldsymbol{\beta}}_{MLE} , v(\mathbf{X}'\mathbf{X})^{-1})</span></li>
<li><span class="math inline">(v \mid \mathbf{y}) \sim \mathcal{IG}((n − k)/2, d/2)</span> with</li>
</ul>
<p><span id="eq-d-unbiased-estimator-v"><span class="math display">
d = (\mathbf{y} − \mathbf{X} \hat{\boldsymbol{\beta}}_{MLE} )′(\mathbf{y} − \mathbf{X} \hat{\boldsymbol{\beta}}_{MLE} )
\tag{90.6}</span></span></p>
<p>where <span class="math inline">\mathcal{IG}(a, b)</span> denotes the inverse-gamma distribution with <em>shape</em> parameter <span class="math inline">a</span> and <em>scale</em> parameter <span class="math inline">b</span>.</p>
<p>with <span class="math inline">k = dim(\boldsymbol\beta)</span>.</p>
<p><mark>Given that <span class="math inline">\mathbb{P}r(\boldsymbol\beta, v \mid \mathbf{y}) = \mathbb{P}r(\boldsymbol\beta \mid v, \mathbf{y})p(v \mid \mathbf{y})</span> the equations above provide a way to directly sample from the posterior distribution of <span class="math inline">\boldsymbol \beta</span> and <span class="math inline">v</span> by first sampling v from the inverse-gamma distribution above and then conditioning on this sampled value of v, sampling <span class="math inline">\boldsymbol \beta</span> from the normal distribution above.</mark></p>
</section>
</section>
<section id="sec-mle-ar1" class="level2 page-columns page-full" data-number="90.2">
<h2 data-number="90.2" class="anchored" data-anchor-id="sec-mle-ar1"><span class="header-section-number">90.2</span> Maximum likelihood estimation in the AR(1) (video)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-s_0041" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-s_0041-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/m1_0041.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;90.1: MLE 1"><img src="images/m1_0041.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-s_0041-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;90.1: MLE 1
</figcaption>
</figure>
</div><div id="fig-s_0042" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-s_0042-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/m1_0042.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;90.2: Full Likelihood MLE"><img src="images/m1_0042.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-s_0042-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;90.2: Full Likelihood MLE
</figcaption>
</figure>
</div><div id="fig-s_0043" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-s_0043-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/m1_0043.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;90.3: Conditional Likelihood MLE"><img src="images/m1_0043.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-s_0043-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;90.3: Conditional Likelihood MLE
</figcaption>
</figure>
</div></div>

<p>There are two main strategies for performing MLE for an AR(1) model:</p>
<ul>
<li>Full Likelihood: Considers the joint distribution of all observations <span class="math inline">y_1, \dots, y_T</span>.</li>
<li>Conditional Likelihood: Conditions on the first observation (<span class="math inline">y_1</span>) and works with the likelihood of the remaining observations (<span class="math inline">y_2, \dots, y_T</span>).</li>
</ul>
<section id="model-setup" class="level3" data-number="90.2.1">
<h3 data-number="90.2.1" class="anchored" data-anchor-id="model-setup"><span class="header-section-number">90.2.1</span> <strong>Model Setup</strong></h3>
<p>The focus is on the <strong>zero-mean AR(1) model</strong>:</p>
<p><span class="math display">
Y_t = \phi Y_{t-1} + \epsilon_t, \quad \epsilon_t \overset{iid}{\sim} \mathcal{N}(0, v), \quad \phi \in (-1,1)
</span></p>
<p>This condition ensures <strong>stationarity</strong> of the process.</p>
</section>
<section id="distributional-assumptions" class="level3" data-number="90.2.2">
<h3 data-number="90.2.2" class="anchored" data-anchor-id="distributional-assumptions"><span class="header-section-number">90.2.2</span> <strong>Distributional Assumptions</strong></h3>
<ul>
<li><span class="math inline">Y_1 \sim \mathcal{N}\left(0, \frac{v}{1 - \phi^2}\right)</span></li>
<li><span class="math inline">Y_t \mid Y_{t-1} \sim \mathcal{N}(\phi Y_{t-1}, v)</span> for <span class="math inline">t \geq 2</span></li>
</ul>
</section>
<section id="likelihood-approaches" class="level3" data-number="90.2.3">
<h3 data-number="90.2.3" class="anchored" data-anchor-id="likelihood-approaches"><span class="header-section-number">90.2.3</span> <strong>Likelihood Approaches</strong></h3>
<p>Two approaches are considered:</p>
<section id="full-likelihood" class="level4" data-number="90.2.3.1">
<h4 data-number="90.2.3.1" class="anchored" data-anchor-id="full-likelihood"><span class="header-section-number">90.2.3.1</span> 1. <strong>Full Likelihood</strong></h4>
<p><span id="eq-full-likelihood-ar1"><span class="math display">
\begin{aligned}
p(y_{1:T} \mid \phi, v) &amp;= p(y_1 \mid \phi, v) \cdot \prod_{t=2}^T p(y_t \mid y_{t-1}, \phi, v) \\
&amp;= \frac{1}{\sqrt{2\pi \frac{v}{1 - \phi^2}}} \exp\left( -\frac{y_1^2 (1 - \phi^2)}{2v} \right) \cdot  \prod_{t=2}^T \frac{1}{\sqrt{2\pi v}} \exp\left( -\frac{(y_t - \phi y_{t-1})^2}{2v} \right) \\
  &amp;= \frac{(1 - \phi^2)^{1/2}}{(2\pi v)^{T/2}} \cdot
\exp\left( -\frac{1}{2v} \left[ \underbrace{ y_1^2(1 - \phi^2) + \sum_{t=2}^T (y_t - \phi y_{t-1})^2 }_{\text{Quadratic Loss } Q^*(\phi)} \right] \right) \\
&amp;= \frac{(1 - \phi^2)^{1/2}}{(2\pi v)^{T/2}} \exp\left( -\frac{Q^*(\phi)}{2v} \right)
\end{aligned}
\tag{90.7}</span></span></p>
<p>where <span class="math inline">Q^*(\phi)</span> is defined as:</p>
<p><span id="eq-qstar-ar1"><span class="math display">
Q^*(\phi) =
\underbrace{y_1^2(1 - \phi^2)\vphantom{\sum_{t=2}^T (y_t - \phi y_{t-1})^2}}_{\text{Initial Loss}}
+ \underbrace{\sum_{t=2}^T (y_t - \phi y_{t-1})^2}_{\text{Remaining Loss } Q(\phi)}
\tag{90.8}</span></span></p>
<p><span id="eq-likelihood-ar1"><span class="math display">
\begin{aligned}
p(y_{1:T} \mid \phi) &amp;= \prod_{t=2}^T \frac{1}{\sqrt{2\pi v}} \exp\left( -\frac{(y_t - \phi y_{t-1})^2}{2v} \right) \\
&amp;= \frac{1}{(2\pi v)^{T/2}} \exp\left( -\sum_{t=2}^T \frac{1}{2v} \left( y_t - \phi y_{t-1} \right)^2 \right) \\
&amp;= \frac{1}{(2\pi v)^{T/2}} \exp\left( -\frac{Q(\phi)}{2v} \right)
\end{aligned}
\tag{90.9}</span></span></p>
<p>where <span class="math inline">Q(\phi)</span> is the quadratic loss function defined as:</p>
<p><span id="eq-ar1-model"><span class="math display">
\underbrace{ \begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_T \end{pmatrix} }_{\utilde{y}} =
\underbrace{ \begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_T \end{pmatrix} }_{\mathbb{X}}
\underbrace{ \phi \vphantom{\begin{pmatrix}y_1 \\ y_2 \\ \vdots \\ y_T \end{pmatrix} }}_{\beta} +
\underbrace{ \begin{pmatrix}\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_T\end{pmatrix}}_{\utilde {\epsilon}}
\tag{90.10}</span></span></p>
<p>where <span class="math inline">\utilde{y}</span> is the vector of observations, <span class="math inline">\mathbb{X}</span> is the design matrix with <span class="math inline">y_1</span> as the first column and <span class="math inline">y_2, \ldots, y_{T-1}</span> as the second column, <span class="math inline">\beta = \phi</span> is the AR coefficient, and <span class="math inline">\utilde{\epsilon} \sim \mathcal{N}(0, vI)</span> is the error term.</p>
<p><span class="math display">
\utilde{y} = \mathbb{X} \beta + \utilde{\epsilon} \qquad \utilde{\epsilon} \sim \mathcal{N}(0, vI)
</span></p>
<p>where <span class="math inline">\mathbb{X}</span> is the design matrix</p>
<p>with <span class="math inline">y_1</span> as the first column and <span class="math inline">y_2, \ldots, y_{T-1}</span> as the second column.</p>
<p>If the matrix <span class="math inline">\mathbb{X}</span> is full rank, the MLE for <span class="math inline">\phi</span> can be obtained as:</p>
<p><span class="math display">
\hat{\beta} = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'y
</span> and the MLE for <span class="math inline">v</span> is given by:</p>
<p><span class="math display">
\hat{v} = S^2 = \frac{(y - \mathbb{X}\hat{\beta})'(y - \mathbb{X}\hat{\beta})}{\dim(y)-\dim(\beta)}
</span></p>
<p>and the MLE for <span class="math inline">\phi</span>. <span id="eq-mle-ar1-phi"><span class="math display">
\hat{\phi}_{MLE} = \frac{\sum_{t=2}^T y_t y_{t-1}}{\sum_{t=2}^T y_{t-1}^2}
\tag{90.11}</span></span></p>
<p>and the unbiased estimator for <span class="math inline">v</span> is given by:</p>
<p><span id="eq-mle-ar1-v-unbiased"><span class="math display">
S^2 = \sum_{t=2}^T (y_t - \hat{\phi}_{MLE} y_{t-1})^2 / (T - 2)
\tag{90.12}</span></span></p>
<p>where <span class="math inline">T</span> is the number of time points and <span class="math inline">S^2</span> is the unbiased estimator for the variance <span class="math inline">v</span>. And we usually use this unbiased estimator for <span class="math inline">v</span> in practice, as the MLE for <span class="math inline">v</span> is biased.</p>
</section>
<section id="conditional-likelihood" class="level4" data-number="90.2.3.2">
<h4 data-number="90.2.3.2" class="anchored" data-anchor-id="conditional-likelihood"><span class="header-section-number">90.2.3.2</span> 2. <strong>Conditional Likelihood</strong></h4>
<p>Maximizing the full likelihood requires <strong>numerical optimization methods</strong> (e.g., Newton-Raphson), as there’s no closed-form solution.</p>
<p>This setup is equivalent to a <strong>linear regression</strong>:</p>
<p><span class="math display">
\mathbf{y} = X\beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, vI)
</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\mathbf{y} = [y_2, \dots, y_T]^T</span></li>
<li><span class="math inline">X = [y_1, \dots, y_{T-1}]^T</span></li>
<li><span class="math inline">\beta = \phi</span></li>
</ul>
<p>Condition on <span class="math inline">y_1</span>:</p>
<p><span class="math display">
\begin{aligned}
p(y_1 \mid \phi) &amp;\sim \mathcal{N}(0, 1/(1 - \phi^2)) \\
p(y_t \mid y_{t-1}, \phi) &amp;\sim \mathcal{N}(\phi y_{t-1}, 1) \\
\end{aligned}
</span></p>
<p><span class="math display">
\begin{aligned}
p(y_{1:T} \mid y_1, \phi, v) &amp;= p(y_1 \mid \phi)  \cdot \prod_{t=2}^T p(y_t \mid y_{t-1}, \phi)\\
&amp;= \frac{(1 - \phi^2)^{1/2}}{(2\pi)^{T/2}} \exp\left(-\frac{y_1^2(1 - \phi^2)}{2}\right) \cdot \prod_{t=2}^T \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(y_t - \phi y_{t-1})^2}{2}\right) \\
&amp;= \frac{(1 - \phi^2)^{1/2}}{(2\pi)^{T/2}} \exp\left(-\frac{1}{2} \left[y_1^2(1 - \phi^2) + \sum_{t=2}^T (y_t - \phi y_{t-1})^2\right]\right) \\
\end{aligned}
</span></p>
<p>So the log-likelihood becomes:</p>
<p><span id="eq-log-likelihood-ar1"><span class="math display">
\log p(y_{1:T} \mid y_1, \phi) = \frac{1}{2} \log(1 - \phi^2) - \frac{1}{2} Q(\phi) + K
\tag{90.13}</span></span></p>
<p>where <span class="math inline">K</span> is a constant that does not depend on <span class="math inline">\phi</span>.</p>
<p>So if I were to look at maximizing this function, we can think about taking first derivatives with respect to phi. And then we will see that again the expression that you obtain doesn’t allow you to obtain a close form expression for <span class="math inline">\hat{\phi}_{MLE}</span>. Instead, we will need to use a numerical optimization method such as Newton Raphson to obtain the maximum likelihood estimator for phi.</p>
</section>
</section>
<section id="conclusion" class="level3" data-number="90.2.4">
<h3 data-number="90.2.4" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">90.2.4</span> Conclusion</h3>
<ul>
<li><strong>Full likelihood</strong> is more general but computationally intensive.</li>
<li><strong>Conditional likelihood</strong> simplifies estimation by leveraging regression theory.</li>
<li>When variance <span class="math inline">v</span> is known (e.g., <span class="math inline">v = 1</span>), the optimization reduces to maximizing a univariate function of <span class="math inline">\phi</span>.</li>
<li>For full likelihood, optimization of <span class="math inline">Q^*(\phi)</span> is required.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 30%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Full Likelihood</th>
<th>Conditional Likelihood (Regression)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accounts for <span class="math inline">Y\_1</span></td>
<td>✅ Yes</td>
<td>❌ No</td>
</tr>
<tr class="even">
<td>MLE for <span class="math inline">\phi</span></td>
<td>❌ No closed form</td>
<td>✅ Closed form</td>
</tr>
<tr class="odd">
<td>MLE for <span class="math inline">v</span></td>
<td>❌ Biased unless adjusted</td>
<td>✅ Unbiased estimator available</td>
</tr>
<tr class="even">
<td>Optimization Needed</td>
<td>✅ Yes (numerical methods)</td>
<td>❌ No (closed-form MLE for <span class="math inline">\phi</span>)</td>
</tr>
<tr class="odd">
<td>Useful when</td>
<td>Modeling full joint process</td>
<td>Estimating <span class="math inline">\phi</span> efficiently in practice</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="r-code-mle-for-the-ar1-examples-reading" class="level2" data-number="90.3">
<h2 data-number="90.3" class="anchored" data-anchor-id="r-code-mle-for-the-ar1-examples-reading"><span class="header-section-number">90.3</span> R code: MLE for the AR(1), examples (reading)</h2>
<p>The following code allows you to compute the MLE of the AR coefficient <span class="math inline">\psi</span>, the unbiased estimator of <span class="math inline">v</span>, <span class="math inline">s^2</span> , and the MLE of v based on a dataset simulated from an AR(1) process and using the conditional likelihood.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>v<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="fu">sqrt</span>(v) <span class="co"># innovation standard deviation</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">500</span> <span class="co"># number of time points</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Case 1: Conditional likelihood</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">2</span><span class="sc">:</span>T]) <span class="co"># response</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]) <span class="co"># design matrix</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>phi_MLE<span class="ot">=</span><span class="fu">as.numeric</span>((<span class="fu">t</span>(X)<span class="sc">%*%</span>y)<span class="sc">/</span><span class="fu">sum</span>(X<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># MLE for phi</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>s2<span class="ot">=</span><span class="fu">sum</span>((y <span class="sc">-</span> phi_MLE<span class="sc">*</span>X)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(y) <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># Unbiased estimate for v </span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>v_MLE<span class="ot">=</span>s2<span class="sc">*</span>(<span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>(<span class="fu">length</span>(y)) <span class="co"># MLE for v</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st"> MLE of conditional likelihood for phi: "</span>, phi_MLE, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"MLE for the variance v: "</span>, v_MLE, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Estimate s2 for the variance v: "</span>, s2, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 MLE of conditional likelihood for phi:  0.9261423 
 MLE for the variance v:  1.048 
 Estimate s2 for the variance v:  1.050104 </code></pre>
</div>
</div>
<p>This code allows you to compute estimates of the AR(1) coefficient and the variance using the <code>arima</code> function in R. The first case uses the conditional sum of squares, the second and third cases use the full likelihood with different starting points for the numerical optimization required to compute the MLE with the full likelihood.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtaining parameter estimates using the arima function in R</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>v<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="fu">sqrt</span>(v) <span class="co"># innovation standard deviation</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">500</span> <span class="co"># number of time points</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Using conditional sum of squares, equivalent to conditional likelihood </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>arima_CSS<span class="ot">=</span><span class="fu">arima</span>(yt,<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">method=</span><span class="st">"CSS"</span>,<span class="at">n.cond=</span><span class="dv">1</span>,<span class="at">include.mean=</span><span class="cn">FALSE</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"AR estimates with conditional sum of squares (CSS) for phi and v:"</span>, arima_CSS<span class="sc">$</span>coef,arima_CSS<span class="sc">$</span>sigma2,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>AR estimates with conditional sum of squares (CSS) for phi and v: 0.9261423 1.048 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Uses ML with full likelihood </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>arima_ML<span class="ot">=</span><span class="fu">arima</span>(yt,<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">method=</span><span class="st">"ML"</span>,<span class="at">include.mean=</span><span class="cn">FALSE</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"AR estimates with full likelihood for phi and v:"</span>, arima_ML<span class="sc">$</span>coef,arima_ML<span class="sc">$</span>sigma2,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>AR estimates with full likelihood for phi and v: 0.9265251 1.048434 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Default: uses conditional sum of squares to find the starting point for ML and </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co">#         then uses ML </span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>arima_CSS_ML<span class="ot">=</span><span class="fu">arima</span>(yt,<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">method=</span><span class="st">"CSS-ML"</span>,<span class="at">n.cond=</span><span class="dv">1</span>,<span class="at">include.mean=</span><span class="cn">FALSE</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"AR estimates with CSS to find starting point for ML for phi and v:"</span>, </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>arima_CSS_ML<span class="sc">$</span>coef,arima_CSS_ML<span class="sc">$</span>sigma2,<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>AR estimates with CSS to find starting point for ML for phi and v: 0.9265252 1.048434 </code></pre>
</div>
</div>
<p>This code shows you how to compute the MLE for <span class="math inline">\psi</span> using the full likelihood and the function optimize in R.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>v<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="fu">sqrt</span>(v) <span class="co"># innovation standard deviation</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">500</span> <span class="co"># number of time points</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) </span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="do">## MLE, full likelihood AR(1) with v=1 assumed known </span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># log likelihood function</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>log_p <span class="ot">&lt;-</span> <span class="cf">function</span>(phi, yt){</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.5</span><span class="sc">*</span>(<span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>phi<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fu">sum</span>((yt[<span class="dv">2</span><span class="sc">:</span>T] <span class="sc">-</span> phi<span class="sc">*</span>yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)])<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> yt[<span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>phi<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a built-in optimization method to obtain maximum likelihood estimates</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>result <span class="ot">=</span><span class="fu">optimize</span>(log_p, <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">tol =</span> <span class="fl">0.0001</span>, <span class="at">maximum =</span> <span class="cn">TRUE</span>, <span class="at">yt =</span> yt)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st"> MLE of full likelihood for phi: "</span>, result<span class="sc">$</span>maximum)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 MLE of full likelihood for phi:  0.9265928</code></pre>
</div>
</div>
</section>
<section id="bayesian-inference-in-the-ar1" class="level2 page-columns page-full" data-number="90.4">
<h2 data-number="90.4" class="anchored" data-anchor-id="bayesian-inference-in-the-ar1"><span class="header-section-number">90.4</span> Bayesian inference in the AR(1)</h2>

<div class="no-row-height column-margin column-container"><div id="fig-c2l1-s1-inference" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-c2l1-s1-inference-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/m1_0051.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;90.4: inference"><img src="images/m1_0051.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-c2l1-s1-inference-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;90.4: inference
</figcaption>
</figure>
</div></div><p><span id="eq-ar1-model"><span class="math display">
y_t= \phi y_{t-1} + \epsilon_t, \quad \epsilon_t \overset{iid}{\sim} \mathcal{N}(0, v), \quad \phi \in (-1,1)
\tag{90.14}</span></span></p>
<p><span id="eq-ar1-model-bayes"><span class="math display">
\utilde{y} = \mathbb{X}\utilde{\beta} + \utilde{\epsilon}
\tag{90.15}</span></span></p>
<p>where <span class="math inline">\mathbb{X}</span> is the design matrix with <span class="math inline">y_1</span> as the first column and <span class="math inline">y_2, \ldots, y_{T-1}</span> as the second column, <span class="math inline">\utilde{\beta} = \phi</span> is the AR coefficient, and <span class="math inline">\utilde{\epsilon} \sim \mathcal{N}(0, v\mathbf{I})</span> is the error term.</p>
<p><span id="eq-utilde-y-ar1"><span class="math display">
\utilde{y} = \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_{T-1}
\end{pmatrix}
\tag{90.16}</span></span></p>
<p><span id="eq-design-matrix-ar1"><span class="math display">
\mathbb{X} = \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_{T-1}
\end{pmatrix}
\tag{90.17}</span></span></p>
<p><span id="eq-ar1-model"><span class="math display">
\utilde{\beta} = \phi
\tag{90.18}</span></span></p>
<p><span id="eq-ar1-model-bayes"><span class="math display">
\utilde{\epsilon} \sim \mathcal{N}(0, v\mathbf{I})
\tag{90.19}</span></span></p>
<p><span id="eq-likelihood-ar1-conditional"><span class="math display">
p(y_{2:T} \mid y_1, \phi, v) =  \frac{1}{(2\pi v)^{\frac{T-1}{2}}} \exp\left(-\frac{(\utilde{y} - \mathbb{X}\utilde{\beta})'(\utilde{y} - \mathbb{X}\utilde{\beta})}{2v}\right)
\tag{90.20}</span></span></p>
<p><span id="eq-posterior-ar1"><span class="math display">
p(\phi, v \mid y_{1:T}) \propto p(\phi, v) p(y_{2:T} \mid y_1, \phi, v)
\tag{90.21}</span></span></p>
<p><span id="eq-prior-ar1"><span class="math display">
\begin{aligned}
p(\phi, v) \propto \frac{1}{v} &amp;\cdot (\utilde{\beta} \mid v, \mathbb{X}, \utilde{y}) &amp;\sim &amp; \mathcal{N}(\hat{\utilde{\beta}}, v(\mathbb{X}'\mathbb{X})^{-1}) \\
  &amp; \cdot (v \mid \mathbb{X}, \utilde{y}) &amp;\sim &amp; \mathcal{IG}\left(\frac{T-2}{2}, \frac{1}{2}Q(\hat{\utilde{\beta}}_{MLE}) \right)
\end{aligned}
\tag{90.22}</span></span></p>
<p><span id="eq-mle-ar1-phi-bayes"><span class="math display">
\begin{aligned}
\hat{\utilde{\beta}_{MLE}} &amp;= (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\utilde{y} \\
&amp;= \hat{\phi}_{MLE} = \frac{\sum_{t=2}^T y_t y_{t-1}}{\sum_{t=2}^T y_{t-1}^2}
\end{aligned}
\tag{90.23}</span></span></p>
<p><span id="eq-q-ar1-q-mle-bayes"><span class="math display">
Q(\hat{\phi}_{MLE}) = \sum_{t=2}^T (y_t - \hat{\phi}_{MLE}\ y_{t-1})^2
\tag{90.24}</span></span></p>
</section>
<section id="bayesian-inference-in-the-ar1-conditional-likelihood-example-video" class="level2" data-number="90.5">
<h2 data-number="90.5" class="anchored" data-anchor-id="bayesian-inference-in-the-ar1-conditional-likelihood-example-video"><span class="header-section-number">90.5</span> Bayesian inference in the AR(1): Conditional likelihood example (video)</h2>
<p>This video walks through the code snippet below and provides examples of how to sample from the posterior distribution of the AR coefficient <span class="math inline">\psi</span> and the variance <span class="math inline">v</span> using the conditional likelihood and a reference prior.</p>
</section>
<section id="r-code-ar1-bayesian-inference-conditional-likelihood-example-reading" class="level2" data-number="90.6">
<h2 data-number="90.6" class="anchored" data-anchor-id="r-code-ar1-bayesian-inference-conditional-likelihood-example-reading"><span class="header-section-number">90.6</span> R Code: AR(1) Bayesian inference, conditional likelihood example (reading)</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="do">####################################################</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="do">#####             MLE for AR(1)               ######</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="do">####################################################</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="dv">1</span> <span class="co"># innovation standard deviation</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">200</span> <span class="co"># number of time points</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) <span class="co"># sample stationary AR(1) process</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">2</span><span class="sc">:</span>T]) <span class="co"># response</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>X<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]) <span class="co"># design matrix</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>phi_MLE<span class="ot">=</span><span class="fu">as.numeric</span>((<span class="fu">t</span>(X)<span class="sc">%*%</span>y)<span class="sc">/</span><span class="fu">sum</span>(X<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># MLE for phi</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>s2<span class="ot">=</span><span class="fu">sum</span>((y <span class="sc">-</span> phi_MLE<span class="sc">*</span>X)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(y) <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># Unbiased estimate for v</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>v_MLE<span class="ot">=</span>s2<span class="sc">*</span>(<span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>(<span class="fu">length</span>(y)) <span class="co"># MLE for v </span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">c</span>(phi_MLE,s2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9178472 1.0491054</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="do">#######################################################</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="do">######     Posterior inference, AR(1)               </span><span class="al">###</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="do">######     Conditional Likelihood + Reference Prior </span><span class="al">###</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="do">######     Direct sampling                          </span><span class="al">###</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="do">#######################################################</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>n_sample<span class="ot">=</span><span class="dv">3000</span>   <span class="co"># posterior sample size</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="do">## step 1: sample posterior distribution of v from inverse gamma distribution</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>v_sample<span class="ot">=</span><span class="dv">1</span><span class="sc">/</span><span class="fu">rgamma</span>(n_sample, (T<span class="dv">-2</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="fu">sum</span>((yt[<span class="dv">2</span><span class="sc">:</span>T] <span class="sc">-</span> phi_MLE<span class="sc">*</span>yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)])<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="do">## step 2: sample posterior distribution of phi from normal distribution</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>phi_sample<span class="ot">=</span><span class="fu">rep</span>(<span class="dv">0</span>,n_sample)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sample){</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>phi_sample[i]<span class="ot">=</span><span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> phi_MLE, <span class="at">sd=</span><span class="fu">sqrt</span>(v_sample[i]<span class="sc">/</span><span class="fu">sum</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]<span class="sc">^</span><span class="dv">2</span>)))}</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="do">## plot histogram of posterior samples of phi and v</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>),<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">cex.lab =</span> <span class="fl">1.3</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(phi_sample, <span class="at">xlab =</span> <span class="fu">bquote</span>(phi), </span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">bquote</span>(<span class="st">"Posterior for "</span><span class="sc">~</span>phi),<span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.75</span>,<span class="fl">1.05</span>), <span class="at">col=</span><span class="st">'lightblue'</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> phi, <span class="at">col =</span> <span class="st">'red'</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(v_sample, <span class="at">xlab =</span> <span class="fu">bquote</span>(v), <span class="at">col=</span><span class="st">'lightblue'</span>, <span class="at">main =</span> <span class="fu">bquote</span>(<span class="st">"Posterior for "</span><span class="sc">~</span>v))</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> sd, <span class="at">col =</span> <span class="st">'red'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/AR(1) inference, conditional likelihood example-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="C4-L03_files/figure-html/AR(1) inference, conditional likelihood example-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="quiz---mle-and-bayesian-inference-in-the-ar1" class="level2" data-number="90.7">
<h2 data-number="90.7" class="anchored" data-anchor-id="quiz---mle-and-bayesian-inference-in-the-ar1"><span class="header-section-number">90.7</span> Quiz - MLE and Bayesian inference in the AR(1)</h2>
<p>Omitted per Coursera honor code</p>
</section>
<section id="practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1" class="level2" data-number="90.8">
<h2 data-number="90.8" class="anchored" data-anchor-id="practice-graded-assignment-mle-and-bayesian-inference-in-the-ar1"><span class="header-section-number">90.8</span> Practice Graded Assignment: MLE and Bayesian inference in the AR(1)</h2>
<p>This peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you’ve learned in R and prepare you for your data analysis project in week 5.</p>
<ol type="1">
<li>Consider the R code below: MLE for the AR(1)</li>
</ol>
<div class="cell">
<div id="lst-ar-1-mle" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-ar-1-mle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;90.1: R Code: MLE for the AR(1) process, conditional likelihood example
</figcaption>
<div aria-describedby="lst-ar-1-mle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="do">####################################################</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="do">#####             MLE for AR(1)               ######</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="do">####################################################</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>v<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="fu">sqrt</span>(v) <span class="co"># innovation standard deviation</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">500</span> <span class="co"># number of time points</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) </span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Case 1: Conditional likelihood</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">2</span><span class="sc">:</span>T]) <span class="co"># response</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>X<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]) <span class="co"># design matrix</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>phi_MLE<span class="ot">=</span><span class="fu">as.numeric</span>((<span class="fu">t</span>(X)<span class="sc">%*%</span>y)<span class="sc">/</span><span class="fu">sum</span>(X<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># MLE for phi</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>s2<span class="ot">=</span><span class="fu">sum</span>((y <span class="sc">-</span> phi_MLE<span class="sc">*</span>X)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(y) <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># Unbiased estimate for v </span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>v_MLE<span class="ot">=</span>s2<span class="sc">*</span>(<span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>(<span class="fu">length</span>(y)) <span class="co"># MLE for v</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st"> MLE of conditional likelihood for phi: "</span>, phi_MLE, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"MLE for the variance v: "</span>, v_MLE, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Estimate s2 for the variance v: "</span>, s2, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
 MLE of conditional likelihood for phi:  0.9048951 
 MLE for the variance v:  1.084559 
 Estimate s2 for the variance v:  1.086737 </code></pre>
</div>
</div>
<p>Modify the code above to sample 800 observations from an AR(1) with AR coefficient <span class="math inline">\psi = -0.8</span> and variance <span class="math inline">v = 2</span>. Plot your simulated data. Obtain the MLE for <span class="math inline">\psi</span> based on the conditional likelihood and the unbiased estimate <span class="math inline">s^2</span> for the variance <span class="math inline">v</span>.</p>
<ol start="2" type="1">
<li>Consider the R code below: AR(1) Bayesian inference, conditional likelihood</li>
</ol>
<div class="cell">
<div id="lst-ar-1-inference" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-ar-1-inference-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;90.2: R Code: AR(1) Bayesian inference, conditional likelihood example
</figcaption>
<div aria-describedby="lst-ar-1-inference-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="do">#######################################################</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="do">######     Posterior inference, AR(1)               </span><span class="al">###</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="do">######     Conditional Likelihood + Reference Prior </span><span class="al">###</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="do">######     Direct sampling                          </span><span class="al">###</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="do">#######################################################</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>n_sample<span class="ot">=</span><span class="dv">3000</span>   <span class="co"># posterior sample size</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="do">## step 1: sample posterior distribution of v from inverse gamma distribution</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>v_sample<span class="ot">=</span><span class="dv">1</span><span class="sc">/</span><span class="fu">rgamma</span>(n_sample, (T<span class="dv">-2</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="fu">sum</span>((yt[<span class="dv">2</span><span class="sc">:</span>T] <span class="sc">-</span> phi_MLE<span class="sc">*</span>yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)])<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="do">## step 2: sample posterior distribution of phi from normal distribution</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>phi_sample<span class="ot">=</span><span class="fu">rep</span>(<span class="dv">0</span>,n_sample)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sample){</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>phi_sample[i]<span class="ot">=</span><span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> phi_MLE, <span class="at">sd=</span><span class="fu">sqrt</span>(v_sample[i]<span class="sc">/</span><span class="fu">sum</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]<span class="sc">^</span><span class="dv">2</span>)))}</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="do">## plot histogram of posterior samples of phi and v</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">cex.lab =</span> <span class="fl">1.3</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(phi_sample, <span class="at">xlab =</span> <span class="fu">bquote</span>(phi), </span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">bquote</span>(<span class="st">"Posterior for "</span><span class="sc">~</span>phi),<span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.75</span>,<span class="fl">1.05</span>), <span class="at">col=</span><span class="st">'lightblue'</span>)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> phi, <span class="at">col =</span> <span class="st">'red'</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(v_sample, <span class="at">xlab =</span> <span class="fu">bquote</span>(v), <span class="at">col=</span><span class="st">'lightblue'</span>, <span class="at">main =</span> <span class="fu">bquote</span>(<span class="st">"Posterior for "</span><span class="sc">~</span>v))</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> sd, <span class="at">col =</span> <span class="st">'red'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-ar-1-inference-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="C4-L03_files/figure-html/lst-ar-1-inference-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
<p>Using your simulated data from part 1 modify the code above to summarize your posterior inference for <span class="math inline">\psi</span> and <span class="math inline">v</span> based on 5000 samples from the joint posterior distribution of <span class="math inline">\psi</span> and <span class="math inline">v</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Grading Criteria
</div>
</div>
<div class="callout-body-container callout-body">
<p>The responses should follow the same template as the sample code provided above but you will submit your code lines in plain text. Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect that :</p>
<ol type="1">
<li>you generate 800 time points from the AR(1) rather than 500 and plot your simulated data.</li>
<li>your simulated data is from an AR(1) with AR coefficients <span class="math inline">\psi = -0.8</span> and variance <span class="math inline">v = 2</span> rather than AR(1) with AR coefficient <span class="math inline">\psi = 0.9</span> and variance <span class="math inline">v = 1</span> and</li>
<li>you obtain 5000 rather than 3000 samples from the posterior distribution from the new simulated process.</li>
</ol>
</div>
</div>
<section id="bayesian-inference-in-the-ar1-full-likelihood-example-reading" class="level3" data-number="90.8.1">
<h3 data-number="90.8.1" class="anchored" data-anchor-id="bayesian-inference-in-the-ar1-full-likelihood-example-reading"><span class="header-section-number">90.8.1</span> Bayesian Inference in the AR(1), : full likelihood example (reading)</h3>
<p>We consider a prior distribution that assumes that <span class="math inline">\phi</span> and <span class="math inline">v</span> are independent:</p>
<p><span class="math display">
\mathbb{P}r(v) \propto \frac{1}{v},
</span></p>
<p><span class="math display">
\mathbb{P}r(\phi) = \frac{1}{2}, \quad \text{for } \phi \in (-1, 1),
</span></p>
<p>i.e., we assume a Uniform prior for <span class="math inline">\phi \in (-1, 1)</span>. Combining this prior with the full likelihood in the AR(1) case, we obtain the following posterior density:</p>
<p><span class="math display">
\mathbb{P}r(\phi, v \mid y_{1:T}) \propto \frac{(1 - \phi^2)^{1/2} }{v^{T/2 + 1}} \exp\left(-\frac{Q^*(\phi)}{2v}\right), \quad -1 &lt; \phi &lt; 1,
</span></p>
<p>with</p>
<p><span class="math display">
Q^*(\phi) = y_1^2(1 - \phi^2) + \sum_{t=2}^{T} (y_t - \phi y_{t-1})^2.
</span></p>
<p>It is not possible to get a closed-form expression for this posterior or to perform direct simulation. Therefore, we use simulation-based Markov Chain Monte Carlo (MCMC) methods to obtain samples from the posterior distribution.</p>
</section>
<section id="transformation-of-phi" class="level3" data-number="90.8.2">
<h3 data-number="90.8.2" class="anchored" data-anchor-id="transformation-of-phi"><span class="header-section-number">90.8.2</span> Transformation of <span class="math inline">\phi</span></h3>
<p>We first consider the following transformation on <span class="math inline">\phi</span>:</p>
<p><span class="math display">
\eta = \log\left(\frac{1 - \phi}{\phi + 1}\right),
</span></p>
<p>so that <span class="math inline">\eta \in (-\infty, \infty)</span>. The inverse transformation on <span class="math inline">\eta</span> is:</p>
<p><span class="math display">
\phi = \frac{1 - \exp(\eta)}{1 + \exp(\eta)}.
</span></p>
<p>Writing down the posterior density for <span class="math inline">\eta</span> and <span class="math inline">v</span>, we obtain</p>
<p><span class="math display">
\mathbb{P}r(\eta, v \mid y_{1:T}) \propto\frac{ (1 - \phi^2)^{1/2} }{v^{T/2 + 1}} \exp\left(-\frac{Q^*(\phi)}{2v}\right) \cdot \frac{2 \exp(\eta)}{(1 + \exp(\eta))^2},
</span></p>
<p>with <span class="math inline">\phi</span> written as a function of <span class="math inline">\eta</span>. We proceed to obtain samples from this posterior distribution using the MCMC algorithm outlined below. Once we have obtained <span class="math inline">M</span> samples from <span class="math inline">\eta</span> and <span class="math inline">v</span> after convergence, we can use the inverse transformation above to obtain posterior samples for <span class="math inline">\phi</span>.</p>
</section>
<section id="mcmc-algorithm-bayesian-inference-for-ar1-full-likelihood" class="level3" data-number="90.8.3">
<h3 data-number="90.8.3" class="anchored" data-anchor-id="mcmc-algorithm-bayesian-inference-for-ar1-full-likelihood"><span class="header-section-number">90.8.3</span> MCMC Algorithm: Bayesian Inference for AR(1), Full Likelihood</h3>
<p><strong>Algorithm</strong>:</p>
<ol type="1">
<li>Initialize <span class="math inline">\eta^{(0)}</span> and <span class="math inline">\beta^{(0)}</span>.</li>
<li>For <span class="math inline">m</span> in <span class="math inline">1:M</span> do:
<ul>
<li>Sample <span class="math inline">v^{(m)} \sim \text{IG}\left(\frac{T}{2}, \frac{Q^*(\phi^{(m-1)})}{2}\right)</span>.</li>
<li>Sample <span class="math inline">\eta^{(m)}</span> using Metropolis-Hastings:
<ol type="1">
<li>Sample <span class="math inline">\eta^* \sim N(\eta^{(m-1)}, c)</span>, where <span class="math inline">c</span> is a tuning parameter.</li>
<li>Compute the importance ratio:</li>
</ol></li>
</ul></li>
</ol>
<p><span class="math display">
        r = \frac{p(\eta^*, v^{(m)} \mid y_{1:T})}{p(\eta^{(m-1)}, v^{(m)} \mid y_{1:T})}.
</span></p>
<ol start="3" type="1">
<li>Set:</li>
</ol>
<p><span class="math display">
        \eta^{(m)} =
        \begin{cases}
        \eta^* &amp; \text{with probability } \min(r, 1), \\
        \eta^{(m-1)} &amp; \text{otherwise}.
        \end{cases}
</span></p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C4-L02.html" class="pagination-link" aria-label="The AR(1) process: definitions and properties - M1L2">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">The AR(1) process: definitions and properties - M1L2</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C4-L04.html" class="pagination-link" aria-label="The AR(p) process - M2L4">
        <span class="nav-page-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">The AR(p) process - M2L4</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2024-11-03</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "The AR(1): MLE and Bayesian inference - M1L3"</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> Time Series Analysis</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "This lesson we will define the AR(1) process, Stationarity, ACF, PACF, differencing, smoothing"</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> </span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - bayesian statistics</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span><span class="co"> </span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">  - notes</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - time series</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - autoregressive models</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">  - stationarity</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">  - MLE</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">  - AR(1) process</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">  - Yule-Walker equations</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">  - Durbin-Levinson recursion</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">  - R code</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[x]</span> Perform maximum likelihood estimation for the full and conditional likelihood in an AR(1) <span class="co">[</span><span class="ot">\#</span><span class="co">](#l2g3)</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[x]</span> Perform Bayesian inference for the **AR(1)** under the conditional likelihood and the reference prior <span class="co">[</span><span class="ot">\#</span><span class="co">](#l2g4)</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## Review of maximum likelihood and Bayesian inference in regression</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regression Models: Maximum Likelihood Estimation</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>\index{regression!linear}</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>\index{Maximum Likelihood Estimation}</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>Assume a regression model with the following structure: </span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>y_i = \beta_1x_{i,1} + \ldots + \beta_kx_{i,k} + \epsilon_i,</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>for $i = 1, \ldots, n$ and $\epsilon_i$ independent random variables with $\epsilon_i \sim \mathcal{N}(0, v) \quad \forall i$. </span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>This model can be written in matrix form as: </span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>y = \mathbf{X} \boldsymbol{\beta} + \boldsymbol\epsilon \qquad \boldsymbol\epsilon \sim \mathcal{N} (0, v\mathbf{I})</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$y = (y_1, \ldots, y_n)′$ is an n-dimensional vector of responses,</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\mathbf{X}$ is an $n × k$ matrix containing the explanatory variables,</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\boldsymbol \beta = (\beta_1, \ldots, \beta_k)'$ is the k-dimensional vector of regression coefficients,</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\boldsymbol \epsilon = (\epsilon_1, \ldots, \epsilon_n)'$ is the n-dimensional vector of errors,</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\mathbf{I}$ is an $n \times n$ identity matrix.</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>If $\mathbf{X}$ is a *full rank* matrix with rank $k$ , the maximum likelihood estimator for $\boldsymbol\beta$, denoted as $\hat{\boldsymbol\beta}_{MLE}$ is given by:</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>\hat{\boldsymbol{\beta}}_{MLE} = (\mathbf{X}'\mathbf{X})^{−1}\mathbf{X}'\mathbf{y},</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mle-estimator-beta}</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>where $(\mathbf{X}'\mathbf{X})^{−1}\mathbf{X}'$ is the <span class="co">[</span><span class="ot">Moore-Penrose pseudoinverse</span><span class="co">](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)</span> of the matrix $\mathbf{X}$. This Moore-Penrose pseudoinverse of the matrix $\mathbf{X}$ is used to obtain the least squares solution to the linear regression problem.</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>and the MLE for $v$ is given by:</span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>\hat{v}_{MLE} = \frac{1}{n} (y − \mathbf{X} \hat{\boldsymbol{\beta}}_{MLE})′(y − \mathbf{X} \hat{\boldsymbol{\beta}}_{MLE})</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mle-estimator-v}</span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>$\hat{v}_{MLE}$ is not an unbiased estimator of $v$, therefore, the following unbiased estimator of $v$ is typically used:</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>s^2 = \frac{1}{n-k}(y − \mathbf{X} \hat{\boldsymbol\beta}_{MLE} )′(y − \mathbf{X} \hat{\boldsymbol\beta}_{MLE} )</span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mle-unbiased-estimator-v}</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regression Models: Bayesian Inference {#sec-bayes-regression}</span></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a>Assume once again we have a model with the structure in (1), which results in a likelihood of the form</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(y \mid \boldsymbol{\beta} , v) = \frac{1}{(2\pi v)^{n/2}}\exp \left<span class="sc">\{</span> -\frac{1}{2} (y − \mathbf{X} \boldsymbol{\beta})′(y − \mathbf{X} \boldsymbol{\beta}) \right<span class="sc">\}</span></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>If a prior of the form :</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\boldsymbol{\beta}, v) \propto \frac{1}{v}</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a>$$ {#eq-prior-regression}</span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a>is used, we obtain that the posterior distribution is given by:</span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\boldsymbol{\beta},v \mid \mathbf{y}) \propto \frac{1}{v^{n/2+1}}\exp \left<span class="sc">\{</span> -\frac{1}{2v} (\mathbf{y} − \mathbf{X} \boldsymbol{\beta})′(\mathbf{y} − \mathbf{X} \boldsymbol{\beta}) \right<span class="sc">\}</span></span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a>$$ {#eq-posterior-regression}</span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a>In addition it can be shown that</span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$(\boldsymbol{\beta}\mid v, \mathbf{y}) \sim \mathcal{N} (\hat{\boldsymbol{\beta}}_{MLE} , v(\mathbf{X}'\mathbf{X})^{-1})$</span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$(v \mid \mathbf{y}) \sim \mathcal{IG}((n − k)/2, d/2)$ with</span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a>d = (\mathbf{y} − \mathbf{X} \hat{\boldsymbol{\beta}}_{MLE} )′(\mathbf{y} − \mathbf{X} \hat{\boldsymbol{\beta}}_{MLE} )</span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a>$$ {#eq-d-unbiased-estimator-v}</span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a>where $\mathcal{IG}(a, b)$ denotes the inverse-gamma distribution with *shape* parameter $a$ and *scale* parameter $b$.</span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a>with $k = dim(\boldsymbol\beta)$.</span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Given that $\mathbb{P}r(\boldsymbol\beta, v \mid \mathbf{y}) = \mathbb{P}r(\boldsymbol\beta \mid v, \mathbf{y})p(v \mid \mathbf{y})$ the equations above provide a way to directly sample from the posterior distribution of $\boldsymbol \beta$ and $v$ by first sampling v from the inverse-gamma distribution above and then conditioning on this sampled value of v, sampling $\boldsymbol \beta$ from the normal distribution above.</span><span class="co">]</span>{.mark}</span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a><span class="fu">## Maximum likelihood estimation in the AR(1) (video) {#sec-mle-ar1}</span></span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a><span class="al">![MLE 1](images/m1_0041.png)</span>{#fig-s_0041 .column-margin  group="slides" width="53mm"}</span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a><span class="al">![Full Likelihood MLE ](images/m1_0042.png)</span>{#fig-s_0042 .column-margin  group="slides" width="53mm"}</span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a><span class="al">![Conditional Likelihood MLE](images/m1_0043.png)</span>{#fig-s_0043 .column-margin  group="slides" width="53mm"}</span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-121"><a href="#cb17-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-122"><a href="#cb17-122" aria-hidden="true" tabindex="-1"></a>There are two main strategies for performing MLE for an AR(1) model:</span>
<span id="cb17-123"><a href="#cb17-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-124"><a href="#cb17-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Full Likelihood: Considers the joint distribution of all observations $y_1, \dots, y_T$.</span>
<span id="cb17-125"><a href="#cb17-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conditional Likelihood: Conditions on the first observation ($y_1$) and works with the likelihood of the remaining observations ($y_2, \dots, y_T$).</span>
<span id="cb17-126"><a href="#cb17-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-127"><a href="#cb17-127" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Model Setup**</span></span>
<span id="cb17-128"><a href="#cb17-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-129"><a href="#cb17-129" aria-hidden="true" tabindex="-1"></a>The focus is on the **zero-mean AR(1) model**:</span>
<span id="cb17-130"><a href="#cb17-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-131"><a href="#cb17-131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-132"><a href="#cb17-132" aria-hidden="true" tabindex="-1"></a>Y_t = \phi Y_{t-1} + \epsilon_t, \quad \epsilon_t \overset{iid}{\sim} \mathcal{N}(0, v), \quad \phi \in (-1,1)</span>
<span id="cb17-133"><a href="#cb17-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-134"><a href="#cb17-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-135"><a href="#cb17-135" aria-hidden="true" tabindex="-1"></a>This condition ensures **stationarity** of the process.</span>
<span id="cb17-136"><a href="#cb17-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-137"><a href="#cb17-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Distributional Assumptions**</span></span>
<span id="cb17-138"><a href="#cb17-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-139"><a href="#cb17-139" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$Y_1 \sim \mathcal{N}\left(0, \frac{v}{1 - \phi^2}\right)$</span>
<span id="cb17-140"><a href="#cb17-140" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$Y_t \mid Y_{t-1} \sim \mathcal{N}(\phi Y_{t-1}, v)$ for $t \geq 2$</span>
<span id="cb17-141"><a href="#cb17-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-142"><a href="#cb17-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-143"><a href="#cb17-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-144"><a href="#cb17-144" aria-hidden="true" tabindex="-1"></a><span class="fu">### **Likelihood Approaches**</span></span>
<span id="cb17-145"><a href="#cb17-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-146"><a href="#cb17-146" aria-hidden="true" tabindex="-1"></a>Two approaches are considered:</span>
<span id="cb17-147"><a href="#cb17-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-148"><a href="#cb17-148" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 1. **Full Likelihood**</span></span>
<span id="cb17-149"><a href="#cb17-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-150"><a href="#cb17-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-151"><a href="#cb17-151" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb17-152"><a href="#cb17-152" aria-hidden="true" tabindex="-1"></a>p(y_{1:T} \mid \phi, v) &amp;= p(y_1 \mid \phi, v) \cdot \prod_{t=2}^T p(y_t \mid y_{t-1}, \phi, v) <span class="sc">\\</span></span>
<span id="cb17-153"><a href="#cb17-153" aria-hidden="true" tabindex="-1"></a> &amp;= \frac{1}{\sqrt{2\pi \frac{v}{1 - \phi^2}}} \exp\left( -\frac{y_1^2 (1 - \phi^2)}{2v} \right) \cdot  \prod_{t=2}^T \frac{1}{\sqrt{2\pi v}} \exp\left( -\frac{(y_t - \phi y_{t-1})^2}{2v} \right) <span class="sc">\\</span></span>
<span id="cb17-154"><a href="#cb17-154" aria-hidden="true" tabindex="-1"></a>  &amp;= \frac{(1 - \phi^2)^{1/2}}{(2\pi v)^{T/2}} \cdot</span>
<span id="cb17-155"><a href="#cb17-155" aria-hidden="true" tabindex="-1"></a>\exp\left( -\frac{1}{2v} \left<span class="co">[</span><span class="ot"> \underbrace{ y_1^2(1 - \phi^2) + \sum_{t=2}^T (y_t - \phi y_{t-1})^2 }_{\text{Quadratic Loss } Q^*(\phi)} \right</span><span class="co">]</span> \right) <span class="sc">\\</span></span>
<span id="cb17-156"><a href="#cb17-156" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{(1 - \phi^2)^{1/2}}{(2\pi v)^{T/2}} \exp\left( -\frac{Q^*(\phi)}{2v} \right)</span>
<span id="cb17-157"><a href="#cb17-157" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb17-158"><a href="#cb17-158" aria-hidden="true" tabindex="-1"></a>$$ {#eq-full-likelihood-ar1}</span>
<span id="cb17-159"><a href="#cb17-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-160"><a href="#cb17-160" aria-hidden="true" tabindex="-1"></a>where $Q^*(\phi)$ is defined as:</span>
<span id="cb17-161"><a href="#cb17-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-162"><a href="#cb17-162" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-163"><a href="#cb17-163" aria-hidden="true" tabindex="-1"></a>Q^*(\phi) = </span>
<span id="cb17-164"><a href="#cb17-164" aria-hidden="true" tabindex="-1"></a>\underbrace{y_1^2(1 - \phi^2)\vphantom{\sum_{t=2}^T (y_t - \phi y_{t-1})^2}}_{\text{Initial Loss}} </span>
<span id="cb17-165"><a href="#cb17-165" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>\underbrace{\sum_{t=2}^T (y_t - \phi y_{t-1})^2}_{\text{Remaining Loss } Q(\phi)}</span>
<span id="cb17-166"><a href="#cb17-166" aria-hidden="true" tabindex="-1"></a>$$ {#eq-qstar-ar1}</span>
<span id="cb17-167"><a href="#cb17-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-168"><a href="#cb17-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-169"><a href="#cb17-169" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb17-170"><a href="#cb17-170" aria-hidden="true" tabindex="-1"></a>p(y_{1:T} \mid \phi) &amp;= \prod_{t=2}^T \frac{1}{\sqrt{2\pi v}} \exp\left( -\frac{(y_t - \phi y_{t-1})^2}{2v} \right) <span class="sc">\\</span></span>
<span id="cb17-171"><a href="#cb17-171" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{(2\pi v)^{T/2}} \exp\left( -\sum_{t=2}^T \frac{1}{2v} \left( y_t - \phi y_{t-1} \right)^2 \right) <span class="sc">\\</span></span>
<span id="cb17-172"><a href="#cb17-172" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{(2\pi v)^{T/2}} \exp\left( -\frac{Q(\phi)}{2v} \right)</span>
<span id="cb17-173"><a href="#cb17-173" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb17-174"><a href="#cb17-174" aria-hidden="true" tabindex="-1"></a>$$ {#eq-likelihood-ar1}</span>
<span id="cb17-175"><a href="#cb17-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-176"><a href="#cb17-176" aria-hidden="true" tabindex="-1"></a>where $Q(\phi)$ is the quadratic loss function defined as:</span>
<span id="cb17-177"><a href="#cb17-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-178"><a href="#cb17-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-179"><a href="#cb17-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-180"><a href="#cb17-180" aria-hidden="true" tabindex="-1"></a>\underbrace{ \begin{pmatrix}y_1 <span class="sc">\\</span> y_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> y_T \end{pmatrix} }_{\utilde{y}} = </span>
<span id="cb17-181"><a href="#cb17-181" aria-hidden="true" tabindex="-1"></a>\underbrace{ \begin{pmatrix}y_1 <span class="sc">\\</span> y_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> y_T \end{pmatrix} }_{\mathbb{X}} </span>
<span id="cb17-182"><a href="#cb17-182" aria-hidden="true" tabindex="-1"></a>\underbrace{ \phi \vphantom{\begin{pmatrix}y_1 <span class="sc">\\</span> y_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> y_T \end{pmatrix} }}_{\beta} + </span>
<span id="cb17-183"><a href="#cb17-183" aria-hidden="true" tabindex="-1"></a>\underbrace{ \begin{pmatrix}\epsilon_1 <span class="sc">\\</span> \epsilon_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> \epsilon_T\end{pmatrix}}_{\utilde {\epsilon}}</span>
<span id="cb17-184"><a href="#cb17-184" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ar1-model}</span>
<span id="cb17-185"><a href="#cb17-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-186"><a href="#cb17-186" aria-hidden="true" tabindex="-1"></a>where $\utilde{y}$ is the vector of observations, $\mathbb{X}$ is the design matrix with $y_1$ as the first column and $y_2, \ldots, y_{T-1}$ as the second column, $\beta = \phi$ is the AR coefficient, and $\utilde{\epsilon} \sim \mathcal{N}(0, vI)$ is the error term.</span>
<span id="cb17-187"><a href="#cb17-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-188"><a href="#cb17-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-189"><a href="#cb17-189" aria-hidden="true" tabindex="-1"></a>\utilde{y} = \mathbb{X} \beta + \utilde{\epsilon} \qquad \utilde{\epsilon} \sim \mathcal{N}(0, vI)</span>
<span id="cb17-190"><a href="#cb17-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-191"><a href="#cb17-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-192"><a href="#cb17-192" aria-hidden="true" tabindex="-1"></a>where $\mathbb{X}$ is the design matrix </span>
<span id="cb17-193"><a href="#cb17-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-194"><a href="#cb17-194" aria-hidden="true" tabindex="-1"></a>with $y_1$ as the first column and $y_2, \ldots, y_{T-1}$ as the second column.</span>
<span id="cb17-195"><a href="#cb17-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-196"><a href="#cb17-196" aria-hidden="true" tabindex="-1"></a>If the matrix $\mathbb{X}$ is full rank, the MLE for $\phi$ can be obtained as:</span>
<span id="cb17-197"><a href="#cb17-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-198"><a href="#cb17-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-199"><a href="#cb17-199" aria-hidden="true" tabindex="-1"></a>\hat{\beta} = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'y</span>
<span id="cb17-200"><a href="#cb17-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-201"><a href="#cb17-201" aria-hidden="true" tabindex="-1"></a>and the MLE for $v$ is given by:</span>
<span id="cb17-202"><a href="#cb17-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-203"><a href="#cb17-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-204"><a href="#cb17-204" aria-hidden="true" tabindex="-1"></a>\hat{v} = S^2 = \frac{(y - \mathbb{X}\hat{\beta})'(y - \mathbb{X}\hat{\beta})}{\dim(y)-\dim(\beta)} </span>
<span id="cb17-205"><a href="#cb17-205" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-206"><a href="#cb17-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-207"><a href="#cb17-207" aria-hidden="true" tabindex="-1"></a>and the MLE for $\phi$.</span>
<span id="cb17-208"><a href="#cb17-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-209"><a href="#cb17-209" aria-hidden="true" tabindex="-1"></a>\hat{\phi}_{MLE} = \frac{\sum_{t=2}^T y_t y_{t-1}}{\sum_{t=2}^T y_{t-1}^2}</span>
<span id="cb17-210"><a href="#cb17-210" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mle-ar1-phi}</span>
<span id="cb17-211"><a href="#cb17-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-212"><a href="#cb17-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-213"><a href="#cb17-213" aria-hidden="true" tabindex="-1"></a>and the unbiased estimator for $v$ is given by:</span>
<span id="cb17-214"><a href="#cb17-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-215"><a href="#cb17-215" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-216"><a href="#cb17-216" aria-hidden="true" tabindex="-1"></a>S^2 = \sum_{t=2}^T (y_t - \hat{\phi}_{MLE} y_{t-1})^2 / (T - 2)</span>
<span id="cb17-217"><a href="#cb17-217" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mle-ar1-v-unbiased}</span>
<span id="cb17-218"><a href="#cb17-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-219"><a href="#cb17-219" aria-hidden="true" tabindex="-1"></a>where $T$ is the number of time points and $S^2$ is the unbiased estimator for the variance $v$.</span>
<span id="cb17-220"><a href="#cb17-220" aria-hidden="true" tabindex="-1"></a>And we usually use this unbiased estimator for $v$ in practice, as the MLE for $v$ is biased.</span>
<span id="cb17-221"><a href="#cb17-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-222"><a href="#cb17-222" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 2. **Conditional Likelihood**</span></span>
<span id="cb17-223"><a href="#cb17-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-224"><a href="#cb17-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-225"><a href="#cb17-225" aria-hidden="true" tabindex="-1"></a>Maximizing the full likelihood requires **numerical optimization methods** (e.g., Newton-Raphson), as there's no closed-form solution.</span>
<span id="cb17-226"><a href="#cb17-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-227"><a href="#cb17-227" aria-hidden="true" tabindex="-1"></a>This setup is equivalent to a **linear regression**:</span>
<span id="cb17-228"><a href="#cb17-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-229"><a href="#cb17-229" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-230"><a href="#cb17-230" aria-hidden="true" tabindex="-1"></a>\mathbf{y} = X\beta + \epsilon, \quad \epsilon \sim \mathcal{N}(0, vI)</span>
<span id="cb17-231"><a href="#cb17-231" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-232"><a href="#cb17-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-233"><a href="#cb17-233" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb17-234"><a href="#cb17-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-235"><a href="#cb17-235" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\mathbf{y} = <span class="co">[</span><span class="ot">y_2, \dots, y_T</span><span class="co">]</span>^T$</span>
<span id="cb17-236"><a href="#cb17-236" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$X = <span class="co">[</span><span class="ot">y_1, \dots, y_{T-1}</span><span class="co">]</span>^T$</span>
<span id="cb17-237"><a href="#cb17-237" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\beta = \phi$</span>
<span id="cb17-238"><a href="#cb17-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-239"><a href="#cb17-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-240"><a href="#cb17-240" aria-hidden="true" tabindex="-1"></a>Condition on $y_1$:</span>
<span id="cb17-241"><a href="#cb17-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-242"><a href="#cb17-242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-243"><a href="#cb17-243" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb17-244"><a href="#cb17-244" aria-hidden="true" tabindex="-1"></a>p(y_1 \mid \phi) &amp;\sim \mathcal{N}(0, 1/(1 - \phi^2)) <span class="sc">\\</span></span>
<span id="cb17-245"><a href="#cb17-245" aria-hidden="true" tabindex="-1"></a>p(y_t \mid y_{t-1}, \phi) &amp;\sim \mathcal{N}(\phi y_{t-1}, 1) <span class="sc">\\</span></span>
<span id="cb17-246"><a href="#cb17-246" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb17-247"><a href="#cb17-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-248"><a href="#cb17-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-249"><a href="#cb17-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-250"><a href="#cb17-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-251"><a href="#cb17-251" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb17-252"><a href="#cb17-252" aria-hidden="true" tabindex="-1"></a>p(y_{1:T} \mid y_1, \phi, v) &amp;= p(y_1 \mid \phi)  \cdot \prod_{t=2}^T p(y_t \mid y_{t-1}, \phi)<span class="sc">\\</span></span>
<span id="cb17-253"><a href="#cb17-253" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{(1 - \phi^2)^{1/2}}{(2\pi)^{T/2}} \exp\left(-\frac{y_1^2(1 - \phi^2)}{2}\right) \cdot \prod_{t=2}^T \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(y_t - \phi y_{t-1})^2}{2}\right) <span class="sc">\\</span></span>
<span id="cb17-254"><a href="#cb17-254" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{(1 - \phi^2)^{1/2}}{(2\pi)^{T/2}} \exp\left(-\frac{1}{2} \left<span class="co">[</span><span class="ot">y_1^2(1 - \phi^2) + \sum_{t=2}^T (y_t - \phi y_{t-1})^2\right</span><span class="co">]</span>\right) <span class="sc">\\</span></span>
<span id="cb17-255"><a href="#cb17-255" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb17-256"><a href="#cb17-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-257"><a href="#cb17-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-258"><a href="#cb17-258" aria-hidden="true" tabindex="-1"></a>So the log-likelihood becomes:</span>
<span id="cb17-259"><a href="#cb17-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-260"><a href="#cb17-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-261"><a href="#cb17-261" aria-hidden="true" tabindex="-1"></a>\log p(y_{1:T} \mid y_1, \phi) = \frac{1}{2} \log(1 - \phi^2) - \frac{1}{2} Q(\phi) + K</span>
<span id="cb17-262"><a href="#cb17-262" aria-hidden="true" tabindex="-1"></a>$$ {#eq-log-likelihood-ar1}</span>
<span id="cb17-263"><a href="#cb17-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-264"><a href="#cb17-264" aria-hidden="true" tabindex="-1"></a>where $K$ is a constant that does not depend on $\phi$.</span>
<span id="cb17-265"><a href="#cb17-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-266"><a href="#cb17-266" aria-hidden="true" tabindex="-1"></a>So if I were to look at maximizing this function, we can think about taking first derivatives with respect to phi. And then we will see that again the expression that you obtain doesn't allow you to obtain a close form expression for $\hat{\phi}_{MLE}$. Instead, we will need to use a numerical optimization method such as Newton Raphson to obtain the maximum likelihood estimator for phi.</span>
<span id="cb17-267"><a href="#cb17-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-268"><a href="#cb17-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-269"><a href="#cb17-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conclusion</span></span>
<span id="cb17-270"><a href="#cb17-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-271"><a href="#cb17-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-272"><a href="#cb17-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-273"><a href="#cb17-273" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Full likelihood** is more general but computationally intensive.</span>
<span id="cb17-274"><a href="#cb17-274" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Conditional likelihood** simplifies estimation by leveraging regression theory.</span>
<span id="cb17-275"><a href="#cb17-275" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>When variance $v$ is known (e.g., $v = 1$), the optimization reduces to maximizing a univariate function of $\phi$.</span>
<span id="cb17-276"><a href="#cb17-276" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For full likelihood, optimization of $Q^*(\phi)$ is required.</span>
<span id="cb17-277"><a href="#cb17-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-278"><a href="#cb17-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-279"><a href="#cb17-279" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Feature               <span class="pp">|</span> Full Likelihood             <span class="pp">|</span> Conditional Likelihood (Regression)       <span class="pp">|</span></span>
<span id="cb17-280"><a href="#cb17-280" aria-hidden="true" tabindex="-1"></a><span class="pp">| ---------------------</span> <span class="pp">| ---------------------------</span> <span class="pp">| ------------------------------------------|</span></span>
<span id="cb17-281"><a href="#cb17-281" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Accounts for $Y<span class="sc">\_</span>1$   <span class="pp">|</span> ✅ Yes                      <span class="pp">|</span> ❌ No                                     <span class="pp">|</span></span>
<span id="cb17-282"><a href="#cb17-282" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MLE for $\phi$        <span class="pp">|</span> ❌ No closed form           <span class="pp">|</span> ✅ Closed form                            <span class="pp">|</span></span>
<span id="cb17-283"><a href="#cb17-283" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MLE for $v$           <span class="pp">|</span> ❌ Biased unless adjusted   <span class="pp">|</span> ✅ Unbiased estimator available           <span class="pp">|</span></span>
<span id="cb17-284"><a href="#cb17-284" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Optimization Needed   <span class="pp">|</span> ✅ Yes (numerical methods)  <span class="pp">|</span> ❌ No (closed-form MLE for $\phi$)        <span class="pp">|</span></span>
<span id="cb17-285"><a href="#cb17-285" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Useful when           <span class="pp">|</span> Modeling full joint process <span class="pp">|</span> Estimating $\phi$ efficiently in practice <span class="pp">|</span></span>
<span id="cb17-286"><a href="#cb17-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-287"><a href="#cb17-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-288"><a href="#cb17-288" aria-hidden="true" tabindex="-1"></a><span class="fu">## R code: MLE for the AR(1), examples (reading)</span></span>
<span id="cb17-289"><a href="#cb17-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-290"><a href="#cb17-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-291"><a href="#cb17-291" aria-hidden="true" tabindex="-1"></a>The following code allows you to compute the MLE of the AR coefficient $\psi$, the unbiased estimator of $v$, $s^2$ , and the MLE of v based on a dataset simulated from an AR(1) process and using the conditional likelihood.</span>
<span id="cb17-292"><a href="#cb17-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-295"><a href="#cb17-295" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb17-296"><a href="#cb17-296" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "MLE for AR(1)"</span></span>
<span id="cb17-297"><a href="#cb17-297" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb17-298"><a href="#cb17-298" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb17-299"><a href="#cb17-299" aria-hidden="true" tabindex="-1"></a>v<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb17-300"><a href="#cb17-300" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="fu">sqrt</span>(v) <span class="co"># innovation standard deviation</span></span>
<span id="cb17-301"><a href="#cb17-301" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">500</span> <span class="co"># number of time points</span></span>
<span id="cb17-302"><a href="#cb17-302" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) </span>
<span id="cb17-303"><a href="#cb17-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-304"><a href="#cb17-304" aria-hidden="true" tabindex="-1"></a><span class="do">## Case 1: Conditional likelihood</span></span>
<span id="cb17-305"><a href="#cb17-305" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">2</span><span class="sc">:</span>T]) <span class="co"># response</span></span>
<span id="cb17-306"><a href="#cb17-306" aria-hidden="true" tabindex="-1"></a>X<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]) <span class="co"># design matrix</span></span>
<span id="cb17-307"><a href="#cb17-307" aria-hidden="true" tabindex="-1"></a>phi_MLE<span class="ot">=</span><span class="fu">as.numeric</span>((<span class="fu">t</span>(X)<span class="sc">%*%</span>y)<span class="sc">/</span><span class="fu">sum</span>(X<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># MLE for phi</span></span>
<span id="cb17-308"><a href="#cb17-308" aria-hidden="true" tabindex="-1"></a>s2<span class="ot">=</span><span class="fu">sum</span>((y <span class="sc">-</span> phi_MLE<span class="sc">*</span>X)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(y) <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># Unbiased estimate for v </span></span>
<span id="cb17-309"><a href="#cb17-309" aria-hidden="true" tabindex="-1"></a>v_MLE<span class="ot">=</span>s2<span class="sc">*</span>(<span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>(<span class="fu">length</span>(y)) <span class="co"># MLE for v</span></span>
<span id="cb17-310"><a href="#cb17-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-311"><a href="#cb17-311" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st"> MLE of conditional likelihood for phi: "</span>, phi_MLE, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb17-312"><a href="#cb17-312" aria-hidden="true" tabindex="-1"></a>    <span class="st">"MLE for the variance v: "</span>, v_MLE, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb17-313"><a href="#cb17-313" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Estimate s2 for the variance v: "</span>, s2, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb17-314"><a href="#cb17-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-315"><a href="#cb17-315" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-316"><a href="#cb17-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-317"><a href="#cb17-317" aria-hidden="true" tabindex="-1"></a>This code allows you to compute estimates of the AR(1) coefficient and the variance using the <span class="in">`arima`</span> function in R. The first case uses the conditional sum of squares, the second and third cases use the full likelihood with different starting points for the numerical optimization required to compute the MLE with the full likelihood.</span>
<span id="cb17-318"><a href="#cb17-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-321"><a href="#cb17-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb17-322"><a href="#cb17-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "MLE for AR(1) with different methods"</span></span>
<span id="cb17-323"><a href="#cb17-323" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtaining parameter estimates using the arima function in R</span></span>
<span id="cb17-324"><a href="#cb17-324" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb17-325"><a href="#cb17-325" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb17-326"><a href="#cb17-326" aria-hidden="true" tabindex="-1"></a>v<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb17-327"><a href="#cb17-327" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="fu">sqrt</span>(v) <span class="co"># innovation standard deviation</span></span>
<span id="cb17-328"><a href="#cb17-328" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">500</span> <span class="co"># number of time points</span></span>
<span id="cb17-329"><a href="#cb17-329" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) </span>
<span id="cb17-330"><a href="#cb17-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-331"><a href="#cb17-331" aria-hidden="true" tabindex="-1"></a><span class="co">#Using conditional sum of squares, equivalent to conditional likelihood </span></span>
<span id="cb17-332"><a href="#cb17-332" aria-hidden="true" tabindex="-1"></a>arima_CSS<span class="ot">=</span><span class="fu">arima</span>(yt,<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">method=</span><span class="st">"CSS"</span>,<span class="at">n.cond=</span><span class="dv">1</span>,<span class="at">include.mean=</span><span class="cn">FALSE</span>)</span>
<span id="cb17-333"><a href="#cb17-333" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"AR estimates with conditional sum of squares (CSS) for phi and v:"</span>, arima_CSS<span class="sc">$</span>coef,arima_CSS<span class="sc">$</span>sigma2,</span>
<span id="cb17-334"><a href="#cb17-334" aria-hidden="true" tabindex="-1"></a><span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb17-335"><a href="#cb17-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-336"><a href="#cb17-336" aria-hidden="true" tabindex="-1"></a><span class="co">#Uses ML with full likelihood </span></span>
<span id="cb17-337"><a href="#cb17-337" aria-hidden="true" tabindex="-1"></a>arima_ML<span class="ot">=</span><span class="fu">arima</span>(yt,<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">method=</span><span class="st">"ML"</span>,<span class="at">include.mean=</span><span class="cn">FALSE</span>)</span>
<span id="cb17-338"><a href="#cb17-338" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"AR estimates with full likelihood for phi and v:"</span>, arima_ML<span class="sc">$</span>coef,arima_ML<span class="sc">$</span>sigma2,</span>
<span id="cb17-339"><a href="#cb17-339" aria-hidden="true" tabindex="-1"></a><span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb17-340"><a href="#cb17-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-341"><a href="#cb17-341" aria-hidden="true" tabindex="-1"></a><span class="co">#Default: uses conditional sum of squares to find the starting point for ML and </span></span>
<span id="cb17-342"><a href="#cb17-342" aria-hidden="true" tabindex="-1"></a><span class="co">#         then uses ML </span></span>
<span id="cb17-343"><a href="#cb17-343" aria-hidden="true" tabindex="-1"></a>arima_CSS_ML<span class="ot">=</span><span class="fu">arima</span>(yt,<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">method=</span><span class="st">"CSS-ML"</span>,<span class="at">n.cond=</span><span class="dv">1</span>,<span class="at">include.mean=</span><span class="cn">FALSE</span>)</span>
<span id="cb17-344"><a href="#cb17-344" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"AR estimates with CSS to find starting point for ML for phi and v:"</span>, </span>
<span id="cb17-345"><a href="#cb17-345" aria-hidden="true" tabindex="-1"></a>arima_CSS_ML<span class="sc">$</span>coef,arima_CSS_ML<span class="sc">$</span>sigma2,<span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb17-346"><a href="#cb17-346" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-347"><a href="#cb17-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-348"><a href="#cb17-348" aria-hidden="true" tabindex="-1"></a>This code shows you how to compute the MLE for $\psi$ using the full likelihood and the function optimize in R.</span>
<span id="cb17-349"><a href="#cb17-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-352"><a href="#cb17-352" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb17-353"><a href="#cb17-353" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "MLE for AR(1) with full likelihood"</span></span>
<span id="cb17-354"><a href="#cb17-354" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb17-355"><a href="#cb17-355" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb17-356"><a href="#cb17-356" aria-hidden="true" tabindex="-1"></a>v<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb17-357"><a href="#cb17-357" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="fu">sqrt</span>(v) <span class="co"># innovation standard deviation</span></span>
<span id="cb17-358"><a href="#cb17-358" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">500</span> <span class="co"># number of time points</span></span>
<span id="cb17-359"><a href="#cb17-359" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) </span>
<span id="cb17-360"><a href="#cb17-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-361"><a href="#cb17-361" aria-hidden="true" tabindex="-1"></a><span class="do">## MLE, full likelihood AR(1) with v=1 assumed known </span></span>
<span id="cb17-362"><a href="#cb17-362" aria-hidden="true" tabindex="-1"></a><span class="co"># log likelihood function</span></span>
<span id="cb17-363"><a href="#cb17-363" aria-hidden="true" tabindex="-1"></a>log_p <span class="ot">&lt;-</span> <span class="cf">function</span>(phi, yt){</span>
<span id="cb17-364"><a href="#cb17-364" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.5</span><span class="sc">*</span>(<span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>phi<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fu">sum</span>((yt[<span class="dv">2</span><span class="sc">:</span>T] <span class="sc">-</span> phi<span class="sc">*</span>yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)])<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> yt[<span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>phi<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb17-365"><a href="#cb17-365" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb17-366"><a href="#cb17-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-367"><a href="#cb17-367" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a built-in optimization method to obtain maximum likelihood estimates</span></span>
<span id="cb17-368"><a href="#cb17-368" aria-hidden="true" tabindex="-1"></a>result <span class="ot">=</span><span class="fu">optimize</span>(log_p, <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">tol =</span> <span class="fl">0.0001</span>, <span class="at">maximum =</span> <span class="cn">TRUE</span>, <span class="at">yt =</span> yt)</span>
<span id="cb17-369"><a href="#cb17-369" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st"> MLE of full likelihood for phi: "</span>, result<span class="sc">$</span>maximum)</span>
<span id="cb17-370"><a href="#cb17-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-371"><a href="#cb17-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-372"><a href="#cb17-372" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian inference in the AR(1)</span></span>
<span id="cb17-373"><a href="#cb17-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-374"><a href="#cb17-374" aria-hidden="true" tabindex="-1"></a><span class="al">![inference](images/m1_0051.png)</span>{#fig-c2l1-s1-inference .column-margin  group="slides" width="53mm"}</span>
<span id="cb17-375"><a href="#cb17-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-376"><a href="#cb17-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-377"><a href="#cb17-377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-378"><a href="#cb17-378" aria-hidden="true" tabindex="-1"></a>y_t= \phi y_{t-1} + \epsilon_t, \quad \epsilon_t \overset{iid}{\sim} \mathcal{N}(0, v), \quad \phi \in (-1,1)</span>
<span id="cb17-379"><a href="#cb17-379" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ar1-model}</span>
<span id="cb17-380"><a href="#cb17-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-381"><a href="#cb17-381" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-382"><a href="#cb17-382" aria-hidden="true" tabindex="-1"></a>\utilde{y} = \mathbb{X}\utilde{\beta} + \utilde{\epsilon}</span>
<span id="cb17-383"><a href="#cb17-383" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ar1-model-bayes}</span>
<span id="cb17-384"><a href="#cb17-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-385"><a href="#cb17-385" aria-hidden="true" tabindex="-1"></a>where $\mathbb{X}$ is the design matrix with $y_1$ as the first column and $y_2, \ldots, y_{T-1}$ as the second column, $\utilde{\beta} = \phi$ is the AR coefficient, and $\utilde{\epsilon} \sim \mathcal{N}(0, v\mathbf{I})$ is the error term.</span>
<span id="cb17-386"><a href="#cb17-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-387"><a href="#cb17-387" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-388"><a href="#cb17-388" aria-hidden="true" tabindex="-1"></a>\utilde{y} = \begin{pmatrix}</span>
<span id="cb17-389"><a href="#cb17-389" aria-hidden="true" tabindex="-1"></a>y_1 <span class="sc">\\</span></span>
<span id="cb17-390"><a href="#cb17-390" aria-hidden="true" tabindex="-1"></a>y_2 <span class="sc">\\</span></span>
<span id="cb17-391"><a href="#cb17-391" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb17-392"><a href="#cb17-392" aria-hidden="true" tabindex="-1"></a>y_{T-1}</span>
<span id="cb17-393"><a href="#cb17-393" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb17-394"><a href="#cb17-394" aria-hidden="true" tabindex="-1"></a>$$ {#eq-utilde-y-ar1}</span>
<span id="cb17-395"><a href="#cb17-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-396"><a href="#cb17-396" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-397"><a href="#cb17-397" aria-hidden="true" tabindex="-1"></a>\mathbb{X} = \begin{pmatrix}</span>
<span id="cb17-398"><a href="#cb17-398" aria-hidden="true" tabindex="-1"></a>y_1 <span class="sc">\\</span></span>
<span id="cb17-399"><a href="#cb17-399" aria-hidden="true" tabindex="-1"></a>y_2 <span class="sc">\\</span></span>
<span id="cb17-400"><a href="#cb17-400" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb17-401"><a href="#cb17-401" aria-hidden="true" tabindex="-1"></a>y_{T-1} </span>
<span id="cb17-402"><a href="#cb17-402" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb17-403"><a href="#cb17-403" aria-hidden="true" tabindex="-1"></a>$$ {#eq-design-matrix-ar1}</span>
<span id="cb17-404"><a href="#cb17-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-405"><a href="#cb17-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-406"><a href="#cb17-406" aria-hidden="true" tabindex="-1"></a>\utilde{\beta} = \phi</span>
<span id="cb17-407"><a href="#cb17-407" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ar1-model}</span>
<span id="cb17-408"><a href="#cb17-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-409"><a href="#cb17-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-410"><a href="#cb17-410" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-411"><a href="#cb17-411" aria-hidden="true" tabindex="-1"></a>\utilde{\epsilon} \sim \mathcal{N}(0, v\mathbf{I})</span>
<span id="cb17-412"><a href="#cb17-412" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ar1-model-bayes}</span>
<span id="cb17-413"><a href="#cb17-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-414"><a href="#cb17-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-415"><a href="#cb17-415" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-416"><a href="#cb17-416" aria-hidden="true" tabindex="-1"></a>p(y_{2:T} \mid y_1, \phi, v) =  \frac{1}{(2\pi v)^{\frac{T-1}{2}}} \exp\left(-\frac{(\utilde{y} - \mathbb{X}\utilde{\beta})'(\utilde{y} - \mathbb{X}\utilde{\beta})}{2v}\right)</span>
<span id="cb17-417"><a href="#cb17-417" aria-hidden="true" tabindex="-1"></a>$$ {#eq-likelihood-ar1-conditional}</span>
<span id="cb17-418"><a href="#cb17-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-419"><a href="#cb17-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-420"><a href="#cb17-420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-421"><a href="#cb17-421" aria-hidden="true" tabindex="-1"></a>p(\phi, v \mid y_{1:T}) \propto p(\phi, v) p(y_{2:T} \mid y_1, \phi, v)</span>
<span id="cb17-422"><a href="#cb17-422" aria-hidden="true" tabindex="-1"></a>$$ {#eq-posterior-ar1}</span>
<span id="cb17-423"><a href="#cb17-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-424"><a href="#cb17-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-425"><a href="#cb17-425" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-426"><a href="#cb17-426" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb17-427"><a href="#cb17-427" aria-hidden="true" tabindex="-1"></a>p(\phi, v) \propto \frac{1}{v} &amp;\cdot (\utilde{\beta} \mid v, \mathbb{X}, \utilde{y}) &amp;\sim &amp; \mathcal{N}(\hat{\utilde{\beta}}, v(\mathbb{X}'\mathbb{X})^{-1}) <span class="sc">\\</span></span>
<span id="cb17-428"><a href="#cb17-428" aria-hidden="true" tabindex="-1"></a>  &amp; \cdot (v \mid \mathbb{X}, \utilde{y}) &amp;\sim &amp; \mathcal{IG}\left(\frac{T-2}{2}, \frac{1}{2}Q(\hat{\utilde{\beta}}_{MLE}) \right)</span>
<span id="cb17-429"><a href="#cb17-429" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb17-430"><a href="#cb17-430" aria-hidden="true" tabindex="-1"></a>$$ {#eq-prior-ar1}</span>
<span id="cb17-431"><a href="#cb17-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-432"><a href="#cb17-432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-433"><a href="#cb17-433" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb17-434"><a href="#cb17-434" aria-hidden="true" tabindex="-1"></a>\hat{\utilde{\beta}_{MLE}} &amp;= (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\utilde{y} <span class="sc">\\</span></span>
<span id="cb17-435"><a href="#cb17-435" aria-hidden="true" tabindex="-1"></a>&amp;= \hat{\phi}_{MLE} = \frac{\sum_{t=2}^T y_t y_{t-1}}{\sum_{t=2}^T y_{t-1}^2}</span>
<span id="cb17-436"><a href="#cb17-436" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb17-437"><a href="#cb17-437" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mle-ar1-phi-bayes}</span>
<span id="cb17-438"><a href="#cb17-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-439"><a href="#cb17-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-440"><a href="#cb17-440" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-441"><a href="#cb17-441" aria-hidden="true" tabindex="-1"></a>Q(\hat{\phi}_{MLE}) = \sum_{t=2}^T (y_t - \hat{\phi}_{MLE}\ y_{t-1})^2</span>
<span id="cb17-442"><a href="#cb17-442" aria-hidden="true" tabindex="-1"></a>$$ {#eq-q-ar1-q-mle-bayes}</span>
<span id="cb17-443"><a href="#cb17-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-444"><a href="#cb17-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-445"><a href="#cb17-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-446"><a href="#cb17-446" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian inference in the AR(1): Conditional likelihood example (video)</span></span>
<span id="cb17-447"><a href="#cb17-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-448"><a href="#cb17-448" aria-hidden="true" tabindex="-1"></a>This video walks through the code snippet below and provides examples of how to sample from the posterior distribution of the AR coefficient $\psi$ and the variance $v$ using the conditional likelihood and a reference prior.</span>
<span id="cb17-449"><a href="#cb17-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-450"><a href="#cb17-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-451"><a href="#cb17-451" aria-hidden="true" tabindex="-1"></a><span class="fu">## R Code: AR(1) Bayesian inference, conditional likelihood example (reading)</span></span>
<span id="cb17-452"><a href="#cb17-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-455"><a href="#cb17-455" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb17-456"><a href="#cb17-456" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "AR(1) inference, conditional likelihood example"</span></span>
<span id="cb17-457"><a href="#cb17-457" aria-hidden="true" tabindex="-1"></a><span class="co">#| </span></span>
<span id="cb17-458"><a href="#cb17-458" aria-hidden="true" tabindex="-1"></a><span class="do">####################################################</span></span>
<span id="cb17-459"><a href="#cb17-459" aria-hidden="true" tabindex="-1"></a><span class="do">#####             MLE for AR(1)               ######</span></span>
<span id="cb17-460"><a href="#cb17-460" aria-hidden="true" tabindex="-1"></a><span class="do">####################################################</span></span>
<span id="cb17-461"><a href="#cb17-461" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2021</span>)</span>
<span id="cb17-462"><a href="#cb17-462" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb17-463"><a href="#cb17-463" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="dv">1</span> <span class="co"># innovation standard deviation</span></span>
<span id="cb17-464"><a href="#cb17-464" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">200</span> <span class="co"># number of time points</span></span>
<span id="cb17-465"><a href="#cb17-465" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) <span class="co"># sample stationary AR(1) process</span></span>
<span id="cb17-466"><a href="#cb17-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-467"><a href="#cb17-467" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">2</span><span class="sc">:</span>T]) <span class="co"># response</span></span>
<span id="cb17-468"><a href="#cb17-468" aria-hidden="true" tabindex="-1"></a>X<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]) <span class="co"># design matrix</span></span>
<span id="cb17-469"><a href="#cb17-469" aria-hidden="true" tabindex="-1"></a>phi_MLE<span class="ot">=</span><span class="fu">as.numeric</span>((<span class="fu">t</span>(X)<span class="sc">%*%</span>y)<span class="sc">/</span><span class="fu">sum</span>(X<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># MLE for phi</span></span>
<span id="cb17-470"><a href="#cb17-470" aria-hidden="true" tabindex="-1"></a>s2<span class="ot">=</span><span class="fu">sum</span>((y <span class="sc">-</span> phi_MLE<span class="sc">*</span>X)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(y) <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># Unbiased estimate for v</span></span>
<span id="cb17-471"><a href="#cb17-471" aria-hidden="true" tabindex="-1"></a>v_MLE<span class="ot">=</span>s2<span class="sc">*</span>(<span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>(<span class="fu">length</span>(y)) <span class="co"># MLE for v </span></span>
<span id="cb17-472"><a href="#cb17-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-473"><a href="#cb17-473" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">c</span>(phi_MLE,s2))</span>
<span id="cb17-474"><a href="#cb17-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-475"><a href="#cb17-475" aria-hidden="true" tabindex="-1"></a><span class="do">#######################################################</span></span>
<span id="cb17-476"><a href="#cb17-476" aria-hidden="true" tabindex="-1"></a><span class="do">######     Posterior inference, AR(1)               </span><span class="al">###</span></span>
<span id="cb17-477"><a href="#cb17-477" aria-hidden="true" tabindex="-1"></a><span class="do">######     Conditional Likelihood + Reference Prior </span><span class="al">###</span></span>
<span id="cb17-478"><a href="#cb17-478" aria-hidden="true" tabindex="-1"></a><span class="do">######     Direct sampling                          </span><span class="al">###</span></span>
<span id="cb17-479"><a href="#cb17-479" aria-hidden="true" tabindex="-1"></a><span class="do">#######################################################</span></span>
<span id="cb17-480"><a href="#cb17-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-481"><a href="#cb17-481" aria-hidden="true" tabindex="-1"></a>n_sample<span class="ot">=</span><span class="dv">3000</span>   <span class="co"># posterior sample size</span></span>
<span id="cb17-482"><a href="#cb17-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-483"><a href="#cb17-483" aria-hidden="true" tabindex="-1"></a><span class="do">## step 1: sample posterior distribution of v from inverse gamma distribution</span></span>
<span id="cb17-484"><a href="#cb17-484" aria-hidden="true" tabindex="-1"></a>v_sample<span class="ot">=</span><span class="dv">1</span><span class="sc">/</span><span class="fu">rgamma</span>(n_sample, (T<span class="dv">-2</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="fu">sum</span>((yt[<span class="dv">2</span><span class="sc">:</span>T] <span class="sc">-</span> phi_MLE<span class="sc">*</span>yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)])<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb17-485"><a href="#cb17-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-486"><a href="#cb17-486" aria-hidden="true" tabindex="-1"></a><span class="do">## step 2: sample posterior distribution of phi from normal distribution</span></span>
<span id="cb17-487"><a href="#cb17-487" aria-hidden="true" tabindex="-1"></a>phi_sample<span class="ot">=</span><span class="fu">rep</span>(<span class="dv">0</span>,n_sample)</span>
<span id="cb17-488"><a href="#cb17-488" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sample){</span>
<span id="cb17-489"><a href="#cb17-489" aria-hidden="true" tabindex="-1"></a>phi_sample[i]<span class="ot">=</span><span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> phi_MLE, <span class="at">sd=</span><span class="fu">sqrt</span>(v_sample[i]<span class="sc">/</span><span class="fu">sum</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]<span class="sc">^</span><span class="dv">2</span>)))}</span>
<span id="cb17-490"><a href="#cb17-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-491"><a href="#cb17-491" aria-hidden="true" tabindex="-1"></a><span class="do">## plot histogram of posterior samples of phi and v</span></span>
<span id="cb17-492"><a href="#cb17-492" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>),<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">cex.lab =</span> <span class="fl">1.3</span>)</span>
<span id="cb17-493"><a href="#cb17-493" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(phi_sample, <span class="at">xlab =</span> <span class="fu">bquote</span>(phi), </span>
<span id="cb17-494"><a href="#cb17-494" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">bquote</span>(<span class="st">"Posterior for "</span><span class="sc">~</span>phi),<span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.75</span>,<span class="fl">1.05</span>), <span class="at">col=</span><span class="st">'lightblue'</span>)</span>
<span id="cb17-495"><a href="#cb17-495" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> phi, <span class="at">col =</span> <span class="st">'red'</span>)</span>
<span id="cb17-496"><a href="#cb17-496" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(v_sample, <span class="at">xlab =</span> <span class="fu">bquote</span>(v), <span class="at">col=</span><span class="st">'lightblue'</span>, <span class="at">main =</span> <span class="fu">bquote</span>(<span class="st">"Posterior for "</span><span class="sc">~</span>v))</span>
<span id="cb17-497"><a href="#cb17-497" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> sd, <span class="at">col =</span> <span class="st">'red'</span>)</span>
<span id="cb17-498"><a href="#cb17-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-499"><a href="#cb17-499" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-500"><a href="#cb17-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-501"><a href="#cb17-501" aria-hidden="true" tabindex="-1"></a><span class="fu">## Quiz - MLE and Bayesian inference in the AR(1)</span></span>
<span id="cb17-502"><a href="#cb17-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-503"><a href="#cb17-503" aria-hidden="true" tabindex="-1"></a>Omitted per Coursera honor code</span>
<span id="cb17-504"><a href="#cb17-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-505"><a href="#cb17-505" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practice Graded Assignment: MLE and Bayesian inference in the AR(1)</span></span>
<span id="cb17-506"><a href="#cb17-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-507"><a href="#cb17-507" aria-hidden="true" tabindex="-1"></a>This peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you've learned in R and prepare you for your data analysis project in week 5.</span>
<span id="cb17-508"><a href="#cb17-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-509"><a href="#cb17-509" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Consider the R code below: MLE for the AR(1)</span>
<span id="cb17-510"><a href="#cb17-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-513"><a href="#cb17-513" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb17-514"><a href="#cb17-514" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "lst-ar-1-mle"</span></span>
<span id="cb17-515"><a href="#cb17-515" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-label: lst-ar-1-mle</span></span>
<span id="cb17-516"><a href="#cb17-516" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-cap: "R Code: MLE for the AR(1) process, conditional likelihood example"</span></span>
<span id="cb17-517"><a href="#cb17-517" aria-hidden="true" tabindex="-1"></a><span class="do">####################################################</span></span>
<span id="cb17-518"><a href="#cb17-518" aria-hidden="true" tabindex="-1"></a><span class="do">#####             MLE for AR(1)               ######</span></span>
<span id="cb17-519"><a href="#cb17-519" aria-hidden="true" tabindex="-1"></a><span class="do">####################################################</span></span>
<span id="cb17-520"><a href="#cb17-520" aria-hidden="true" tabindex="-1"></a>phi<span class="ot">=</span><span class="fl">0.9</span> <span class="co"># ar coefficient</span></span>
<span id="cb17-521"><a href="#cb17-521" aria-hidden="true" tabindex="-1"></a>v<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb17-522"><a href="#cb17-522" aria-hidden="true" tabindex="-1"></a>sd<span class="ot">=</span><span class="fu">sqrt</span>(v) <span class="co"># innovation standard deviation</span></span>
<span id="cb17-523"><a href="#cb17-523" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="dv">500</span> <span class="co"># number of time points</span></span>
<span id="cb17-524"><a href="#cb17-524" aria-hidden="true" tabindex="-1"></a>yt<span class="ot">=</span><span class="fu">arima.sim</span>(<span class="at">n =</span> T, <span class="at">model =</span> <span class="fu">list</span>(<span class="at">ar =</span> phi), <span class="at">sd =</span> sd) </span>
<span id="cb17-525"><a href="#cb17-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-526"><a href="#cb17-526" aria-hidden="true" tabindex="-1"></a><span class="do">## Case 1: Conditional likelihood</span></span>
<span id="cb17-527"><a href="#cb17-527" aria-hidden="true" tabindex="-1"></a>y<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">2</span><span class="sc">:</span>T]) <span class="co"># response</span></span>
<span id="cb17-528"><a href="#cb17-528" aria-hidden="true" tabindex="-1"></a>X<span class="ot">=</span><span class="fu">as.matrix</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]) <span class="co"># design matrix</span></span>
<span id="cb17-529"><a href="#cb17-529" aria-hidden="true" tabindex="-1"></a>phi_MLE<span class="ot">=</span><span class="fu">as.numeric</span>((<span class="fu">t</span>(X)<span class="sc">%*%</span>y)<span class="sc">/</span><span class="fu">sum</span>(X<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># MLE for phi</span></span>
<span id="cb17-530"><a href="#cb17-530" aria-hidden="true" tabindex="-1"></a>s2<span class="ot">=</span><span class="fu">sum</span>((y <span class="sc">-</span> phi_MLE<span class="sc">*</span>X)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="fu">length</span>(y) <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># Unbiased estimate for v </span></span>
<span id="cb17-531"><a href="#cb17-531" aria-hidden="true" tabindex="-1"></a>v_MLE<span class="ot">=</span>s2<span class="sc">*</span>(<span class="fu">length</span>(y)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>(<span class="fu">length</span>(y)) <span class="co"># MLE for v</span></span>
<span id="cb17-532"><a href="#cb17-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-533"><a href="#cb17-533" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st"> MLE of conditional likelihood for phi: "</span>, phi_MLE, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb17-534"><a href="#cb17-534" aria-hidden="true" tabindex="-1"></a>    <span class="st">"MLE for the variance v: "</span>, v_MLE, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb17-535"><a href="#cb17-535" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Estimate s2 for the variance v: "</span>, s2, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb17-536"><a href="#cb17-536" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-537"><a href="#cb17-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-538"><a href="#cb17-538" aria-hidden="true" tabindex="-1"></a>Modify the code above to sample 800 observations from an AR(1) with AR coefficient $\psi = -0.8$ and variance $v = 2$. Plot your simulated data. Obtain the MLE for $\psi$ based on the conditional likelihood and the unbiased estimate $s^2$ for the variance $v$.</span>
<span id="cb17-539"><a href="#cb17-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-540"><a href="#cb17-540" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Consider the R code below: AR(1) Bayesian inference, conditional likelihood</span>
<span id="cb17-541"><a href="#cb17-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-544"><a href="#cb17-544" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb17-545"><a href="#cb17-545" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lst-ar-1-inference</span></span>
<span id="cb17-546"><a href="#cb17-546" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-label: lst-ar-1-inference</span></span>
<span id="cb17-547"><a href="#cb17-547" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-cap: "R Code: AR(1) Bayesian inference, conditional likelihood example"</span></span>
<span id="cb17-548"><a href="#cb17-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-549"><a href="#cb17-549" aria-hidden="true" tabindex="-1"></a><span class="do">#######################################################</span></span>
<span id="cb17-550"><a href="#cb17-550" aria-hidden="true" tabindex="-1"></a><span class="do">######     Posterior inference, AR(1)               </span><span class="al">###</span></span>
<span id="cb17-551"><a href="#cb17-551" aria-hidden="true" tabindex="-1"></a><span class="do">######     Conditional Likelihood + Reference Prior </span><span class="al">###</span></span>
<span id="cb17-552"><a href="#cb17-552" aria-hidden="true" tabindex="-1"></a><span class="do">######     Direct sampling                          </span><span class="al">###</span></span>
<span id="cb17-553"><a href="#cb17-553" aria-hidden="true" tabindex="-1"></a><span class="do">#######################################################</span></span>
<span id="cb17-554"><a href="#cb17-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-555"><a href="#cb17-555" aria-hidden="true" tabindex="-1"></a>n_sample<span class="ot">=</span><span class="dv">3000</span>   <span class="co"># posterior sample size</span></span>
<span id="cb17-556"><a href="#cb17-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-557"><a href="#cb17-557" aria-hidden="true" tabindex="-1"></a><span class="do">## step 1: sample posterior distribution of v from inverse gamma distribution</span></span>
<span id="cb17-558"><a href="#cb17-558" aria-hidden="true" tabindex="-1"></a>v_sample<span class="ot">=</span><span class="dv">1</span><span class="sc">/</span><span class="fu">rgamma</span>(n_sample, (T<span class="dv">-2</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="fu">sum</span>((yt[<span class="dv">2</span><span class="sc">:</span>T] <span class="sc">-</span> phi_MLE<span class="sc">*</span>yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)])<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb17-559"><a href="#cb17-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-560"><a href="#cb17-560" aria-hidden="true" tabindex="-1"></a><span class="do">## step 2: sample posterior distribution of phi from normal distribution</span></span>
<span id="cb17-561"><a href="#cb17-561" aria-hidden="true" tabindex="-1"></a>phi_sample<span class="ot">=</span><span class="fu">rep</span>(<span class="dv">0</span>,n_sample)</span>
<span id="cb17-562"><a href="#cb17-562" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sample){</span>
<span id="cb17-563"><a href="#cb17-563" aria-hidden="true" tabindex="-1"></a>phi_sample[i]<span class="ot">=</span><span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> phi_MLE, <span class="at">sd=</span><span class="fu">sqrt</span>(v_sample[i]<span class="sc">/</span><span class="fu">sum</span>(yt[<span class="dv">1</span><span class="sc">:</span>(T<span class="dv">-1</span>)]<span class="sc">^</span><span class="dv">2</span>)))}</span>
<span id="cb17-564"><a href="#cb17-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-565"><a href="#cb17-565" aria-hidden="true" tabindex="-1"></a><span class="do">## plot histogram of posterior samples of phi and v</span></span>
<span id="cb17-566"><a href="#cb17-566" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">cex.lab =</span> <span class="fl">1.3</span>)</span>
<span id="cb17-567"><a href="#cb17-567" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(phi_sample, <span class="at">xlab =</span> <span class="fu">bquote</span>(phi), </span>
<span id="cb17-568"><a href="#cb17-568" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="fu">bquote</span>(<span class="st">"Posterior for "</span><span class="sc">~</span>phi),<span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">0.75</span>,<span class="fl">1.05</span>), <span class="at">col=</span><span class="st">'lightblue'</span>)</span>
<span id="cb17-569"><a href="#cb17-569" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> phi, <span class="at">col =</span> <span class="st">'red'</span>)</span>
<span id="cb17-570"><a href="#cb17-570" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(v_sample, <span class="at">xlab =</span> <span class="fu">bquote</span>(v), <span class="at">col=</span><span class="st">'lightblue'</span>, <span class="at">main =</span> <span class="fu">bquote</span>(<span class="st">"Posterior for "</span><span class="sc">~</span>v))</span>
<span id="cb17-571"><a href="#cb17-571" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> sd, <span class="at">col =</span> <span class="st">'red'</span>)</span>
<span id="cb17-572"><a href="#cb17-572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-573"><a href="#cb17-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-574"><a href="#cb17-574" aria-hidden="true" tabindex="-1"></a>Using your simulated data from part 1 modify the code above to summarize your posterior inference for $\psi$ and $v$ based on 5000 samples from the joint posterior distribution of $\psi$ and $v$.</span>
<span id="cb17-575"><a href="#cb17-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-576"><a href="#cb17-576" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb17-577"><a href="#cb17-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-578"><a href="#cb17-578" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Grading Criteria</span></span>
<span id="cb17-579"><a href="#cb17-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-580"><a href="#cb17-580" aria-hidden="true" tabindex="-1"></a>The responses should follow the same template as the sample code provided above but you will submit your code lines in plain text. Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect that :</span>
<span id="cb17-581"><a href="#cb17-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-582"><a href="#cb17-582" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>you generate 800 time points from the AR(1) rather than 500 and plot your simulated data.</span>
<span id="cb17-583"><a href="#cb17-583" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>your simulated data is from an AR(1) with AR coefficients $\psi = -0.8$ and variance $v = 2$ rather than AR(1) with AR coefficient $\psi = 0.9$ and variance $v = 1$ and</span>
<span id="cb17-584"><a href="#cb17-584" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>you obtain 5000 rather than 3000 samples from the posterior distribution from the new simulated process.</span>
<span id="cb17-585"><a href="#cb17-585" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-586"><a href="#cb17-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-587"><a href="#cb17-587" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Inference in the AR(1), : full likelihood example (reading)</span></span>
<span id="cb17-588"><a href="#cb17-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-589"><a href="#cb17-589" aria-hidden="true" tabindex="-1"></a>We consider a prior distribution that assumes that $\phi$ and $v$ are independent:</span>
<span id="cb17-590"><a href="#cb17-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-591"><a href="#cb17-591" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-592"><a href="#cb17-592" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(v) \propto \frac{1}{v},</span>
<span id="cb17-593"><a href="#cb17-593" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-594"><a href="#cb17-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-595"><a href="#cb17-595" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-596"><a href="#cb17-596" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\phi) = \frac{1}{2}, \quad \text{for } \phi \in (-1, 1),</span>
<span id="cb17-597"><a href="#cb17-597" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-598"><a href="#cb17-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-599"><a href="#cb17-599" aria-hidden="true" tabindex="-1"></a>i.e., we assume a Uniform prior for $\phi \in (-1, 1)$. Combining this prior with the full likelihood in the AR(1) case, we obtain the following posterior density:</span>
<span id="cb17-600"><a href="#cb17-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-601"><a href="#cb17-601" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-602"><a href="#cb17-602" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\phi, v \mid y_{1:T}) \propto \frac{(1 - \phi^2)^{1/2} }{v^{T/2 + 1}} \exp\left(-\frac{Q^*(\phi)}{2v}\right), \quad -1 &lt; \phi &lt; 1,</span>
<span id="cb17-603"><a href="#cb17-603" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-604"><a href="#cb17-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-605"><a href="#cb17-605" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb17-606"><a href="#cb17-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-607"><a href="#cb17-607" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-608"><a href="#cb17-608" aria-hidden="true" tabindex="-1"></a>Q^*(\phi) = y_1^2(1 - \phi^2) + \sum_{t=2}^{T} (y_t - \phi y_{t-1})^2.</span>
<span id="cb17-609"><a href="#cb17-609" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-610"><a href="#cb17-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-611"><a href="#cb17-611" aria-hidden="true" tabindex="-1"></a>It is not possible to get a closed-form expression for this posterior or to perform direct simulation. Therefore, we use simulation-based Markov Chain Monte Carlo (MCMC) methods to obtain samples from the posterior distribution.</span>
<span id="cb17-612"><a href="#cb17-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-613"><a href="#cb17-613" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transformation of $\phi$</span></span>
<span id="cb17-614"><a href="#cb17-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-615"><a href="#cb17-615" aria-hidden="true" tabindex="-1"></a>We first consider the following transformation on $\phi$:</span>
<span id="cb17-616"><a href="#cb17-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-617"><a href="#cb17-617" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-618"><a href="#cb17-618" aria-hidden="true" tabindex="-1"></a>\eta = \log\left(\frac{1 - \phi}{\phi + 1}\right),</span>
<span id="cb17-619"><a href="#cb17-619" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-620"><a href="#cb17-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-621"><a href="#cb17-621" aria-hidden="true" tabindex="-1"></a>so that $\eta \in (-\infty, \infty)$. The inverse transformation on $\eta$ is:</span>
<span id="cb17-622"><a href="#cb17-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-623"><a href="#cb17-623" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-624"><a href="#cb17-624" aria-hidden="true" tabindex="-1"></a>\phi = \frac{1 - \exp(\eta)}{1 + \exp(\eta)}.</span>
<span id="cb17-625"><a href="#cb17-625" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-626"><a href="#cb17-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-627"><a href="#cb17-627" aria-hidden="true" tabindex="-1"></a>Writing down the posterior density for $\eta$ and $v$, we obtain</span>
<span id="cb17-628"><a href="#cb17-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-629"><a href="#cb17-629" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-630"><a href="#cb17-630" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\eta, v \mid y_{1:T}) \propto\frac{ (1 - \phi^2)^{1/2} }{v^{T/2 + 1}} \exp\left(-\frac{Q^*(\phi)}{2v}\right) \cdot \frac{2 \exp(\eta)}{(1 + \exp(\eta))^2},</span>
<span id="cb17-631"><a href="#cb17-631" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-632"><a href="#cb17-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-633"><a href="#cb17-633" aria-hidden="true" tabindex="-1"></a>with $\phi$ written as a function of $\eta$. We proceed to obtain samples from this posterior distribution using the MCMC algorithm outlined below. Once we have obtained $M$ samples from $\eta$ and $v$ after convergence, we can use the inverse transformation above to obtain posterior samples for $\phi$.</span>
<span id="cb17-634"><a href="#cb17-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-635"><a href="#cb17-635" aria-hidden="true" tabindex="-1"></a><span class="fu">### MCMC Algorithm: Bayesian Inference for AR(1), Full Likelihood</span></span>
<span id="cb17-636"><a href="#cb17-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-637"><a href="#cb17-637" aria-hidden="true" tabindex="-1"></a>**Algorithm**:</span>
<span id="cb17-638"><a href="#cb17-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-639"><a href="#cb17-639" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Initialize $\eta^{(0)}$ and $\beta^{(0)}$.</span>
<span id="cb17-640"><a href="#cb17-640" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>For $m$ in $1:M$ do:</span>
<span id="cb17-641"><a href="#cb17-641" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Sample $v^{(m)} \sim \text{IG}\left(\frac{T}{2}, \frac{Q^*(\phi^{(m-1)})}{2}\right)$.</span>
<span id="cb17-642"><a href="#cb17-642" aria-hidden="true" tabindex="-1"></a><span class="ss">    -   </span>Sample $\eta^{(m)}$ using Metropolis-Hastings:</span>
<span id="cb17-643"><a href="#cb17-643" aria-hidden="true" tabindex="-1"></a><span class="ss">        1.  </span>Sample $\eta^* \sim N(\eta^{(m-1)}, c)$, where $c$ is a tuning parameter.</span>
<span id="cb17-644"><a href="#cb17-644" aria-hidden="true" tabindex="-1"></a><span class="ss">        2.  </span>Compute the importance ratio:</span>
<span id="cb17-645"><a href="#cb17-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-646"><a href="#cb17-646" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-647"><a href="#cb17-647" aria-hidden="true" tabindex="-1"></a>        r = \frac{p(\eta^*, v^{(m)} \mid y_{1:T})}{p(\eta^{(m-1)}, v^{(m)} \mid y_{1:T})}.</span>
<span id="cb17-648"><a href="#cb17-648" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-649"><a href="#cb17-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-650"><a href="#cb17-650" aria-hidden="true" tabindex="-1"></a><span class="ss"> 3. </span>Set: </span>
<span id="cb17-651"><a href="#cb17-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-652"><a href="#cb17-652" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-653"><a href="#cb17-653" aria-hidden="true" tabindex="-1"></a>        \eta^{(m)} =</span>
<span id="cb17-654"><a href="#cb17-654" aria-hidden="true" tabindex="-1"></a>        \begin{cases}</span>
<span id="cb17-655"><a href="#cb17-655" aria-hidden="true" tabindex="-1"></a>        \eta^* &amp; \text{with probability } \min(r, 1), <span class="sc">\\</span></span>
<span id="cb17-656"><a href="#cb17-656" aria-hidden="true" tabindex="-1"></a>        \eta^{(m-1)} &amp; \text{otherwise}.</span>
<span id="cb17-657"><a href="#cb17-657" aria-hidden="true" tabindex="-1"></a>        \end{cases}</span>
<span id="cb17-658"><a href="#cb17-658" aria-hidden="true" tabindex="-1"></a>$$</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>