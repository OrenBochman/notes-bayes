<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="dcterms.date" content="2024-10-25">
<meta name="keywords" content="Time Series, Filtering, Kalman filtering, Smoothing, NDLM, Normal Dynamic Linear Models, Polynomial Trend Models, Regression Models, Superposition Principle, R code">
<meta name="description" content="Normal Dynamic Linear Models (NDLMs) are a class of models used for time series analysis that allow for flexible modeling of temporal dependencies.">

<title>91&nbsp; Normal Dynamic Linear Models, Part 1 – Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C4-L04.html" rel="next">
<link href="./C4-L02.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(images/banner_deep.jpg);
background-size: cover;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C4-L03.html"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
            <p class="subtitle lead">Time Series Analysis</p>
                  <div>
        <div class="description">
          Normal Dynamic Linear Models (NDLMs) are a class of models used for time series analysis that allow for flexible modeling of temporal dependencies.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Bayesian Statistics</div>
                <div class="quarto-category">Time Series</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 25, 2024</p>
      </div>
    </div>
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Time Series, Filtering, Kalman filtering, Smoothing, NDLM, Normal Dynamic Linear Models, Polynomial Trend Models, Regression Models, Superposition Principle, R code</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">About</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">M1L1 - Probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Homework on paradigms of probability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">M1L2 - Bayes’ Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">M1L3 - Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Homework on Random Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Homework on Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">M2L4 - Frequentist Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Homework - Frequentist MLE</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">M2L5 - Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">M3L6 - Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Homework on Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">M3L8 - Poisson Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Homework on Poisson Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Honors Quiz - Beta Bernoulli</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Homework exponential data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">M4L10 - Normally distributed Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Homework Normal data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">M4L11 - Non-Informative Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Homework Alternative Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">M4L12 - Brief Review of Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">—</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Honors Homework On Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">M1L1 - Statistical Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">M1L3 - Monte Carlo estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">M2L4 - Metropolis-Hastings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">M2L5 - Gibbs sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">HW - Gibbs-Sampling algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">M2L5 - Assessing Convergence</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Honnors Homework on M-H algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">M3L7 - Linear regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">HW on Linear Regression Model Part 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">HW - Deviance information criterion (DIC)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">M3L8 - ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">HW on ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">HW+ - Multiple Factor ANOVA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">M3L9 - Logistic regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">M4L10 - Poisson regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Homework on Poisson regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">M4L11 -Hierarchical modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">M4L12 - Capstone Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">M1L1 - Definitions of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">M1L2 - Likelihood functions for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">HW - The likelihood function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">HW - Identifiability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">HW - The likelihood function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">HW - Simulating from a Mixture Model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">HW+ - Sim mixture of exponential distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">M2L3 - The EM algorithm for Mixture models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">M4L1 - MCMC for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">M4L5 - Density Estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">M4L6 - Clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">M4L7 -Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Bayesian Mixture Models for Classification of Banknotes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">M5L8 - Computational Considerations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">M5L9 - Determining the number of components</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">M5HW2 - Bayesian Information Criteria (BIC)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">M5HW2 - Estimating the number of components in Bayesian settings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">M5HW3 - Estimating the partition structure in Bayesian models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">M5HW4 - BIC for zero-inflated mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Week 0: Introductions to time series analysis and the AR(1) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">Introductions to Time Series analysis &amp; the AR(1) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(p) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Time Series Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Bayes by backprop</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-normal-dynamic-linear-model-definition-model-classes-the-superposition-principle" id="toc-the-normal-dynamic-linear-model-definition-model-classes-the-superposition-principle" class="nav-link active" data-scroll-target="#the-normal-dynamic-linear-model-definition-model-classes-the-superposition-principle"><span class="header-section-number">92</span> The Normal Dynamic Linear Model: Definition, Model classes &amp; The Superposition Principle</a>
  <ul class="collapse">
  <li><a href="#ndlm-definition-video" id="toc-ndlm-definition-video" class="nav-link" data-scroll-target="#ndlm-definition-video"><span class="header-section-number">92.1</span> NDLM Definition (Video)</a>
  <ul class="collapse">
  <li><a href="#white-noise---a-motivating-example" id="toc-white-noise---a-motivating-example" class="nav-link" data-scroll-target="#white-noise---a-motivating-example"><span class="header-section-number">92.1.1</span> White Noise - A motivating example</a></li>
  <li><a href="#a-random-walk-model-with-a-slowly-changing-mean" id="toc-a-random-walk-model-with-a-slowly-changing-mean" class="nav-link" data-scroll-target="#a-random-walk-model-with-a-slowly-changing-mean"><span class="header-section-number">92.1.2</span> A Random walk model with a slowly changing mean</a></li>
  <li><a href="#general-form-of-the-ndlm" id="toc-general-form-of-the-ndlm" class="nav-link" data-scroll-target="#general-form-of-the-ndlm"><span class="header-section-number">92.1.3</span> General form of the NDLM</a></li>
  <li><a href="#inference-in-the-ndlm" id="toc-inference-in-the-ndlm" class="nav-link" data-scroll-target="#inference-in-the-ndlm"><span class="header-section-number">92.1.4</span> Inference in the NDLM</a></li>
  <li><a href="#the-forecast-function-for-the-ndlm" id="toc-the-forecast-function-for-the-ndlm" class="nav-link" data-scroll-target="#the-forecast-function-for-the-ndlm"><span class="header-section-number">92.1.5</span> The forecast function for the NDLM</a></li>
  <li><a href="#ndlm-short-form-notation" id="toc-ndlm-short-form-notation" class="nav-link" data-scroll-target="#ndlm-short-form-notation"><span class="header-section-number">92.1.6</span> NDLM short form notation</a></li>
  </ul></li>
  <li><a href="#polynomial-trend-models-video" id="toc-polynomial-trend-models-video" class="nav-link" data-scroll-target="#polynomial-trend-models-video"><span class="header-section-number">92.2</span> Polynomial Trend Models (Video)</a>
  <ul class="collapse">
  <li><a href="#first-order-polynomial-model" id="toc-first-order-polynomial-model" class="nav-link" data-scroll-target="#first-order-polynomial-model"><span class="header-section-number">92.2.1</span> First order polynomial model</a></li>
  <li><a href="#second-order-polynomial-model-aka-linear-growth-model" id="toc-second-order-polynomial-model-aka-linear-growth-model" class="nav-link" data-scroll-target="#second-order-polynomial-model-aka-linear-growth-model"><span class="header-section-number">92.2.2</span> Second order Polynomial model AKA Linear Growth model</a></li>
  <li><a href="#general-p-th-order-polynomial-model" id="toc-general-p-th-order-polynomial-model" class="nav-link" data-scroll-target="#general-p-th-order-polynomial-model"><span class="header-section-number">92.2.3</span> General p-th order polynomial model</a></li>
  </ul></li>
  <li><a href="#summary-of-polynomial-trend-models-reading" id="toc-summary-of-polynomial-trend-models-reading" class="nav-link" data-scroll-target="#summary-of-polynomial-trend-models-reading"><span class="header-section-number">92.3</span> Summary of polynomial trend models (Reading)</a>
  <ul class="collapse">
  <li><a href="#polynomial-trend-models" id="toc-polynomial-trend-models" class="nav-link" data-scroll-target="#polynomial-trend-models"><span class="header-section-number">92.3.1</span> Polynomial Trend Models</a></li>
  </ul></li>
  <li><a href="#regression-models-video" id="toc-regression-models-video" class="nav-link" data-scroll-target="#regression-models-video"><span class="header-section-number">92.4</span> Regression models (Video)</a>
  <ul class="collapse">
  <li><a href="#simple-dynamic-regression" id="toc-simple-dynamic-regression" class="nav-link" data-scroll-target="#simple-dynamic-regression"><span class="header-section-number">92.4.1</span> Simple dynamic regression</a></li>
  <li><a href="#general-dynamic-regression" id="toc-general-dynamic-regression" class="nav-link" data-scroll-target="#general-dynamic-regression"><span class="header-section-number">92.4.2</span> General dynamic regression</a></li>
  </ul></li>
  <li><a href="#summary-of-regression-models-reading" id="toc-summary-of-regression-models-reading" class="nav-link" data-scroll-target="#summary-of-regression-models-reading"><span class="header-section-number">92.5</span> Summary of Regression Models (Reading)</a>
  <ul class="collapse">
  <li><a href="#dynamic-regression-models" id="toc-dynamic-regression-models" class="nav-link" data-scroll-target="#dynamic-regression-models"><span class="header-section-number">92.5.1</span> Dynamic Regression Models</a></li>
  </ul></li>
  <li><a href="#the-superposition-principle-video" id="toc-the-superposition-principle-video" class="nav-link" data-scroll-target="#the-superposition-principle-video"><span class="header-section-number">92.6</span> The superposition principle (Video)</a></li>
  <li><a href="#superposition-principle-general-case-reading" id="toc-superposition-principle-general-case-reading" class="nav-link" data-scroll-target="#superposition-principle-general-case-reading"><span class="header-section-number">92.7</span> Superposition principle: General case (Reading)</a>
  <ul class="collapse">
  <li><a href="#general-case" id="toc-general-case" class="nav-link" data-scroll-target="#general-case"><span class="header-section-number">92.7.1</span> General Case</a></li>
  </ul></li>
  <li><a href="#quiz-the-normal-dynamic-linear-model" id="toc-quiz-the-normal-dynamic-linear-model" class="nav-link" data-scroll-target="#quiz-the-normal-dynamic-linear-model"><span class="header-section-number">92.8</span> Quiz: The Normal Dynamic Linear Model</a></li>
  </ul></li>
  <li><a href="#bayesian-inference-in-the-ndlm-part-1" id="toc-bayesian-inference-in-the-ndlm-part-1" class="nav-link" data-scroll-target="#bayesian-inference-in-the-ndlm-part-1"><span class="header-section-number">93</span> Bayesian Inference in the NDLM: Part 1</a>
  <ul class="collapse">
  <li><a href="#filtering-video" id="toc-filtering-video" class="nav-link" data-scroll-target="#filtering-video"><span class="header-section-number">93.1</span> Filtering (Video)</a></li>
  <li><a href="#summary-of-filtering-distributions-reading" id="toc-summary-of-filtering-distributions-reading" class="nav-link" data-scroll-target="#summary-of-filtering-distributions-reading"><span class="header-section-number">93.2</span> Summary of filtering distributions (Reading)</a>
  <ul class="collapse">
  <li><a href="#bayesian-inference-in-ndlm-known-variances" id="toc-bayesian-inference-in-ndlm-known-variances" class="nav-link" data-scroll-target="#bayesian-inference-in-ndlm-known-variances"><span class="header-section-number">93.2.1</span> Bayesian Inference in NDLM: Known Variances</a></li>
  </ul></li>
  <li><a href="#rcode-filtering-in-the-ndlm-example-reading" id="toc-rcode-filtering-in-the-ndlm-example-reading" class="nav-link" data-scroll-target="#rcode-filtering-in-the-ndlm-example-reading"><span class="header-section-number">93.3</span> Rcode Filtering in the NDLM: Example (Reading)</a></li>
  <li><a href="#smoothing-and-forecasting-video" id="toc-smoothing-and-forecasting-video" class="nav-link" data-scroll-target="#smoothing-and-forecasting-video"><span class="header-section-number">93.4</span> Smoothing and forecasting (Video)</a></li>
  <li><a href="#summary-of-the-smoothing-and-forecasting-distributions-reading" id="toc-summary-of-the-smoothing-and-forecasting-distributions-reading" class="nav-link" data-scroll-target="#summary-of-the-smoothing-and-forecasting-distributions-reading"><span class="header-section-number">93.5</span> Summary of the smoothing and forecasting distributions (reading)</a>
  <ul class="collapse">
  <li><a href="#bayesian-inference-in-ndlm-known-variances-1" id="toc-bayesian-inference-in-ndlm-known-variances-1" class="nav-link" data-scroll-target="#bayesian-inference-in-ndlm-known-variances-1"><span class="header-section-number">93.5.1</span> Bayesian Inference in NDLM: Known Variances</a></li>
  </ul></li>
  <li><a href="#smoothing-in-the-ndlm-example-video" id="toc-smoothing-in-the-ndlm-example-video" class="nav-link" data-scroll-target="#smoothing-in-the-ndlm-example-video"><span class="header-section-number">93.6</span> Smoothing in the NDLM, Example (Video)</a></li>
  <li><a href="#r-code-smoothing-in-the-ndlm-example-reading" id="toc-r-code-smoothing-in-the-ndlm-example-reading" class="nav-link" data-scroll-target="#r-code-smoothing-in-the-ndlm-example-reading"><span class="header-section-number">93.7</span> R-code: Smoothing in the NDLM, Example (Reading)</a></li>
  <li><a href="#second-order-polynomial-filtering-and-smoothing-example-video" id="toc-second-order-polynomial-filtering-and-smoothing-example-video" class="nav-link" data-scroll-target="#second-order-polynomial-filtering-and-smoothing-example-video"><span class="header-section-number">93.8</span> Second order polynomial: Filtering and smoothing example (Video)</a></li>
  <li><a href="#using-the-dlm-package-in-r-video" id="toc-using-the-dlm-package-in-r-video" class="nav-link" data-scroll-target="#using-the-dlm-package-in-r-video"><span class="header-section-number">93.9</span> Using the dlm package in R (Video)</a></li>
  <li><a href="#r-code-using-the-dlm-package-in-r-reading" id="toc-r-code-using-the-dlm-package-in-r-reading" class="nav-link" data-scroll-target="#r-code-using-the-dlm-package-in-r-reading"><span class="header-section-number">93.10</span> R-code: Using the <code>dlm</code> package in R (Reading)</a></li>
  <li><a href="#practice-graded-assignment-ndlm-sensitivity-to-the-model-parameters" id="toc-practice-graded-assignment-ndlm-sensitivity-to-the-model-parameters" class="nav-link" data-scroll-target="#practice-graded-assignment-ndlm-sensitivity-to-the-model-parameters"><span class="header-section-number">93.11</span> Practice Graded Assignment: NDLM – sensitivity to the model parameters</a></li>
  <li><a href="#quiz---ndlm-part-i-review" id="toc-quiz---ndlm-part-i-review" class="nav-link" data-scroll-target="#quiz---ndlm-part-i-review"><span class="header-section-number">93.12</span> Quiz - NDLM, Part I: Review</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<!-- 
The challenge in taking notes in this courses is that the instructors speak out the maths as they go along. I supply the math but I want to extract the non-math parts and put them in the right place. 
-->
<p><mark>Normal Dynamic Linear Models (NDLMs) are defined and illustrated in this module using several examples Model building based on the forecast function via the superposition principle is explained. Methods for Bayesian filtering, smoothing and forecasting for NDLMs in the case of known observational variances and known system covariance matrices are discussed and illustrated.</mark>.</p>
<p>The Normal Dynamic Linear Model (DLM) is covered <span class="citation" data-cites="prado2023time">(<a href="#ref-prado2023time" role="doc-biblioref">R. Prado, Ferreira, and West 2023, 117–44</a>)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Learning Objectives
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Use R for analysis and forecasting of time series using NDLM (case of known observational and system variances) <a href="#m3g1">#</a></label></li>
<li><label><input type="checkbox" checked="">Derive the equations to obtain posterior inference and forecasting in the NDLM with known observational and system variances, including the filtering, smoothing and forecasting equations <a href="#m3g2">#</a></label></li>
<li><label><input type="checkbox" checked="">Apply the NDLM superposition principle and explain the role of the forecast function <a href="#m3g3">#</a></label></li>
<li><label><input type="checkbox" checked="">Define trend and regression normal DLMs <a href="#m3g4">#</a></label></li>
<li><label><input type="checkbox" checked="">Explain the general normal dynamic linear model (NDLM) representation <a href="#m3g5">#</a></label></li>
</ul>
</div>
</div>
</div>
<section id="the-normal-dynamic-linear-model-definition-model-classes-the-superposition-principle" class="level1 page-columns page-full" data-number="92">
<h1 data-number="92"><span class="header-section-number">92</span> The Normal Dynamic Linear Model: Definition, Model classes &amp; The Superposition Principle</h1>
<p>Dynamic Linear Models (DLMs) extend classical linear regression to time-indexed data, introducing dependencies between observations through latent evolving parameters. A Normal DLM (NDLM) assumes Gaussian noise at both observation and system levels, enabling tractable Bayesian inference through the Kalman filter.</p>
<p>While superficially complex, NDLMs are conceptually close to linear regression. Instead of I.I.D. observations indexed by <span class="math inline">i</span>, we index data by time <span class="math inline">t</span> and allow parameters to evolve with time, resulting in a two-level hierarchical model. At the top level is the observation equation. Below this there is the evolution equation(s) that can be understood as a latent state transition model that can capture trends, periodicity, and regression. The evolution equations can have more than one level however we will see that with some work these are summarized into a matrix form.</p>
<p>To make things simpler this is demonstrated using a white noise process and then a random walk model. What makes the NDLM somewhat different is that that there are two variance elements at two levels, necessitating learning more parameters. Once we cover these to models the instructor walks us though all the bits and pieces of the notation. Later we will see that we can add trends, periodicity, regression components in a more or less systematic way. However we need to pick and choose these components to get a suitable forecast function. This approach require an intimate familiarity with the data generating process to model.</p>
<p>This approach is Bayesian in that we draw our parameters from a multivariate normal and use updating to improve this initial estimate by incorporating the data and we end up with a posterior i.e.&nbsp;we have distributional view of the time series incorporating uncertainties. Additionally we have a number of Bayesian quantities that can be derived from the model, such as</p>
<ul>
<li>the <strong>filtering distribution</strong> that estimates the current state <span class="math inline">\mathbb{P}r(\theta_t \mid \mathcal{D}_t)</span>,</li>
<li>the <strong>forecasting distribution</strong> - to predict future observation: <span class="math inline">\mathbb{P}r(y_{t+h} \mid \mathcal{D}_t)</span>,</li>
<li>the <strong>smoothing distribution</strong> - retrospective estimate of past state: <span class="math inline">\mathbb{P}r(\theta_t \mid \mathcal{D}_{T})\quad t&lt;T</span> and</li>
<li>the <strong>forecast function</strong> when <span class="math inline">F_t=F</span> and <span class="math inline">\mathbf{G}_t=\mathbf{G}</span> <span class="math inline">f_t(h)=\mathbb{E}[y_{t+h} \mid \mathcal{D}_{T}] = F'G^h \mathbb{E}[\theta_{t} \mid \mathcal{D}_{T}]</span></li>
<li>the usual credible intervals for forecasts and parameter estimates.</li>
</ul>
<p>However the DLM framework is quite flexible and once you understand it it can ve adapted to support features like seasonality using the superposition principle. NDLMs don’t need to be non-stationary time series.</p>
<p>As far as I cen tell NDLMs are just DLM with their errors distributed normally at the different levels.</p>
<section id="ndlm-definition-video" class="level2 page-columns page-full" data-number="92.1">
<h2 data-number="92.1" class="anchored" data-anchor-id="ndlm-definition-video"><span class="header-section-number">92.1</span> NDLM Definition (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0001.png" class="lightbox" data-gallery="slides" title="NDLM Motivation"><img src="images/m3_0001.png" class="img-fluid figure-img" width="200" alt="NDLM Motivation"></a></p>
<figcaption>NDLM Motivation</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0002.png" class="lightbox" data-gallery="slides" title="NDLM general form"><img src="images/m3_0002.png" class="img-fluid figure-img" width="200" alt="NDLM general form"></a></p>
<figcaption>NDLM general form</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0003.png" class="lightbox" data-gallery="slides" title="the forecast function"><img src="images/m3_0003.png" class="img-fluid figure-img" width="200" alt="the forecast function"></a></p>
<figcaption>the forecast function</figcaption>
</figure>
</div></div>

<!--
::: {.callout-caution collapse="true"}

### Hard to follow :weary:

- I found this video hard to follow. I watched it carefully a number of times and reworked the math and video's transcript. But in the end this and the next two video are more like a summary of the material rather than a motivated development of the models. 

- The text also diverge when it comes to the $F$ in the state space representation equation. In @west2013bayesian it is a matrix and in @prado2023time it is called a vector. In some places it is referenced as a $p \times 1$. As far as I can tell this reflects different state space representation of the model which neither text is particularly clear. 
- Although we are give a couple of motivational examples the way they generalize to the NDLM is not as clear as in @west2013bayesian but this text is very verbose and covers much more material.

- @west2013bayesian also discusses how the math relates to more common statistical scenarios and how the model can be adapted to these scenarios e.g. replacing the obs. normal distribution with a t-distribution or a mixture of normals to handle outliers. I.e. see NDLMs as a looser framework with the assumptions of independence and normality as guidelines rather than strict assumptions.

In @prado2023time we quickly get a large number of mathematically concise format, that illustrate how NDLMs generalize other time series models. However these examples assume we are sufficiently familiar with this models.

- The NDLM is also a hierarchical model which we have not looked into in much depth in this specialization. This is another missed opportunity to connect to previous material and deepen our understanding. Particularly as the bayesian formulation should be able to overcome the limitations of the frequentist approach where we need to make strong assumptions on IID of the shocks but in a time series local observations often highly correlated.

- NDLM have efficient algorithms for inference and forecasting mostly by using the Kalman filter, again this is not mentioned not is a bayesian formulation presented

- So I guess the main issue is that the vide just lists lots of equations and does not really without providing a good intuition behind the model.

- I hope that with the rest of this and the next lesson we can get a better understanding of the NDLM.

- I ended up skimming through large portions of [@west2013bayesian] which goes into much greater detail on the NDLM. The first and second polynomial models developed, explained and motivation is clearer. The state space representation is not explained but the model is developed in a more coherent way and this is a good starting point for developing a more intuitive understanding of the NDLMs. The notation in this book is also easier to follow.

  - We use $\theta_t$ for the parameters of the vector and \theta_{t,1} and \theta_{t,2} for the first two components of the parameter vector. This continues the convention of greek characters for parameters from the previous courses and many texts. However if the parameters have a clear meaning it much easier to follow math where we use a more descriptive notation and avoid the second subscripts.
- The state space representation here is omitted. The $F$ vector or matrix is not really motivates as a latent state space representation.

:::
-->
<p>In this module, <mark>we will motivate and develop a class of models suitable for for analyzing and forecasting <strong>non-stationary time series</strong> called <strong>normal dynamic linear models</strong> . We will talk about Bayesian inference and forecasting within this class of models and describe model building as well</mark>.</p>
<section id="white-noise---a-motivating-example" class="level3" data-number="92.1.1">
<h3 data-number="92.1.1" class="anchored" data-anchor-id="white-noise---a-motivating-example"><span class="header-section-number">92.1.1</span> White Noise - A motivating example</h3>
<p>Let’s begin with a very simple model that has no temporal structure, just a mean value with some variation that is:</p>
<p><span id="eq-white-noise-model"><span class="math display">
y_t = \mu + v_t \qquad v_t \overset{\text{iid}}{\sim}  \mathcal{N}(0, \nu) \qquad  \text{(white noise model)}
\tag{92.1}</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">y_t</span> is the observed time series at time <span class="math inline">t</span>,</li>
<li><span class="math inline">\mu</span> is the expected value of <span class="math inline">y_t</span> this is characteristic we are interested in,</li>
<li><span class="math inline">\nu_t</span> is a white noise process as usual iid standard normal N(0,1).</li>
</ul>
<p>If we plot this model we might see the following graph:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mu <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, V)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="st">"Time"</span>, <span class="at">ylab =</span> <span class="st">"y"</span>, <span class="at">main =</span> <span class="st">"Model with no temporal structure"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-NDLM-white-noise" class="quarto-float quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-NDLM-white-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C4-L03_files/figure-html/fig-NDLM-white-noise-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;92.1: "><img src="C4-L03_files/figure-html/fig-NDLM-white-noise-1.png" id="fig-NDLM-white-noise" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-NDLM-white-noise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;92.1
</figcaption>
</figure>
</div>
</div>
</div>
<p>For this model the mean of the time series is <span class="math inline">\mu</span> will be the the expected value of <span class="math inline">y_t</span>, which is <span class="math inline">\mu</span>. And the variance of <span class="math inline">y_t</span> is <span class="math inline">\nu</span>.</p>
<p><span id="eq-NDLM-mean-variance"><span class="math display">
\mathbb{E}[y_t] = \mu \qquad \text{and} \qquad \mathbb{V}ar[y_t] = \nu \qquad
\tag{92.2}</span></span></p>
</section>
<section id="a-random-walk-model-with-a-slowly-changing-mean" class="level3" data-number="92.1.2">
<h3 data-number="92.1.2" class="anchored" data-anchor-id="a-random-walk-model-with-a-slowly-changing-mean"><span class="header-section-number">92.1.2</span> A Random walk model with a slowly changing mean</h3>
<p><mark>Next we incorporate some temporal structure, we allow the expected value of the time series, to change over time. To can achieve this, by update the model definition with a <span class="math inline">\mu_t</span> where the index indicates that it can change at every time step. And let us keep the noise unchanged. i.e.&nbsp;we set it to <span class="math inline">\mu_t \in N(0,\nu)</span>.</mark></p>
<p>We get the following model:</p>
<p><span id="eq-NDLM-model-random-walk"><span class="math display">
y_t = \mu_t + \nu_t \quad \nu_t \overset{\text{iid}}{\sim} N(0, V) \qquad \text{(radom walk model)}
\tag{92.3}</span></span></p>
<p>To complete this we need to also decide how to incorporate the the changes over time in the parameter <span class="math inline">\mu_t</span>. We might consider different options but we should pick the simplest possible to start with. One option is to assume that the expected value of <span class="math inline">\mu_t</span> is just the expected value of <span class="math inline">\mu_{t-1}</span> plus some noise.</p>
<p>We now have that random walk type of structure where <span class="math inline">\mu_t</span> can be written in terms of <span class="math inline">\mu(t-1)</span>. The expected value of <span class="math inline">\mu_t</span>, we can think of it as <span class="math inline">\mu_{t-1} + \text{some noise}</span>. This error is once again, assumed to be normally distributed random variable centered at zero and with variance <span class="math inline">W</span>. <mark>Another assumption that we have made here is that the <span class="math inline">\nu_t</span> and <span class="math inline">\omega_t</span>, are also independent of each other</mark>.</p>
<p>putting this together we get:</p>
<p><span id="eq-random-walk-model-hierarchical"><span class="math display">
\begin{aligned}
y_t &amp;= \mu_t + \nu_t &amp; \nu_t &amp; \overset{\text{iid}}{\sim}  \mathcal{N}(0, V)  &amp; \text{(Observation eq.)} \\
\mu_t &amp;= \mu_{t-1} + \omega_t  &amp; \omega_t &amp; \overset{\text{iid}}{\sim}  \mathcal{N}(0, W) &amp; \text{(System/evolution eq.)}
\end{aligned}
\tag{92.4}</span></span></p>
<p>With this model, what we are assuming is that the mean level of the series is changing over time. Note that this is an example of a <strong>Gaussian or Normal dynamic linear model</strong>.</p>
<p>NDLMs are a two level hierarchical models where :</p>
<ol type="1">
<li>At the top is an <strong>observation level equation</strong> relating observations y at time t to some time dependent, (hidden) state parameters and some observation level iid distributed error.</li>
<li>The <strong>system evolution level equation</strong> describes the dynamics of parameters over time and incorporates some system iid distributed error.</li>
<li><mark>These equations have a linear structure, in the sense that the expected value of y at time t is a linear function of the parameters.</mark></li>
<li>We have the <strong>assumption of normality for the noise terms</strong> in both these equations as well as independence within and between levels.</li>
</ol>
<p>This is our first example. Next we will be discuss the general class of models. Later we will consider how to incorporate different structures into the model, and how to perform Bayesian inference for filtering smoothing and forecasting.</p>
</section>
<section id="general-form-of-the-ndlm" class="level3" data-number="92.1.3">
<h3 data-number="92.1.3" class="anchored" data-anchor-id="general-form-of-the-ndlm"><span class="header-section-number">92.1.3</span> General form of the NDLM</h3>
<p>The general class of dynamic linear models can be written as follows:</p>
<p>We are going to have two equations. One is the so-called observation equation that relates the observations to the parameters in the model, and the notation we are going to use is as follows.</p>
<p><span id="eq-NDLM-general-form"><span class="math display">\begin{aligned}
y_t &amp;= \vec{F}_t' \vec{\theta}_t   + \nu_t &amp;&amp; \nu_t \overset{\text{iid}}{\sim}  \mathcal{N}(0, V_t) &amp;&amp; \text{(obs)} \\
\vec{\theta}_t &amp;= G_t \vec{\theta}_{t-1} + \vec{\omega}_t &amp;&amp; \vec{\omega}_t \overset{\text{iid}}{\sim}  \mathcal{N}(0, W_t) &amp;&amp; \text{(system)}
\end{aligned}
\tag{92.5}</span></span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">y_t</span> a univariate observation at time <span class="math inline">t</span>.</li>
<li><span class="math inline">\vec{\theta}_t</span> the <strong>state vector</strong> is a k-dimensional vector of unknown parameters at time t.</li>
<li><span class="math inline">\vec{F_t}</span> the <strong>observation operator</strong> a <span class="math inline">k*1</span>-dimensional vector at time t that transforms the <strong>state parameters</strong> into observations.</li>
<li><span class="math inline">\nu_t</span> is the observation noise at time t from a Normal distribution with variance <span class="math inline">V_t</span>.</li>
<li><span class="math inline">G_t</span> the <strong>state evolution operator</strong> is a <span class="math inline">k \times k</span> matrix (known)</li>
<li><span class="math inline">\omega_t</span> the <strong>innovation</strong> or state evolution noise at time t distributed as <span class="math inline">N(0,W_t)</span>(known)</li>
<li>the noise at the observation level and the system level are each iid and mutually iid.</li>
</ul>
<p>We also have the prior distribution for the state vector at time 0:</p>
<ul>
<li><span class="math inline">\vec{\theta}_0 \sim N(\vec{m}_0,c_0)</span> a prior k-dimensional Normal distribution.
<ul>
<li><span class="math inline">m_0</span> the mean in the prior is a k-dimensional vector of means. (known)</li>
<li><span class="math inline">c_0</span> is the covariance matrix k by k. (known)</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Some Thoughts on NDLM the definition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Q. Why are <span class="math inline">F_t</span> and <span class="math inline">G_t</span> a vector and a matrix respectively?</p>
<blockquote class="blockquote">
<p>It may helps to think about <span class="math inline">F</span> and <span class="math inline">G</span> as follows:</p>
<p><span class="math inline">F_t'</span> acts as a linear transformation that maps the latent state <span class="math inline">\vec{\theta}_t</span> into the observation space, of <span class="math inline">y</span>.</p>
<p><span class="math inline">G_t</span> is a linear transformation that describes how the state vector evolves over time. I like to think about it as a Hidden Markov state transition matrix.</p>
<p>In other words, <span class="math inline">F_t</span> takes the current hidden state <span class="math inline">\theta_t</span> and produces an observation <span class="math inline">y_t</span>, while <span class="math inline">G_t</span> takes the current state and produces the next state.</p>
</blockquote>
<p>Q. Why is this called a linear model?</p>
<blockquote class="blockquote">
<p>This is because both the observation equation is a linear equation that relates the observations to the parameters in the model and the system equation is a linear equation that tells us how the time-varying parameter is going to be changing over time. This is why we call this a linear model.</p>
</blockquote>
<p>Q. Why are the noise terms <span class="math inline">\nu_t</span> and <span class="math inline">\omega_t</span> assumed to be normally distributed?</p>
<blockquote class="blockquote">
<p>This is a common assumption in time series analysis. It is a convenient assumption that allows us to perform Bayesian inference and forecasting in a very simple way. And this is why we call this a <strong>normal</strong> dynamic linear model.</p>
</blockquote>
<p>Q. Isn’t this just a hierarchical model?</p>
<blockquote class="blockquote">
<p>Indeed, this is a hierarchical model. We have a model for the observations and a model for the system level. The system level is changing over time and the observations are related to the system level through the observation equation. And so it is possible to extend this model to more complex structures if we wish to do so by adding another level, etc… However adding more levels leads to extra dynamics that are captured in <span class="math inline">G</span> without changing the overall framework!</p>
</blockquote>
</div>
</div>
</div>
</section>
<section id="inference-in-the-ndlm" class="level3 page-columns page-full" data-number="92.1.4">
<h3 data-number="92.1.4" class="anchored" data-anchor-id="inference-in-the-ndlm"><span class="header-section-number">92.1.4</span> Inference in the NDLM</h3>
<p>In terms of the inference, there are a few different kinds of densities and quantities that we are interested in:</p>
<div class="page-columns page-full"><p> One of the distributions that we are interested in finding is the so-called <strong>filtering distribution</strong>. We may be interested here in finding what is the density of <span class="math inline">\theta_t</span> <em>given all the observations that we have up to time</em> <span class="math inline">t</span>.</p><div class="no-row-height column-margin column-container"><span class="">Filtering distribution</span></div></div>
<p><span id="eq-NDLM-filtering-distribution"><span class="math display">
\mathcal{D}_t= \{\mathcal{D}_0, y_{1:T}\}
\tag{92.6}</span></span></p>
<p>We will denote information as <span class="math inline">\mathcal{D}_t</span>. Usually, it is all the information we have at time zero (i.e.&nbsp;our prior), coupled with all the data points I have up to time <span class="math inline">t</span>.</p>
<p>Here we conditioning on all the observed quantities and the prior information up to time <span class="math inline">t</span>, and I may be interested in just finding what is the distribution for <span class="math inline">\theta_t</span>. This is called filtering.</p>
<p><span id="eq-NDLM-filtering"><span class="math display">
\mathbb{P}r(\theta_t \mid \mathcal{D}_t) \qquad \text{filtering distribution}
\tag{92.7}</span></span></p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">forecasting distribution</span></div></div>
<p>Another distribution that is very important in time series analysis is the forecasting distribution. We may be interested in the distribution of <span class="math inline">y{t+h}</span>? where we consider <span class="math inline">h</span> lags into the future and we have all the information <span class="math inline">\mathcal{D}_t</span>, up to time t. We want to do a predictions here</p>
<p><span id="eq-NDLM-forecasting-distribution"><span class="math display">
\mathbb{P}r(y_{t+h} \mid \mathcal{D}_t) \qquad \text{forecasting distribution}
\tag{92.8}</span></span></p>
<div class="page-columns page-full"><p> Another important quantity or an important set of distributions is what we call the smoothing distribution. Usually, you have a time series, when you get your data, you observe, I don’t know, 300 data points. As you go with the filtering, you are going to start from zero all the way to 300 and you’re going to update these filtering distributions as you go and move forward. <mark>We may want instead to revisit the parameter at time 10, for example, given that you now have observed all these 300 observations. In that case, you’re interested in densities that are of the form. Let’s say that you observe capital T in your process and now you are going to revisit that density for <span class="math inline">\theta_t</span>. This is now in the past. Here we assume that <span class="math inline">t&lt;T</span>. This is called <strong>smoothing</strong>.</mark></p><div class="no-row-height column-margin column-container"><span class="">Smoothing Distribution</span></div></div>
<p>So you have more observation once you have seen the data. We will talk about how to perform Bayesian inference to obtain all these distributions under this model setting.</p>
<p><span id="eq-NDLM-smoothing-distribution"><span class="math display">
\mathbb{P}r(\theta_t \mid \mathcal{D}_T)  \qquad t &lt; T \qquad \text{smoothing distribution}
\tag{92.9}</span></span></p>
</section>
<section id="the-forecast-function-for-the-ndlm" class="level3" data-number="92.1.5">
<h3 data-number="92.1.5" class="anchored" data-anchor-id="the-forecast-function-for-the-ndlm"><span class="header-section-number">92.1.5</span> The forecast function for the NDLM</h3>
<p>In addition to all the structure that we described before and all the densities that we are interested in finding, we also have as usual, the so-called <em>forecast function</em>, which instead of being the density is just <span class="math inline">\mathbb{E}[y(t+h)\mid \mathcal{D}_t]</span> i.e.&nbsp;expected value of y at time t given all the information we have before time t.</p>
<p><span id="eq-forecast-function-for-NDLM"><span class="math display">
\mathbb{E}[y(t+h)\mid \mathcal(D_t)] = F'_{t+h} G_{t+h} \ldots G_{t+1} \mathbb{E}[\theta_t \mid \mathcal{D}_t]
\tag{92.10}</span></span></p>
<p>This is the form of the forecast function.</p>
<p>There are particular cases and particular models that we will be discussing in which the <span class="math inline">F_t=F</span>, i.e.&nbsp;constant and also <span class="math inline">G_t = G</span> is also constant for all t. In these cases, the forecast function can be simplified and written as:</p>
<p><span id="eq-forecast-function-for-NDLM-constant-F-G"><span class="math display">
f_t(h) = \mathbb{E}(y_{t+h} \mid D_t) = F'G^h \mathbb{E}(\theta_t \mid \mathcal{D}_t)
\tag{92.11}</span></span></p>
<p>One thing that we will learn is that the eigen-structure of this matrix is very important to define the form of the forecast function, and it’s very important for model building and for adding components into your model.</p>
</section>
<section id="ndlm-short-form-notation" class="level3" data-number="92.1.6">
<h3 data-number="92.1.6" class="anchored" data-anchor-id="ndlm-short-form-notation"><span class="header-section-number">92.1.6</span> NDLM short form notation</h3>
<p>Finally, just in terms of short notation, we can always write down when we’re working with normal dynamic linear models, we may be referring to the model instead of writing the two equations, the system and the observation equation. I can just write all the components that define my model. <span id="eq-NDLM-model-shothand-notation"><span class="math display">
\{F_t, G_t, v_t, W_t\}
\tag{92.12}</span></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/mH7Ol/ndlm-definition -->
<p>In this part of the course, I will discuss the class of normal dynamic linear models for analyzing and forecasting non-stationary time series. We will talk about Bayesian inference and forecasting within this class of models and describe model building as well.</p>
<section id="motivating-example" class="level3" data-number="92.1.7">
<h3 data-number="92.1.7" class="anchored" data-anchor-id="motivating-example"><span class="header-section-number">92.1.7</span> Motivating example</h3>
<p>I want to begin first with a motivating example. Suppose you have a model that is very simple and has no temporal structure here, just a model that looks like this. You have your time series <span class="math inline">y_t</span>. Then you’re interested in just thinking about what is the mean level of that time series. That mean level, I’m going to call it <span class="math inline">\mu</span> and then I have some noise and the noise is normally distributed. They are all independent, identically distributed normal random variables <span class="math inline">\Normal(0,v)</span>. Again, I can think of my time series. Suppose that I have my time series here, and then I’m plotting y_t. Then I have something that looks like this. In this model that <span class="math inline">\mu</span> is going to try to get the mean of that time series, this expected value of y_t, which is <span class="math inline">\mu</span>. The variance here of <span class="math inline">y_t</span> is <span class="math inline">v</span> under this model. What may happen in practice again, this model has no temporal structure, I may want to incorporate some temporal structure that says, well, I think that the level of this, the expected value of this time series, should be changing over time. If you were to do that, you will write down a model where the <span class="math inline">mu</span> changes over time, so it’s indexed in time. Then you have still your same noise here. Let’s again assume <span class="math inline">\Normal(0,v)</span>. I have now to make a decision on how I’m going to incorporate temporal structure by modeling the changes over time in this parameter <span class="math inline">\mu_t</span>. You could consider different options.</p>
<p>The simplest possible, probably that you can consider is something that looks like this. You have that random walk type of structure where <span class="math inline">\mu_t</span> is now going to be written as <span class="math inline">\mu_{t-1}</span>. The expected value of <span class="math inline">\mu_t</span>, you’ll think of it as <span class="math inline">\mu_{t-1}</span> plus some noise. That error here is going to be again, assume normally distributed random variable centered at zero and with variance w. There is another assumption that we can make here and is that the nu t and omega t here, are also independent of each other. When I have this model, what am assuming here is that the mean level of the series is changing over time.</p>
<p>These type of models have a few characteristics. This is an example of a normal dynamic linear model, as we will see later. In this models, we usually have a few things.</p>
<p>The first thing is we have two equations. One is the so-called observation equation that is relating your <span class="math inline">y_t</span>, your observed process to some parameters in the model that are changing over time. The next equation is the so-called system level equation or evolution equation that tells me how that time varying parameter is going to be changing over time. The other thing you may notice is that we have a linear structure both in the observational level and in the system level. The linear structure, in the sense of the expected value of <span class="math inline">y_t</span> is just a linear function of that <span class="math inline">\mu_t</span>. It happens to be <span class="math inline">\mu_t</span> in this particular case. In the second level, I can think of the expected value of <span class="math inline">\mu_t</span> as a linear function given <span class="math inline">\mu_{t-1}</span>, so it’s a function that is linear on <span class="math inline">\mu_{t-1}</span>. There is that linear structure. The other thing that we have here is at both levels, we have the assumption of normality for the noise terms in those equations. This is an example of a Gaussian or normal dynamic. These are time-varying parameters linear model. We will be discussing the general class of models. This is just an example. We will also discuss how to build different structures into the model, as well as how to perform Bayesian inference and forecasting.</p>
</section>
<section id="general-form-of-the-model" class="level3" data-number="92.1.8">
<h3 data-number="92.1.8" class="anchored" data-anchor-id="general-form-of-the-model"><span class="header-section-number">92.1.8</span> General form of the model</h3>
<p>The general class of dynamic linear models can be written as follows. Again, we are going to have two equations. One is the so-called observation equation that relates the observations to the parameters in the model, and the notation we are going to use is as follows. Here, my observations are univariate. We are discussing models for univariate time series. I have that related to a vector of parameters, Theta_t plus some noise here. This is the noise. The noise are assumed to be independent, identically distributed normal random variables, 0, <span class="math inline">V_t</span>. Then I have another equation which is a system equation that has this form. There is a general G_t matrix. This is going to be depending on <span class="math inline">\theta_{t-1}</span>. This is a vector, and then I have again, these are iid multivariate <span class="math inline">\Normal(0, W_t)</span>. This is the observation equation. This is the system equation or evolution equation. This defines a normal dynamic linear model. Here, we are going to say that <span class="math inline">F_t</span> is a vector. The dimension of the vector is going to be the same as the number of parameters in the model. Let’s say we have k. This is a vector of known values. For each t, we are going to assume that we know what that vector is. Then we have the vector of parameters here is also of dimension k of parameters. The G is the next thing we need to define is a known matrix. That one is also assumed to be known, and then I have <span class="math inline">V_t</span> is variance at the observational level. The W_t we are going to assume at the beginning that these two quantities are also known for all the values t. This is the variance-covariance matrix at the system level. Again, if we think about these two equations, we have the model defined in this way.</p>
<p>There is a next piece that we need to consider if we are going to perform based in inference for the model parameters. The next piece that we need to consider to just fully specify the model is what is the prior distribution. In a normal dynamic linear model, the prior distribution is assumed to be conjugate here. In the case again in which <span class="math inline">V_t</span> and <span class="math inline">W_t</span> are known, we are going to be assuming that, say that zero, the parameter vector before observing any data is going to be normally distributed Multivariate normal with <span class="math inline">M_0</span> and <span class="math inline">C_0</span>. The mean is a vector, again of the same dimension as <span class="math inline">\theta_0</span>. Then I have k by k covariance matrix there as well. These are assumed to be also given to move forward with the model.</p>
</section>
<section id="inference-forcasting-smoothing-and-filtering." class="level3" data-number="92.1.9">
<h3 data-number="92.1.9" class="anchored" data-anchor-id="inference-forcasting-smoothing-and-filtering."><span class="header-section-number">92.1.9</span> Inference, forcasting, smoothing, and filtering.</h3>
<p>In terms of the inference, there are different kinds of densities and quantities that we are interested in. One distribution that we are interested in finding is the so-called <strong>filtering distribution</strong>. We may be interested here in finding what is the density of <span class="math inline">\theta_{t}</span> given all the observations that we have up to time <span class="math inline">t</span>. I’m going to call and all the information that I have up to time t. I’m going to call that <span class="math inline">D_t</span> . It can also be, in some cases, I will just write down. So <span class="math inline">D_t</span>, you can view with all the info up to time <span class="math inline">t</span>. Usually, it is all the information I have at time zero. Then coupled, if there is no additional information that’s going to be coupled with all the data points I have up to that time. Here I’m conditioning on all the observed quantities and the prior information up to time t, and I may be interested in just finding what is the distribution for <span class="math inline">\theta_{t}</span>.</p>
<p>This is called filtering. Another quantity that is very important in time series analysis is forecasting.</p>
<p>I may be interested in just what is the <strong>density</strong>, the distribution of <span class="math inline">y_{t+h}</span> ? Again, the number of steps ahead here, here I’m thinking of <span class="math inline">h</span>, given that I have all this information up to time <span class="math inline">t</span>. I’m interested in <strong>predictions</strong> here. We will be talking about <strong>forecasting</strong>. Then another important quantity or an important set of distributions is what we call the <strong>smoothing distribution</strong>. Usually, you have a time series, when you get your data, you observe, I don’t know, 300 data points. As you go with the filtering, you are going to start from zero all the way to 300 and you’re going to update these filtering distributions as you go and move forward. But then you may want to revisit your parameter at time 10, for example, given that you now have observed all these 300 observations. In that case, you’re interested in densities that are of the form. Let’s say that you observe capital <span class="math inline">T</span> in your process and now you are going to revisit that density for <span class="math inline">\theta_t</span>. This is now in the past. Here we assume that t is smaller than capital T. This is called smoothing. So you have more observation once you have seen the data. We will talk about how to perform Bayesian inference to obtain all these distributions under this model setting.</p>
</section>
<section id="the-forecast-function" class="level3 unumbered" data-number="92.1.10">
<h3 class="unumbered anchored" data-number="92.1.10" data-anchor-id="the-forecast-function"><span class="header-section-number">92.1.10</span> The forecast function</h3>
<p>In addition to all the structure that we described before and all the densities that we are interested in finding, we also have as usual, the so-called forecast function, which is just instead of being the density is just expected value of <span class="math inline">y(t+h)</span> given all the information I have up to time t. In the case of a general normal dynamic linear model, we have the structure for these just using the equations, the observation and the system of equations.</p>
<p>We’re going to have here <span class="math inline">G_{t+h}</span>. We multiply all these all the way to <span class="math inline">G_(t+1)</span>, and then we have the <span class="math inline">\mathbb{E}[\theta_{t}\mid D_t]</span>. This is the form of the <strong>forecast function</strong>. There are particular cases and particular models that we will be discussing in which the <span class="math inline">F_t</span> is equal to <span class="math inline">F</span>, so is constant for all <span class="math inline">t</span> and <span class="math inline">G_t</span> is also constant for all <span class="math inline">t</span>. In those cases, the forecast function can be simplified and written as <span class="math inline">F'G^h</span> expected value. One thing that we will learn is that the eigen-structure of this matrix is very important to define the form of the forecast function, and it’s very important for model building and for adding components into your model.</p>
</section>
<section id="short-form-notation" class="level3 unumbered" data-number="92.1.11">
<h3 class="unumbered anchored" data-number="92.1.11" data-anchor-id="short-form-notation"><span class="header-section-number">92.1.11</span> Short-form notation</h3>
<p>Finally, just in terms of short notation, we can always write down when we’re working with normal dynamic linear models, we may be referring to the model instead of writing the two equations, the system and the observation equation. I can just write all the components that define my model. This fully specifies the model in terms of the two equations. If I know what Ft is, what Gt is, what Vt is, and the covariance at the system level. I sometimes will be just talking about a short notation like this for defining the model.</p>
</section>
</div>
</div>
</div>
</section>
</section>
<section id="polynomial-trend-models-video" class="level2 page-columns page-full" data-number="92.2">
<h2 data-number="92.2" class="anchored" data-anchor-id="polynomial-trend-models-video"><span class="header-section-number">92.2</span> Polynomial Trend Models (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0011.png" class="lightbox" data-gallery="slides" title="first and second order polynomial model"><img src="images/m3_0011.png" class="img-fluid figure-img" width="200" alt="first and second order polynomial model"></a></p>
<figcaption>first and second order polynomial model</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0012.png" class="lightbox" data-gallery="slides" title="p-order polynomial model"><img src="images/m3_0012.png" class="img-fluid figure-img" width="200" alt="p-order polynomial model"></a></p>
<figcaption>p-order polynomial model</figcaption>
</figure>
</div></div>
<p>While we haven’t talked about the superposition principle yet we start at looking at adding different components to the DLM.</p>
<p>We might :</p>
<ul>
<li>setting a baseline mean and variance</li>
<li>adding a random walk with its variance</li>
<li>add a trend</li>
<li>add a regression</li>
<li>add seasonality</li>
</ul>
<p>Next we want to extend the random walk model to include different types of trends and this will be covered by the polynomial trend models. <mark>These are models that are useful to model linear trends or polynomial trends in your time series. So if you have a data set, where you have an increasing trend, or a decreasing trend, you would use one of those <strong>components</strong> in your model.</mark> Also</p>
<section id="first-order-polynomial-model" class="level3" data-number="92.2.1">
<h3 data-number="92.2.1" class="anchored" data-anchor-id="first-order-polynomial-model"><span class="header-section-number">92.2.1</span> First order polynomial model</h3>
<p>The first order model is developed at great detail in chapter In <span class="citation" data-cites="west2013bayesian">(<a href="#ref-west2013bayesian" role="doc-biblioref">West and Harrison 2013</a> ch.&nbsp;2)</span>. I don’t know what to make of it, isn’t this a trivial white noise model?</p>
<p>The math for Bayesian updating is fairly straight forward and must be much more complex with more sophisticated dynamics. So this is used by the authors to introduce their DLM and an 30 pages of the book is dedicated to in depth analysis and Bayesian development of this specific model and different distribution of interests as well as including comparison to other models and a look at the signal to noise ratio in the model.</p>
<p>It is worthwhile pointing out that these models get their name from their forecast function which will takes the general form <a href="#eq-DLM-n-order-polynomial-forecast-function" class="quarto-xref">Equation&nbsp;<span>92.22</span></a></p>
<p>The first order polynomial model is a model that is useful to describe linear trends in your time series. If you have a data set where you have an increasing trend or a decreasing trend, you would use one of those components in your model.</p>
<p>So the all those can be incorporated using the general p-order polynomial model, so I will just describe the form of this model.</p>
<p>A first order polynomial is of the form <span class="math inline">Ax+B</span> where A is the slope and B is the intercept. This is the same random walk model we saw above.</p>
<p><span id="eq-DLM-first-order-polynomial-model"><span class="math display">
\begin{aligned}
y_t &amp;= \theta_t + \nu_t, \qquad &amp; \nu_t &amp; \overset{\text{iid}}{\sim}  \mathcal{N}(0, V_t) \\
\theta_t &amp;= \theta_{t-1} + \omega_t, \qquad &amp; \omega_t &amp; \overset{\text{iid}}{\sim}  \mathcal{N}(0, W_t) \\
&amp;\{1,1,v_t,W_t\} &amp;&amp; \text{(short form)}\\
f_t(h) &amp;= \mathbb{E}[\theta_t \mid \mathcal{D}_t] &amp;&amp; \text{(forecast fn)}\\
\end{aligned}
\tag{92.13}</span></span></p>
<p>In the observation equation, <span class="math inline">\theta_{t}</span> is the level of the series at time t and <span class="math inline">\nu_t</span> is the observation error. In the evolution equation we see the mean for this parameter changing over time as a random walk or a local constant mean with evolution noise <span class="math inline">\omega_t</span>.</p>
<p><span class="citation" data-cites="west2013bayesian">(<a href="#ref-west2013bayesian" role="doc-biblioref">West and Harrison 2013, sec. 2.1</a>)</span> gives the following representation of the model:</p>
<p>It is useful to think of <span class="math inline">\theta_t</span> as a smooth function of time <span class="math inline">\theta(t)</span> with an associated Taylor series representation</p>
<p><span id="eq-DLM-taylor-series-representation"><span class="math display">
\theta(t + \delta t) = \theta(t) + \text{higher-order terms}
\tag{92.14}</span></span></p>
<p>where the higher-order terms are assumed to be zero-mean noise. This is a very important point, because it means that we are not trying to model the higher-order terms explicitly, but rather we are assuming that they are just noise.</p>
<p>with the model simply describing the higher-order terms as zero-mean noise.</p>
<p>This is the genesis of the first-order polynomial DLM: the level model is a locally constant (first-order polynomial) proxy for the underlying evolution.·</p>
<p>We can write it down in short form with the following quadruple/</p>
<p><span id="eq-DLM-1-order-polynomial-model-short"><span class="math display">
\{1, 1, V_t, W_t\} \qquad f_t(h) = \mathbb{E}[\theta_t \mid \mathcal{D}_t] = k_t \ \forall   h&gt;0
\tag{92.15}</span></span></p>
<p>Next we can write the forecast function <span class="math inline">f_t(h)</span> of this model using the representation we gave in <a href="#eq-DLM-1-order-polynomial-model-short" class="quarto-xref">Equation&nbsp;<span>92.15</span></a>.</p>
<p>Again, we’re going to have something of the form <span class="math inline">F</span> transposed <span class="math inline">G</span> to the power of h and then the expected value of that <span class="math inline">\theta_t</span> given <span class="math inline">\mathcal{D}_t</span>. <span class="math inline">F</span> is 1, <span class="math inline">G</span> is 1, therefore I’m going to end up having just expected value of <span class="math inline">\theta_t</span> given <span class="math inline">\mathcal{D}_t</span>.</p>
<p>Which depending on the data that you have is you’re just going to have something that is a value that depends on <span class="math inline">t</span> and it doesn’t depend on <span class="math inline">h</span>. What this model is telling you is that the forecast function, how you expect to see future values of the series h steps ahead is something that looks like the level that you estimated at time <span class="math inline">t</span>.</p>
</section>
<section id="second-order-polynomial-model-aka-linear-growth-model" class="level3" data-number="92.2.2">
<h3 data-number="92.2.2" class="anchored" data-anchor-id="second-order-polynomial-model-aka-linear-growth-model"><span class="header-section-number">92.2.2</span> Second order Polynomial model AKA Linear Growth model</h3>
<p><span class="citation" data-cites="west2013bayesian">(<a href="#ref-west2013bayesian" role="doc-biblioref">West and Harrison 2013, secs. 7.1–7.2</a>)</span> gives a detailed analysis of this model.</p>
<p>Now we want to create a model in which captures things that has a linear trend either increasing or decreasing. To do thus we need to have two components in our parameter vector of the state vector. For this we will need two components in our parameter vector of the <strong>state vector</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>So we have again something that looks like in my observation equation. I’m going to have, I’m going to call it say <span class="math inline">\theta_{t,1} \sim  \mathcal{N}(v_t)</span>, and then I’m going to have say <span class="math inline">\theta_{t,1}</span> is going to be of the form to <span class="math inline">\theta_{t-1,1}</span> and there is another component here. The other component enters this equation plus let’s call this <span class="math inline">\theta_{t-1,2}</span>. And then I have finally also I need an evolution for the second component of the process which is going to be again having a random walk type of behavior.</p>
<p><span id="eq-NDLM-2-order-polynomial-model"><span class="math display">\begin{aligned}
  y_t &amp;= \theta_{t,1} + \nu_t \quad &amp;\nu_t &amp;\overset{\text{iid}}{\sim}  \mathcal{N}(0, v_t) \\
  \theta_{t,1} &amp;= \theta_{t-1,1} + \theta_{t-1,2} + \omega_{t,1} \qquad &amp;\omega_{t,1} &amp;\overset{\text{iid}}{\sim}  \mathcal{N}(0, w_{t,11}) \\
  \theta_{t,2} &amp;= \theta_{t-1,2} + \omega_{t,2} \qquad &amp;\omega_{t,2} &amp;\overset{\text{iid}}{\sim}  \mathcal{N}(0, w_{t,22})
\end{aligned}
\tag{92.16}</span></span></p>
<p>So there are different ways in which you can interpret this two parameters but essentially:</p>
<ul>
<li><span class="math inline">\theta_{t-1,1}</span> is related to the <strong>baseline</strong> level of the series</li>
<li><span class="math inline">\theta_{t-1,2}</span> is related to the <strong>rate of change</strong> of the of the series.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Short form DLM notation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Having the short form notation makes the model easier to understand in relation to other DLM models.</li>
<li>It will soon be instrumental in communicating the model structure with different software packages.</li>
</ul>
</div>
</div>
</div>
<p>Next we should summarize this model using the familiar short form DLM representation, which requires a bit of creative algebra.</p>
<p><span class="math display">
\mathbf{\theta}_t = (\theta_{t,1}, \theta_{t,2}) \qquad \{\mathbf{F}, \mathbf{G}, V_t, \mathbf{W}_t\}
</span></p>
<p>First we collect the two variances for the evolution two components into the vector <span class="math inline">\utilde{w}_t</span> and then assume that this <span class="math inline">w_t</span> is Normal. Now this is a bi-variate normal.</p>
<p><span class="math display">
\utilde{\omega}_t = (\omega_{t,1},\omega_{t,2})' \qquad \utilde{\omega}_t \sim  \mathcal{N}(0,W_t)
</span></p>
<p>So what would be my <span class="math inline">F</span> and my <span class="math inline">G</span> in this model? So again my theta vector has two components, thus my <span class="math inline">G</span>, so my <span class="math inline">F</span> is going to be a two dimensional. We can write down <span class="math inline">F</span> transposed as the only component that appears at this level is the first component of the vector. I’m going to have 1 and then a zero for <span class="math inline">F</span> transposed. c.f. <a href="#eq-DLM-2-order-polynomial-model-short-form-simple" class="quarto-xref">Equation&nbsp;<span>92.17</span></a> And then my <span class="math inline">G</span> here if you think about writing down <span class="math inline">\theta_t</span> times <span class="math inline">G</span> say the <span class="math inline">t-1 + \omega_t</span>. Then you have that you’re <span class="math inline">G</span> is going to have this form.</p>
<p><span id="eq-DLM-2-order-polynomial-model-short-form-simple"><span class="math display">
\begin{aligned}
\mathbf{F} &amp;= (1,0)' &amp; V_t &amp;= v_t \\
\mathbf{G} &amp;= \begin{pmatrix} 1 &amp; h \\ 0 &amp; 1 \end{pmatrix}
&amp; \mathbf{W}_t &amp;= \begin{pmatrix} w_{t,11} &amp; 0 \\ 0 &amp; w_{t,22} \end{pmatrix}
\end{aligned}
\tag{92.17}</span></span></p>
<p>this is the form from the video <span id="eq-DLM-2-order-polynomial-model-short-form-complex"><span class="math display">
\begin{aligned}
\mathbf{F} &amp;= (1,0)' &amp; V_t &amp;= v_t \\
\mathbf{G} &amp;= \begin{pmatrix} 1 &amp; h \\ 0 &amp; 1 \end{pmatrix}
&amp; \mathbf{W}_t &amp;= \begin{pmatrix} w_{t,11} &amp; w_{t,12} \\ w_{t,21} &amp; w_{t,22} \end{pmatrix}
\end{aligned}
\tag{92.18}</span></span></p>
<p>this is the more general form from the handout. Note that in this case we have <span class="math inline">w_{t,12}=w_{t,21}</span> so there is just one extra parameter.</p>
<p>The lesson videos and the handouts differ in the form <span class="math inline">\mathbf{W}_t</span>. In the lecture we assumed zero covariance but in the handout the covariance was snuck in. This gives us a slightly more general model. The covariance though is symmetric so we get an extra parameter we need to infer and include in the prior. Anyhow I kept the more general form, though in most cases we will keep the off diagonal terms at zero.</p>
<p>So for the first component, I have past values of both components. That’s why I have a 1 and 1 here for the second component I only have the past value of the second component. So there is a zero and a 1. So this tells me what is the structure of this second order polynomial. If I think about how to obtain the forecast function for this second order polynomial is going to be very similar to what we did before. So you can write it down as F transposed G to the power of h, expected value of theta t given Dt. Now the expected value is going to be vector also with two components because theta_t is a two dimensional vector. The structure here if you look at what G is G to the power of h going to be a matrix, that is going to look like 1, h, 0 1. When you multiply that matrix time this times this F what you’re going to end up having is something that looks like 1 h times this expected value of theta t given Dt. So I can think of two components here, so this gives you a constant on h, this part is not going to depend on h. So I can write this down as k t 11 component multiplied by 1 and then I have another constant, multiplied by h. So you can see what happens now is that your forecast function has the form of a linear polynomial. So it’s just a linear function on the number of steps ahead. The slope and the intercept related to that linear function are going to depend on the expected value of, theta_t given the all the information I have up to time t. But essentially is a way to model linear trends. So this is what happens with the second order polynomial model.</p>
<p>As we included linear trends and constant values in the forecast function, we may want to also incorporate other kinds of trends, polynomial trends in the model. So you may want to have a quadratic form, the forecast function or a cubic forecast function as a function of h.</p>
<p><span class="math display">
\theta_t = (\theta_{t,1}, \theta_{t,2})' \qquad \mathbf{G} = \mathbf{J}_2(1) \qquad \mathbf{E}_2 = (1, 0)'
</span></p>
<p><span class="math display">
\mathbf{G^h} = \begin{pmatrix} 1 &amp; h \\ 0 &amp; 1 \end{pmatrix}
</span></p>
<p><span id="eq-second-order-poly-prediction-fn"><span class="math display">
\begin{aligned}
f_t(h) &amp;= F' G^h \mathbb{E}[\theta_t \mid \mathcal{D}_t] \\
&amp;= (1,h) \mathbb{E}[\theta_{t}\mid D_t] \\
&amp;= (1,h)(K_{t,0}, K_{t,1})' \\
&amp;= (K_{t,0} + K_{t,1} h)
\end{aligned}
\tag{92.19}</span></span></p>
<p><span id="eq-second-order-poly-Gh"><span class="math display">
\begin{aligned}
\mathbf{G^h} &amp;= \begin{pmatrix} 1 &amp; h \\ 0 &amp; 1 \end{pmatrix}
\end{aligned}
\tag{92.20}</span></span></p>
</section>
<section id="general-p-th-order-polynomial-model" class="level3" data-number="92.2.3">
<h3 data-number="92.2.3" class="anchored" data-anchor-id="general-p-th-order-polynomial-model"><span class="header-section-number">92.2.3</span> General p-th order polynomial model</h3>
<p>We can consider a so called p-th order polynomial model. This model will have a state-space vector of dimension p and a polynomial of order <span class="math inline">p − 1</span> forecast function on <span class="math inline">h</span>. The model can be written as</p>
<p><span class="math display">
\{E_p, J_p(1), v_t, W_t\}
</span></p>
<p>with <span class="math inline">F_t = E_p = (1, 0, \ldots, 0)′</span> and <span class="math inline">G_t = J_p(1)</span>, with</p>
<p><span id="eq-Jordan-form-dynamics"><span class="math display">
J_p(1) = \begin{pmatrix}
1 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0  \\
0 &amp; 1 &amp; 1 &amp; \cdots &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1
\end{pmatrix}
\tag{92.21}</span></span></p>
<p>The forecast function is given by <span id="eq-DLM-n-order-polynomial-forecast-function"><span class="math display">
f_t(k) = a_{t_0} +  a_{t_1}k + \ldots + a_{t_{n-1}} k^{n-1} \qquad k \in \mathbb{N}
\tag{92.22}</span></span></p>
<p>where <span class="math inline">a_{t_i}</span> are the coefficients of the polynomial and <span class="math inline">k</span> is the number of steps ahead we need in our forecast. There is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function given by <span class="math inline">\{Ep, Lp, vt, W t\}</span>, with</p>
<p><span id="eq-triangular-form-dynamics"><span class="math display">
L_p = \begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 &amp; 1  \\
0 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 &amp; 1   \\
0 &amp; 0 &amp; 1 &amp; \cdots &amp; 1 &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1
\end{pmatrix}
\tag{92.23}</span></span></p>
<p>And in this type of model, the forecast function is going to have order <span class="math inline">p-1</span>. So the parameter vector is going to have dimension <span class="math inline">p</span>. So you’re going to have <span class="math inline">\theta_t =  \theta_{t1:p}</span>.</p>
<p>The observation operator <span class="math inline">F</span> is just a constant and if we write it as a row vector we get <span class="math inline">F'</span> as a p-dimensional vector with the one in the first entry and zeros everywhere else.</p>
<p>The dynamics matrix <span class="math inline">G</span> may be written using either a <span class="math inline">J</span> Jordan form <a href="#eq-Jordan-form-dynamics" class="quarto-xref">Equation&nbsp;<span>92.21</span></a> or as a triangular form <a href="#eq-triangular-form-dynamics" class="quarto-xref">Equation&nbsp;<span>92.23</span></a>. These result in different parameterization of this model and we will talk a little bit about this.</p>
<p>In the <a href="#eq-Jordan-form-dynamics" class="quarto-xref">Equation&nbsp;<span>92.21</span></a> we have a matrix with ones on the diagonal and the super diagonal, the matrix is needs to be <span class="math inline">p \times p</span> i.e.&nbsp;with dimension <span class="math inline">p</span> to be compatible with the dimension of the hidden state vector <span class="math inline">\theta</span>. So this matrix <span class="math inline">G</span> is what we call a Jordan block of dimension p of 1. So here 1 is the number that appears in the diagonal. And then I have a p <span class="math inline">I_p</span> matrix, I have ones in the upper diagonal part. So this is the form of the model, so once again I have the <span class="math inline">F</span> the <span class="math inline">G</span>, and the <span class="math inline">W_t</span>. I have my model.</p>
<p>The forecast function in this case again can be written as <span class="math inline">F' G^h \mathbb{E}[\theta_t \mid \mathcal{D}_t]</span>. And when you simplify times expected value of <span class="math inline">\theta_t</span>, given <span class="math inline">D_t</span>. Once you simplify those functions you get something that is a polynomial of order <span class="math inline">p-1</span> in <span class="math inline">h</span>. So I just can write this down as <span class="math inline">k_t + k_{t,1} h + k_{t, p-1} h^{p-1}</span>, so that’s my forecast function.</p>
<p>There is an alternative parameterization of this model that has the same <span class="math inline">F</span> and the same algebraic form of the forecast function, the same form of the forecast function. But instead of using <a href="#eq-Jordan-form-dynamics" class="quarto-xref">Equation&nbsp;<span>92.21</span></a> form of the <span class="math inline">G</span> matrix, it has a <a href="#eq-triangular-form-dynamics" class="quarto-xref">Equation&nbsp;<span>92.23</span></a> form that has ones in the diagonal and ones everywhere above the diagonal. So it’s an upper triangular matrix with ones in the diagonal and above the diagonal. That’s a different parameterization of the same model but it leads to the same general form of the forecast function just with a different parameterization.</p>
<p>So again, we can consider the way you think about these models?</p>
<ul>
<li>What is you think what kind of forecast function makes sense here ?</li>
<li>What is the type of predictions that I expect to have in my model?</li>
<li>If they look like a linear trend, I use a second order polynomial.</li>
<li>If it looks like a quadratic trend in the forecast then I would use 3rd order polynomial model representation.</li>
</ul>
<p>Note that the third order polynomial model is covered in</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/653eR/polynomial-trend-models -->
<section id="first-order-polynomial-models" class="level3" data-number="92.2.4">
<h3 data-number="92.2.4" class="anchored" data-anchor-id="first-order-polynomial-models"><span class="header-section-number">92.2.4</span> First order Polynomial Models</h3>
<p>I will begin describing the structure of a particular class of models now, the polynomial trend models. These are models that are useful to describe linear trends or polynomial trends in your time series. So if you have a data set, where you have an increasing trend, or a decreasing trend, you would use one of those components in your model.</p>
<p>We will begin with the first order polynomial model, which we have already described. It’s the one that has <span class="math inline">y_t</span> is a single parameter, I’m going to call it just <span class="math inline">\theta_t + \nu_t</span>. And then a random walk evolution for that single parameter, so that’s the mean level of the series. And then we assume that it changes as a random walk, so this is the first order polynomial model.</p>
<p>So in general, I’m going to begin with the first order polynomial model, which we have already described. It’s the one that has <span class="math inline">y_t</span> is a single parameter, I’m going to call it just <span class="math inline">\theta_t + \nu_t</span>. And then a random walk evolution for that single parameters, so that’s the mean level of the series. And then we assume that it changes As a random walk, so this is the first order polynomial model. In this model if I want to write it down in short form I would have a quadruple that looks like this. So the <span class="math inline">F</span> here that goes <span class="math inline">F</span> transposed times the parameter vector in this case we have a scalar vector, scalar parameter. It’s going to be 1 my <span class="math inline">G</span> that goes next to the state of <span class="math inline">t-1</span> is going to also be 1. And then I have vt and Wt here. So this fully defines my model if I think about the forecast function of this model using the representation we had before. Again, we’re going to have something of the form <span class="math inline">F'G^h</span> and then the expected value of that <span class="math inline">\theta_t | \mathcal{D}_t</span>. <span class="math inline">F</span> is 1, <span class="math inline">G</span> is 1, therefore I’m going to end up having just expected value of <span class="math inline">\theta_t | \mathcal{D}_t</span>. Which depending on the data that you have is you’re just going to have something that is a value that depends on t and it doesn’t depend on <span class="math inline">h</span>.</p>
<p>What this model is telling you is that the forecast function, how you expect to see future values of the series h steps ahead is something that looks like the level that you estimated at time t.</p>
<p>So that’s the forecast function, you is a first order is a zero order polynomial is a constant on h and it’s called the first order polynomial model.</p>
</section>
<section id="second-order-polynomial-models" class="level3" data-number="92.2.5">
<h3 data-number="92.2.5" class="anchored" data-anchor-id="second-order-polynomial-models"><span class="header-section-number">92.2.5</span> Second order Polynomial Models</h3>
<p>In the case of a second order polynomial We are going to now think about about a model in which we want to capture things that are not a constant over time but may have an increasing or decreasing linear trend. In this case we’re going to need two components in your parameter vector in the state vector.</p>
<p>So we have again something that looks like in my observation equation. I’m going to have, I’m going to call it say theta{t,1} Normal vt, and then I’m going to have say theta_{t,1} is going to be of the form to theta_{t-1,1} and there is another component here. The other component enters this equation plus let’s call this And then I have finally also I need an evolution for the second component of the process which is going to be again having a random walk type of behavior. So there is different ways in which you can interpret this two parameters but essentially one of them is related to the baseline level of the series the other one is related to the rate of change of the of the series. So if you think about the dlm representation again, these two components, I can collect into the vector wt. and then assume that this wt Is normal. Now this is a bivariate normal. So what would be my F and my G in this model? So again my theta vector has two components My G, so my F is going to be a two dimensional vectors. So I can write down F transposed as the only component that appears at this level is the first component of the vector. I’m going to have 1 and then a zero for F transposed. And then my G here if you think about writing down theta t times G say the t -1 +wt. Then you have that you’re G is going to have this form. So for the first component, I have past values of both components. That’s why I have a 1 and 1 here for the second component I only have the past value of the second component. So there is a zero and a 1. So this tells me what is the structure of this second order polynomial. If I think about how to obtain the forecast function for this second order polynomial is going to be very similar to what we did before. So you can write it down as F transposed G to the power of h, expected value of theta t given Dt. Now the expected value is going to be vector also with two components because theta_t is a two dimensional vector. The structure here if you look at what G is G to the power of h going to be a matrix, that is going to look like 1, h, 0 1. When you multiply that matrix time this times this F what you’re going to end up having is something that looks like 1 h times this expected value of theta t given Dt. So I can think of two components here, so this gives you a constant on h, this part is not going to depend on h. So I can write this down as k t 11 component multiplied by 1 and then I have another constant, multiplied by h. So you can see what happens now is that your forecast function has the form of a linear polynomial. So it’s just a linear function on the number of steps ahead. The slope and the intercept related to that linear function are going to depend on the expected value of, theta_t given the all the information I have up to time t. But essentially is a way to model linear trends. So this is what happens with the second order polynomial model.</p>
</section>
<section id="p-th-order-polynomial-models" class="level3" data-number="92.2.6">
<h3 data-number="92.2.6" class="anchored" data-anchor-id="p-th-order-polynomial-models"><span class="header-section-number">92.2.6</span> P-th Order Polynomial Models</h3>
<p>As we included linear trends and constant values in the forecast function, we may want to also incorporate other kinds of trends, polynomial trends in the model. So you may want to have a quadratic form, the forecast function or a cubic forecast function as a function of h. So the all those can be incorporated using the general p-order polynomial model, so I will just describe the form of this model. And in this type of model, the forecast function is going to have order p-1. So your parameter vector is going to have dimension p.&nbsp;So you’re going to have theta_t theta t1 to tp. Your F matrix is going to be constant if I write it as a row vector. F transpose is going to be a p dimensional vector with the one in the first entry and zeros everywhere else. My G matrix is going to have this form and there is different parameterizations of this model and I will talk a little bit about this. But one way to parameterize the model is something that looks like this. So you have ones in the diagonal of the matrix, the matrix is going to be a p by p has to be the dimension of the p compatible with the dimension of the state vector. And then you have zeros’s below the diagonal above that set of ones that are also ones above the diagonal. So this matrix G is what we call a Jordan block of dimension p of 1. So here 1 is the number that appears in the diagonal. And then I have a p Ip matrix, I have ones in the upper diagonal part. So this is the form of the model, so once again I have the F the G, and the wt. I have my model. The forecast function in this case again can be written as F transposed G to the power of h. And when you simplify times expected value of theta_t, given Dt. Once you simplify those functions you get something that is a polynomial of order p-1 in h. So I just can write this down as kt constant.Plus kt1 h + kt p- 1, h to the p -1, so that’s my forecast function. There is an alternative parameterization of this model that has the same F and the same algebraic form of the forecast function, the same form of the forecast function. But instead of having this G matrix, it has a matrix that has ones in the diagonal and ones everywhere above the diagonal. So it’s an upper triangular matrix with ones in the diagonal and above the diagonal. That’s a different parameterization of the same model is going to have the same general form of the forecast function is a different parameterization. So again, you can consider the way you think about these models is you think what kind of forecast function I want to have for my future? What is the type of predictions that I expect to have in my model? And if they look like a linear trend, I use a second order polynomial. If it looks like a quadratic trend in the forecast then I would use 3rd order polynomial model representation.</p>
</section>
</div>
</div>
</div>
</section>
</section>
<section id="summary-of-polynomial-trend-models-reading" class="level2" data-number="92.3">
<h2 data-number="92.3" class="anchored" data-anchor-id="summary-of-polynomial-trend-models-reading"><span class="header-section-number">92.3</span> Summary of polynomial trend models (Reading)</h2>
<section id="polynomial-trend-models" class="level3" data-number="92.3.1">
<h3 data-number="92.3.1" class="anchored" data-anchor-id="polynomial-trend-models"><span class="header-section-number">92.3.1</span> Polynomial Trend Models</h3>
<section id="first-order-polynomial" class="level4" data-number="92.3.1.1">
<h4 data-number="92.3.1.1" class="anchored" data-anchor-id="first-order-polynomial"><span class="header-section-number">92.3.1.1</span> First-Order Polynomial</h4>
<p><span class="math display">
\begin{aligned}
y_t &amp;= \mu_t + \nu_t, \qquad &amp; \nu_t &amp;\sim  \mathcal{N}(0, v_t) \\
\mu_t &amp;= \mu_{t-1} + \omega_t, \qquad &amp; \omega_t &amp;\sim  \mathcal{N}(0, w_t)
\end{aligned}
</span></p>
<p>In this case, we have:</p>
<p><span class="math inline">\theta_t = \mu_t \quad \forall t</span></p>
<p><span class="math display">
F_t = 1 \quad \forall t \qquad G_t = 1 \quad \forall t
</span></p>
<p>resulting in:</p>
<p><span class="math display">
\{1, 1, v_t, w_t\} \qquad \text{(short notation)}
</span></p>
<p>The forecast function is:</p>
<p><span class="math display">
f_t(h) = E(\mu_t \mid \mathcal{D}_t) = k_t, \quad \forall h &gt; 0.
</span></p>
</section>
<section id="second-order-polynomial" class="level4" data-number="92.3.1.2">
<h4 data-number="92.3.1.2" class="anchored" data-anchor-id="second-order-polynomial"><span class="header-section-number">92.3.1.2</span> Second-Order Polynomial</h4>
<p><span class="math display">\begin{aligned}
  y_t &amp;= \theta_{t,1} + \nu_t, \quad &amp;\nu_t &amp;\sim  \mathcal{N}(0, v_t) \\
  \theta_{t,1} &amp;= \theta_{t-1,1} + \theta_{t-1,2} + \omega_{t,1}, \qquad &amp;\omega_{t,1} &amp;\sim  \mathcal{N}(0, w_{t,11}) \\
  \theta_{t,2} &amp;= \theta_{t-1,2} + \omega_{t,2}, \qquad &amp;\omega_{t,2} &amp;\sim  \mathcal{N}(0, w_{t,22}),
\end{aligned}
</span></p>
<p>where we can also have:</p>
<p><span class="math display">
\text{Cov}(\theta_{t,1}, \theta_{t,2} ) = w_{t,12} = w_{t,21}
</span></p>
<p>This can be written as a DLM with the state-space vector <span class="math inline">\theta_t = (\theta_{t,1}, \theta_{t,2})'</span>, and</p>
<p><span class="math display">
\{\mathbf{F}, \mathbf{G}, v_t, \mathbf{W}_t\}  \qquad \text{(short notation)}
</span></p>
<p>with <span class="math inline">\mathbf{F} = (1, 0)'</span> and</p>
<p><span class="math display">
\mathbf{G} =
\begin{pmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}, \quad \mathbf{W}_t =
\begin{pmatrix}
w_{t,11} &amp; w_{t,12} \\
w_{t,21} &amp; w_{t,22}
\end{pmatrix}.
</span></p>
<p>Note that</p>
<p><span class="math display">
\mathbf{G}^2 =
\begin{pmatrix}
1 &amp; 2 \\
0 &amp; 1
\end{pmatrix}, \quad \mathbf{G}^h =
\begin{pmatrix}
1 &amp; h \\
0 &amp; 1
\end{pmatrix},
</span></p>
<p>and so:</p>
<p><span class="math display">
f_t(h) = (1, h) E(\mathbf{\theta}_t \mid \mathcal{D}_t) = (1, h) (k_{t,0}, k_{t,1})' = (k_{t,0} + h k_{t,1}).
</span></p>
<p>Here <span class="math inline">\mathbf{G} = \mathbf{J}_2(1)</span> (see below).</p>
<p>Also, we denote <span class="math inline">\mathbf{E}_2 = (1, 0)'</span>, and so the short notation for this model is</p>
<p><span class="math display">
\{E_2, J_2(1), \cdot, \cdot\}
</span></p>
</section>
<section id="general-p-th-order-polynomial-model-1" class="level4" data-number="92.3.1.3">
<h4 data-number="92.3.1.3" class="anchored" data-anchor-id="general-p-th-order-polynomial-model-1"><span class="header-section-number">92.3.1.3</span> General <span class="math inline">p</span>-th Order Polynomial Model</h4>
<p>We can consider a <span class="math inline">p</span>-th order polynomial model. This model will have a state-space vector of dimension <span class="math inline">p</span> and a polynomial of order <span class="math inline">p-1</span> forecast function on <span class="math inline">h</span>. The model can be written as</p>
<p><span class="math display">\{E_p, J_p(1), v_t, W_t\}  \qquad \text{(short notation)}
</span></p>
<p>with <span class="math inline">\mathbf{F}_t = \mathbf{E}_p = (1, 0, \dots, 0)'</span> and <span class="math inline">\mathbf{G}_t = \mathbf{J}_p(1)</span>, with</p>
<p><span class="math display">
\mathbf{J}_p(1) =
\begin{pmatrix}
1 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 1
\end{pmatrix}.
</span></p>
<p>The forecast function is given by</p>
<p><span class="math display">
f_t(h) = k_{t,0} + k_{t,1} h + \dots + k_{t,p-1} h^{p-1}.
</span></p>
<p>There is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function, given by <span class="math inline">\{E_p, L_p, v_t, W_t\}</span>, with</p>
<p><span class="math display">
L_p =
\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 \\
0 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1
\end{pmatrix}.
</span></p>
</section>
</section>
</section>
<section id="regression-models-video" class="level2 page-columns page-full" data-number="92.4">
<h2 data-number="92.4" class="anchored" data-anchor-id="regression-models-video"><span class="header-section-number">92.4</span> Regression models (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0021.png" class="lightbox" data-gallery="slides" title="Regression models"><img src="images/m3_0021.png" class="img-fluid figure-img" width="200" alt="Regression models"></a></p>
<figcaption>Regression models</figcaption>
</figure>
</div></div><section id="simple-dynamic-regression" class="level3" data-number="92.4.1">
<h3 data-number="92.4.1" class="anchored" data-anchor-id="simple-dynamic-regression"><span class="header-section-number">92.4.1</span> Simple dynamic regression</h3>
<p><span id="eq-simple-dynamic-regression"><span class="math display">
\begin{aligned}
y_t &amp;= \beta_{t,0} + \beta_{t,1}x_t + ν_t \\
\beta_{t,0} &amp;= \beta_{t−1,0} + \omega_{t,0} \\
\beta_{t,1} &amp;= \beta_{t−1,1} + \omega_{t,1}
\end{aligned}
\tag{92.24}</span></span></p>
<p>and so <span class="math inline">\theta_t = (\beta_t,0, \beta_{t,1})′</span>, <span class="math inline">F_t = (1, x_t)′</span> and <span class="math inline">G = I_2</span>.</p>
<p>This results in a forecast function of the form</p>
<p><span id="eq-simple-dynamic-regression-forecast-function"><span class="math display">
f_t(h) = k_{t,0} + k_{t,1}x_{t+h}
\tag{92.25}</span></span></p>
<p>where <span class="math inline">k_{t,0} = \mathbb{E}[\beta_{t,0} \mid \mathcal{D}_t]</span> and <span class="math inline">k_{t,1} = \mathbb{E}[\beta_{t,1} \mid \mathcal{D}_t]</span>.</p>
</section>
<section id="general-dynamic-regression" class="level3" data-number="92.4.2">
<h3 data-number="92.4.2" class="anchored" data-anchor-id="general-dynamic-regression"><span class="header-section-number">92.4.2</span> General dynamic regression</h3>
<p><span id="eq-regression-in-short-form"><span class="math display">
\begin{aligned}
y_t &amp;= \beta_{t,0} + \beta_{t,1}x_{t,1} + \ldots \beta_{t,M} x_{t,M} + ν_t \\
\beta_{t,m} &amp;= \beta_{t−1,m} + \omega_{t,m,} &amp; m = 0 : M.
\end{aligned}
\tag{92.26}</span></span></p>
<p>Then, <span class="math inline">\theta = (\beta_t,0, \ldots , \beta_{t,M} )′</span>, <span class="math inline">F_t = (1, x_{t,1}, \ldots , x_{t,M} )′</span> and <span class="math inline">G = I_M</span> . The forecast function is given by</p>
<p><span id="eq-gen-regression-forecast-function"><span class="math display">
f_t(h) = k_{t,0} + k_{t,1}x_{t+h,1} + \ldots + k_{t+h,M}x_{t+h,M}
\tag{92.27}</span></span></p>
<p>A particular case is of dynamic regressions is the case of time-varying auto-regressions (TVAR) with</p>
<p><span id="eq-tvar"><span class="math display">
\begin{aligned}
y_t &amp;= \varphi_{t,1}y_{t−1} + \varphi_{t,2}y_{t−2} + \ldots + \varphi_{t,p} y_{t−p} + ν_t,\\
\varphi_{t,m} &amp;= \varphi_{t−1,m} + \omega_{t,m,} &amp; m = 1 : p
\end{aligned}
\tag{92.28}</span></span></p>
<p>There is a paper <span class="citation" data-cites="prado2000bayesian">(<a href="#ref-prado2000bayesian" role="doc-biblioref">Raquel Prado, Huerta, and West 2000</a>)</span> on TVAR models that is a good reference for this model.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/fmnRh/regression-models -->
<section id="regression-models" class="level3" data-number="92.4.3">
<h3 data-number="92.4.3" class="anchored" data-anchor-id="regression-models"><span class="header-section-number">92.4.3</span> Regression Models</h3>
<p>In regression models, we may also have additional covariates that are also measured sequentially over time. We may want to regress the <span class="math inline">y_t</span> time series and see what relationships they have with other covariates that are also measured over time. The simplest possible case is the dynamic simple regression model. In this case, I can write down. I have a single covariate, that covariate is <span class="math inline">X_t</span> that is observed here, and then I have the usual. In this case, I have an intercept and a slope, and this is representing my simple linear regression. It’s just the regression where both the intercept and the slope are time-varying. I can define the variation. I need to specify what’s the evolution of the two components, and we are going to use this random walk. We could use other structures, but again, in the normal linear case, we are going to be using these evolution equations. Then I collect here my W’s as a single vector. The <span class="math inline">\omega_t</span> is going to have the two components in here. These are normally distributed zero and variance covariance matrix <span class="math inline">W_t</span>, that is a two-by-two matrix. This is the case of the simple regression model. In the case of this model, we have F now is time-varying. This is going to change depending on the value of X_t. I can write Ft transpose as one and X_t. My Theta vector. Again, if I think about what it is, is just Beta t, 0 Beta t, 1. I have those two components.</p>
<p>The <span class="math inline">G</span> matrix is going to be the identity, and you can see that essentially the first component is related to the first component in t minus one, and the second component at time t is related to the second component at time t minus 1. So the identity matrix will be the G. Therefore, if I think about my forecast function in the simple linear regression case, this is going to be my F transpose, which is 1 xt times the G, the G is the identity, times the expected value of Theta t, given Dt. For the expected value of Theta t given Dt, This is a two-dimensional vector, so I’m going to have components in there. I can write this down as K_t0 plus K_t1 Xt. We can see that the forecast function is again has that form that depends on that covariate at the time. This should be t plus h because we are evaluating this at t plus h. You need to have the covariate evaluated at t plus h here.</p>
</section>
<section id="general-dynamic-regression-model" class="level3" data-number="92.4.4">
<h3 data-number="92.4.4" class="anchored" data-anchor-id="general-dynamic-regression-model"><span class="header-section-number">92.4.4</span> General Dynamic Regression Model</h3>
<p>In the case of general dynamic regression model, we’re going to have a set of covariates. We can have, let’s say k of those covariates or p of those covariates, X_t1. This is my observation equation. Instead of having a single covariate, now I’m going to have p of them. I’m going to have coefficients that go with each of those and I may have the Beta t0 coefficient. My G matrix now, if I think about my parameter vector is just p plus 1 dimensional, p plus 1. Yeah, so that I have the 0 and then the p values, so is a p plus 1 vector. Then my G is the identity. My F_t is going to be a vector, is also p plus 1 dimension. The first entry is one, the second is X_t1 X_tp. My forecast function is going to be similar to this, but now we are going to have more than one covariate, so we end up with a forecast function that has this form, p.&nbsp;This is the case for the dynamic regression.</p>
</section>
<section id="tvar" class="level3" data-number="92.4.5">
<h3 data-number="92.4.5" class="anchored" data-anchor-id="tvar"><span class="header-section-number">92.4.5</span> TVAR</h3>
<p>One particular example of dynamic regression model is the case of a time-varying autoregressive process. This brings us back to those autoregressive processes that we were discussing earlier in the course. When you you’re regressing each of the X’s correspond to pass values, you have a regression model that we call a time-varying ARP. In this case, your observation equation is going to have the AR coefficients, but the AR coefficients are going to be varying over time. If we assume that we put all the coefficients together and have a random walk evolution equation for those. If I said, I call Phi_t the vector that contains all the components with all the coefficients from one to p, then I can now define this evolution equation. Then my Omega_t here is a p-dimensional vector, and I have Omega t, normal zero, WT, and my epsilon t normal 0 vt.</p>
<p>This defines a time-varying AR. It’s the same structure that we had before. The only difference is my covariates are just past values of the time series. Therefore my forecast function for the time-varying AR is going to have this form where every_thing is going to depend on past values of the time series. We will study this model in particular and make connections with the AR that we studied earlier in the class.</p>
</section>
</div>
</div>
</div>
</section>
</section>
<section id="summary-of-regression-models-reading" class="level2 page-columns page-full" data-number="92.5">
<h2 data-number="92.5" class="anchored" data-anchor-id="summary-of-regression-models-reading"><span class="header-section-number">92.5</span> Summary of Regression Models (Reading)</h2>
<section id="dynamic-regression-models" class="level3 page-columns page-full" data-number="92.5.1">
<h3 data-number="92.5.1" class="anchored" data-anchor-id="dynamic-regression-models"><span class="header-section-number">92.5.1</span> Dynamic Regression Models</h3>
<section id="simple-dynamic-regression-1" class="level4" data-number="92.5.1.1">
<h4 data-number="92.5.1.1" class="anchored" data-anchor-id="simple-dynamic-regression-1"><span class="header-section-number">92.5.1.1</span> Simple Dynamic Regression</h4>
<p><span class="math display">
\begin{aligned}
  y_t &amp;= \beta_{t,0} + \beta_{t,1} x_t + \nu_t \\
  \beta_{t,0} &amp;= \beta_{t-1,0} + \omega_{t,0} \\
  \beta_{t,1} &amp;= \beta_{t-1,1} + \omega_{t,1}
\end{aligned}
</span></p>
<p>Thus:</p>
<p><span class="math display">
\theta_t = (\beta_{t,0}, \beta_{t,1})'
</span></p>
<p><span class="math display">
F_t = (1, x_t)'
</span></p>
<p>and</p>
<p><span class="math display">
G = I_2
</span></p>
<p>This results in a forecast function of the form</p>
<p><span class="math display">
f_t(h) = k_{t,0} + k_{t,1} x_{t+h}.
</span></p>
</section>
<section id="general-dynamic-regression-1" class="level4 page-columns page-full" data-number="92.5.1.2">
<h4 data-number="92.5.1.2" class="anchored" data-anchor-id="general-dynamic-regression-1"><span class="header-section-number">92.5.1.2</span> General Dynamic Regression</h4>
<p><span id="eq-general-dynamic-regression"><span class="math display">
\begin{aligned}
y_t &amp;= \beta_{t,0} + \beta_{t,1} x_{t,1} + \dots + \beta_{t,M} x_{t,M} + \nu_t \\
\beta_{t,m} &amp;= \beta_{t-1,m} + \omega_{t,m}, \quad &amp;m = 0:M.
\end{aligned}
\tag{92.29}</span></span></p>
<p>Then,</p>
<p><span class="math inline">\theta_t = (\beta_{t,0}, \dots, \beta_{t,M})'</span>,</p>
<p><span class="math inline">\mathbf{F}_t = (1, x_{t,1}, \dots, x_{t,M})'</span> and</p>
<p><span class="math inline">\mathbf{G} = \mathbf{I}_M</span>.</p>
<p>The forecast function is given by:</p>
<p><span id="eq-forecast-function-general-dynamic-regression"><span class="math display">
f_t(h) = k_{t,0} + k_{t,1} x_{t+h,1} + \dots + k_{t,M} x_{t+h,M}.
\tag{92.30}</span></span></p>
<div class="page-columns page-full"><p>A particular case of dynamic regressions is the case of <strong>time-varying autoregressive (TVAR)</strong> with </p><div class="no-row-height column-margin column-container"><span class="" width="200px" data-group="slides">time-varying autoregressive (TVAR)</span></div></div>
<p><span class="math display">
\begin{aligned}
  y_t &amp;= \phi_{t,1} y_{t-1} + \phi_{t,2} y_{t-2} + \dots + \phi_{t,p} y_{t-p} + \nu_t \\
  \phi_{t,m} &amp;= \phi_{t-1,m} + \omega_{t,m}, \quad m = 1:p.
\end{aligned}
</span></p>
</section>
</section>
</section>
<section id="the-superposition-principle-video" class="level2 page-columns page-full" data-number="92.6">
<h2 data-number="92.6" class="anchored" data-anchor-id="the-superposition-principle-video"><span class="header-section-number">92.6</span> The superposition principle (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0031.png" class="lightbox" data-gallery="slides" title="The superposition principle"><img src="images/m3_0031.png" class="img-fluid figure-img" width="200" alt="The superposition principle"></a></p>
<figcaption>The superposition principle</figcaption>
</figure>
</div></div><p><mark>We can use the superposition principle to build models that have different kinds of components. The main idea is to think about what is the general structure we want for the forecast function and then isolate the different components of the forecast function and think about the classes of dynamic linear models that are represented in each of those components.</mark> Each of those components has a class and then we can build the general dynamic linear model with all those pieces together using this principle.</p>
<p>Two references for the Superposition principle are</p>
<ul>
<li><span class="citation" data-cites="west2013bayesian">(<a href="#ref-west2013bayesian" role="doc-biblioref">West and Harrison 2013, sec. 3.1</a> p.&nbsp;98)</span></li>
<li><span class="citation" data-cites="prado2023time">(<a href="#ref-prado2023time" role="doc-biblioref">R. Prado, Ferreira, and West 2023, sec. 4.2.1</a> p.&nbsp;136)</span></li>
</ul>
<div id="imp-superposition" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important&nbsp;92.1: Superposition Principle
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the first the author state:</p>
<blockquote class="blockquote">
<p>Conditional independence also features strongly in initial model building and in choosing an appropriate parametrization. For example, the linear superposition principle states that any linear combination of deterministic linear models is a linear model. This extends to a normal linear superposition principle:</p>
<p><mark>Any linear combination of independent normal DLMs is a normal DLM.</mark> - &gt; – <span class="citation" data-cites="west2013bayesian">(<a href="#ref-west2013bayesian" role="doc-biblioref">West and Harrison 2013, sec. 3.1</a> p.&nbsp;98)</span></p>
</blockquote>
</div>
</div>
<p>We will illustrate how to do that with an example:</p>
<p>Let’s say that we want to create a model here with a forecast function that has a linear trend component. Let’s say we have a linear function as a function of the number of steps ahead that you want to consider. Then suppose you also have a covariate here that you want to include in your model as a regression component. <span id="eq-superposition-principle-example"><span class="math display">
f_t(h) = \underbrace{(k_{t,0} + k_{t,1}\; h)}_{\text{linear trend component}} + \underbrace{(k_{t,2}\; x_{t+h})}_{\text{regression component}}
\tag{92.31}</span></span></p>
<p>where:</p>
<ul>
<li><span class="math inline">f_t(h)</span> is our forecast function.</li>
<li><span class="math inline">k_{t,0}</span>, <span class="math inline">k_{t,1}</span> and <span class="math inline">k_{t,2}</span> are just constants (that we index using time <span class="math inline">t</span> and a second subscript).</li>
<li><span class="math inline">x_{t+h}</span> is a time dependent regression covariate.</li>
</ul>
<p>When we look at the forecast function, we can isolate a linear trend and a regression components as indicated. Each of these can be set in terms of two forecast functions]{.mark}. I’m going to call the forecast function <span class="math inline">f_{1,t}(h)</span>, this is just the first piece.</p>
<p><span id="eq-superposition-breaking-it-down"><span class="math display">
\begin{aligned}
f_t(h) &amp;= f_{1,t}(h) + f_{2,t}(h) \\
f_{1,t}(h) &amp;= k_{t,0} + k_{t,1} &amp; \text{(linear trend component)} \\
f_{2,t}(h) &amp;= k_{t,2}x_{t+h} &amp; \text{(regression component)}
\end{aligned}
\tag{92.32}</span></span></p>
<p>We know how to represent forecast function <span class="math inline">f_{1,t}</span> and <span class="math inline">f_{2,t}</span> in terms of dynamic linear models.</p>
<p>For the linear trend component, <span class="math inline">f_{1,t}(h)</span> , we have a 2-dimensional state vector, <span class="math inline">\theta_t = (\theta_{t,1}, \theta_{t,2})'</span>, which yields the following DLM shortform:</p>
<p><span id="eq-superposition-linear-trend-short-form"><span class="math display">
\{F_1, G_1, \cdot, \cdot\}  \qquad \text{(short notation)}
\tag{92.33}</span></span></p>
<ul>
<li>Where we don’t explicitly specify the observational and system variances, <span class="math inline">V</span> and <span class="math inline">W</span></li>
<li>The important bit are <span class="math inline">F</span> and <span class="math inline">G</span>. The forecast function is given by:</li>
</ul>
<p><span id="eq-superposition-F1"><span class="math display">
F_{1} = E_2 = (1, 0)'
\tag{92.34}</span></span></p>
<p><span id="eq-superposition-G1"><span class="math display">
G_{1} =
\begin{pmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}
\tag{92.35}</span></span></p>
<p>for the regression component <span class="math inline">f_{2,t}(h)</span> we have the following DLM representation:</p>
<p><span id="eq-superposition-regression-short-form"><span class="math display">
\{F_2,t, G_2, \cdot, \cdot\}  \qquad \text{(short notation)}
\tag{92.36}</span></span></p>
<p>where we have <span class="math inline">F_{2t}</span> is <span class="math inline">X_t</span> and my <span class="math inline">G</span> is simply going to be 1. This is a one-dimensional vector in terms of the state parameter vector.</p>
<p><span id="eq-superposition-F2"><span class="math display">
F_{2,t} = x_{t+h}
\tag{92.37}</span></span></p>
<p><span id="eq-superposition-G2"><span class="math display">
G_{2} = 1
\tag{92.38}</span></span></p>
<p>Once we have these, we can assemble them into our final model. <span class="math inline">\{F_t, G, \cdot, \cdot\}</span></p>
<p>We care more about <span class="math inline">F</span>, <span class="math inline">G</span>, and less about the observational variance and some covariance also for the system where the</p>
<p>F is going to be an F that has, you just concatenate the two Fs. You’re going to get 1, 0 and then you’re going to put the next component here. Again, this one is dependent on time because this component is time dependent and</p>
<p>The model with forecast function <span class="math inline">f_t(h)</span> above is a model with a 3-dimensional state vector with</p>
<p><span class="math display">
F_t = (F_1', F_{2,t})' = (1, 0, x_t)'
</span></p>
<p>Then the G, you can create it just taking a block diagonal structure by concatenating <span class="math inline">G_1</span> and <span class="math inline">G_2</span>. though formally there must be a better term for this operation.</p>
<p><span class="math display">
G = \text{blockdiag}[G_1, G_2] =
\begin{pmatrix}
1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}.
</span></p>
<p>This gives us the full <span class="math inline">G</span> dynamics matrix for the model. A model with this <span class="math inline">F_t</span> and this <span class="math inline">G</span> that is constant over time will give us this particular forecast function <a href="#eq-superposition-principle-example" class="quarto-xref">Equation&nbsp;<span>92.31</span></a> we started with.</p>
<p>We used the superposition principle to build this model. If we need additional components, we will learn how to incorporate seasonal components, regression components, trend components. One can build a fairly sophisticated model with different structures into this particular model using the superposition principle.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/gh9qW/the-superposition-principle -->
<section id="the-superposition-principle" class="level3" data-number="92.6.1">
<h3 data-number="92.6.1" class="anchored" data-anchor-id="the-superposition-principle"><span class="header-section-number">92.6.1</span> The superposition principle</h3>
<p>We can use the superposition principle to build models that have different kinds of components. The main idea is to think about what is the general structure we want for the forecast function and then isolate the different components of the forecast function and think about the classes of dynamic linear models that are represented in each of those components. Each of those components has a class and then we can build the general dynamic linear model with all those pieces together using this principle. I will illustrate how to do that with an example. Let’s say that you want to create a model here with a forecast function that has a linear trend component. Let’s say we have a linear function as a function of the number of steps ahead that you want to consider. Then suppose you also have a covariate here that you want to include in your model as a regression component. Let’s say we have a K_t2 and then we have X_t plus h, this is my covariate. Again, the k’s here are just constants, as of constants in terms of h, they are dependent on time here. This is the general structure we want to have for the forecast function. Now you can see that when I look at the forecast function, I can isolate here and separate these two components. I have a component that looks like a linear trend and then I have a component that is a regression component. Each of this can be set in terms of two forecast functions. I’m going to call the forecast function F_1t h, this is just the first piece. Then I have my second piece here. I’m going to call it F_2t, is just this piece here with the regression component. We know how to represent this forecast function in terms of a dynamic linear model. I can write down a model that has an F, G, and some V, and some W that I’m going to just leave here and not specify them explicitly because the important components for the structure of the model are the F and the G. If you’ll recall the F in the case of a forecast function with a linear trend like this, is just my E_2 vector, which is a two-dimensional vector. The first entry is one, and the second one is a zero. Then the G in this case is just this upper triangular matrix that has 1, 1 in the first row and 0, 1 in the second one. Remember, in this case we have a two-dimensional state vector where one of the components in the vector is telling me information about the level of the time series, the other component is telling me about the rate of change in that level. This is a representation that corresponds to this forecast function. For this other forecast function, we have a single covariate, it’s just a regression and I can represent these in terms of an F_2, G_2, and then some observational variance and some system variance here in the case of a single covariate and this one depends on t. We have F_2t is X_t and my G here is simply going to be one. This is a one-dimensional vector in terms of the state parameter vector. We have a single state vector and it’s just going to tell me about the changes, the coefficient that goes with the X_t covariate. Once I have these, I can create my final model and I’m going to just say that my final model is F, G, and then I have some observational variance and some covariance also for the system where the F is going to be an F that has, you just concatenate the two Fs. You’re going to get 1, 0 and then you’re going to put the next component here. Again, this one is dependent on time because this component is time dependent and then the G, you can create it just taking a block diagonal structure, G_1 and G_2. You just put together, the first one is 1, 1, 0, 1 and then I concatenate this one as a block diagonal. This should be one. This gives me the full G function for the model. Now a model with this F_t and this G that is constant over time will give me this particular forecast function. I’m using the superposition principle to build this model. If you want additional components, we will learn how to incorporate seasonal components, regression components, trend components. You can build a fairly sophisticated model with different structures into this particular model using the superposition principle.</p>
</section>
</div>
</div>
</div>
</section>
<section id="superposition-principle-general-case-reading" class="level2" data-number="92.7">
<h2 data-number="92.7" class="anchored" data-anchor-id="superposition-principle-general-case-reading"><span class="header-section-number">92.7</span> Superposition principle: General case (Reading)</h2>
<p><mark>You can build dynamic models with different components, for example, a trend component plus a regression component, by using the principle of superposition. The idea is to think about the general form of the forecast function you want to have for prediction. You then write that forecast function as a sum of different components where each component corresponds to a class of DLM with its own state-space representation. The final DLM can then be written by combining the pieces of the different components.</mark></p>
<p>For example, suppose you are interested in a model with a forecast function that includes a linear polynomial trend and a single covariate <span class="math inline">x_t</span>, i.e.,</p>
<p><span class="math display">
f_t(h) = k_{t,0} + k_{t,1}h + k_{t,3}x_{t+h}.
</span></p>
<p>This forecast function can be written as <span class="math inline">f_t(h) = f_{1,t}(h) + f_{2,t}(h)</span>, with</p>
<p><span class="math display">
f_{1,t}(h) = (k_{t,0} + k_{t,1}h), \quad f_{2,t}(h) = k_{t,3}x_{t+h}.
</span></p>
<p>The first component in the forecast function corresponds to a model with a 2-dimensional state vector, <span class="math inline">F_{1,t} = F_1 = (1, 0)'</span>,</p>
<p><span class="math display">
G_{1,t} = G_1 =
\begin{pmatrix}
1 &amp; 1 \\
0 &amp; 1
\end{pmatrix}.
</span></p>
<p>The second component corresponds to a model with a 1-dimensional state vector, <span class="math inline">F_{2,t} = x_t</span>, <span class="math inline">G_{2,t} = G_2 = 1</span>.</p>
<p>The model with forecast function <span class="math inline">f_t(h)</span> above is a model with a 3-dimensional state vector with <span class="math inline">F_t = (F_1', F_{2,t})' = (1, 0, x_t)'</span> and</p>
<p><span class="math display">
G_t = \text{blockdiag}[G_1, G_2] =
\begin{pmatrix}
1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix}.
</span></p>
<section id="general-case" class="level3" data-number="92.7.1">
<h3 data-number="92.7.1" class="anchored" data-anchor-id="general-case"><span class="header-section-number">92.7.1</span> General Case</h3>
<p>The general case wasn’t covered in the video and we didn’t have a proper statement of the superposition principle. However, in <a href="#imp-superposition" class="quarto-xref">Important&nbsp;<span>92.1</span></a> I extracted the statement of the principle above. This statement clarifies that the principle arises via conditional independence, a tool we also used extensively in the previous course on mixture models. Now let us consider the general case from the handout.</p>
<p>Assume that you have a time series process <span class="math inline">y_t</span> with a forecast function</p>
<p><span class="math display">
f_t(h) = \sum_{i=1}^{m} f_{i,t}(h),
</span></p>
<p>where each <span class="math inline">f_{i,t}(h)</span> is the forecast function of a DLM with representation <span class="math inline">\{F_{i,t}, G_{i,t}, v_{i,t}, W_{i,t}\}</span>.</p>
<p>Then, <span class="math inline">f_t(h)</span> has a DLM representation <span class="math inline">\{F_t, G_t, v_t, W_t\}</span> with</p>
<p><span class="math display">
F_t = (F_{1,t}', F_{2,t}', \dots, F_{m,t}')',
</span></p>
<p><span class="math display">
G_t = \text{blockdiag}[G_{1,t}, \dots, G_{m,t}],
</span></p>
<p><span class="math display">
v_t = \sum_{i=1}^{m} v_{i,t},
</span></p>
<p>and</p>
<p><span class="math display">
W_t = \text{blockdiag}[W_{1,t}, \dots, W_{m,t}].
</span></p>
</section>
</section>
<section id="quiz-the-normal-dynamic-linear-model" class="level2" data-number="92.8">
<h2 data-number="92.8" class="anchored" data-anchor-id="quiz-the-normal-dynamic-linear-model"><span class="header-section-number">92.8</span> Quiz: The Normal Dynamic Linear Model</h2>
<p>Omitted due to Coursera honor code</p>
</section>
</section>
<section id="bayesian-inference-in-the-ndlm-part-1" class="level1 page-columns page-full" data-number="93">
<h1 data-number="93"><span class="header-section-number">93</span> Bayesian Inference in the NDLM: Part 1</h1>
<figure class="quarto-float quarto-float-vid figure">
<div aria-describedby="vid-sean-law-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0O6dlq6a4rA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-vid" id="vid-sean-law-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Video&nbsp;93.1: Sean Law - STUMPY: Modern Time Series Analysis with Matrix Profiles
</figcaption>
</figure>
<p>We will now delve into Bayesian inference in the case of the normal dynamic linear model where both the observational variance and the system variance are known. We will talk about filtering equations, smoothing equations and also forecasting in this setting using Bayesian approach</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Reality check
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A large portion of the next video is dedicated to laying the ground for use bayesian inference in the NDLM. I was thinking we will infer <span class="math inline">F, G,V, W</span> from the data, once we supply a normal prior <span class="math inline">\mathcal{N}(M,C)</span>. But this is not the case. In fact we don’t seem to be particularly Bayesian when going about constructing the NDLM model.</p>
<p>(i.e.&nbsp;filtering smoothing, of the parameters <span class="math inline">\theta_t</span> given the data and forecasting <span class="math inline">y_t</span> based on that)</p>
<p>I was initially disappointed that we assume that we know everything I want to infer. This is an open question for the proverbial <a href="./C4-L06.html">Feynman Notebook</a>. In reality that is just another problem. But that not the process with time series analysis.</p>
<p>However, at some point I saw a talk about <a href="https://stumpy.readthedocs.io/en/latest/Tutorial_STUMPY_Basics.html">STUMPY</a> which does many cool time series stuff in python. And the speaker Sean Law talks about all the different Data Science &amp; DSP Mojos <code>#magicspells</code> one can use, the nascent issues when comparing lots of times series or even just time series with lots of data. He makes another point that there is <code>#NoFreeLunch</code> - each Mojo comes with with its own assumptions and limitations before making the case for using <a href="https://stumpy.readthedocs.io/en/latest/Tutorial_STUMPY_Basics.html">Matrix Profiles</a>.</p>
<p>Prado lays the ground for working with a time series with around 300 data points - i.e.&nbsp;something we can still plot and view on the screen then inspect it. This might unlock for us, the NDLM framework which while flexible requires us to have a detailed form of the the model and its priors. This is easy enough if we deal with a synthetic data set, less so if we have need to analyze an novel time series for which we lack much intuition. Here we will need todo a preliminary exploratory data analysis before we can step in and construct our NDLM.</p>
<p>I suppose they expect that you will use some other non-bayesian tools to do this as we haven’t really covered this in her course.</p>
<p>Splitting a TS into trend periods and a stationary residual isn’t too tricky in R. Getting the inverse periods is then possible. Doing regression on the residuals is also possible. So if we have done all that we should be able to write down an NDLM based on all that we learned. And this will allow us to do filtering, smoothing and forecasting with error estimates.</p>
</div>
</div>
</div>
<section id="filtering-video" class="level2 page-columns page-full" data-number="93.1">
<h2 data-number="93.1" class="anchored" data-anchor-id="filtering-video"><span class="header-section-number">93.1</span> Filtering (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0041.png" class="lightbox" data-gallery="slides" title="Derivation for the Prior and Forecast at Time t"><img src="images/m3_0041.png" class="img-fluid figure-img" width="200" alt="Derivation for the Prior and Forecast at Time t"></a></p>
<figcaption>Derivation for the Prior and Forecast at Time t</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0042.png" class="lightbox" data-gallery="slides" title="Derivation of the Posterior at Time t"><img src="images/m3_0042.png" class="img-fluid figure-img" width="200" alt="Derivation of the Posterior at Time t"></a></p>
<figcaption>Derivation of the Posterior at Time t</figcaption>
</figure>
</div></div>
<p>Recall we are working in a Bayesian setting where a NDLM model with a normal prior would like this:</p>
<p><span id="eq-generic-NDLM"><span class="math display">
\begin{aligned}
  y_t &amp;= F_t' \theta_t + \nu_t &amp; \nu_t &amp;\sim \mathcal{N}(0, v_t) &amp; \text{(observation)}\\
  \theta_t &amp;= G_t \theta_{t-1} + \omega_t &amp; \omega_t &amp;\sim  \mathcal{N}(0, W_t) &amp; \text{(evolution)} \\
  &amp; &amp;(\theta_0 \mid \mathcal{D}_0) &amp; \sim  \mathcal{N}(m_0, C_0) &amp; \text{(prior)}
\end{aligned}
\tag{93.1}</span></span></p>
<ul>
<li>In the prior <span class="math inline">\mathcal{D}_0</span> stands for the information that we have before collecting any data and</li>
<li>We are assuming <span class="math inline">\theta_0</span> follows a normal distribution with</li>
<li><span class="math inline">m_0</span> mean</li>
<li><span class="math inline">C_0</span> variance covariance matrix.</li>
</ul>
<p>Since we are doing filtering which is a retrospective analysis, of past states we assume that we know <span class="math inline">m_0, C_0, \nu_t, \omega_t, F_t, G_t \qquad \forall t</span>.</p>
<p>However, there is often great interest in looking back in time in order to get a clearer picture of what happened.</p>
<p>We are interested in performing Bayesian inference in this setting and we talked about different kinds of distributions.</p>
<ul>
<li>One is the <strong>filtering distribution</strong> that allows us to update the distribution of <span class="math inline">\theta_t</span> as we receive observations and information over time.</li>
<li>The other one is <strong>smoothing equations</strong> that allows us to just revisit the past once we have observed a chunk of data.</li>
</ul>
<p>In a Bayesian setting, you have to set <em>a prior distribution</em>. <mark>We will work with the prior distribution that is conjugate.</mark></p>
<p>In this case we have to begin with a distribution at time zero for <span class="math inline">\theta_0</span>. So before we have seen any data at all, I have this prior distribution.</p>
<p>We also assume a prior distribution of the form:</p>
<p><span id="eq-filtering-assumption-1"><span class="math display">
(\theta_{t} \mid \mathcal{D}_{t-1}) \sim \mathcal{N}(m_{t-1}, C_{t-1}).
\tag{93.2}</span></span></p>
<p>We assume that this the filtering distribution follows this normal distribution based on</p>
<ol type="1">
<li>the prior in <a href="#eq-generic-NDLM" class="quarto-xref">Equation&nbsp;<span>93.9</span></a> being conjugate of the normal and<br>
</li>
<li>the linearity of the model in <a href="#eq-generic-NDLM" class="quarto-xref">Equation&nbsp;<span>93.9</span></a>.</li>
</ol>
<p>These result in updates to the model parameters and uncertainty, at each time step, preserving the normal structure from the prior.</p>
<p>Then, we can obtain the following distributions:</p>
<ol type="1">
<li>Prior at Time <span class="math inline">t</span></li>
</ol>
<p><span id="eq-prior-at-time-t"><span class="math display">
  (\theta_t \mid \mathcal{D}_{t-1}) \sim  \mathcal{N}(a_t, R_t) \qquad \text{(prior at time t)} \qquad
   \tag{93.3}</span></span></p>
<p>with</p>
<p><span id="eq-derivation-for-a-t-and-R-t"><span class="math display"> \begin{aligned}
  a_t \doteq&amp; \mathbb{E}[\theta_t \mid \mathcal{D}_{t-1}] =&amp; G_t  \mathbb{E}[G_t \theta_{t-1} \mid \mathcal{D}_{t-1} ] =&amp; G_t m_{t-1} \\
  R_t \doteq&amp; \mathbb{V}ar[\theta_t \mid \mathcal{D}_{t-1}] =&amp; G_t \mathbb{V}ar[\theta_t \mid \mathcal{D}_{t-1}] =&amp; G_t C_{t-1} G_t' + W_t.
  \end{aligned}
   \tag{93.4}</span></span></p>
<p>Where we simply took the first and second moments of the system equation from <a href="#eq-generic-NDLM" class="quarto-xref">Equation&nbsp;<span>93.9</span></a> conditioned on our information set <span class="math inline">\mathcal{D}_{t-1}</span></p>
<ol start="2" type="1">
<li>One-Step Forecast</li>
</ol>
<p><span id="eq-one-step-forecast-fn"><span class="math display">
  (y_t \mid \mathcal{D}_{t-1}) \sim  \mathcal{N}(f_t, q_t) \qquad \text{(one step forecast fn)} \qquad
   \tag{93.5}</span></span></p>
<p>with</p>
<p><span id="eq-derivation-for-f-t-and-q-t"><span class="math display">\begin{aligned}
  f_t
     &amp; \doteq \mathbb{E}[ y_t \mid \mathcal{D}_{t-1} ]
     &amp; = F_t' \mathbb{E}[ y_t \mid \mathcal{D}_{t-1} ]
     &amp; = F_t' a_t \\
  q_t
     &amp; \doteq \mathbb{V}ar[y_t \mid \mathcal{D}_{t-1}]
     &amp; = F_t' \mathbb{V}ar[y_t \mid \mathcal{D}_{t-1}]  
     &amp; = F_t' R_t F_t + v_t
  \end{aligned}
   \tag{93.6}</span></span></p>
<p>Where we took the first moments on the observation equation conditioned on the information set <span class="math inline">\mathcal{D}_t</span> and substituted <a href="#eq-one-step-forecast-fn" class="quarto-xref">Equation&nbsp;<span>93.5</span></a></p>
<ol start="3" type="1">
<li>Posterior at Time <span class="math inline">t</span></li>
</ol>
<p><span class="math display">
  (\theta_t \mid \mathcal{D}_t) \sim  \mathcal{N}(m_t, C_t)
  </span></p>
<p>with</p>
<p><span id="eq-posterior-at-time-t"><span class="math display">\begin{aligned}
  m_t &amp;= a_t + R_t F_t q_t^{-1} (y_t - f_t), \\
  C_t &amp;= R_t - R_t F_t q_t^{-1} F_t' R_t.
  \end{aligned}
   \tag{93.7}</span></span></p>
<p>These can be derived via Normal theory or via the Multivariate Bayes’ theorem. The background for both seems to be provided in <span class="citation" data-cites="west2013bayesian">(<a href="#ref-west2013bayesian" role="doc-biblioref">West and Harrison 2013, secs. 17.2.3 p.639</a>)</span></p>
<p>Now, denoting <span class="math inline">e_t = (y_t - f_t)</span> and <span class="math inline">A_t = R_t F_t q_t^{-1}</span>, we can rewrite the equations above as:</p>
<p>It follows that</p>
<p><span class="math display">
\begin{pmatrix}Y \\ \theta\end{pmatrix} \sim \mathcal{N}
\left(
  \begin{pmatrix}F'a \\ a \end{pmatrix},
  \begin{pmatrix} F'RF + V &amp; F'R \\ RF &amp; R \end{pmatrix}
\right)
</span></p>
<p>Therefore, identifying <span class="math inline">Y</span> with <span class="math inline">X_1</span> and <span class="math inline">\theta</span> with <span class="math inline">X_2</span> in the partition of <span class="math inline">X</span> in 17.2.2, we have RF R )]</p>
<p>Therefore, identifying Y with X1 and θ with X2 in the partition of X in 17.2.2, we have <span class="math display">
Y \sim \mathcal{N}[F'a, F'RF + V]
</span></p>
<p><span class="math display">
(\theta \mid Y) \sim \mathcal{N}[m, C],
</span></p>
<p>where</p>
<p><span class="math display">
m = a + RF[F′RF + V]−1[Y − F′a]
</span></p>
<p>and <span class="math display">
C = R − RF[F′RF + V]−1F′R.
</span></p>
<p><span id="eq-posterior-distribution"><span class="math display">
  \begin{aligned}
  \theta \mid \mathcal{D}_t &amp;\sim \mathcal{N}(m_t,C_t)\\
  m_t &amp;\doteq a_t + A_t e_t, \\
  C_t &amp;\doteq R_t - A_t q_t A_t'
  \end{aligned}
\tag{93.8}</span></span></p>
<p><a href="#eq-derivation-for-a-t-and-R-t" class="quarto-xref">Equation&nbsp;<span>93.4</span></a> , <a href="#eq-derivation-for-f-t-and-q-t" class="quarto-xref">Equation&nbsp;<span>93.6</span></a> and <a href="#eq-posterior-distribution" class="quarto-xref">Equation&nbsp;<span>93.8</span></a> are often referred to as the <a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filtering</a> equations.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/EILYM/filtering -->
<p><mark>I will now discuss Bayesian inference in the case of the normal dynamic linear model where both the observational variance and the system variance are known. We will talk about filtering equations, smoothing equations and also forecasting in this setting using Bayesian approach.</mark></p>
<p><del>So recall we are working with a model that looks like this: … And then this is my first equation, the observation equation and I have a system equation that looks like this.</del></p>
<p><del>We are going to assume that <span class="math inline">V_t</span> and <span class="math inline">W_t</span> are known for every t. And we also know what the <span class="math inline">F_t</span>’s and the <span class="math inline">G_t</span>’s are here. So the response is a uni-dimensional <span class="math inline">y_t</span> and then I have, say, <span class="math inline">\theta_t</span> is a vector of a given dimension, depending on the structure of the model.</del></p>
<p><mark>We are interested in performing <strong>Bayesian inference</strong> in this setting and we talked about different kinds of distributions</mark></p>
<ul>
<li><p><mark>One is the <strong>filtering distribution</strong> that allows us to update the distribution of <span class="math inline">\theta_t</span> as we receive observations and information over time.</mark></p></li>
<li><p><mark>The other one is <strong>smoothing equations</strong> that allows us to just revisit the past once we have observed a chunk of data.</mark></p></li>
</ul>
<p><del>So I will be talking about those and also smoothing.</del></p>
<p><del>In a Bayesian setting, you have to set a prior distribution.</del> <mark>We will work with the prior distribution that is conjugate.</mark></p>
<p><del>In this case I have to begin with distribution at time zero. So before I know, I have seen any data at all,</del> <mark>I have this prior distribution. <span class="math inline">D_0</span> stands for the information that I have before collecting any data. And we are going to assume, That this <span class="math inline">\theta_0</span> follows a normal distribution with <span class="math inline">m_0</span> mean and variance covariance matrix <span class="math inline">C_0</span>. So these are also specified when you’re working with this model.</mark></p>
<p>So we assume that this <span class="math inline">m_0</span> and <span class="math inline">C_0</span> is known.</p>
<p><mark>Once we have this setting using these equations, we can obtain the filtering equations.</mark></p>
<p>So the first assumption is going to be that we have, a structure.</p>
<p>So for <span class="math inline">\theta_{t -1} \mid \mathcal{D}_{t-1}</span> is going to have this normal structure which is going to happen basically because we’re using this conjugate prior. And because we have normal structure in the model, is going to lead to the following distribution. So the first one is the prior at time <span class="math inline">t</span>.</p>
<p><mark>So if I want to think about why my distribution for the t is given the information I have up to <span class="math inline">t-1</span>, I can look at the equations of the model and use this second equation. And by looking at this equation, if I condition on the information I have up to <span class="math inline">t-1</span>, I can see that, say, <span class="math inline">\theta_t</span> is written as a linear function of, <span class="math inline">\theta_{t -1}</span> and I have the assumption of normality here.</mark></p>
<p><del>Therefore, say, <span class="math inline">\theta_t</span> going to follow a normal distribution with some mean and some variance. So now we’re going to compute this mean and this variance using this equation. So if you think about the expected value of <span class="math inline">\theta_t</span>, given <span class="math inline">D_{t -1}</span>, that’s just going to be <span class="math inline">G_t</span> is a constant here. So I have my <span class="math inline">G_t</span> and then I have expected value of <span class="math inline">\theta_{t -1}</span> given <span class="math inline">G_{t -1}</span> plus expect the value of this <span class="math inline">\omega_t</span>.</del></p>
<p>But <span class="math inline">\omega_t</span> is a zero mean, normally distributed quantity, so it’s just going to be zero. Using the assumption that I have this structure, then I have that the <span class="math inline">\mathbb{E}[\theta_t \mid \mathcal{D}_{t -1}] = G_t \times m_{t-1}</span>. We’re going to call this quantity <span class="math inline">a_t</span>, so we have here <span class="math inline">a_t</span>. For the variance covariance matrix, then we just have to compute, do the same type of operation. And again, we can use this equation and see that we obtain this <span class="math inline">G_t</span> variance of <span class="math inline">\theta_{t-1} \mid \mathcal{D}_{t -1} G_t'</span>. And then we have now the variance of the omega, the variance of the omega is just <span class="math inline">W_t</span>. So we have <span class="math inline">G_t = C_{t -1} G_t' + W_t</span>. So we can call this quantity <span class="math inline">R_t</span> and just have the form of this prior distribution at time <span class="math inline">t</span>.</p>
<p>I can now think about another distribution which is the distribution of <span class="math inline">y_t \mid \mathcal{D}_{t-1}</span>. So this is the so called one-step ahead, Forecast, And in the one-step ahead forecast again is a similar type of structure. So now we’re going to use the first equation rather than the second equation and we see that <span class="math inline">y_t</span> is written in terms of a linear function of <span class="math inline">\theta_t</span>. And we have also the Gaussian in assumption here. So again the <span class="math inline">y_t</span> is going to be normally distributed, And we just have to compute the mean and the variance for this <span class="math inline">y_t</span>. So using the first equation, we have the expected value of <span class="math inline">y_t</span> given <span class="math inline">D_{t -1}</span> is just <span class="math inline">F_t' \mathbb{E}[\theta_t \mid D_{t -1}]</span>. And we computed this before, so this is, again, the expected value of <span class="math inline">\theta_t</span> given <span class="math inline">D_{t -1}</span> is what we computed here. So this is to be <span class="math inline">F_t' a_t</span>. And we are going to call this little <span class="math inline">f_t</span>. Then, for the variance, Again, we use this equation, we have this component, so we are going to get <span class="math inline">F_t' R_t F_t + D_t</span>. And I’m going to call this <span class="math inline">q_t</span>. So my final distribution, the one-step ahead forecast distribution, tells me that this follows a normal <span class="math inline">f_t q_t</span>. The next equations we are going to discuss are the equations that tell me about what is the distribution of <span class="math inline">\theta_t</span> once we incorporate the information provided by <span class="math inline">y_t</span>. The next distribution is the posterior of <span class="math inline">\theta_t</span> given <span class="math inline">D_t</span>. So that’s, <span class="math inline">\theta_t</span> given <span class="math inline">D_t</span>. And we can write <span class="math inline">D_t</span> as whatever information we have at time <span class="math inline">t- 1</span>. And the new data point with this just <span class="math inline">y_t</span>. So we just want to update the distribution of <span class="math inline">\theta_t</span> given that we have received this additional data point at time <span class="math inline">t</span>. There are two ways of computing this distribution. One uses normal theory, the other one uses Bayes’ theorem. And you obtain that the distribution of <span class="math inline">\theta_t</span> given <span class="math inline">D_t</span> is going to be a normal, with mean we call it <span class="math inline">m_t</span> and variance <span class="math inline">C_t</span>. We will see how to obtain this distribution or the moments of this distribution using normal theory.</p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>So, again, we can write down, if we think about just combining the vector <span class="math inline">\theta_t</span> with the observation</p>
</blockquote>
</blockquote>
<p><span class="math inline">Y_t</span> given <span class="math inline">D_{t -1}</span>, right? We have information about <span class="math inline">\theta_t \mid t-1</span>. That’s the prior for <span class="math inline">\theta_{ta,t}</span>, based on the information at <span class="math inline">t -1</span>. And then we also computed before the one step ahead forecast distribution for <span class="math inline">y_t| \mathcal{D}_{t -1}</span>. So we know that when we combine these two in a single vector, we’re going to have a multivariate normal distribution and the first component is going to be <span class="math inline">a_t</span>. The second component is what we have called <span class="math inline">F_t</span>, so that’s the mean. And then for the covariance matrix. We’re going to have now, what goes here is just the variance of <span class="math inline">\theta_t</span> given <span class="math inline">D_{t -1}</span>, which we have called <span class="math inline">R_t</span>. What goes here is the variance of <span class="math inline">y_t \mid \mathcal{D}_{t -1}</span> and we have called this <span class="math inline">q_t</span>. And now we have to compute the covariance between <span class="math inline">\theta_t</span> and <span class="math inline">y_t</span>, and that goes here. And the covariance between <span class="math inline">y_t</span> and <span class="math inline">\theta_t</span>, which is just the transpose of that, is going to go here. So if I think about computing the covariance of <span class="math inline">\theta_t</span> and <span class="math inline">y_t \mid \mathcal{D}_{t -1}</span>, I can write <span class="math inline">y_t</span> using the first equation here as a function of <span class="math inline">\theta_t</span>. That’s going to give us, <span class="math inline">F_t' \theta_t + v_t</span> given <span class="math inline">D_{t -1}</span>. And in this one we can see that this is going to give us basically the variance of <span class="math inline">\theta_t</span> given <span class="math inline">D_{t -1}</span> and then multiplied by <span class="math inline">F_t' F_t</span> which gives me the <span class="math inline">F_t</span>. So this is going to be variance of <span class="math inline">\theta_t</span> given <span class="math inline">D_{t -1}</span> times <span class="math inline">F_t</span>. And then there is a term that combines the <span class="math inline">\theta_t</span> with the noise but they are independent, so the covariance is going to be zero. So this one is simply going to be my <span class="math inline">R_t F_t</span>, so this goes here, And what goes here is just the covariance of <span class="math inline">y_t</span> with <span class="math inline">\theta_t</span> or the transpose of this. So this is going to give me <span class="math inline">F_t' R_t'</span>, but <span class="math inline">R_t</span> is a covariance matrix, so <span class="math inline">R_t' = R_t</span>. So now I have my full multivariate distribution and I can use properties of the multivariate distribution to compute the distribution of, <span class="math inline">\theta_t</span>, given <span class="math inline">y_t</span> and <span class="math inline">D_{t -1}</span>. So that’s going to be a conditional distribution, I’m going to condition on the <span class="math inline">y_t</span>. And when I combine <span class="math inline">y_t</span> and <span class="math inline">D_{t -1}</span> that gives me just the information up to time <span class="math inline">t</span>. So we are interested in just finding, say, <span class="math inline">\theta_t</span> given <span class="math inline">y_t</span> and <span class="math inline">D_{t -1}</span> which is the same as <span class="math inline">\theta_t</span> given <span class="math inline">D_t</span>. We partition the normal distribution in this way, so I can just think about this is the first component and then I have these different pieces in my covariance matrix. And we know from normal theory that if we have a distribution, if we have a vector that is partitioned into vectors here where they are normally distributed. And I have my mean partition here and let’s say I have one component here, Then we know that if I wanted to compute the distribution of <span class="math inline">X_1</span> conditional on <span class="math inline">X_2</span>, that’s going to give me normal, let’s say <span class="math inline">\alpha^*</span>. And let’s call this one the <span class="math inline">\sigma^*</span>, where <span class="math inline">\alpha^*</span> is going to be my <span class="math inline">\alpha_1 + \sigma_{12}^{-1}</span>. And then I have <span class="math inline">_1 - \alpha_2</span> and then I have my <span class="math inline">\sigma^*</span>. And this one gives me my <span class="math inline">\sigma_{11} - \sigma_{21}</span>. So this is a result from normal theory. So if I want my conditional distribution of <span class="math inline">X_1</span> given <span class="math inline">X_2</span> I can apply these equations. So we notice we have the same type of structure here. If I partition my vector and in <span class="math inline">\theta_t</span> and <span class="math inline">y_t</span>. And now I condition on, I take the distribution of <span class="math inline">\theta_t</span> conditioning on <span class="math inline">y_t</span>. I’m going to have that same structure where this is normal, <span class="math inline">m_t C_t</span>. And my <span class="math inline">m_t</span> using normal theory, again, is going to be <span class="math inline">a_t + \sigma_{22}^{-1}</span>. And then I have <span class="math inline">y_t - f_t</span>. So that’s my mean and my covariance matrix. It’s going to be <span class="math inline">R_t - q_t^{-1}</span> and then I have this transpose again. So if we simplify things a bit here and we call <span class="math inline">e_t</span>, it’s just the error that we make when we compare <span class="math inline">y_t</span>, which is the observation with the prediction, right? And then I also use the notation I call <span class="math inline">a_t</span>, let’s call here <span class="math inline">A_t R_t F_t q_t^{-1}</span>. Then we can write this down, to mean, we can write as <span class="math inline">a_t + A_t</span>. And the covariance matrix. We can write it as <span class="math inline">R_t, A_t q_t A_t'</span>. So this gives me the posterior mean after receiving this <span class="math inline">y_t</span> observation. And you can see that you can write down the posterior mean, has this usual form of the prior plus something that relates to the error that I make with the prediction.</p>
<p>So the <span class="math inline">y_t</span> appears there and then is weighted by this quantity that we just call <span class="math inline">a_t</span>.</p>
<p>And for the covariance structure, we are also incorporating information about the prior and what the <span class="math inline">y_t</span> observation provides. So this gives us our filtering equation for <span class="math inline">\theta_t</span> given <span class="math inline">D_t</span>. And now we can apply all these equations as we receive observations from <span class="math inline">t = 1</span> all the way to <span class="math inline">T</span>. If we happen to have <span class="math inline">T</span> observations in the time series, we can do this filtering process and obtain these distributions as we receive information.</p>
</div>
</div>
</div>
</section>
<section id="summary-of-filtering-distributions-reading" class="level2" data-number="93.2">
<h2 data-number="93.2" class="anchored" data-anchor-id="summary-of-filtering-distributions-reading"><span class="header-section-number">93.2</span> Summary of filtering distributions (Reading)</h2>
<section id="bayesian-inference-in-ndlm-known-variances" class="level3" data-number="93.2.1">
<h3 data-number="93.2.1" class="anchored" data-anchor-id="bayesian-inference-in-ndlm-known-variances"><span class="header-section-number">93.2.1</span> Bayesian Inference in NDLM: Known Variances</h3>
<p>Consider an NDLM given by:</p>
<p><span id="eq-generic-NDLM"><span class="math display">
\begin{aligned}
y_t &amp;= F_t' \theta_t + \nu_t, \quad \nu_t \sim  \mathcal{N}(0, v_t), \\
\theta_t &amp;= G_t \theta_{t-1} + \omega_t, \quad \omega_t \sim  \mathcal{N}(0, W_t),
\end{aligned}
\tag{93.9}</span></span></p>
<p>with <span class="math inline">F_t</span>, <span class="math inline">G_t</span>, <span class="math inline">v_t</span>, and <span class="math inline">W_t</span> known. We also assume a prior distribution of the form <span class="math inline">(\theta_0 \mid \mathcal{D}_0) \sim  \mathcal{N}(m_0, C_0)</span>, with <span class="math inline">m_0</span>, <span class="math inline">C_0</span> known.</p>
<section id="filtering" class="level4" data-number="93.2.1.1">
<h4 data-number="93.2.1.1" class="anchored" data-anchor-id="filtering"><span class="header-section-number">93.2.1.1</span> Filtering</h4>
<p>We are interested in finding <span class="math inline">\mathbb{P}r(\theta_t \mid \mathcal{D}_t)</span> for all <span class="math inline">t</span>. Assume that the posterior at <span class="math inline">t-1</span> is such that:</p>
<p><span id="eq-filtering"><span class="math display">
(\theta_{t-1} \mid \mathcal{D}_{t-1}) \sim  \mathcal{N}(m_{t-1}, C_{t-1}).
\tag{93.10}</span></span></p>
<p>Then, we can obtain the following:</p>
<ol type="1">
<li>Prior at Time <span class="math inline">t</span></li>
</ol>
<p><span class="math display">
(\theta_t \mid \mathcal{D}_{t-1}) \sim  \mathcal{N}(a_t, R_t),
</span></p>
<p>with</p>
<p><span class="math display">
a_t = G_t m_{t-1} \qquad R_t = G_t C_{t-1} G_t' + W_t.
</span></p>
<ol start="2" type="1">
<li>One-Step Forecast</li>
</ol>
<p><span class="math display">
(y_t \mid D_{t-1}) \sim  \mathcal{N}(f_t, q_t),
</span></p>
<p>with</p>
<p><span class="math display">
f_t = F_t' a_t, \quad q_t = F_t' R_t F_t + v_t.
</span></p>
<ol start="3" type="1">
<li>Posterior at Time <span class="math inline">t: (\theta_t \mid \mathcal{D}_t) \sim  \mathcal{N}(m_t, C_t)</span> with</li>
</ol>
<p><span class="math display">\begin{aligned}
m_t &amp;= a_t + R_t F_t q_t^{-1} (y_t - f_t), \\
C_t &amp;= R_t - R_t F_t q_t^{-1} F_t' R_t.
\end{aligned}
</span></p>
<p>Now, denoting <span class="math inline">e_t = (y_t - f_t)</span> and <span class="math inline">A_t = R_t F_t q_t^{-1}</span>, we can rewrite the equations above as:</p>
<p><span class="math display">\begin{aligned}
m_t &amp;= a_t + A_t e_t, \\
C_t &amp;= R_t - A_t q_t A_t'
\end{aligned}
</span></p>
</section>
</section>
</section>
<section id="rcode-filtering-in-the-ndlm-example-reading" class="level2" data-number="93.3">
<h2 data-number="93.3" class="anchored" data-anchor-id="rcode-filtering-in-the-ndlm-example-reading"><span class="header-section-number">93.3</span> Rcode Filtering in the NDLM: Example (Reading)</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="do">##### Univariate DLM: Known, constant variances</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>set_up_dlm_matrices <span class="ot">&lt;-</span> <span class="cf">function</span>(FF, GG, VV, WW){</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">FF=</span>FF, <span class="at">GG=</span>GG, <span class="at">VV=</span>VV, <span class="at">WW=</span>WW))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>set_up_initial_states <span class="ot">&lt;-</span> <span class="cf">function</span>(m0, C0){</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">m0=</span>m0, <span class="at">C0=</span>C0))</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="do">### forward update equations </span><span class="al">###</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>forward_filter <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, initial_states){</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve dataset</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve a set of quadruples </span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># FF, GG, VV, WW are scalar</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF  </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve initial states</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>  m0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>m0</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>  C0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>C0</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for results</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(GG)[<span class="dv">1</span>]</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>  et <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T){</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of priors at t</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(m0)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> C0 <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[, , i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[, , i])</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[, , i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[, , i])</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of one-step forecast:</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> (at[i, ]) </span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of posterior at t:</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    At <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">/</span> Qt[i]</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    et[i] <span class="ot">&lt;-</span> y_t[i] <span class="sc">-</span> ft[i]</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    mt[i, ] <span class="ot">&lt;-</span> at[i, ] <span class="sc">+</span> <span class="fu">t</span>(At) <span class="sc">*</span> et[i]</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    Ct[, , i] <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">-</span> Qt[i] <span class="sc">*</span> At <span class="sc">%*%</span> <span class="fu">t</span>(At)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    Ct[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Ct[, , i]<span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Ct[, , i])</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forward filtering is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mt =</span> mt, <span class="at">Ct =</span> Ct, <span class="at">at =</span> at, <span class="at">Rt =</span> </span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>                Rt, <span class="at">ft =</span> ft, <span class="at">Qt =</span> Qt))</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>forecast_function <span class="ot">&lt;-</span> <span class="cf">function</span>(posterior_states, k, matrices){</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>  <span class="do">## set up matrices</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>] <span class="co"># time points</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>] <span class="co"># dimension of state-space parameter vector</span></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>  <span class="do">## placeholder for results</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> k, <span class="at">ncol =</span> d)</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, k))</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of state distribution</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[T, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , T] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[, , i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[, , i])</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(at[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Rt[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[, , i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[, , i])</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of forecast distribution</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(at[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forecasting is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">at=</span>at, <span class="at">Rt=</span>Rt, <span class="at">ft=</span>ft, <span class="at">Qt=</span>Qt))</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a><span class="do">## obtain 95% credible interval</span></span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>get_credible_interval <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma2, </span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>                          <span class="at">quantile =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)){</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>  z_quantile <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(quantile)</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>  bound <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">length</span>(mu), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">1</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> </span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>    z_quantile[<span class="dv">1</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># lower bound</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">2</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> </span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>    z_quantile[<span class="dv">2</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># upper bound</span></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(bound)</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a><span class="do">####################### Example: Lake Huron Data ######################</span></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(LakeHuron,<span class="at">main=</span><span class="st">"Lake Huron Data"</span>,</span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"level in feet"</span>) <span class="co"># Total of 98 observations </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-filtering-in-the-NDLM-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="C4-L03_files/figure-html/lst-filtering-in-the-NDLM-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>k<span class="ot">=</span><span class="dv">4</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(LakeHuron)<span class="sc">-</span>k <span class="co"># We take the first 94 observations </span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                      <span class="co"># only as our data</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>ts_data<span class="ot">=</span>LakeHuron[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>ts_validation_data <span class="ot">&lt;-</span> LakeHuron[(T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">98</span>]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y_t =</span> ts_data)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># First order polynomial model </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="do">## set up the DLM matrices </span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>FF <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>GG <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">570</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>C0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fl">1e4</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="do">## wrap up all matrices and initial values</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>matrices <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF, GG, VV, WW)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>initial_states <span class="ot">&lt;-</span> <span class="fu">set_up_initial_states</span>(m0, C0)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>results_filtered <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices, </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>                                   initial_states)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Forward filtering is completed!</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(results_filtered)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "mt" "Ct" "at" "Rt" "ft" "Qt"</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>ci_filtered <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_filtered<span class="sc">$</span>mt, </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                                     results_filtered<span class="sc">$</span>Ct)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="do">## forecasting </span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>results_forecast <span class="ot">&lt;-</span> <span class="fu">forecast_function</span>(results_filtered,k, </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                                      matrices)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Forecasting is completed!</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>ci_forecast <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_forecast<span class="sc">$</span>ft, </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>                                     results_forecast<span class="sc">$</span>Qt)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>index<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1875</span>, <span class="dv">1972</span>, <span class="at">length.out =</span> <span class="fu">length</span>(LakeHuron))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>index_filt<span class="ot">=</span>index[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>index_forecast<span class="ot">=</span>index[(T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">98</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index, LakeHuron, <span class="at">ylab =</span> <span class="st">"level"</span>, </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Lake Huron Level"</span>,<span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">574</span>,<span class="dv">584</span>))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_filtered<span class="sc">$</span>mt, <span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered[, <span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered[, <span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'red'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, results_forecast<span class="sc">$</span>ft, <span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, ci_forecast[, <span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, ci_forecast[, <span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">'bottomleft'</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"filtered"</span>,<span class="st">"forecast"</span>),</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>), <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-filtering-in-the-NDLM-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="C4-L03_files/figure-html/lst-filtering-in-the-NDLM-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Now consider a 100 times smaller signal to noise ratio </span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fl">0.01</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>matrices_2 <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF,GG, VV, WW)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>results_filtered_2 <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices_2, </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>                                     initial_states)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Forward filtering is completed!</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>ci_filtered_2 <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_filtered_2<span class="sc">$</span>mt, </span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>                                       results_filtered_2<span class="sc">$</span>Ct)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>results_forecast_2 <span class="ot">&lt;-</span> <span class="fu">forecast_function</span>(results_filtered_2, </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">length</span>(ts_validation_data), </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>                             matrices_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Forecasting is completed!</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>ci_forecast_2 <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_forecast_2<span class="sc">$</span>ft, </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>                                       results_forecast_2<span class="sc">$</span>Qt)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index, LakeHuron, <span class="at">ylab =</span> <span class="st">"level"</span>, </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Lake Huron Level"</span>,<span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">574</span>,<span class="dv">584</span>))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_filtered_2<span class="sc">$</span>mt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'magenta'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered_2[, <span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'magenta'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered_2[, <span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'magenta'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, results_forecast_2<span class="sc">$</span>ft, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, ci_forecast_2[, <span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, ci_forecast_2[, <span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">'bottomleft'</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"filtered"</span>,<span class="st">"forecast"</span>),</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"magenta"</span>, <span class="st">"green"</span>), <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-filtering-in-the-NDLM-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="C4-L03_files/figure-html/lst-filtering-in-the-NDLM-3.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index_filt,results_filtered<span class="sc">$</span>mt,<span class="at">type=</span><span class="st">'l'</span>,<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">574</span>,<span class="dv">584</span>),<span class="at">ylab=</span><span class="st">"level"</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt,results_filtered_2<span class="sc">$</span>mt,<span class="at">col=</span><span class="st">'magenta'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index,LakeHuron,<span class="at">lty=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-filtering-in-the-NDLM-4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="C4-L03_files/figure-html/lst-filtering-in-the-NDLM-4.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="smoothing-and-forecasting-video" class="level2 page-columns page-full" data-number="93.4">
<h2 data-number="93.4" class="anchored" data-anchor-id="smoothing-and-forecasting-video"><span class="header-section-number">93.4</span> Smoothing and forecasting (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0051.png" class="lightbox" data-gallery="slides" title="Smoothing"><img src="images/m3_0051.png" class="img-fluid figure-img" width="200" alt="Smoothing"></a></p>
<figcaption>Smoothing</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/m3_0052.png" class="lightbox" data-gallery="slides" title="Forecasting"><img src="images/m3_0052.png" class="img-fluid figure-img" width="200" alt="Forecasting"></a></p>
<figcaption>Forecasting</figcaption>
</figure>
</div></div>
<p>We now discuss the <strong>smoothing equations</strong> for the case of the NDLM, where we are assuming that the variance at the <em>observation level</em> <span class="math inline">\nu_t</span> and the covariance matrix at the <em>system level</em> <span class="math inline">\mathbf{W}_t</span> are both known.</p>
<p><span id="eq-inference-NDLM"><span class="math display"> \begin{aligned}
y_t &amp;= \mathbf{F}_t' \mathbf{\theta}_t + \nu_t, &amp;\nu_t &amp;\sim \mathcal{N} (0, v_t), &amp; \text{(observation)} \\
\mathbf{\theta}_t &amp; = \mathbf{G}_t \mathbf{\theta}_{t-1} + \mathbf{\omega}_t, &amp;\mathbf{\omega}_t &amp; \sim \mathcal{N} (0, \mathbf{W}_t), &amp; \text{(evolution)} \\
&amp;\{ \mathbf{F}_t, \mathbf{G}_t, v_t, \mathbf{W}_t \}  &amp;(\mathbf{\omega}_0 \mid \mathcal{D}_0) &amp; \sim \mathcal{N}(\mathbf{m}_0, \mathbf{C}_0) &amp; \text{(prior)}
\end{aligned}
\tag{93.11}</span></span></p>
<p>with <span class="math inline">F_t</span>, <span class="math inline">G_t</span>, <span class="math inline">v_t</span>, <span class="math inline">W_t</span>, <span class="math inline">m_0</span> and <span class="math inline">C_0</span> known.</p>
<p>We have discussed the filtering equations, i.e.&nbsp;the process for obtaining the distributions of <span class="math inline">\theta_t \mid \mathcal{D}_t</span>, as we collect observations over time, called filtering.</p>
<p>We do this by updating the distribution of <span class="math inline">\theta_t</span> given the data we have collected step by step, as we move forward in time - updating the from the prior distribution.</p>
<p>Now we will discuss what happens when we do smoothing, meaning when we revisit the distributions of <span class="math inline">\theta_t</span>, given now that we have received a set of observations.</p>
<section id="smoothing" class="level4" data-number="93.4.0.1">
<h4 data-number="93.4.0.1" class="anchored" data-anchor-id="smoothing"><span class="header-section-number">93.4.0.1</span> Smoothing</h4>
<p>For <span class="math inline">t &lt; T</span>, we have that:</p>
<p><span class="math display">
(\theta_t \mid D_T) \sim  \mathcal{N}(a_T(t - T), R_T(t - T)),
</span></p>
<p>where</p>
<p><span class="math display">
a_T(t - T) = m_t - B_t [a_{t+1} - a_T(t - T + 1)],
</span></p>
<p><span class="math display">
R_T(t - T) = C_t - B_t [R_{t+1} - R_T(t - T + 1)] B_t',
</span></p>
<p>for <span class="math inline">t = (T - 1), (T - 2), \dots, 0</span>, with <span class="math inline">B_t = C_t G_t' R_{t+1}^{-1}</span>, and <span class="math inline">a_T(0) = m_T</span>, <span class="math inline">R_T(0) = C_T</span>. Here <span class="math inline">a_t</span>, <span class="math inline">m_t</span>, <span class="math inline">R_t</span>, and <span class="math inline">C_t</span> are obtained using the filtering equations as explained before.</p>
</section>
<section id="forecasting" class="level4" data-number="93.4.0.2">
<h4 data-number="93.4.0.2" class="anchored"><span class="header-section-number">93.4.0.2</span> Forecasting</h4>
<p>For <span class="math inline">h \geq 0</span>, it is possible to show that:</p>
<p><span class="math display">
(\theta_{t+h} \mid D_t) \sim  \mathcal{N}(a_t(h), R_t(h)),
</span></p>
<p><span class="math display">
(y_{t+h} \mid D_t) \sim  \mathcal{N}(f_t(h), q_t(h)),
</span></p>
<p>with</p>
<p><span class="math display">
a_t(h) = G_{t+h} a_t(h - 1),
</span></p>
<p><span class="math display">
R_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},
</span></p>
<p><span class="math display">
f_t(h) = F_{t+h}' a_t(h),
</span></p>
<p><span class="math display">
q_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},
</span></p>
<p>and</p>
<p><span class="math display">
a_t(0) = m_t, \quad R_t(0) = C_t.
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/EIciI/smoothing-and-forecasting -->
<section id="smoothing-1" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="smoothing-1">Smoothing</h3>
<blockquote class="blockquote">
<p>We know, we now discuss the smoothing equations for the case of the normal dynamic linear model. When we are assuming that both the variance at the observation level is known and the covariance matrix at the system level is also known.</p>
</blockquote>
</section>
<section id="the-ndlm-we-will-be-inferring" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="the-ndlm-we-will-be-inferring">the NDLM we will be inferring</h3>
<blockquote class="blockquote">
<p>Recall we have two equations here, we have the observation equation, where <span class="math inline">y_t</span> is modeled as <span class="math inline">F_t'\theta_t + \text{noise}</span> the noise is <span class="math inline">\mathcal{N}(0,\nu_t)</span>. And we’re assuming that the vt is given. We are also assuming that we know Ft for all t. And then in the evolution equation we have <span class="math inline">\theta_t= G_t \theta(t-1) + noise</span>. And then again, the assumption for the <span class="math inline">w_t</span> is here is that they are normally distributed with mean zero, and these variants co variance matrix, capital <span class="math inline">W_t</span>. So we can summarize the model in terms of <span class="math inline">F_t</span>, <span class="math inline">G_t</span>, <span class="math inline">Vt</span> and <span class="math inline">W_t</span>, that are given for all t. We have discussed the filtering equations.</p>
</blockquote>
</section>
<section id="recall-what-is-filtering" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="recall-what-is-filtering">Recall what is Filtering?</h3>
<blockquote class="blockquote">
<p>So the process for obtaining the distributions of <span class="math inline">\theta_t \mid \mathcal{D}_t</span>, as we collect observations over time is called filtering.</p>
</blockquote>
</section>
<section id="recall-what-is-smoothing" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="recall-what-is-smoothing">Recall what is Smoothing?</h3>
<blockquote class="blockquote">
<p>Now we will discuss what happens when we do smoothing, meaning when we revisit the distributions of <span class="math inline">\theta_t</span>, given now that we have received a set of observations.</p>
</blockquote>
</section>
<section id="filtering-illustrated" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="filtering-illustrated">Filtering illustrated</h3>
<blockquote class="blockquote">
<p>So Just to illustrate the process, we have here, <span class="math inline">\theta_0</span>,<span class="math inline">\theta_1</span> all way up to <span class="math inline">\theta_4</span>. And we can assume just for the sake of the example, that we are going to receive three observations. So we are going to proceed with the filtering, and then once we receive the last observation at time three, we’re going to go backwards and we’re going to revisit the distributions for the state parameters.</p>
<p>So just to remind you how the filtering works, we move forward, before we receive any observations. In the NDLM, when we have all the variances known. The <em>conjugate prior</em> distribution is a <span class="math inline">\mathcal{N}(m_0,C_0)</span>, and this is specified by the user, before collecting any observations.</p>
<p>We can then use the structure of the model, meaning the system equation and the observation equation to obtain the distribution of <span class="math inline">\theta_t \mid \mathcal{D}_0</span>. Before observing the first <span class="math inline">y</span>. This gives us first the distribution of <span class="math inline">\theta_t</span>, <span class="math inline">\theta_1 \mid \mathcal{D}_0</span>, which is <span class="math inline">\mathcal{N}(a_1, R_1)</span>. And then we can also get the one step ahead forecast distribution for <span class="math inline">y_1 \mid  \mathcal{D}_0</span>, which is a <span class="math inline">\mathcal{N}(f_1, q_1)</span>. And we have discussed how to obtain these moments using the filtering equations.</p>
<p>Then we received the first observation, and the first observation can allows us to update the distribution of . So we obtain now the distribution of <span class="math inline">\theta1 \mid y_1</span>, and whatever information we have at <span class="math inline">\mathcal{D}_0</span>. So this gives us <span class="math inline">\mathcal{N}(m_1, C_1)</span>. And using again the structure of the model, we can get the prior distribution for <span class="math inline">\theta_2</span> given the one and that’s a <span class="math inline">\mathcal{N}(a_2, R_2)</span>. And then the one step ahead forecast distribution now for <span class="math inline">y_2 \mid \mathcal{D}_1</span> and that’s a <span class="math inline">\mathcal{N}(f_2, q_2)</span>. So we can receive <span class="math inline">y_2</span> update the distribution of and we can continue this process, now get the priors at <span class="math inline">T=3</span>. And then once we get the observation at <span class="math inline">T=3</span>, we update the distribution. And we can continue like this with the prior for <span class="math inline">\theta_4</span> and so on. Let’s say that we stop here, at <span class="math inline">T=3</span>.</p>
</blockquote>
<blockquote class="blockquote">
<p>And <mark>now we are interested in answering the question. Well, what is the distribution for example of <span class="math inline">\theta_2</span> given that, now, I obtain not only <span class="math inline">y_1</span> and <span class="math inline">y_2</span>, but also <span class="math inline">y_3</span>. I want to revisit that distribution using all that information. Same thing for say, the distribution of <span class="math inline">\theta_0 \mid D_0, y_1, y_2, y_3</span>. So that’s what it’s called smoothing.</mark></p>
<p>So the smoothing equations, allow us to obtain those distributions. So just to talk a little bit about the notation again, in the normal dynamic linear model where <span class="math inline">v_t</span> and <span class="math inline">w_t</span> are known for all t’s. We have that this is a normal, so the notation here, the <span class="math inline">T &gt;t</span>, here. So we’re looking at the distribution of <span class="math inline">\theta_t</span>, now in the past and that one follows a normal distribution with mean aT(t-T). So the notation here for the subscript T means that I’m conditioning on all the information I have to T. And then the variance covariance matrix is given by this, RT(t-T). So this is just going to indicate how many steps I’m going to go backwards as you will see in the example.</p>
</blockquote>
<blockquote class="blockquote">
<p>So we have some recursions in the same way that we have the filtering equations. Now we have the smoothing equations. And for these smoothing equations we have that the mean. You can see here, that whenever you’re computing a particular step t- T, you’re going to need a quantity that you computed in the previous step, t-T+1. So you’re going to need that, is a recursion, but you’re also going to need mt and and at+1. So those are quantities that you computed using the filtering equations. So in order to get the smoothing equations, you first have to proceed with the filtering. Similarly for RT(t-T), you have also that depends on something you previously obtained. And then you also have the Ct, the Rt+1 and so on. So those quantities you computed when you were updating the filtering equations. The recursion begins with aT(0) meaning that you are not going to go backwards any points in time. So that is precisely the mean is going to be whatever you computed with the filtering equations of up to T, that’s mT. And then RT(0) is going to be CT. So just to again illustrate how this would work in the example, if we start here right? If we condition, so the first step would be to compute again to initialize using the distribution of given D3. And that is a normal with mean a3(0) and variance covariance matrix R3(0), But those are precisely m3 and C3 respectively. Then we go backwards one step. And if we want to look at what is the distribution of <span class="math inline">\theta^2</span>, now conditional on D3. That’s a normal with mean a3(-1) and variance covariance matrix R3(-1). So if you look at the equations down here, you will see that, in order to compute a3 (-1), and R3(-1). You’re going to need m2,C2, a3,R3 and then what you computed here these moments in the previous step, a3(0) and R3(0). Then you obtain that distribution and you can now look at the distribution of given D3, that’s the normal a3(-2), R3(-2). And once again, to compute these moments, you’re going to need <span class="math inline">m1,C1,a2,R2</span> and then you’re going to need a3(-1),R3(-1). And you can continue all the way down to given D3 using these recursions. So the smoothing equations allow us to, just compute all these distributions. And the important equations work basically because of the linear and Gaussian structure in the normal dynamic linear model.</p>
</blockquote>
</section>
<section id="forecasting-1" class="level3" data-number="93.4.1">
<h3 data-number="93.4.1" class="anchored" data-anchor-id="forecasting-1"><span class="header-section-number">93.4.1</span> Forecasting</h3>
<blockquote class="blockquote">
<p>In a similar way, we can compute the forecasting distributions. Now we are going to be looking forward, and in the case of forecasting, we are interested in the distribution of <span class="math inline">\theta(t+h)</span> given <span class="math inline">D_t</span>. And now h is a positive lag. So here we assume that is <span class="math inline">h≥0</span>. So we are going to have the recursion is a <span class="math inline">N(a_t(h), R_t(h))</span>. The mean is <span class="math inline">a_t(h)</span> and we are going to use the structure of the model to obtain these recursions, again. So here we are using the system equation, and the moment at(h) depends on what you computed at <span class="math inline">a_t(h-1)</span> the previous lag, times <span class="math inline">G_{t+h}</span>. And then, would you initialize the recursion with <span class="math inline">a_t(0)=m_t</span>.</p>
</blockquote>
<blockquote class="blockquote">
<p>Similarly, for the covariance matrix h steps ahead, you’re going to have a recursion that depends on <span class="math inline">Rt(h-1)</span>. And then you’re going to need to input also <span class="math inline">G_{t+h}</span> and <span class="math inline">W_{t+h}</span>. To initialize, the recursion with <span class="math inline">Rt(0)= Ct</span>. So you can see that in order to compute these moments, you’re going to need mt and Ct to start with. And then you’re also going to have to input all the G’s and the W’s for the number of steps ahead that you require.</p>
</blockquote>
<blockquote class="blockquote">
<p>Similarly, you can compute the distribution, the h steps ahead distribution of y_t+h given Dt. And that one also follows a normal, with mean <span class="math inline">f_t(h)</span>, <span class="math inline">q_t(h)</span>. And now we also have a recursion here, <span class="math inline">ft(h)</span> depends on <span class="math inline">at(h)</span> and as we said, <span class="math inline">a_t(h)</span> depends on <span class="math inline">a_t(h-1)</span> and so on. And <span class="math inline">q_t(h)</span> is just given by these equations. So once again, you have to have access to <span class="math inline">F_{t+h}</span> for all the <span class="math inline">h</span>, a number of steps ahead that you are trying to compute this distribution. And then you also have to provide the observational variance for every h value. So that you get <span class="math inline">v_{t+h}</span>. So this is specified in the modeling framework as well. If you want proceed with the forecasting distributions.</p>
</blockquote>
</section>
</div>
</div>
</div>
</section>
</section>
<section id="summary-of-the-smoothing-and-forecasting-distributions-reading" class="level2" data-number="93.5">
<h2 data-number="93.5" class="anchored" data-anchor-id="summary-of-the-smoothing-and-forecasting-distributions-reading"><span class="header-section-number">93.5</span> Summary of the smoothing and forecasting distributions (reading)</h2>
<!-- start -->
<section id="bayesian-inference-in-ndlm-known-variances-1" class="level3" data-number="93.5.1">
<h3 data-number="93.5.1" class="anchored" data-anchor-id="bayesian-inference-in-ndlm-known-variances-1"><span class="header-section-number">93.5.1</span> Bayesian Inference in NDLM: Known Variances</h3>
<p>Consider the NDLM given by:</p>
<p><span id="eq-inference-NDLM"><span class="math display"> \begin{aligned}
y_t &amp;= \mathbf{F}_t' \mathbf{\theta}_t + \nu_t, &amp;\nu_t &amp;\sim \mathcal{N} (0, v_t), \\
\mathbf{\theta}_t &amp;= \mathbf{G}_t \mathbf{\theta}_{t-1} + \mathbf{\omega}_t, &amp;\mathbf{\omega}_t &amp;\sim \mathcal{N} (0, \mathbf{W}_t), \\
&amp;\{ \mathbf{F}_t, \mathbf{G}_t, v_t, \mathbf{W}_t \}  &amp;(\mathbf{\omega}_0 \mid \mathcal{D}_0) &amp;\sim  \mathcal{N}(\mathbf{m}_0, \mathbf{C}_0)
\end{aligned}
\tag{93.12}</span></span></p>
<p>with <span class="math inline">F_t</span>, <span class="math inline">G_t</span>, <span class="math inline">v_t</span>, and <span class="math inline">W_t</span> known.</p>
<p>We also assume a prior distribution of the form <span class="math inline">(\theta_0 \mid D_0) \sim  \mathcal{N}(m_0, C_0)</span>, with <span class="math inline">m_0</span> and <span class="math inline">C_0</span> known.</p>
<section id="smoothing-2" class="level4" data-number="93.5.1.1">
<h4 data-number="93.5.1.1" class="anchored" data-anchor-id="smoothing-2"><span class="header-section-number">93.5.1.1</span> Smoothing</h4>
<p>For <span class="math inline">t &lt; T</span>, we have that:</p>
<p><span class="math display">
(\theta_t \mid D_T) \sim  \mathcal{N}(a_T(t - T), R_T(t - T)),
</span></p>
<p>where</p>
<p><span class="math display">
a_T(t - T) = m_t - B_t [a_{t+1} - a_T(t - T + 1)],
</span></p>
<p><span class="math display">
R_T(t - T) = C_t - B_t [R_{t+1} - R_T(t - T + 1)] B_t',
</span></p>
<p>for <span class="math inline">t = (T - 1), (T - 2), \dots, 0</span>, with <span class="math inline">B_t = C_t G_t' R_{t+1}^{-1}</span>, and <span class="math inline">a_T(0) = m_T</span>, <span class="math inline">R_T(0) = C_T</span>. Here <span class="math inline">a_t</span>, <span class="math inline">m_t</span>, <span class="math inline">R_t</span>, and <span class="math inline">C_t</span> are obtained using the filtering equations as explained before.</p>
</section>
<section id="forecasting-2" class="level4" data-number="93.5.1.2">
<h4 data-number="93.5.1.2" class="anchored" data-anchor-id="forecasting-2"><span class="header-section-number">93.5.1.2</span> Forecasting</h4>
<p>For <span class="math inline">h \geq 0</span>, it is possible to show that:</p>
<p><span class="math display">
(\theta_{t+h} \mid D_t) \sim  \mathcal{N}(a_t(h), R_t(h)),
</span></p>
<p><span class="math display">
(y_{t+h} \mid D_t) \sim  \mathcal{N}(f_t(h), q_t(h)),
</span></p>
<p>with</p>
<p><span class="math display">
a_t(h) = G_{t+h} a_t(h - 1),
</span></p>
<p><span class="math display">
R_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},
</span></p>
<p><span class="math display">
f_t(h) = F_{t+h}' a_t(h),
</span></p>
<p><span class="math display">
q_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},
</span></p>
<p>and</p>
<p><span class="math display">
a_t(0) = m_t, \quad R_t(0) = C_t.
</span></p>
<!-- end -->
</section>
</section>
</section>
<section id="smoothing-in-the-ndlm-example-video" class="level2" data-number="93.6">
<h2 data-number="93.6" class="anchored" data-anchor-id="smoothing-in-the-ndlm-example-video"><span class="header-section-number">93.6</span> Smoothing in the NDLM, Example (Video)</h2>
</section>
<section id="r-code-smoothing-in-the-ndlm-example-reading" class="level2" data-number="93.7">
<h2 data-number="93.7" class="anchored" data-anchor-id="r-code-smoothing-in-the-ndlm-example-reading"><span class="header-section-number">93.7</span> R-code: Smoothing in the NDLM, Example (Reading)</h2>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="do">##### Univariate DLM: Known, constant variances</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>set_up_dlm_matrices <span class="ot">&lt;-</span> <span class="cf">function</span>(FF, GG, VV, WW){</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">FF=</span>FF, <span class="at">GG=</span>GG, <span class="at">VV=</span>VV, <span class="at">WW=</span>WW))</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>set_up_initial_states <span class="ot">&lt;-</span> <span class="cf">function</span>(m0, C0){</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">m0=</span>m0, <span class="at">C0=</span>C0))</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="do">### forward update equations </span><span class="al">###</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>forward_filter <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, initial_states){</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve dataset</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve a set of quadruples </span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># FF, GG, VV, WW are scalar</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF  </span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve initial states</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>  m0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>m0</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>  C0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>C0</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for results</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(GG)[<span class="dv">1</span>]</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>  et <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T){</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of priors at t</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(m0)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> C0 <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of one-step forecast:</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> (at[i, ]) </span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of posterior at t:</span></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>    At <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">/</span> Qt[i]</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>    et[i] <span class="ot">&lt;-</span> y_t[i] <span class="sc">-</span> ft[i]</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>    mt[i, ] <span class="ot">&lt;-</span> at[i, ] <span class="sc">+</span> <span class="fu">t</span>(At) <span class="sc">*</span> et[i]</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>    Ct[, , i] <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">-</span> Qt[i] <span class="sc">*</span> At <span class="sc">%*%</span> <span class="fu">t</span>(At)</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>    Ct[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Ct[,,i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Ct[,,i]) </span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forward filtering is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mt =</span> mt, <span class="at">Ct =</span> Ct, <span class="at">at =</span> at, <span class="at">Rt =</span> Rt, </span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>              <span class="at">ft =</span> ft, <span class="at">Qt =</span> Qt))</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>forecast_function <span class="ot">&lt;-</span> <span class="cf">function</span>(posterior_states, k, matrices){</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>  <span class="do">## set up matrices</span></span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>] <span class="co"># time points</span></span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>] <span class="co"># dimension of state parameter vector</span></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>  <span class="do">## placeholder for results</span></span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> k, <span class="at">ncol =</span> d)</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, k))</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of state distribution</span></span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[T, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , T] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(at[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Rt[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of forecast distribution</span></span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(at[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forecasting is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">at=</span>at, <span class="at">Rt=</span>Rt, <span class="at">ft=</span>ft, <span class="at">Qt=</span>Qt))</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a><span class="do">## obtain 95% credible interval</span></span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>get_credible_interval <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma2, </span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a>                          <span class="at">quantile =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)){</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a>  z_quantile <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(quantile)</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>  bound <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">length</span>(mu), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">1</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> z_quantile[<span class="dv">1</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># lower bound</span></span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">2</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> z_quantile[<span class="dv">2</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># upper bound</span></span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(bound)</span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a><span class="do">### smoothing equations </span><span class="al">###</span></span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>backward_smoothing <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, </span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>                               posterior_states){</span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve data </span></span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t) </span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>at</span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Rt</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for posterior moments </span></span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>  mnt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>], <span class="at">ncol =</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>])</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a>  Cnt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim =</span> <span class="fu">dim</span>(Ct))</span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a>  fnt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>  Qnt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> T<span class="sc">:</span><span class="dv">1</span>){</span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for the distributions of the state vector given D_T</span></span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> T){</span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a>      mnt[i, ] <span class="ot">&lt;-</span> mt[i, ]</span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> Ct[, , i]</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Cnt[, , i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Cnt[, , i]) </span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>      inv_Rtp1<span class="ot">&lt;-</span><span class="fu">solve</span>(Rt[,,i<span class="sc">+</span><span class="dv">1</span>])</span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>      Bt <span class="ot">&lt;-</span> Ct[, , i] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">%*%</span> inv_Rtp1</span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>      mnt[i, ] <span class="ot">&lt;-</span> mt[i, ] <span class="sc">+</span> Bt <span class="sc">%*%</span> (mnt[i<span class="sc">+</span><span class="dv">1</span>, ] <span class="sc">-</span> at[i<span class="sc">+</span><span class="dv">1</span>, ])</span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> Ct[, , i] <span class="sc">+</span> Bt <span class="sc">%*%</span> (Cnt[, , i <span class="sc">+</span> <span class="dv">1</span>] <span class="sc">-</span> Rt[, , i<span class="sc">+</span><span class="dv">1</span>]) <span class="sc">%*%</span> <span class="fu">t</span>(Bt)</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a>      Cnt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Cnt[,,i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Cnt[,,i]) </span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for the smoothed distribution of the mean response of the series</span></span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a>    fnt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(mnt[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a>    Qnt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(Cnt[, , i]) <span class="sc">%*%</span> FF</span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Backward smoothing is completed!"</span>)</span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mnt =</span> mnt, <span class="at">Cnt =</span> Cnt, <span class="at">fnt=</span>fnt, <span class="at">Qnt=</span>Qnt))</span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a><span class="do">####################### Example: Lake Huron Data ######################</span></span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(LakeHuron,<span class="at">main=</span><span class="st">"Lake Huron Data"</span>,<span class="at">ylab=</span><span class="st">"level in feet"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-smoothing-in-the-NDLM-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="C4-L03_files/figure-html/lst-smoothing-in-the-NDLM-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 98 observations total </span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>k<span class="ot">=</span><span class="dv">4</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(LakeHuron)<span class="sc">-</span>k <span class="co"># We take the first 94 observations </span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                     <span class="co">#  as our data</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>ts_data<span class="ot">=</span>LakeHuron[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>ts_validation_data <span class="ot">&lt;-</span> LakeHuron[(T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">98</span>]</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y_t =</span> ts_data)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="do">## set up matrices</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>FF <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>GG <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">570</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>C0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fl">1e4</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="do">## wrap up all matrices and initial values</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>matrices <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF,GG,VV,WW)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>initial_states <span class="ot">&lt;-</span> <span class="fu">set_up_initial_states</span>(m0, C0)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>results_filtered <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices, </span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>                                   initial_states)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Forward filtering is completed!</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>ci_filtered<span class="ot">&lt;-</span><span class="fu">get_credible_interval</span>(results_filtered<span class="sc">$</span>mt,</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>                                   results_filtered<span class="sc">$</span>Ct)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="do">## smoothing</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>results_smoothed <span class="ot">&lt;-</span> <span class="fu">backward_smoothing</span>(data, matrices, </span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>                                       results_filtered)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Backward smoothing is completed!</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>ci_smoothed <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_smoothed<span class="sc">$</span>mnt, </span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>                                     results_smoothed<span class="sc">$</span>Cnt)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>index<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1875</span>, <span class="dv">1972</span>, <span class="at">length.out =</span> <span class="fu">length</span>(LakeHuron))</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>index_filt<span class="ot">=</span>index[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index, LakeHuron, <span class="at">main =</span> <span class="st">"Lake Huron Level "</span>,<span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">ylab=</span><span class="st">"level in feet"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">575</span>,<span class="dv">583</span>))</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_filtered<span class="sc">$</span>mt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered[,<span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered[,<span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_smoothed<span class="sc">$</span>mnt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_smoothed[,<span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_smoothed[,<span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">'bottomleft'</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"filtered"</span>,<span class="st">"smoothed"</span>),</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-smoothing-in-the-NDLM-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="C4-L03_files/figure-html/lst-smoothing-in-the-NDLM-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="second-order-polynomial-filtering-and-smoothing-example-video" class="level2" data-number="93.8">
<h2 data-number="93.8" class="anchored" data-anchor-id="second-order-polynomial-filtering-and-smoothing-example-video"><span class="header-section-number">93.8</span> Second order polynomial: Filtering and smoothing example (Video)</h2>
<p>In this video walk through the code provided in the section below the comment</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/EMgQY/second-order-polynomial-filtering-and-smoothing-example -->
<p>We now consider another example where instead of fitting a first order polynomial we’re fitting a second order polynomial DLM. So I just want to show you how to set up the structure of the model in a case in which you have a state parameter vector. That is of dimension larger than one in this particular case we have a bivariate state parameter vector. So once again we are going to source this file that has all the DLM functions for the case in which the <span class="math inline">F</span>, <span class="math inline">G</span>, <span class="math inline">V</span> and <span class="math inline">W</span> are known. So we’re just assuming that this is the case and then we’re assuming that <span class="math inline">F</span>, <span class="math inline">G</span>, <span class="math inline">V</span> and <span class="math inline">W</span> are constant over time in these examples. So we just I’m going to use a new data set which is also data set available in R this data set corresponds to the atmospheric CO2 concentrations in parts per million in the location of Mauna Loa. And this is monthly data so I’m just plotting the data here. If you look at the data you can see that it has two important features. One of them is an increasing trend as the time increases the concentration increases. And then the other very specific feature that you can see in this data set is this seasonal behavior. So right now what I’m going to do with this example is we are going to ignore the seasonal behavior, and we are going to try to fit the model that captures the linear increasing trend using a second order polynomial model.</p>
<p>So I’m going to just specify everything here. We are going to use the entire data set here. We’re going to analyze the entire data. We are going to read in this into a list and then we’re going to set up the DLM in matrices. So here because the model it’s a second order polynomial we are going to have a state vector. That is of dimension two the F matrix is going to be, so it’s a vector that has 1 in the first entry and 0 in the second one. And then G is this upper triangular matrix that has 1s in the diagonal and 1 above the diagonal as well. So the two parameters that we’re fitting here one of them you can view the two components in the state of theta_t parameter vector. The first component corresponds to the baseline of the level and then the second component corresponds to the rate of growth in that level that we are fitting. So just defining the F and G like that. And then V the observational variance I’m just going to set it at 10. You can play with different numbers here, and the W is a diagonal matrix with .0001 in each of the elements in the diagonal. So these models are not as flexible as the ones that we are going to consider later. So in particular we are using an assumption that the two components in the state sector are independent over time which is usually not very realistic. And we can consider more flexible models later but just to show you here how to fit these models, for the prior distribution I have again two components. So I’m going to say that a priori my baseline is 315 parts per million. And then for the second, the rate of growth is going to be 0 a priori. And then I have C0 which is this 10 times the diagonal of dimension 2 so this is an identity matrix. So is we have a diagonal with the elements in the diagonal equal to 10. So we wrap up all the DLM matrices with the functions that we defined before. And then we proceed with the filtering equations just using the forward filter function. We can obtain credible intervals for the expected value of y_t via this filtering equations.</p>
<p>So the reason why I’m calling it the expected value of <span class="math inline">y_t</span> via filtering it’s just the first component of the say that theta_t vectors. So that corresponds to the level of the series, the expected value of that <span class="math inline">y_t</span>. And then, I can compute the smoothing equations using the backward smoothing. And again I have to pass the data, the structure of the model in terms of the matrices and the results that I obtained via the filtering equations. And I can compute credible intervals for this expected value via smoothing and as we mentioned before, it has the same structure the smoothing and the filtering is just that, we call the mean and the variance mt and Ct. In the case of the filtering equations for the smoothing equations we just call them mnt and Cnt. So now we can plot all the results here. I’m just going to plot the results that correspond to the smoothing distributions just for you to see. And we can see here that is this trend that is estimated here is capturing the structure of this linear increasing trend. And you can play with different values of the signal to noise ratio. So different values of the V and the W. And if you change the values so that there is more or less signal to noise ratio, you will see that you will capture more of the seasonal structure and less of this linear trend structure. If you were to change those values. So if I go back a little bit here you can see that I have a very low signal to noise ratio and I picked this on purpose, because I didn’t want to capture any of the seasonal behavior that I observe in the series through these parameters. So I’m assuming that a lot of the variation that I see now I’m just keeping it in the noise. Just because I want to just get a very smooth estimate for this linear trend through a second order polynomial model. In practice what we’re going to do later is we really want to construct a model in which we have a component for the linear trend using the second order polynomial model. And then we add another component that will allow us to capture also the seasonal behavior that we observe in this series using a Fourier component model. So we will illustrate that later, in a separate example here is just again to show you how to use the code for specifying a second order polynomial.</p>
</div>
</div>
</div>
</section>
<section id="using-the-dlm-package-in-r-video" class="level2" data-number="93.9">
<h2 data-number="93.9" class="anchored" data-anchor-id="using-the-dlm-package-in-r-video"><span class="header-section-number">93.9</span> Using the dlm package in R (Video)</h2>
<p>The <code>dlm</code> package in R is a powerful tool for working with dynamic linear models. The package provides a wide range of functions for filtering, smoothing, forecasting, and parameter estimation in DLMs. In this video, we walk through the code provided in <a href="#lst-dlm-package" class="quarto-xref">Listing&nbsp;<span>93.7</span></a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Video Transcript
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<!-- transcript of https://www.coursera.org/learn/bayesian-statistics-time-series-analysis/lecture/lg1g0/using-the-dlm-package-in-r -->
<p>So here I’m going to show you how to use the <code>dlm</code> package to fit these dynamic linear models as well. So the dlm is package that is available from Cran. And it allows you to compute the filtering smoothing and forecasting equations for dynamic linear models. So I’m just going to show you how to do the same thing we’ve been doing with the code that I provided just using the dlm package. So I’m going to just run here the first examples that we ran. And I’m going to show you how to do the same again. So here, I’m just going through the Lake Huron data. So just setting up every_thing as we did before. And then going through the filtering and smoothing equations. And so we can now plot the results and just want to have all the results here. So we have the red line corresponds to the posterior mean for the distribution of <span class="math inline">\theta_t</span> given the <span class="math inline">Dt</span> using a first order polynomial model to fit the data. And the blue line corresponds to the smoothing mean. So the mean of the posterior distribution of the smoothing equations here. So now we can look at how to fit this with the dlm package. So you have to call, install the package if you don’t have it installed. And then just call that library once you have installed the package. And the dlm package has a different set of functions to construct the model first.</p>
<p>So I’m going to use the function that is called the <code>dlmModPoly</code>, which allows you to fit polynomial models. So it constructs the polynomial models. The default function as you can see here is a function in that assumes that the polynomial model is of order 2. So here I want to polynomial model of all the 1. And then I’m going to specify the variance at the observational level, which is called dV in that package. dW is the variance at the evolution level. And then I have my prior mean for theta and the prior variance. I’m just using exactly the same prior distribution. And the package provides two functions of the dlm filter function allows you to providing the data. And the model that you just define computes the filtering recursions here. And then there is another function that is called the dlmSmooth that you essentially pass the results of the filtering equations. And then you obtain the smoothing distributions. So we’re just going to do that. And now I’m going to plot the results that I obtained from those filtering equations. One thing that you can see here, if I do names of, let’s say results_filter_dlm. You can see that the way in which the dlm functions from the dlm package keep the results. It has a particular format. So in the case of the dlm package, you’re going to have the information about what model you fitted. Then you have the mean of theta_t given Dt is kept in this m object. And then you have a is the prior mean of theta_t, given the t -1. And then f is the mean of the one step ahead forecast distribution. And then you have these U.C, D.C, U.R, D.R, those are just decompositions of the C variance matrix. So each of the Cs at time t. And then if you have also the composition of the R matrices. So the model, the way in which the functions are implemented in this <code>dlm</code> package. Assume used an SVD decomposition of all the matrices. So you have to keep in mind if you’re going to recover the structure here for the different components in the model. You have to keep this in mind. So for the filtering results, this is the structure. If you do names of the results, smooth, with the dlm package. You’re going to have again, here is the mean here that is called S and then you have the decomposition of the matrix as well. So, I’m just going to plot now for the filtering results. I’m just going to plot the mean here. And then for the smoothing distribution, I’m also going to plot that means. In this case, we’re working with the first order polynomial. So the dimension of the state vector is 1. So you can see that we obtain exactly the same results. And you can compare them numerically. The upper plot corresponds to the results we get with the code that we’ve been using. And the second block corresponds to just using the code from the dlm package. We can also run the example with the second order polynomial. So again, if I use the specification of the model that we use before with the functions that we described. I can keep my results there. And if I use the dlm package, I can use again, this is a second order polynomial model. I say that the order of the polynomial is 2, I use this dlmModPoly function. I specify the observational variance, the system variance m0 and C0. So I’m using exactly the same priors in this case. And then I use the dlm filter function and the dlm smooth just to compute the moments of the filtering and smoothing distributions. And then I can plot every_thing here. We are plotting just the first component here. The posterior distribution for the first component of the theta vector. Which also corresponds to the expected value of the <span class="math inline">y_t</span>. And then if I do the same with the dlm package, you can see that you obtain the same results. So again, the upper plot corresponds to the results that we get from the code that we’ve been using. And then the bottom plot corresponds to the results that we get from the dlm package. So I just wanted to illustrate this. You’re welcome to always use the dlm package. Just keep in mind the structure in which the matrices are kept is a little bit different than what we have been discussing. Because the dlm package uses and SVD decomposition of the covariance matrices and keeps every_thing like that. So there are some differences. But you can also use this package to obtain inference in the case of dynamic linear models.</p>
</div>
</div>
</div>
</section>
<section id="r-code-using-the-dlm-package-in-r-reading" class="level2" data-number="93.10">
<h2 data-number="93.10" class="anchored" data-anchor-id="r-code-using-the-dlm-package-in-r-reading"><span class="header-section-number">93.10</span> R-code: Using the <code>dlm</code> package in R (Reading)</h2>
<div class="cell">
<div id="lst-dlm-package" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;93.1: Using the <code>dlm</code> package for dynamic linear models
</figcaption>
<div aria-describedby="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="do">##### Univariate DLM: Known, constant variances</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>set_up_dlm_matrices <span class="ot">&lt;-</span> <span class="cf">function</span>(FF, GG, VV, WW){</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">FF=</span>FF, <span class="at">GG=</span>GG, <span class="at">VV=</span>VV, <span class="at">WW=</span>WW))</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>set_up_initial_states <span class="ot">&lt;-</span> <span class="cf">function</span>(m0, C0){</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">m0=</span>m0, <span class="at">C0=</span>C0))</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="do">### forward update equations </span><span class="al">###</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>forward_filter <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, initial_states){</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve dataset</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve a set of quadruples </span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># FF, GG, VV, WW are scalar</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF  </span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve initial states</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>  m0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>m0</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>  C0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>C0</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for results</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(GG)[<span class="dv">1</span>]</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>  et <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T){</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of priors at t</span></span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(m0)</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> C0 <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of one-step forecast:</span></span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> (at[i, ]) </span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of posterior at t:</span></span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>    At <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">/</span> Qt[i]</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>    et[i] <span class="ot">&lt;-</span> y_t[i] <span class="sc">-</span> ft[i]</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a>    mt[i, ] <span class="ot">&lt;-</span> at[i, ] <span class="sc">+</span> <span class="fu">t</span>(At) <span class="sc">*</span> et[i]</span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a>    Ct[, , i] <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">-</span> Qt[i] <span class="sc">*</span> At <span class="sc">%*%</span> <span class="fu">t</span>(At)</span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a>    Ct[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Ct[,,i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Ct[,,i]) </span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forward filtering is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mt =</span> mt, <span class="at">Ct =</span> Ct, <span class="at">at =</span> at, <span class="at">Rt =</span> Rt, </span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>              <span class="at">ft =</span> ft, <span class="at">Qt =</span> Qt))</span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>forecast_function <span class="ot">&lt;-</span> <span class="cf">function</span>(posterior_states, k, matrices){</span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a>  <span class="do">## set up matrices</span></span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>] <span class="co"># time points</span></span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>] <span class="co"># dimension of state parameter vector</span></span>
<span id="cb22-81"><a href="#cb22-81" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-82"><a href="#cb22-82" aria-hidden="true" tabindex="-1"></a>  <span class="do">## placeholder for results</span></span>
<span id="cb22-83"><a href="#cb22-83" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> k, <span class="at">ncol =</span> d)</span>
<span id="cb22-84"><a href="#cb22-84" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, k))</span>
<span id="cb22-85"><a href="#cb22-85" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb22-86"><a href="#cb22-86" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb22-87"><a href="#cb22-87" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-88"><a href="#cb22-88" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-89"><a href="#cb22-89" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb22-90"><a href="#cb22-90" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of state distribution</span></span>
<span id="cb22-91"><a href="#cb22-91" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb22-92"><a href="#cb22-92" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[T, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb22-93"><a href="#cb22-93" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , T] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb22-94"><a href="#cb22-94" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb22-95"><a href="#cb22-95" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb22-96"><a href="#cb22-96" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(at[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb22-97"><a href="#cb22-97" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Rt[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb22-98"><a href="#cb22-98" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb22-99"><a href="#cb22-99" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb22-100"><a href="#cb22-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-101"><a href="#cb22-101" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of forecast distribution</span></span>
<span id="cb22-102"><a href="#cb22-102" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(at[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb22-103"><a href="#cb22-103" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb22-104"><a href="#cb22-104" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb22-105"><a href="#cb22-105" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forecasting is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb22-106"><a href="#cb22-106" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">at=</span>at, <span class="at">Rt=</span>Rt, <span class="at">ft=</span>ft, <span class="at">Qt=</span>Qt))</span>
<span id="cb22-107"><a href="#cb22-107" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-108"><a href="#cb22-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-109"><a href="#cb22-109" aria-hidden="true" tabindex="-1"></a><span class="do">## obtain 95% credible interval</span></span>
<span id="cb22-110"><a href="#cb22-110" aria-hidden="true" tabindex="-1"></a>get_credible_interval <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma2, </span>
<span id="cb22-111"><a href="#cb22-111" aria-hidden="true" tabindex="-1"></a>                          <span class="at">quantile =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)){</span>
<span id="cb22-112"><a href="#cb22-112" aria-hidden="true" tabindex="-1"></a>  z_quantile <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(quantile)</span>
<span id="cb22-113"><a href="#cb22-113" aria-hidden="true" tabindex="-1"></a>  bound <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">length</span>(mu), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb22-114"><a href="#cb22-114" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">1</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> z_quantile[<span class="dv">1</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># lower bound</span></span>
<span id="cb22-115"><a href="#cb22-115" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">2</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> z_quantile[<span class="dv">2</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># upper bound</span></span>
<span id="cb22-116"><a href="#cb22-116" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(bound)</span>
<span id="cb22-117"><a href="#cb22-117" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-118"><a href="#cb22-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-119"><a href="#cb22-119" aria-hidden="true" tabindex="-1"></a><span class="do">### smoothing equations </span><span class="al">###</span></span>
<span id="cb22-120"><a href="#cb22-120" aria-hidden="true" tabindex="-1"></a>backward_smoothing <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, </span>
<span id="cb22-121"><a href="#cb22-121" aria-hidden="true" tabindex="-1"></a>                               posterior_states){</span>
<span id="cb22-122"><a href="#cb22-122" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve data </span></span>
<span id="cb22-123"><a href="#cb22-123" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb22-124"><a href="#cb22-124" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t) </span>
<span id="cb22-125"><a href="#cb22-125" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-126"><a href="#cb22-126" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb22-127"><a href="#cb22-127" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb22-128"><a href="#cb22-128" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb22-129"><a href="#cb22-129" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-130"><a href="#cb22-130" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb22-131"><a href="#cb22-131" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb22-132"><a href="#cb22-132" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb22-133"><a href="#cb22-133" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>at</span>
<span id="cb22-134"><a href="#cb22-134" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Rt</span>
<span id="cb22-135"><a href="#cb22-135" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb22-136"><a href="#cb22-136" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for posterior moments </span></span>
<span id="cb22-137"><a href="#cb22-137" aria-hidden="true" tabindex="-1"></a>  mnt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>], <span class="at">ncol =</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>])</span>
<span id="cb22-138"><a href="#cb22-138" aria-hidden="true" tabindex="-1"></a>  Cnt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim =</span> <span class="fu">dim</span>(Ct))</span>
<span id="cb22-139"><a href="#cb22-139" aria-hidden="true" tabindex="-1"></a>  fnt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb22-140"><a href="#cb22-140" aria-hidden="true" tabindex="-1"></a>  Qnt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb22-141"><a href="#cb22-141" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> T<span class="sc">:</span><span class="dv">1</span>){</span>
<span id="cb22-142"><a href="#cb22-142" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for the distributions of the state vector given D_T</span></span>
<span id="cb22-143"><a href="#cb22-143" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> T){</span>
<span id="cb22-144"><a href="#cb22-144" aria-hidden="true" tabindex="-1"></a>      mnt[i, ] <span class="ot">&lt;-</span> mt[i, ]</span>
<span id="cb22-145"><a href="#cb22-145" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> Ct[, , i]</span>
<span id="cb22-146"><a href="#cb22-146" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Cnt[, , i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Cnt[, , i]) </span>
<span id="cb22-147"><a href="#cb22-147" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb22-148"><a href="#cb22-148" aria-hidden="true" tabindex="-1"></a>      inv_Rtp1<span class="ot">&lt;-</span><span class="fu">solve</span>(Rt[,,i<span class="sc">+</span><span class="dv">1</span>])</span>
<span id="cb22-149"><a href="#cb22-149" aria-hidden="true" tabindex="-1"></a>      Bt <span class="ot">&lt;-</span> Ct[, , i] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">%*%</span> inv_Rtp1</span>
<span id="cb22-150"><a href="#cb22-150" aria-hidden="true" tabindex="-1"></a>      mnt[i, ] <span class="ot">&lt;-</span> mt[i, ] <span class="sc">+</span> Bt <span class="sc">%*%</span> (mnt[i<span class="sc">+</span><span class="dv">1</span>, ] <span class="sc">-</span> at[i<span class="sc">+</span><span class="dv">1</span>, ])</span>
<span id="cb22-151"><a href="#cb22-151" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> Ct[, , i] <span class="sc">+</span> Bt <span class="sc">%*%</span> (Cnt[, , i <span class="sc">+</span> <span class="dv">1</span>] <span class="sc">-</span> Rt[, , i<span class="sc">+</span><span class="dv">1</span>]) <span class="sc">%*%</span> <span class="fu">t</span>(Bt)</span>
<span id="cb22-152"><a href="#cb22-152" aria-hidden="true" tabindex="-1"></a>      Cnt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Cnt[,,i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Cnt[,,i]) </span>
<span id="cb22-153"><a href="#cb22-153" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb22-154"><a href="#cb22-154" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for the smoothed distribution of the mean response of the series</span></span>
<span id="cb22-155"><a href="#cb22-155" aria-hidden="true" tabindex="-1"></a>    fnt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(mnt[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb22-156"><a href="#cb22-156" aria-hidden="true" tabindex="-1"></a>    Qnt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(Cnt[, , i]) <span class="sc">%*%</span> FF</span>
<span id="cb22-157"><a href="#cb22-157" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb22-158"><a href="#cb22-158" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Backward smoothing is completed!"</span>)</span>
<span id="cb22-159"><a href="#cb22-159" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mnt =</span> mnt, <span class="at">Cnt =</span> Cnt, <span class="at">fnt=</span>fnt, <span class="at">Qnt=</span>Qnt))</span>
<span id="cb22-160"><a href="#cb22-160" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-161"><a href="#cb22-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-162"><a href="#cb22-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-163"><a href="#cb22-163" aria-hidden="true" tabindex="-1"></a><span class="do">####################### Example: Lake Huron Data ######################</span></span>
<span id="cb22-164"><a href="#cb22-164" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(LakeHuron) <span class="co"># 98 observations total </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-dlm-package-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="C4-L03_files/figure-html/lst-dlm-package-1.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div id="lst-dlm-package" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;93.2: Using the <code>dlm</code> package for dynamic linear models
</figcaption>
<div aria-describedby="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>k<span class="ot">=</span><span class="dv">4</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(LakeHuron)<span class="sc">-</span>k <span class="co"># We take the first </span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>                      <span class="co"># 94 observations only as our data</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>ts_data<span class="ot">=</span>LakeHuron[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>ts_validation_data <span class="ot">&lt;-</span> LakeHuron[(T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">98</span>]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y_t =</span> ts_data)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="do">## set up dlm matrices</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>GG <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>FF <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">570</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>C0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fl">1e4</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="do">## wrap up all matrices and initial values</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>matrices <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF, GG, VV, WW)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>initial_states <span class="ot">&lt;-</span> <span class="fu">set_up_initial_states</span>(m0, C0)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering and smoothing </span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>results_filtered <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices, </span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>                                   initial_states)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Forward filtering is completed!</code></pre>
</div>
<div id="lst-dlm-package" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;93.3: Using the <code>dlm</code> package for dynamic linear models
</figcaption>
<div aria-describedby="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>results_smoothed <span class="ot">&lt;-</span> <span class="fu">backward_smoothing</span>(data, matrices, </span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                                       results_filtered)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Backward smoothing is completed!</code></pre>
</div>
<div id="lst-dlm-package" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;93.4: Using the <code>dlm</code> package for dynamic linear models
</figcaption>
<div aria-describedby="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>index<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1875</span>, <span class="dv">1972</span>, <span class="at">length.out =</span> <span class="fu">length</span>(LakeHuron))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>index_filt<span class="ot">=</span>index[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index, LakeHuron, <span class="at">main =</span> <span class="st">"Lake Huron Level "</span>,<span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">ylab=</span><span class="st">"feet"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">575</span>,<span class="dv">583</span>))</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_filtered<span class="sc">$</span>mt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_smoothed<span class="sc">$</span>mnt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Now let's look at the DLM package </span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dlm)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>model<span class="ot">=</span><span class="fu">dlmModPoly</span>(<span class="at">order=</span><span class="dv">1</span>,<span class="at">dV=</span><span class="dv">1</span>,<span class="at">dW=</span><span class="dv">1</span>,<span class="at">m0=</span><span class="dv">570</span>,<span class="at">C0=</span><span class="fl">1e4</span>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>results_filtered_dlm<span class="ot">=</span><span class="fu">dlmFilter</span>(LakeHuron[<span class="dv">1</span><span class="sc">:</span>T],model)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>results_smoothed_dlm<span class="ot">=</span><span class="fu">dlmSmooth</span>(results_filtered_dlm)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index_filt, LakeHuron[<span class="dv">1</span><span class="sc">:</span>T], <span class="at">ylab =</span> <span class="st">"level"</span>, </span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Lake Huron Level"</span>,</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">'l'</span>, <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">575</span>,<span class="dv">583</span>))</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index_filt,LakeHuron[<span class="dv">1</span><span class="sc">:</span>T],<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt,results_filtered_dlm<span class="sc">$</span>m[<span class="sc">-</span><span class="dv">1</span>],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt,results_smoothed_dlm<span class="sc">$</span>s[<span class="sc">-</span><span class="dv">1</span>],<span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-dlm-package-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img src="C4-L03_files/figure-html/lst-dlm-package-2.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
<div id="lst-dlm-package" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;93.5: Using the <code>dlm</code> package for dynamic linear models
</figcaption>
<div aria-describedby="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Similarly, for the second order polynomial and the co2 data:</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(co2)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>data<span class="ot">=</span><span class="fu">list</span>(<span class="at">y_t =</span> co2)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>FF <span class="ot">&lt;-</span> (<span class="fu">as.matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>)))</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>GG <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span>T)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">200</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fl">0.01</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">c</span>(<span class="dv">320</span>,<span class="dv">0</span>)))</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>C0 <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="do">## wrap up all matrices and initial values</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>matrices <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF,GG, VV, WW)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>initial_states <span class="ot">&lt;-</span> <span class="fu">set_up_initial_states</span>(m0, C0)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering and smoothing </span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>results_filtered <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices, </span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>                                   initial_states)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Forward filtering is completed!</code></pre>
</div>
<div id="lst-dlm-package" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;93.6: Using the <code>dlm</code> package for dynamic linear models
</figcaption>
<div aria-describedby="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>results_smoothed <span class="ot">&lt;-</span> <span class="fu">backward_smoothing</span>(data, matrices, </span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>                                       results_filtered)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Backward smoothing is completed!</code></pre>
</div>
<div id="lst-dlm-package" class="r cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;93.7: Using the <code>dlm</code> package for dynamic linear models
</figcaption>
<div aria-describedby="lst-dlm-package-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="do">#### Now, using the DLM package: </span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>model<span class="ot">=</span><span class="fu">dlmModPoly</span>(<span class="at">order=</span><span class="dv">2</span>,<span class="at">dV=</span><span class="dv">200</span>,<span class="at">dW=</span><span class="fl">0.01</span><span class="sc">*</span><span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">2</span>),</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">m0=</span><span class="fu">c</span>(<span class="dv">320</span>,<span class="dv">0</span>),<span class="at">C0=</span><span class="dv">10</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>))</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># filtering and smoothing </span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>results_filtered_dlm<span class="ot">=</span><span class="fu">dlmFilter</span>(data<span class="sc">$</span>y_t,model)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>results_smoothed_dlm<span class="ot">=</span><span class="fu">dlmSmooth</span>(results_filtered_dlm)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),co2,<span class="at">type=</span><span class="st">'l'</span>,<span class="at">xlab=</span><span class="st">"time"</span>,</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">300</span>,<span class="dv">380</span>))</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),results_filtered<span class="sc">$</span>mt[,<span class="dv">1</span>],</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),results_smoothed<span class="sc">$</span>mnt[,<span class="dv">1</span>],</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),co2,<span class="at">type=</span><span class="st">'l'</span>,<span class="at">xlab=</span><span class="st">"time"</span>,</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">300</span>,<span class="dv">380</span>))</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),results_filtered_dlm<span class="sc">$</span>m[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),results_smoothed_dlm<span class="sc">$</span>s[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</figure>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><a href="C4-L03_files/figure-html/lst-dlm-package-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img src="C4-L03_files/figure-html/lst-dlm-package-3.png" class="img-fluid figure-img" width="672"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="practice-graded-assignment-ndlm-sensitivity-to-the-model-parameters" class="level2" data-number="93.11">
<h2 data-number="93.11" class="anchored" data-anchor-id="practice-graded-assignment-ndlm-sensitivity-to-the-model-parameters"><span class="header-section-number">93.11</span> Practice Graded Assignment: NDLM – sensitivity to the model parameters</h2>
<p>Omitted due to the Coursera honor code.</p>
</section>
<section id="quiz---ndlm-part-i-review" class="level2" data-number="93.12">
<h2 data-number="93.12" class="anchored" data-anchor-id="quiz---ndlm-part-i-review"><span class="header-section-number">93.12</span> Quiz - NDLM, Part I: Review</h2>
<p>Omitted due to the Coursera honor code.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-prado2000bayesian" class="csl-entry" role="listitem">
Prado, Raquel, Gabriel Huerta, and Mike West. 2000. <span>“Bayesian Time-Varying Autoregressions: Theory, Methods and Applications.”</span> <em>Resenhas Do Instituto de Matem<span>á</span>tica e Estat<span>ı́</span>stica Da Universidade de S<span>ã</span>o Paulo</em> 4 (4): 405–22. <a href="https://www2.stat.duke.edu/~mw/MWextrapubs/Prado2001.pdf">https://www2.stat.duke.edu/~mw/MWextrapubs/Prado2001.pdf</a>.
</div>
<div id="ref-prado2023time" class="csl-entry" role="listitem">
Prado, R., M. A. R. Ferreira, and M. West. 2023. <em>Time Series: Modeling, Computation, and Inference</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press. <a href="https://books.google.co.il/books?id=pZ6lzgEACAAJ">https://books.google.co.il/books?id=pZ6lzgEACAAJ</a>.
</div>
<div id="ref-west2013bayesian" class="csl-entry" role="listitem">
West, M., and J. Harrison. 2013. <em>Bayesian Forecasting and Dynamic Models</em>. Springer Series in Statistics. Springer New York. <a href="https://books.google.co.il/books?id=NmfaBwAAQBAJ">https://books.google.co.il/books?id=NmfaBwAAQBAJ</a>.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>the state makes it’s appearance<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C4-L02.html" class="pagination-link" aria-label="The AR(p) process">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(p) process</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C4-L04.html" class="pagination-link" aria-label="Normal Dynamic Linear Models, Part 2">
        <span class="nav-page-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 2</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb33" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2024-10-25</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Normal Dynamic Linear Models, Part 1"</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> Time Series Analysis</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Normal Dynamic Linear Models (NDLMs) are a class of models used for time series analysis that allow for flexible modeling of temporal dependencies."</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> </span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - Bayesian Statistics</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - Time Series</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span><span class="co"> </span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - Time Series</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - Filtering</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co">  - Kalman filtering</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co">  - Smoothing</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co">  - NDLM</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a><span class="co">  - Normal Dynamic Linear Models</span></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="co">  - Polynomial Trend Models</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="co">  - Regression Models</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co">  - Superposition Principle</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="co">  - R code</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> course-banner.png</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a><span class="an">fig-caption:</span><span class="co"> Notes about ... Bayesian Statistics</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a><span class="an">title-block-banner:</span><span class="co"> images/banner_deep.jpg</span></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a><span class="co">The challenge in taking notes in this courses is that the instructors speak out the maths as they go along. I supply the math but I want to extract the non-math parts and put them in the right place. </span></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Normal Dynamic Linear Models (NDLMs) are defined and illustrated in this module using several examples Model building based on the forecast function via the superposition principle is explained. Methods for Bayesian filtering, smoothing and forecasting for NDLMs in the case of known observational variances and known system covariance matrices are discussed and illustrated.</span><span class="co">]</span>{.mark}.</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>The Normal Dynamic Linear Model (DLM) is covered  <span class="co">[</span><span class="ot">@prado2023time pp. 117-144</span><span class="co">]</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>:::{.callout-note collapse="true"}</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Objectives {.unnumbered}</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Use R for analysis and forecasting of time series using NDLM (case of known observational and system variances) <span class="co">[</span><span class="ot">\#</span><span class="co">](#m3g1)</span></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Derive the equations to obtain posterior inference and forecasting in the NDLM with known observational and system variances, including the filtering, smoothing and forecasting equations <span class="co">[</span><span class="ot">\#</span><span class="co">](#m3g2)</span></span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Apply the NDLM superposition principle and explain the role of the forecast function <span class="co">[</span><span class="ot">\#</span><span class="co">](#m3g3)</span></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Define trend and regression normal DLMs <span class="co">[</span><span class="ot">\#</span><span class="co">](#m3g4)</span></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="va">[x]</span> Explain the general normal dynamic linear model (NDLM) representation <span class="co">[</span><span class="ot">\#</span><span class="co">](#m3g5)</span></span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Normal Dynamic Linear Model: Definition, Model classes &amp;  The Superposition Principle</span></span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a>Dynamic Linear Models (DLMs) extend classical linear regression to time-indexed data, introducing dependencies between observations through latent evolving parameters. A Normal DLM (NDLM) assumes Gaussian noise at both observation and system levels, enabling tractable Bayesian inference through the Kalman filter.</span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>While superficially complex, NDLMs are conceptually close to linear regression. Instead of I.I.D. observations indexed by $i$, we index data by time $t$ and allow parameters to evolve with time, resulting in a two-level hierarchical model. At the top level is the observation equation. Below this there is the evolution equation(s) that can be understood as a latent state transition model that can capture trends, periodicity, and regression. The evolution equations can have more than one level however we will see that with some work these are summarized into a matrix form.</span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>To make things simpler this is demonstrated using a white noise process and then a random walk model. What makes the NDLM somewhat different is that that there are two variance elements at two levels, necessitating learning more parameters. Once we cover these to models the instructor walks us though all the bits and pieces of the notation. Later we will see that we can add trends, periodicity, regression components in a more or less systematic way. However we need to pick and choose these components to get a suitable forecast function. This approach require an intimate familiarity with the data generating process to model.</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>This approach is Bayesian in that we draw our parameters from a multivariate normal and use updating to improve this initial estimate by incorporating the data and we end up with a posterior i.e. we have distributional view of the time series incorporating uncertainties.</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>Additionally we have a number of Bayesian quantities that can be derived from the model, such as </span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the **filtering distribution** that estimates the current state $\mathbb{P}r(\theta_t \mid \mathcal{D}_t)$, </span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the **forecasting distribution** - to predict future observation: $\mathbb{P}r(y_{t+h} \mid \mathcal{D}_t)$, </span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the **smoothing distribution** - retrospective estimate of past state: $\mathbb{P}r(\theta_t \mid \mathcal{D}_{T})\quad t&lt;T$ and </span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the **forecast function** when $F_t=F$ and $\mathbf{G}_t=\mathbf{G}$ $f_t(h)=\mathbb{E}[y_{t+h} \mid \mathcal{D}_{T}] = F'G^h \mathbb{E}[\theta_{t} \mid \mathcal{D}_{T}]$ </span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the usual credible intervals for forecasts and parameter estimates.</span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>However the DLM framework is quite flexible and once you understand it it can ve adapted to support features like seasonality using the superposition principle. NDLMs don't need to be non-stationary time series.</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a>As far as I cen tell NDLMs are just DLM with their errors distributed normally at the different levels.</span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a><span class="fu">## NDLM Definition (Video)</span></span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a><span class="al">![NDLM Motivation](images/m3_0001.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-72"><a href="#cb33-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-73"><a href="#cb33-73" aria-hidden="true" tabindex="-1"></a><span class="al">![NDLM general form](images/m3_0002.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-74"><a href="#cb33-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-75"><a href="#cb33-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-76"><a href="#cb33-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-77"><a href="#cb33-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-78"><a href="#cb33-78" aria-hidden="true" tabindex="-1"></a><span class="al">![the forecast function](images/m3_0003.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-79"><a href="#cb33-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-80"><a href="#cb33-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-81"><a href="#cb33-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-82"><a href="#cb33-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-83"><a href="#cb33-83" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb33-84"><a href="#cb33-84" aria-hidden="true" tabindex="-1"></a><span class="co">::: {.callout-caution collapse="true"}</span></span>
<span id="cb33-85"><a href="#cb33-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-86"><a href="#cb33-86" aria-hidden="true" tabindex="-1"></a><span class="al">###</span><span class="co"> Hard to follow :weary:</span></span>
<span id="cb33-87"><a href="#cb33-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-88"><a href="#cb33-88" aria-hidden="true" tabindex="-1"></a><span class="co">- I found this video hard to follow. I watched it carefully a number of times and reworked the math and video's transcript. But in the end this and the next two video are more like a summary of the material rather than a motivated development of the models. </span></span>
<span id="cb33-89"><a href="#cb33-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-90"><a href="#cb33-90" aria-hidden="true" tabindex="-1"></a><span class="co">- The text also diverge when it comes to the $F$ in the state space representation equation. In @west2013bayesian it is a matrix and in @prado2023time it is called a vector. In some places it is referenced as a $p \times 1$. As far as I can tell this reflects different state space representation of the model which neither text is particularly clear. </span></span>
<span id="cb33-91"><a href="#cb33-91" aria-hidden="true" tabindex="-1"></a><span class="co">- Although we are give a couple of motivational examples the way they generalize to the NDLM is not as clear as in @west2013bayesian but this text is very verbose and covers much more material.</span></span>
<span id="cb33-92"><a href="#cb33-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-93"><a href="#cb33-93" aria-hidden="true" tabindex="-1"></a><span class="co">- @west2013bayesian also discusses how the math relates to more common statistical scenarios and how the model can be adapted to these scenarios e.g. replacing the obs. normal distribution with a t-distribution or a mixture of normals to handle outliers. I.e. see NDLMs as a looser framework with the assumptions of independence and normality as guidelines rather than strict assumptions.</span></span>
<span id="cb33-94"><a href="#cb33-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-95"><a href="#cb33-95" aria-hidden="true" tabindex="-1"></a><span class="co">In @prado2023time we quickly get a large number of mathematically concise format, that illustrate how NDLMs generalize other time series models. However these examples assume we are sufficiently familiar with this models.</span></span>
<span id="cb33-96"><a href="#cb33-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-97"><a href="#cb33-97" aria-hidden="true" tabindex="-1"></a><span class="co">- The NDLM is also a hierarchical model which we have not looked into in much depth in this specialization. This is another missed opportunity to connect to previous material and deepen our understanding. Particularly as the bayesian formulation should be able to overcome the limitations of the frequentist approach where we need to make strong assumptions on IID of the shocks but in a time series local observations often highly correlated.</span></span>
<span id="cb33-98"><a href="#cb33-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-99"><a href="#cb33-99" aria-hidden="true" tabindex="-1"></a><span class="co">- NDLM have efficient algorithms for inference and forecasting mostly by using the Kalman filter, again this is not mentioned not is a bayesian formulation presented</span></span>
<span id="cb33-100"><a href="#cb33-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-101"><a href="#cb33-101" aria-hidden="true" tabindex="-1"></a><span class="co">- So I guess the main issue is that the vide just lists lots of equations and does not really without providing a good intuition behind the model.</span></span>
<span id="cb33-102"><a href="#cb33-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-103"><a href="#cb33-103" aria-hidden="true" tabindex="-1"></a><span class="co">- I hope that with the rest of this and the next lesson we can get a better understanding of the NDLM.</span></span>
<span id="cb33-104"><a href="#cb33-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-105"><a href="#cb33-105" aria-hidden="true" tabindex="-1"></a><span class="co">- I ended up skimming through large portions of [@west2013bayesian] which goes into much greater detail on the NDLM. The first and second polynomial models developed, explained and motivation is clearer. The state space representation is not explained but the model is developed in a more coherent way and this is a good starting point for developing a more intuitive understanding of the NDLMs. The notation in this book is also easier to follow.</span></span>
<span id="cb33-106"><a href="#cb33-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-107"><a href="#cb33-107" aria-hidden="true" tabindex="-1"></a><span class="co">  - We use $\theta_t$ for the parameters of the vector and \theta_{t,1} and \theta_{t,2} for the first two components of the parameter vector. This continues the convention of greek characters for parameters from the previous courses and many texts. However if the parameters have a clear meaning it much easier to follow math where we use a more descriptive notation and avoid the second subscripts.</span></span>
<span id="cb33-108"><a href="#cb33-108" aria-hidden="true" tabindex="-1"></a><span class="co">- The state space representation here is omitted. The $F$ vector or matrix is not really motivates as a latent state space representation.</span></span>
<span id="cb33-109"><a href="#cb33-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-110"><a href="#cb33-110" aria-hidden="true" tabindex="-1"></a><span class="co">:::</span></span>
<span id="cb33-111"><a href="#cb33-111" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb33-112"><a href="#cb33-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-113"><a href="#cb33-113" aria-hidden="true" tabindex="-1"></a>In this module, <span class="co">[</span><span class="ot">we will motivate and develop a class of models suitable for for analyzing and forecasting **non-stationary time series** called  **normal dynamic linear models** . We will talk about Bayesian inference and forecasting within this class of models and describe model building as well</span><span class="co">]</span>{.mark}. </span>
<span id="cb33-114"><a href="#cb33-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-115"><a href="#cb33-115" aria-hidden="true" tabindex="-1"></a><span class="fu">### White Noise - A motivating example</span></span>
<span id="cb33-116"><a href="#cb33-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-117"><a href="#cb33-117" aria-hidden="true" tabindex="-1"></a>Let's begin with a very simple model that has no temporal structure, just a mean value with some variation that is: </span>
<span id="cb33-118"><a href="#cb33-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-119"><a href="#cb33-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-120"><a href="#cb33-120" aria-hidden="true" tabindex="-1"></a>y_t = \mu + v_t \qquad v_t \overset{\text{iid}}{\sim}  \mathcal{N}(0, \nu) \qquad  \text{(white noise model)}</span>
<span id="cb33-121"><a href="#cb33-121" aria-hidden="true" tabindex="-1"></a>$$ {#eq-white-noise-model}</span>
<span id="cb33-122"><a href="#cb33-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-123"><a href="#cb33-123" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb33-124"><a href="#cb33-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-125"><a href="#cb33-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y_t$ is the observed time series at time $t$,</span>
<span id="cb33-126"><a href="#cb33-126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu$ is the expected value of $y_t$ this is characteristic we are interested in, </span>
<span id="cb33-127"><a href="#cb33-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu_t$ is a white noise process  as usual iid standard normal N(0,1).</span>
<span id="cb33-128"><a href="#cb33-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-129"><a href="#cb33-129" aria-hidden="true" tabindex="-1"></a>If we plot this model we might see the following graph:</span>
<span id="cb33-130"><a href="#cb33-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-133"><a href="#cb33-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb33-134"><a href="#cb33-134" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-NDLM-white-noise</span></span>
<span id="cb33-135"><a href="#cb33-135" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb33-136"><a href="#cb33-136" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb33-137"><a href="#cb33-137" aria-hidden="true" tabindex="-1"></a>V <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb33-138"><a href="#cb33-138" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb33-139"><a href="#cb33-139" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mu <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, V)</span>
<span id="cb33-140"><a href="#cb33-140" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(y, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="st">"Time"</span>, <span class="at">ylab =</span> <span class="st">"y"</span>, <span class="at">main =</span> <span class="st">"Model with no temporal structure"</span>)</span>
<span id="cb33-141"><a href="#cb33-141" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-142"><a href="#cb33-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-143"><a href="#cb33-143" aria-hidden="true" tabindex="-1"></a>For this model the mean of the time series is $\mu$ will be the the expected value of $y_t$, which is $\mu$. And the variance of $y_t$ is $\nu$.</span>
<span id="cb33-144"><a href="#cb33-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-145"><a href="#cb33-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-146"><a href="#cb33-146" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">y_t</span><span class="co">]</span> = \mu \qquad \text{and} \qquad \mathbb{V}ar<span class="co">[</span><span class="ot">y_t</span><span class="co">]</span> = \nu \qquad</span>
<span id="cb33-147"><a href="#cb33-147" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-mean-variance}</span>
<span id="cb33-148"><a href="#cb33-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-149"><a href="#cb33-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-150"><a href="#cb33-150" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Random walk model with a slowly changing mean</span></span>
<span id="cb33-151"><a href="#cb33-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-152"><a href="#cb33-152" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Next we incorporate some temporal structure, we allow the expected value of the time series, to change over time. To can achieve this, by update the model definition with a $\mu_t$ where the index indicates that it can change at every time step. And let us keep the noise unchanged. i.e. we set it to $\mu_t \in N(0,\nu)$.</span><span class="co">]</span>{.mark}  </span>
<span id="cb33-153"><a href="#cb33-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-154"><a href="#cb33-154" aria-hidden="true" tabindex="-1"></a>We get the following model:</span>
<span id="cb33-155"><a href="#cb33-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-156"><a href="#cb33-156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-157"><a href="#cb33-157" aria-hidden="true" tabindex="-1"></a>y_t = \mu_t + \nu_t \quad \nu_t \overset{\text{iid}}{\sim} N(0, V) \qquad \text{(radom walk model)}</span>
<span id="cb33-158"><a href="#cb33-158" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-model-random-walk}</span>
<span id="cb33-159"><a href="#cb33-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-160"><a href="#cb33-160" aria-hidden="true" tabindex="-1"></a>To complete this we need to also decide how to incorporate the the changes over time in the parameter $\mu_t$. We might consider different options but we should pick the simplest possible to start with. One option is to assume that the expected value of $\mu_t$ is just the expected value of $\mu_{t-1}$ plus some noise. </span>
<span id="cb33-161"><a href="#cb33-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-162"><a href="#cb33-162" aria-hidden="true" tabindex="-1"></a>We now have that random walk type of structure where $\mu_t$ can be written in terms of $\mu(t-1)$. The expected value of $\mu_t$, we can think of it as $\mu_{t-1} + \text{some noise}$. This error is once again, assumed to be normally distributed random variable centered at zero and with variance $W$. </span>
<span id="cb33-163"><a href="#cb33-163" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Another assumption that we have made here is that the $\nu_t$ and $\omega_t$, are also independent of each other</span><span class="co">]</span>{.mark}. </span>
<span id="cb33-164"><a href="#cb33-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-165"><a href="#cb33-165" aria-hidden="true" tabindex="-1"></a>putting this together we get:</span>
<span id="cb33-166"><a href="#cb33-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-167"><a href="#cb33-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-168"><a href="#cb33-168" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-169"><a href="#cb33-169" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \mu_t + \nu_t &amp; \nu_t &amp; \overset{\text{iid}}{\sim}  \mathcal{N}(0, V)  &amp; \text{(Observation eq.)} <span class="sc">\\</span></span>
<span id="cb33-170"><a href="#cb33-170" aria-hidden="true" tabindex="-1"></a>\mu_t &amp;= \mu_{t-1} + \omega_t  &amp; \omega_t &amp; \overset{\text{iid}}{\sim}  \mathcal{N}(0, W) &amp; \text{(System/evolution eq.)}</span>
<span id="cb33-171"><a href="#cb33-171" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-172"><a href="#cb33-172" aria-hidden="true" tabindex="-1"></a>$$ {#eq-random-walk-model-hierarchical}</span>
<span id="cb33-173"><a href="#cb33-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-174"><a href="#cb33-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-175"><a href="#cb33-175" aria-hidden="true" tabindex="-1"></a>With this model, what we are assuming is that the mean level of the series is changing over time. </span>
<span id="cb33-176"><a href="#cb33-176" aria-hidden="true" tabindex="-1"></a>Note that this is an example of a **Gaussian or Normal dynamic linear model**. </span>
<span id="cb33-177"><a href="#cb33-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-178"><a href="#cb33-178" aria-hidden="true" tabindex="-1"></a>NDLMs are a two level hierarchical models where :</span>
<span id="cb33-179"><a href="#cb33-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-180"><a href="#cb33-180" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>At the top is an **observation level equation** relating observations y at time t to some time dependent, (hidden) state parameters and some observation level iid distributed error.</span>
<span id="cb33-181"><a href="#cb33-181" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The **system evolution level equation** describes the dynamics of parameters over time and incorporates some system iid distributed error.</span>
<span id="cb33-182"><a href="#cb33-182" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="co">[</span><span class="ot">These equations have a linear structure, in the sense that the expected value of y at time t is a linear function of the parameters.</span><span class="co">]</span>{.mark} </span>
<span id="cb33-183"><a href="#cb33-183" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>We have the **assumption of normality for the noise terms** in both these equations as well as independence within and between levels. </span>
<span id="cb33-184"><a href="#cb33-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-185"><a href="#cb33-185" aria-hidden="true" tabindex="-1"></a>This is our first example. Next we will be discuss the general class of models. Later we will consider how to incorporate different structures into the model, and how to perform Bayesian inference for filtering smoothing and forecasting.</span>
<span id="cb33-186"><a href="#cb33-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-187"><a href="#cb33-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-188"><a href="#cb33-188" aria-hidden="true" tabindex="-1"></a><span class="fu">### General form of the NDLM</span></span>
<span id="cb33-189"><a href="#cb33-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-190"><a href="#cb33-190" aria-hidden="true" tabindex="-1"></a>The general class of dynamic linear models can be written as follows:</span>
<span id="cb33-191"><a href="#cb33-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-192"><a href="#cb33-192" aria-hidden="true" tabindex="-1"></a>We are going to have two equations. One is the so-called observation equation that relates the observations to the parameters in the model, and the notation we are going to use is as follows.</span>
<span id="cb33-193"><a href="#cb33-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-194"><a href="#cb33-194" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb33-195"><a href="#cb33-195" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \vec{F}_t' \vec{\theta}_t   + \nu_t &amp;&amp; \nu_t \overset{\text{iid}}{\sim}  \mathcal{N}(0, V_t) &amp;&amp; \text{(obs)} <span class="sc">\\</span></span>
<span id="cb33-196"><a href="#cb33-196" aria-hidden="true" tabindex="-1"></a>\vec{\theta}_t &amp;= G_t \vec{\theta}_{t-1} + \vec{\omega}_t &amp;&amp; \vec{\omega}_t \overset{\text{iid}}{\sim}  \mathcal{N}(0, W_t) &amp;&amp; \text{(system)}</span>
<span id="cb33-197"><a href="#cb33-197" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-198"><a href="#cb33-198" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-general-form}</span>
<span id="cb33-199"><a href="#cb33-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-200"><a href="#cb33-200" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb33-201"><a href="#cb33-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-202"><a href="#cb33-202" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y_t$ a univariate observation at time $t$.</span>
<span id="cb33-203"><a href="#cb33-203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\vec{\theta}_t$ the **state vector** is a k-dimensional vector of unknown parameters at time t.</span>
<span id="cb33-204"><a href="#cb33-204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\vec{F_t}$ the **observation operator** a $k*1$-dimensional vector at time t that transforms  the **state parameters** into observations.</span>
<span id="cb33-205"><a href="#cb33-205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu_t$ is the observation noise at time t from a Normal distribution with variance $V_t$.</span>
<span id="cb33-206"><a href="#cb33-206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$G_t$ the **state evolution operator** is a  $k \times k$ matrix (known)</span>
<span id="cb33-207"><a href="#cb33-207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\omega_t$ the **innovation** or state evolution noise  at time t distributed as $N(0,W_t)$(known)</span>
<span id="cb33-208"><a href="#cb33-208" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the noise at the observation level and the system level are each iid and mutually iid.</span>
<span id="cb33-209"><a href="#cb33-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-210"><a href="#cb33-210" aria-hidden="true" tabindex="-1"></a>We also have the prior distribution for the state vector at time 0:</span>
<span id="cb33-211"><a href="#cb33-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-212"><a href="#cb33-212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\vec{\theta}_0 \sim N(\vec{m}_0,c_0)$ a prior k-dimensional Normal distribution.</span>
<span id="cb33-213"><a href="#cb33-213" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$m_0$ the mean in the prior is a k-dimensional vector of means. (known)</span>
<span id="cb33-214"><a href="#cb33-214" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$c_0$ is the covariance matrix k by k. (known)</span>
<span id="cb33-215"><a href="#cb33-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-216"><a href="#cb33-216" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-217"><a href="#cb33-217" aria-hidden="true" tabindex="-1"></a><span class="fu">## Some Thoughts on NDLM the definition {.unnumbered}</span></span>
<span id="cb33-218"><a href="#cb33-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-219"><a href="#cb33-219" aria-hidden="true" tabindex="-1"></a>Q. Why are $F_t$ and $G_t$ a vector and a matrix respectively? </span>
<span id="cb33-220"><a href="#cb33-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-221"><a href="#cb33-221" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It may helps to think about $F$ and $G$ as follows:</span></span>
<span id="cb33-222"><a href="#cb33-222" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb33-223"><a href="#cb33-223" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $F_t'$ acts as a linear transformation that maps the latent state $\vec{\theta}_t$ into the observation space, of $y$. </span></span>
<span id="cb33-224"><a href="#cb33-224" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb33-225"><a href="#cb33-225" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $G_t$ is a linear transformation that describes how the state vector evolves over time. I like to think about it as a Hidden Markov state transition matrix.</span></span>
<span id="cb33-226"><a href="#cb33-226" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb33-227"><a href="#cb33-227" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In other words, $F_t$ takes the current hidden state $\theta_t$ and produces an observation $y_t$, while $G_t$ takes the current state and produces the next state.</span></span>
<span id="cb33-228"><a href="#cb33-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-229"><a href="#cb33-229" aria-hidden="true" tabindex="-1"></a>Q. Why is this called a linear model?</span>
<span id="cb33-230"><a href="#cb33-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-231"><a href="#cb33-231" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This is because both the observation equation is a linear equation that relates the observations to the parameters in the model and the system equation is a linear equation that tells us how the time-varying parameter is going to be changing over time. This is why we call this a linear model.</span></span>
<span id="cb33-232"><a href="#cb33-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-233"><a href="#cb33-233" aria-hidden="true" tabindex="-1"></a>Q. Why are the noise terms $\nu_t$ and $\omega_t$ assumed to be normally distributed?</span>
<span id="cb33-234"><a href="#cb33-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-235"><a href="#cb33-235" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This is a common assumption in time series analysis. It is a convenient assumption that allows us to perform Bayesian inference and forecasting in a very simple way. And this is why we call this a **normal** dynamic linear model.</span></span>
<span id="cb33-236"><a href="#cb33-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-237"><a href="#cb33-237" aria-hidden="true" tabindex="-1"></a>Q. Isn't this just a hierarchical model?</span>
<span id="cb33-238"><a href="#cb33-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-239"><a href="#cb33-239" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Indeed, this is a hierarchical model. We have a model for the observations and a model for the system level. The system level is changing over time and the observations are related to the system level through the observation equation. And so it is possible to extend this model to more complex structures if we wish to do so by adding another level, etc... However adding more levels leads to extra dynamics that are captured in $G$ without changing the overall framework!</span></span>
<span id="cb33-240"><a href="#cb33-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-241"><a href="#cb33-241" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-242"><a href="#cb33-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-243"><a href="#cb33-243" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference in the NDLM</span></span>
<span id="cb33-244"><a href="#cb33-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-245"><a href="#cb33-245" aria-hidden="true" tabindex="-1"></a>In terms of the inference, there are a few different kinds of densities and quantities that we are interested in:</span>
<span id="cb33-246"><a href="#cb33-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-247"><a href="#cb33-247" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Filtering distribution</span><span class="co">]</span>{.column-margin}  One of the distributions that we are interested in finding is the so-called **filtering distribution**. We may be interested here in finding what is the density of $\theta_t$ *given all the observations that we have up to time* $t$. </span>
<span id="cb33-248"><a href="#cb33-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-249"><a href="#cb33-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-250"><a href="#cb33-250" aria-hidden="true" tabindex="-1"></a>\mathcal{D}_t= \{\mathcal{D}_0, y_{1:T}<span class="sc">\}</span> </span>
<span id="cb33-251"><a href="#cb33-251" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-filtering-distribution}</span>
<span id="cb33-252"><a href="#cb33-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-253"><a href="#cb33-253" aria-hidden="true" tabindex="-1"></a>We will denote information as $\mathcal{D}_t$. Usually, it is all the information we have at time zero (i.e. our prior), coupled with all the data points I have up to time $t$. </span>
<span id="cb33-254"><a href="#cb33-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-255"><a href="#cb33-255" aria-hidden="true" tabindex="-1"></a>Here we conditioning on all the observed quantities and the prior information up to time $t$, and I may be interested in just finding what is the distribution for $\theta_t$. This is called filtering.</span>
<span id="cb33-256"><a href="#cb33-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-257"><a href="#cb33-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-258"><a href="#cb33-258" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta_t \mid \mathcal{D}_t) \qquad \text{filtering distribution}</span>
<span id="cb33-259"><a href="#cb33-259" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-filtering}</span>
<span id="cb33-260"><a href="#cb33-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-261"><a href="#cb33-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-262"><a href="#cb33-262" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">forecasting distribution</span><span class="co">]</span>{.column-margin}  </span>
<span id="cb33-263"><a href="#cb33-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-264"><a href="#cb33-264" aria-hidden="true" tabindex="-1"></a>Another distribution that is very important in time series analysis is the  forecasting distribution.</span>
<span id="cb33-265"><a href="#cb33-265" aria-hidden="true" tabindex="-1"></a>We may be interested in the distribution of $y{t+h}$? where we consider $h$ lags into the future and we have all the information $\mathcal{D}_t$, up to time t. </span>
<span id="cb33-266"><a href="#cb33-266" aria-hidden="true" tabindex="-1"></a>We want to do a predictions here</span>
<span id="cb33-267"><a href="#cb33-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-268"><a href="#cb33-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-269"><a href="#cb33-269" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(y_{t+h} \mid \mathcal{D}_t) \qquad \text{forecasting distribution}</span>
<span id="cb33-270"><a href="#cb33-270" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-forecasting-distribution}</span>
<span id="cb33-271"><a href="#cb33-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-272"><a href="#cb33-272" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Smoothing Distribution</span><span class="co">]</span>{.column-margin} Another important quantity or an important set of distributions is what we call the smoothing distribution. Usually, you have a time series, when you get your data, you observe, I don't know, 300 data points. As you go with the filtering, you are going to start from zero all the way to 300 and you're going to update these filtering distributions as you go and move forward. <span class="co">[</span><span class="ot">We may want instead to revisit the parameter at time 10, for example, given that you now have observed all these 300 observations. In that case, you're interested in densities that are of the form. Let's say that you observe capital T in your process and now you are going to revisit that density for $\theta_t$. This is now in the past. Here we assume that $t&lt;T$. This is called **smoothing**.</span><span class="co">]</span>{.mark}</span>
<span id="cb33-273"><a href="#cb33-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-274"><a href="#cb33-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-275"><a href="#cb33-275" aria-hidden="true" tabindex="-1"></a>So you have more observation once you have seen the data. We will talk about how to perform Bayesian inference to obtain all these distributions under this model setting. </span>
<span id="cb33-276"><a href="#cb33-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-277"><a href="#cb33-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-278"><a href="#cb33-278" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta_t \mid \mathcal{D}_T)  \qquad t &lt; T \qquad \text{smoothing distribution}</span>
<span id="cb33-279"><a href="#cb33-279" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-smoothing-distribution}</span>
<span id="cb33-280"><a href="#cb33-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-281"><a href="#cb33-281" aria-hidden="true" tabindex="-1"></a><span class="fu">### The forecast function for the NDLM</span></span>
<span id="cb33-282"><a href="#cb33-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-283"><a href="#cb33-283" aria-hidden="true" tabindex="-1"></a>In addition to all the structure that we described before and all the densities that we are interested in finding, we also have as usual, the so-called *forecast function*, which instead of being the density is just $\mathbb{E}<span class="co">[</span><span class="ot">y(t+h)\mid \mathcal{D}_t</span><span class="co">]</span>$ i.e. expected value of y at time t given all the information we have before time t. </span>
<span id="cb33-284"><a href="#cb33-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-285"><a href="#cb33-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-286"><a href="#cb33-286" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">y(t+h)\mid \mathcal(D_t)</span><span class="co">]</span> = F'_{t+h} G_{t+h} \ldots G_{t+1} \mathbb{E}<span class="co">[</span><span class="ot">\theta_t \mid \mathcal{D}_t</span><span class="co">]</span></span>
<span id="cb33-287"><a href="#cb33-287" aria-hidden="true" tabindex="-1"></a>$$ {#eq-forecast-function-for-NDLM}</span>
<span id="cb33-288"><a href="#cb33-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-289"><a href="#cb33-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-290"><a href="#cb33-290" aria-hidden="true" tabindex="-1"></a>This is the form of the forecast function. </span>
<span id="cb33-291"><a href="#cb33-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-292"><a href="#cb33-292" aria-hidden="true" tabindex="-1"></a>There are particular cases and particular models that we will be discussing in which the $F_t=F$, i.e. constant and also $G_t = G$ is also constant for all t. </span>
<span id="cb33-293"><a href="#cb33-293" aria-hidden="true" tabindex="-1"></a>In these cases, the forecast function can be simplified and written as:</span>
<span id="cb33-294"><a href="#cb33-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-295"><a href="#cb33-295" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-296"><a href="#cb33-296" aria-hidden="true" tabindex="-1"></a>f_t(h) = \mathbb{E}(y_{t+h} \mid D_t) = F'G^h \mathbb{E}(\theta_t \mid \mathcal{D}_t)</span>
<span id="cb33-297"><a href="#cb33-297" aria-hidden="true" tabindex="-1"></a>$$ {#eq-forecast-function-for-NDLM-constant-F-G}</span>
<span id="cb33-298"><a href="#cb33-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-299"><a href="#cb33-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-300"><a href="#cb33-300" aria-hidden="true" tabindex="-1"></a>One thing that we will learn is that the eigen-structure of this matrix is very important to define the form of the forecast function, and it's very important for model building and for adding components into your model. </span>
<span id="cb33-301"><a href="#cb33-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-302"><a href="#cb33-302" aria-hidden="true" tabindex="-1"></a><span class="fu">### NDLM short form notation</span></span>
<span id="cb33-303"><a href="#cb33-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-304"><a href="#cb33-304" aria-hidden="true" tabindex="-1"></a>Finally, just in terms of short notation, we can always write down when we're working with normal dynamic linear models, we may be referring to the model instead of writing the two equations, the system and the observation equation. I can just write all the components that define my model.</span>
<span id="cb33-305"><a href="#cb33-305" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb33-306"><a href="#cb33-306" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>F_t, G_t, v_t, W_t<span class="sc">\}</span> </span>
<span id="cb33-307"><a href="#cb33-307" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-model-shothand-notation}</span>
<span id="cb33-308"><a href="#cb33-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-309"><a href="#cb33-309" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-310"><a href="#cb33-310" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript {.unnumbered}</span></span>
<span id="cb33-311"><a href="#cb33-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-312"><a href="#cb33-312" aria-hidden="true" tabindex="-1"></a>{{&lt; include _C4-L03-T01.qmd &gt;}}</span>
<span id="cb33-313"><a href="#cb33-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-314"><a href="#cb33-314" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-315"><a href="#cb33-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-316"><a href="#cb33-316" aria-hidden="true" tabindex="-1"></a><span class="fu">## Polynomial Trend Models (Video)</span></span>
<span id="cb33-317"><a href="#cb33-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-318"><a href="#cb33-318" aria-hidden="true" tabindex="-1"></a><span class="al">![first and second order polynomial model](images/m3_0011.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-319"><a href="#cb33-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-320"><a href="#cb33-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-321"><a href="#cb33-321" aria-hidden="true" tabindex="-1"></a><span class="al">![p-order polynomial model](images/m3_0012.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-322"><a href="#cb33-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-323"><a href="#cb33-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-324"><a href="#cb33-324" aria-hidden="true" tabindex="-1"></a>While we haven't talked about the superposition principle yet we start at looking at adding different components to the DLM. </span>
<span id="cb33-325"><a href="#cb33-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-326"><a href="#cb33-326" aria-hidden="true" tabindex="-1"></a>We might :</span>
<span id="cb33-327"><a href="#cb33-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-328"><a href="#cb33-328" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>setting a baseline mean and variance</span>
<span id="cb33-329"><a href="#cb33-329" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>adding a random walk with its variance</span>
<span id="cb33-330"><a href="#cb33-330" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>add a trend </span>
<span id="cb33-331"><a href="#cb33-331" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>add a regression </span>
<span id="cb33-332"><a href="#cb33-332" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>add seasonality</span>
<span id="cb33-333"><a href="#cb33-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-334"><a href="#cb33-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-335"><a href="#cb33-335" aria-hidden="true" tabindex="-1"></a>Next we want to extend the random walk model to include different types of trends and this will be covered by the polynomial trend models. <span class="co">[</span><span class="ot">These are models that are useful to model linear trends or polynomial trends in your time series. So if you have a data set, where you have an increasing trend, or a decreasing trend, you would use one of those **components** in your model.</span><span class="co">]</span>{.mark} Also </span>
<span id="cb33-336"><a href="#cb33-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-337"><a href="#cb33-337" aria-hidden="true" tabindex="-1"></a><span class="fu">### First order polynomial model</span></span>
<span id="cb33-338"><a href="#cb33-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-339"><a href="#cb33-339" aria-hidden="true" tabindex="-1"></a>The first order model is developed at great detail in chapter In <span class="co">[</span><span class="ot">@west2013bayesian ch. 2</span><span class="co">]</span>. I don't know what to make of it, isn't this a trivial white noise model?</span>
<span id="cb33-340"><a href="#cb33-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-341"><a href="#cb33-341" aria-hidden="true" tabindex="-1"></a>The math for Bayesian updating is fairly straight forward and must be much more complex with more sophisticated dynamics. So this is used by the authors to introduce their DLM and an 30 pages of the book is dedicated to in depth analysis and Bayesian development of this specific model and different distribution of interests as well as including comparison to other models and a look at the signal to noise ratio in the model.</span>
<span id="cb33-342"><a href="#cb33-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-343"><a href="#cb33-343" aria-hidden="true" tabindex="-1"></a>It is worthwhile pointing out that these models get their name from their forecast function which will takes the general form <span class="co">[</span><span class="ot">@eq-DLM-n-order-polynomial-forecast-function</span><span class="co">]</span></span>
<span id="cb33-344"><a href="#cb33-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-345"><a href="#cb33-345" aria-hidden="true" tabindex="-1"></a>The first order polynomial model is a model that is useful to describe linear trends in your time series. </span>
<span id="cb33-346"><a href="#cb33-346" aria-hidden="true" tabindex="-1"></a>If you have a data set where you have an increasing trend or a decreasing trend, you would use one of those components in your model.</span>
<span id="cb33-347"><a href="#cb33-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-348"><a href="#cb33-348" aria-hidden="true" tabindex="-1"></a>So the all those can be incorporated using the general p-order polynomial model, so I will just describe the form of this model.</span>
<span id="cb33-349"><a href="#cb33-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-350"><a href="#cb33-350" aria-hidden="true" tabindex="-1"></a>A first order polynomial is of the form $Ax+B$ where A is the slope and B is the intercept. This is the same random walk model we saw  above.</span>
<span id="cb33-351"><a href="#cb33-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-352"><a href="#cb33-352" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-353"><a href="#cb33-353" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-354"><a href="#cb33-354" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \theta_t + \nu_t, \qquad &amp; \nu_t &amp; \overset{\text{iid}}{\sim}  \mathcal{N}(0, V_t) <span class="sc">\\</span></span>
<span id="cb33-355"><a href="#cb33-355" aria-hidden="true" tabindex="-1"></a>\theta_t &amp;= \theta_{t-1} + \omega_t, \qquad &amp; \omega_t &amp; \overset{\text{iid}}{\sim}  \mathcal{N}(0, W_t) <span class="sc">\\</span></span>
<span id="cb33-356"><a href="#cb33-356" aria-hidden="true" tabindex="-1"></a>&amp;<span class="sc">\{</span>1,1,v_t,W_t<span class="sc">\}</span> &amp;&amp; \text{(short form)}<span class="sc">\\</span></span>
<span id="cb33-357"><a href="#cb33-357" aria-hidden="true" tabindex="-1"></a>f_t(h) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">\theta_t \mid \mathcal{D}_t</span><span class="co">]</span> &amp;&amp; \text{(forecast fn)}<span class="sc">\\</span></span>
<span id="cb33-358"><a href="#cb33-358" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb33-359"><a href="#cb33-359" aria-hidden="true" tabindex="-1"></a>$$ {#eq-DLM-first-order-polynomial-model}</span>
<span id="cb33-360"><a href="#cb33-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-361"><a href="#cb33-361" aria-hidden="true" tabindex="-1"></a>In the observation equation, $\theta_{t}$ is the level of the series at time t and $\nu_t$ is the observation error. In the evolution equation we see the mean for this parameter changing over time as a random walk or a local constant mean with evolution noise $\omega_t$.</span>
<span id="cb33-362"><a href="#cb33-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-363"><a href="#cb33-363" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@west2013bayesian §2.1</span><span class="co">]</span> gives the following representation of the model:</span>
<span id="cb33-364"><a href="#cb33-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-365"><a href="#cb33-365" aria-hidden="true" tabindex="-1"></a>It is useful to think of $\theta_t$ as a smooth function of time $\theta(t)$ with an associated Taylor series representation </span>
<span id="cb33-366"><a href="#cb33-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-367"><a href="#cb33-367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-368"><a href="#cb33-368" aria-hidden="true" tabindex="-1"></a>\theta(t + \delta t) = \theta(t) + \text{higher-order terms}</span>
<span id="cb33-369"><a href="#cb33-369" aria-hidden="true" tabindex="-1"></a>$$ {#eq-DLM-taylor-series-representation}</span>
<span id="cb33-370"><a href="#cb33-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-371"><a href="#cb33-371" aria-hidden="true" tabindex="-1"></a>where the higher-order terms are assumed to be zero-mean noise. This is a very important point, because it means that we are not trying to model the higher-order terms explicitly, but rather we are assuming that they are just noise.</span>
<span id="cb33-372"><a href="#cb33-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-373"><a href="#cb33-373" aria-hidden="true" tabindex="-1"></a>with the model simply describing the higher-order terms as zero-mean noise.</span>
<span id="cb33-374"><a href="#cb33-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-375"><a href="#cb33-375" aria-hidden="true" tabindex="-1"></a>This is the genesis of the first-order polynomial DLM: the level model is a locally constant (first-order polynomial) proxy for the underlying evolution.·</span>
<span id="cb33-376"><a href="#cb33-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-377"><a href="#cb33-377" aria-hidden="true" tabindex="-1"></a>We can write it down in short form with the following quadruple/</span>
<span id="cb33-378"><a href="#cb33-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-379"><a href="#cb33-379" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-380"><a href="#cb33-380" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>1, 1, V_t, W_t<span class="sc">\}</span> \qquad f_t(h) = \mathbb{E}<span class="co">[</span><span class="ot">\theta_t \mid \mathcal{D}_t</span><span class="co">]</span> = k_t \ \forall   h&gt;0</span>
<span id="cb33-381"><a href="#cb33-381" aria-hidden="true" tabindex="-1"></a>$$ {#eq-DLM-1-order-polynomial-model-short}</span>
<span id="cb33-382"><a href="#cb33-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-383"><a href="#cb33-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-384"><a href="#cb33-384" aria-hidden="true" tabindex="-1"></a>Next we can write the forecast function $f_t(h)$ of this model using the representation we gave in  <span class="co">[</span><span class="ot">@eq-DLM-1-order-polynomial-model-short</span><span class="co">]</span>.</span>
<span id="cb33-385"><a href="#cb33-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-386"><a href="#cb33-386" aria-hidden="true" tabindex="-1"></a>Again, we're going to have something of the form $F$ transposed $G$ to the power of h and then the expected value of that $\theta_t$ given $\mathcal{D}_t$. $F$ is 1, $G$ is 1, therefore I'm going to end up having just expected value of $\theta_t$ given $\mathcal{D}_t$.</span>
<span id="cb33-387"><a href="#cb33-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-388"><a href="#cb33-388" aria-hidden="true" tabindex="-1"></a>Which depending on the data that you have is you're just going to have something that is a value that depends on $t$ and it doesn't depend on $h$. What this model is telling you is that the forecast function, how you expect to see future values of the series h steps ahead is something that looks like the level that you estimated at time $t$.</span>
<span id="cb33-389"><a href="#cb33-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-390"><a href="#cb33-390" aria-hidden="true" tabindex="-1"></a><span class="fu">### Second order Polynomial model AKA Linear Growth model</span></span>
<span id="cb33-391"><a href="#cb33-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-392"><a href="#cb33-392" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@west2013bayesian §7.1-7.2</span><span class="co">]</span> gives a detailed analysis of this model.</span>
<span id="cb33-393"><a href="#cb33-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-394"><a href="#cb33-394" aria-hidden="true" tabindex="-1"></a>Now we want to create a model in which captures things that has a linear trend either increasing or decreasing. To do thus we need to have two components in our parameter vector of the state vector. For this we will need two components in our parameter vector of the **state vector**^<span class="co">[</span><span class="ot">the state makes it's appearance</span><span class="co">]</span>. </span>
<span id="cb33-395"><a href="#cb33-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-396"><a href="#cb33-396" aria-hidden="true" tabindex="-1"></a>So we have again something that looks like in my observation equation. </span>
<span id="cb33-397"><a href="#cb33-397" aria-hidden="true" tabindex="-1"></a>I'm going to have, </span>
<span id="cb33-398"><a href="#cb33-398" aria-hidden="true" tabindex="-1"></a>I'm going to call it say $\theta_{t,1} \sim  \mathcal{N}(v_t)$, and then I'm going to have say $\theta_{t,1}$ is going to be of the form to $\theta_{t-1,1}$ and there is another component here. The other component enters this equation plus let's call this $\theta_{t-1,2}$. And then I have finally also I need an evolution for the second component of the process which is going to be again having a random walk type of behavior.</span>
<span id="cb33-399"><a href="#cb33-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-400"><a href="#cb33-400" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb33-401"><a href="#cb33-401" aria-hidden="true" tabindex="-1"></a>  y_t &amp;= \theta_{t,1} + \nu_t \quad &amp;\nu_t &amp;\overset{\text{iid}}{\sim}  \mathcal{N}(0, v_t) <span class="sc">\\</span></span>
<span id="cb33-402"><a href="#cb33-402" aria-hidden="true" tabindex="-1"></a>  \theta_{t,1} &amp;= \theta_{t-1,1} + \theta_{t-1,2} + \omega_{t,1} \qquad &amp;\omega_{t,1} &amp;\overset{\text{iid}}{\sim}  \mathcal{N}(0, w_{t,11}) <span class="sc">\\</span></span>
<span id="cb33-403"><a href="#cb33-403" aria-hidden="true" tabindex="-1"></a>  \theta_{t,2} &amp;= \theta_{t-1,2} + \omega_{t,2} \qquad &amp;\omega_{t,2} &amp;\overset{\text{iid}}{\sim}  \mathcal{N}(0, w_{t,22})</span>
<span id="cb33-404"><a href="#cb33-404" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-405"><a href="#cb33-405" aria-hidden="true" tabindex="-1"></a>$$ {#eq-NDLM-2-order-polynomial-model}</span>
<span id="cb33-406"><a href="#cb33-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-407"><a href="#cb33-407" aria-hidden="true" tabindex="-1"></a>So there are different ways in which you can interpret this two parameters but essentially:</span>
<span id="cb33-408"><a href="#cb33-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-409"><a href="#cb33-409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta_{t-1,1}$ is related to the **baseline** level of the series </span>
<span id="cb33-410"><a href="#cb33-410" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta_{t-1,2}$ is related to the **rate of change** of the of the series. </span>
<span id="cb33-411"><a href="#cb33-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-412"><a href="#cb33-412" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb33-413"><a href="#cb33-413" aria-hidden="true" tabindex="-1"></a><span class="fu">## Short form DLM notation {.unnumbered}</span></span>
<span id="cb33-414"><a href="#cb33-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-415"><a href="#cb33-415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Having the short form notation makes the model easier to understand in relation to other DLM models.</span>
<span id="cb33-416"><a href="#cb33-416" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It will soon be instrumental in communicating the model structure with different software packages.</span>
<span id="cb33-417"><a href="#cb33-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-418"><a href="#cb33-418" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-419"><a href="#cb33-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-420"><a href="#cb33-420" aria-hidden="true" tabindex="-1"></a>Next we should summarize this model using the familiar short form DLM representation, which requires a bit of creative algebra.</span>
<span id="cb33-421"><a href="#cb33-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-422"><a href="#cb33-422" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-423"><a href="#cb33-423" aria-hidden="true" tabindex="-1"></a>\mathbf{\theta}_t = (\theta_{t,1}, \theta_{t,2}) \qquad <span class="sc">\{</span>\mathbf{F}, \mathbf{G}, V_t, \mathbf{W}_t<span class="sc">\}</span></span>
<span id="cb33-424"><a href="#cb33-424" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-425"><a href="#cb33-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-426"><a href="#cb33-426" aria-hidden="true" tabindex="-1"></a>First we collect the two variances for the evolution two components into the vector $\utilde{w}_t$ and then assume that this $w_t$ is Normal. Now this is a bi-variate normal.</span>
<span id="cb33-427"><a href="#cb33-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-428"><a href="#cb33-428" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-429"><a href="#cb33-429" aria-hidden="true" tabindex="-1"></a>\utilde{\omega}_t = (\omega_{t,1},\omega_{t,2})' \qquad \utilde{\omega}_t \sim  \mathcal{N}(0,W_t)</span>
<span id="cb33-430"><a href="#cb33-430" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-431"><a href="#cb33-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-432"><a href="#cb33-432" aria-hidden="true" tabindex="-1"></a>So what would be my $F$ and my $G$ in this model? So again my theta vector has two components, thus my $G$, so my $F$ is going to be a two dimensional. We can write down $F$ transposed as the only component that appears at this level is the first component of the vector. </span>
<span id="cb33-433"><a href="#cb33-433" aria-hidden="true" tabindex="-1"></a>I'm going to have 1 and then a zero for $F$ transposed. c.f. <span class="co">[</span><span class="ot">@eq-DLM-2-order-polynomial-model-short-form-simple</span><span class="co">]</span></span>
<span id="cb33-434"><a href="#cb33-434" aria-hidden="true" tabindex="-1"></a>And then my $G$ here if you think about writing down $\theta_t$ times $G$ say the $t-1 + \omega_t$. Then you have that you're $G$ is going to have this form.</span>
<span id="cb33-435"><a href="#cb33-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-436"><a href="#cb33-436" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-437"><a href="#cb33-437" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-438"><a href="#cb33-438" aria-hidden="true" tabindex="-1"></a>\mathbf{F} &amp;= (1,0)' &amp; V_t &amp;= v_t <span class="sc">\\</span></span>
<span id="cb33-439"><a href="#cb33-439" aria-hidden="true" tabindex="-1"></a>\mathbf{G} &amp;= \begin{pmatrix} 1 &amp; h <span class="sc">\\</span> 0 &amp; 1 \end{pmatrix} </span>
<span id="cb33-440"><a href="#cb33-440" aria-hidden="true" tabindex="-1"></a>&amp; \mathbf{W}_t &amp;= \begin{pmatrix} w_{t,11} &amp; 0 \\ 0 &amp; w_{t,22} \end{pmatrix}</span>
<span id="cb33-441"><a href="#cb33-441" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-442"><a href="#cb33-442" aria-hidden="true" tabindex="-1"></a>$$ {#eq-DLM-2-order-polynomial-model-short-form-simple}</span>
<span id="cb33-443"><a href="#cb33-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-444"><a href="#cb33-444" aria-hidden="true" tabindex="-1"></a>this is the form from the video</span>
<span id="cb33-445"><a href="#cb33-445" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-446"><a href="#cb33-446" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-447"><a href="#cb33-447" aria-hidden="true" tabindex="-1"></a>\mathbf{F} &amp;= (1,0)' &amp; V_t &amp;= v_t <span class="sc">\\</span></span>
<span id="cb33-448"><a href="#cb33-448" aria-hidden="true" tabindex="-1"></a>\mathbf{G} &amp;= \begin{pmatrix} 1 &amp; h <span class="sc">\\</span> 0 &amp; 1 \end{pmatrix} </span>
<span id="cb33-449"><a href="#cb33-449" aria-hidden="true" tabindex="-1"></a>&amp; \mathbf{W}_t &amp;= \begin{pmatrix} w_{t,11} &amp; w_{t,12} \\ w_{t,21} &amp; w_{t,22} \end{pmatrix}</span>
<span id="cb33-450"><a href="#cb33-450" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-451"><a href="#cb33-451" aria-hidden="true" tabindex="-1"></a>$$ {#eq-DLM-2-order-polynomial-model-short-form-complex}</span>
<span id="cb33-452"><a href="#cb33-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-453"><a href="#cb33-453" aria-hidden="true" tabindex="-1"></a>this is the more general form from the handout. Note that in this case we have </span>
<span id="cb33-454"><a href="#cb33-454" aria-hidden="true" tabindex="-1"></a>$w_{t,12}=w_{t,21}$ so there is just one extra parameter.</span>
<span id="cb33-455"><a href="#cb33-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-456"><a href="#cb33-456" aria-hidden="true" tabindex="-1"></a>The lesson videos and the handouts differ in the form $\mathbf{W}_t$. In the lecture we assumed zero covariance but in the handout the covariance was snuck in. This gives us a slightly more general model. The covariance though is symmetric so we get an extra parameter we need to infer and include in the prior. Anyhow I kept the more general form, though in most cases we will keep the off diagonal terms at zero.</span>
<span id="cb33-457"><a href="#cb33-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-458"><a href="#cb33-458" aria-hidden="true" tabindex="-1"></a>So for the first component, I have past values of both components. That's why I have a 1 and 1 here for the second component I only have the past value of the second component. So there is a zero and a 1. So this tells me what is the structure of this second order polynomial. If I think about how to obtain the forecast function for this second order polynomial is going to be very similar to what we did before. So you can write it down as F transposed G to the power of h, expected value of theta t given Dt. Now the expected value is going to be vector also with two components because theta_t is a two dimensional vector. The structure here if you look at what G is G to the power of h going to be a matrix, that is going to look like 1, h, 0 1. When you multiply that matrix time this times this F what you're going to end up having is something that looks like 1 h times this expected value of theta t given Dt. So I can think of two components here, so this gives you a constant on h, this part is not going to depend on h. So I can write this down as k t 11 component multiplied by 1 and then I have another constant, multiplied by h. So you can see what happens now is that your forecast function has the form of a linear polynomial. So it's just a linear function on the number of steps ahead. The slope and the intercept related to that linear function are going to depend on the expected value of, theta_t given the all the information I have up to time t. But essentially is a way to model linear trends. So this is what happens with the second order polynomial model.</span>
<span id="cb33-459"><a href="#cb33-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-460"><a href="#cb33-460" aria-hidden="true" tabindex="-1"></a>As we included linear trends and constant values in the forecast function, we may want to also incorporate other kinds of trends, polynomial trends in the model. So you may want to have a quadratic form, the forecast function or a cubic forecast function as a function of h. </span>
<span id="cb33-461"><a href="#cb33-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-462"><a href="#cb33-462" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-463"><a href="#cb33-463" aria-hidden="true" tabindex="-1"></a>\theta_t = (\theta_{t,1}, \theta_{t,2})' \qquad \mathbf{G} = \mathbf{J}_2(1) \qquad \mathbf{E}_2 = (1, 0)'</span>
<span id="cb33-464"><a href="#cb33-464" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-465"><a href="#cb33-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-466"><a href="#cb33-466" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-467"><a href="#cb33-467" aria-hidden="true" tabindex="-1"></a>\mathbf{G^h} = \begin{pmatrix} 1 &amp; h <span class="sc">\\</span> 0 &amp; 1 \end{pmatrix}</span>
<span id="cb33-468"><a href="#cb33-468" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-469"><a href="#cb33-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-470"><a href="#cb33-470" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-471"><a href="#cb33-471" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-472"><a href="#cb33-472" aria-hidden="true" tabindex="-1"></a>f_t(h) &amp;= F' G^h \mathbb{E}<span class="co">[</span><span class="ot">\theta_t \mid \mathcal{D}_t</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb33-473"><a href="#cb33-473" aria-hidden="true" tabindex="-1"></a>&amp;= (1,h) \mathbb{E}<span class="co">[</span><span class="ot">\theta_{t}\mid D_t</span><span class="co">]</span> <span class="sc">\\</span> </span>
<span id="cb33-474"><a href="#cb33-474" aria-hidden="true" tabindex="-1"></a>&amp;= (1,h)(K_{t,0}, K_{t,1})' <span class="sc">\\</span> </span>
<span id="cb33-475"><a href="#cb33-475" aria-hidden="true" tabindex="-1"></a>&amp;= (K_{t,0} + K_{t,1} h)</span>
<span id="cb33-476"><a href="#cb33-476" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-477"><a href="#cb33-477" aria-hidden="true" tabindex="-1"></a>$$ {#eq-second-order-poly-prediction-fn}</span>
<span id="cb33-478"><a href="#cb33-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-479"><a href="#cb33-479" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-480"><a href="#cb33-480" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-481"><a href="#cb33-481" aria-hidden="true" tabindex="-1"></a>\mathbf{G^h} &amp;= \begin{pmatrix} 1 &amp; h <span class="sc">\\</span> 0 &amp; 1 \end{pmatrix}</span>
<span id="cb33-482"><a href="#cb33-482" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-483"><a href="#cb33-483" aria-hidden="true" tabindex="-1"></a>$$ {#eq-second-order-poly-Gh}</span>
<span id="cb33-484"><a href="#cb33-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-485"><a href="#cb33-485" aria-hidden="true" tabindex="-1"></a><span class="fu">### General p-th order polynomial model </span></span>
<span id="cb33-486"><a href="#cb33-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-487"><a href="#cb33-487" aria-hidden="true" tabindex="-1"></a>We can consider a so called p-th order polynomial model. This model will have a state-space vector of dimension p and a polynomial of order $p − 1$ forecast function on $h$. The model can be written as</span>
<span id="cb33-488"><a href="#cb33-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-489"><a href="#cb33-489" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-490"><a href="#cb33-490" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>E_p, J_p(1), v_t, W_t<span class="sc">\}</span></span>
<span id="cb33-491"><a href="#cb33-491" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-492"><a href="#cb33-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-493"><a href="#cb33-493" aria-hidden="true" tabindex="-1"></a>with $F_t = E_p = (1, 0, \ldots, 0)′$ and $G_t = J_p(1)$, with</span>
<span id="cb33-494"><a href="#cb33-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-495"><a href="#cb33-495" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-496"><a href="#cb33-496" aria-hidden="true" tabindex="-1"></a>J_p(1) = \begin{pmatrix}</span>
<span id="cb33-497"><a href="#cb33-497" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0  <span class="sc">\\</span></span>
<span id="cb33-498"><a href="#cb33-498" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 1 &amp; \cdots &amp; 0 &amp; 0  <span class="sc">\\</span></span>
<span id="cb33-499"><a href="#cb33-499" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb33-500"><a href="#cb33-500" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb33-501"><a href="#cb33-501" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-502"><a href="#cb33-502" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1</span>
<span id="cb33-503"><a href="#cb33-503" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb33-504"><a href="#cb33-504" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Jordan-form-dynamics}</span>
<span id="cb33-505"><a href="#cb33-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-506"><a href="#cb33-506" aria-hidden="true" tabindex="-1"></a>The forecast function is given by</span>
<span id="cb33-507"><a href="#cb33-507" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-508"><a href="#cb33-508" aria-hidden="true" tabindex="-1"></a>f_t(k) = a_{t_0} +  a_{t_1}k + \ldots + a_{t_{n-1}} k^{n-1} \qquad k \in \mathbb{N}</span>
<span id="cb33-509"><a href="#cb33-509" aria-hidden="true" tabindex="-1"></a>$$ {#eq-DLM-n-order-polynomial-forecast-function}</span>
<span id="cb33-510"><a href="#cb33-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-511"><a href="#cb33-511" aria-hidden="true" tabindex="-1"></a>where $a_{t_i}$ are the coefficients of the polynomial and $k$ is the number of steps ahead we need in our forecast. There is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function given by $<span class="sc">\{</span>Ep, Lp, vt, W t<span class="sc">\}</span>$, with</span>
<span id="cb33-512"><a href="#cb33-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-513"><a href="#cb33-513" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-514"><a href="#cb33-514" aria-hidden="true" tabindex="-1"></a>L_p = \begin{pmatrix}</span>
<span id="cb33-515"><a href="#cb33-515" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 &amp; 1  <span class="sc">\\</span></span>
<span id="cb33-516"><a href="#cb33-516" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 &amp; 1   <span class="sc">\\</span></span>
<span id="cb33-517"><a href="#cb33-517" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 1 &amp; \cdots &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-518"><a href="#cb33-518" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb33-519"><a href="#cb33-519" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-520"><a href="#cb33-520" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1</span>
<span id="cb33-521"><a href="#cb33-521" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb33-522"><a href="#cb33-522" aria-hidden="true" tabindex="-1"></a>$$ {#eq-triangular-form-dynamics}</span>
<span id="cb33-523"><a href="#cb33-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-524"><a href="#cb33-524" aria-hidden="true" tabindex="-1"></a>And in this type of model, the forecast function is going to have order $p-1$. So the parameter vector is going to have dimension $p$. So you're going to have $\theta_t =  \theta_{t1:p}$.</span>
<span id="cb33-525"><a href="#cb33-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-526"><a href="#cb33-526" aria-hidden="true" tabindex="-1"></a>The observation operator $F$ is just a constant and if we write it as a row vector we get $F'$ as a p-dimensional vector with the one in the first entry and zeros everywhere else.</span>
<span id="cb33-527"><a href="#cb33-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-528"><a href="#cb33-528" aria-hidden="true" tabindex="-1"></a>The dynamics  matrix $G$ may be written using either a $J$ Jordan form @eq-Jordan-form-dynamics or as a triangular form @eq-triangular-form-dynamics. These result in different parameterization of this model and we  will talk a little bit about this. </span>
<span id="cb33-529"><a href="#cb33-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-530"><a href="#cb33-530" aria-hidden="true" tabindex="-1"></a>In the @eq-Jordan-form-dynamics we have a matrix with ones on the diagonal and the super diagonal, the matrix is needs to be $p \times p$ i.e. with dimension $p$ to be compatible with the dimension of the hidden state vector $\theta$. So this matrix $G$ is what we call a Jordan block of dimension p of 1. So here 1 is the number that appears in the diagonal. And then I have a p $I_p$ matrix, I have ones in the upper diagonal part. So this is the form of the model, so once again I have the $F$ the $G$, and the $W_t$. I have my model. </span>
<span id="cb33-531"><a href="#cb33-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-532"><a href="#cb33-532" aria-hidden="true" tabindex="-1"></a>The forecast function in this case again can be written as $F' G^h \mathbb{E}<span class="co">[</span><span class="ot">\theta_t \mid \mathcal{D}_t</span><span class="co">]</span>$. And when you simplify times expected value of $\theta_t$, given $D_t$. Once you simplify those functions you get something that is a polynomial of order $p-1$ in $h$. So I just can write this down as $k_t + k_{t,1} h + k_{t, p-1} h^{p-1}$, so that's my forecast function. </span>
<span id="cb33-533"><a href="#cb33-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-534"><a href="#cb33-534" aria-hidden="true" tabindex="-1"></a>There is an alternative parameterization of this model that has the same $F$ and the same algebraic form of the forecast function, the same form of the forecast function. But instead of using @eq-Jordan-form-dynamics form of the $G$ matrix, it has a @eq-triangular-form-dynamics form that has ones in the diagonal and ones everywhere above the diagonal. So it's an upper triangular matrix with ones in the diagonal and above the diagonal. That's a different parameterization of the same model but it leads to the same general form of the forecast function just with a different parameterization. </span>
<span id="cb33-535"><a href="#cb33-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-536"><a href="#cb33-536" aria-hidden="true" tabindex="-1"></a>So again, we can consider the way you think about these models?</span>
<span id="cb33-537"><a href="#cb33-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-538"><a href="#cb33-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What is you think what kind of forecast function makes sense here ? </span>
<span id="cb33-539"><a href="#cb33-539" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What is the type of predictions that I expect to have in my model? </span>
<span id="cb33-540"><a href="#cb33-540" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If they look like a linear trend, I use a second order polynomial. </span>
<span id="cb33-541"><a href="#cb33-541" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If it looks like a quadratic trend in the forecast then I would use 3rd order polynomial model representation.</span>
<span id="cb33-542"><a href="#cb33-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-543"><a href="#cb33-543" aria-hidden="true" tabindex="-1"></a>Note that the third order polynomial model is covered in </span>
<span id="cb33-544"><a href="#cb33-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-545"><a href="#cb33-545" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-546"><a href="#cb33-546" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript {.unnumbered}</span></span>
<span id="cb33-547"><a href="#cb33-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-548"><a href="#cb33-548" aria-hidden="true" tabindex="-1"></a>{{&lt; include _C4-L03-T02.qmd &gt;}}</span>
<span id="cb33-549"><a href="#cb33-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-550"><a href="#cb33-550" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-551"><a href="#cb33-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-552"><a href="#cb33-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-553"><a href="#cb33-553" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary of polynomial trend  models (Reading)</span></span>
<span id="cb33-554"><a href="#cb33-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-555"><a href="#cb33-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-556"><a href="#cb33-556" aria-hidden="true" tabindex="-1"></a><span class="fu">### Polynomial Trend Models</span></span>
<span id="cb33-557"><a href="#cb33-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-558"><a href="#cb33-558" aria-hidden="true" tabindex="-1"></a><span class="fu">#### First-Order Polynomial</span></span>
<span id="cb33-559"><a href="#cb33-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-560"><a href="#cb33-560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-561"><a href="#cb33-561" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-562"><a href="#cb33-562" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \mu_t + \nu_t, \qquad &amp; \nu_t &amp;\sim  \mathcal{N}(0, v_t) <span class="sc">\\</span></span>
<span id="cb33-563"><a href="#cb33-563" aria-hidden="true" tabindex="-1"></a>\mu_t &amp;= \mu_{t-1} + \omega_t, \qquad &amp; \omega_t &amp;\sim  \mathcal{N}(0, w_t)</span>
<span id="cb33-564"><a href="#cb33-564" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-565"><a href="#cb33-565" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-566"><a href="#cb33-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-567"><a href="#cb33-567" aria-hidden="true" tabindex="-1"></a>In this case, we have:</span>
<span id="cb33-568"><a href="#cb33-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-569"><a href="#cb33-569" aria-hidden="true" tabindex="-1"></a>$\theta_t = \mu_t \quad \forall t$</span>
<span id="cb33-570"><a href="#cb33-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-571"><a href="#cb33-571" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-572"><a href="#cb33-572" aria-hidden="true" tabindex="-1"></a>F_t = 1 \quad \forall t \qquad G_t = 1 \quad \forall t</span>
<span id="cb33-573"><a href="#cb33-573" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-574"><a href="#cb33-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-575"><a href="#cb33-575" aria-hidden="true" tabindex="-1"></a>resulting in:</span>
<span id="cb33-576"><a href="#cb33-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-577"><a href="#cb33-577" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-578"><a href="#cb33-578" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>1, 1, v_t, w_t<span class="sc">\}</span> \qquad \text{(short notation)}</span>
<span id="cb33-579"><a href="#cb33-579" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-580"><a href="#cb33-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-581"><a href="#cb33-581" aria-hidden="true" tabindex="-1"></a>The forecast function is:</span>
<span id="cb33-582"><a href="#cb33-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-583"><a href="#cb33-583" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-584"><a href="#cb33-584" aria-hidden="true" tabindex="-1"></a>f_t(h) = E(\mu_t \mid \mathcal{D}_t) = k_t, \quad \forall h &gt; 0.</span>
<span id="cb33-585"><a href="#cb33-585" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-586"><a href="#cb33-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-587"><a href="#cb33-587" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Second-Order Polynomial</span></span>
<span id="cb33-588"><a href="#cb33-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-589"><a href="#cb33-589" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb33-590"><a href="#cb33-590" aria-hidden="true" tabindex="-1"></a>  y_t &amp;= \theta_{t,1} + \nu_t, \quad &amp;\nu_t &amp;\sim  \mathcal{N}(0, v_t) <span class="sc">\\</span></span>
<span id="cb33-591"><a href="#cb33-591" aria-hidden="true" tabindex="-1"></a>  \theta_{t,1} &amp;= \theta_{t-1,1} + \theta_{t-1,2} + \omega_{t,1}, \qquad &amp;\omega_{t,1} &amp;\sim  \mathcal{N}(0, w_{t,11}) <span class="sc">\\</span></span>
<span id="cb33-592"><a href="#cb33-592" aria-hidden="true" tabindex="-1"></a>  \theta_{t,2} &amp;= \theta_{t-1,2} + \omega_{t,2}, \qquad &amp;\omega_{t,2} &amp;\sim  \mathcal{N}(0, w_{t,22}),</span>
<span id="cb33-593"><a href="#cb33-593" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-594"><a href="#cb33-594" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-595"><a href="#cb33-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-596"><a href="#cb33-596" aria-hidden="true" tabindex="-1"></a>where we can also have:</span>
<span id="cb33-597"><a href="#cb33-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-598"><a href="#cb33-598" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-599"><a href="#cb33-599" aria-hidden="true" tabindex="-1"></a>\text{Cov}(\theta_{t,1}, \theta_{t,2} ) = w_{t,12} = w_{t,21}</span>
<span id="cb33-600"><a href="#cb33-600" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-601"><a href="#cb33-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-602"><a href="#cb33-602" aria-hidden="true" tabindex="-1"></a>This can be written as a DLM with the state-space vector $\theta_t = (\theta_{t,1}, \theta_{t,2})'$, and </span>
<span id="cb33-603"><a href="#cb33-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-604"><a href="#cb33-604" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-605"><a href="#cb33-605" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>\mathbf{F}, \mathbf{G}, v_t, \mathbf{W}_t<span class="sc">\}</span>  \qquad \text{(short notation)}</span>
<span id="cb33-606"><a href="#cb33-606" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb33-607"><a href="#cb33-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-608"><a href="#cb33-608" aria-hidden="true" tabindex="-1"></a>with $\mathbf{F} = (1, 0)'$ and </span>
<span id="cb33-609"><a href="#cb33-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-610"><a href="#cb33-610" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-611"><a href="#cb33-611" aria-hidden="true" tabindex="-1"></a>\mathbf{G} = </span>
<span id="cb33-612"><a href="#cb33-612" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-613"><a href="#cb33-613" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-614"><a href="#cb33-614" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb33-615"><a href="#cb33-615" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}, \quad \mathbf{W}_t = </span>
<span id="cb33-616"><a href="#cb33-616" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-617"><a href="#cb33-617" aria-hidden="true" tabindex="-1"></a>w_{t,11} &amp; w_{t,12} <span class="sc">\\</span></span>
<span id="cb33-618"><a href="#cb33-618" aria-hidden="true" tabindex="-1"></a>w_{t,21} &amp; w_{t,22}</span>
<span id="cb33-619"><a href="#cb33-619" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb33-620"><a href="#cb33-620" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-621"><a href="#cb33-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-622"><a href="#cb33-622" aria-hidden="true" tabindex="-1"></a>Note that </span>
<span id="cb33-623"><a href="#cb33-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-624"><a href="#cb33-624" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-625"><a href="#cb33-625" aria-hidden="true" tabindex="-1"></a>\mathbf{G}^2 = </span>
<span id="cb33-626"><a href="#cb33-626" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-627"><a href="#cb33-627" aria-hidden="true" tabindex="-1"></a>1 &amp; 2 <span class="sc">\\</span></span>
<span id="cb33-628"><a href="#cb33-628" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb33-629"><a href="#cb33-629" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}, \quad \mathbf{G}^h = </span>
<span id="cb33-630"><a href="#cb33-630" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-631"><a href="#cb33-631" aria-hidden="true" tabindex="-1"></a>1 &amp; h <span class="sc">\\</span></span>
<span id="cb33-632"><a href="#cb33-632" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb33-633"><a href="#cb33-633" aria-hidden="true" tabindex="-1"></a>\end{pmatrix},</span>
<span id="cb33-634"><a href="#cb33-634" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-635"><a href="#cb33-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-636"><a href="#cb33-636" aria-hidden="true" tabindex="-1"></a>and so:</span>
<span id="cb33-637"><a href="#cb33-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-638"><a href="#cb33-638" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-639"><a href="#cb33-639" aria-hidden="true" tabindex="-1"></a>f_t(h) = (1, h) E(\mathbf{\theta}_t \mid \mathcal{D}_t) = (1, h) (k_{t,0}, k_{t,1})' = (k_{t,0} + h k_{t,1}).</span>
<span id="cb33-640"><a href="#cb33-640" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-641"><a href="#cb33-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-642"><a href="#cb33-642" aria-hidden="true" tabindex="-1"></a>Here $\mathbf{G} = \mathbf{J}_2(1)$ (see below). </span>
<span id="cb33-643"><a href="#cb33-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-644"><a href="#cb33-644" aria-hidden="true" tabindex="-1"></a>Also, we denote $\mathbf{E}_2 = (1, 0)'$, and so the short notation for this model is </span>
<span id="cb33-645"><a href="#cb33-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-646"><a href="#cb33-646" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-647"><a href="#cb33-647" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>E_2, J_2(1), \cdot, \cdot<span class="sc">\}</span></span>
<span id="cb33-648"><a href="#cb33-648" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-649"><a href="#cb33-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-650"><a href="#cb33-650" aria-hidden="true" tabindex="-1"></a><span class="fu">#### General $p$-th Order Polynomial Model</span></span>
<span id="cb33-651"><a href="#cb33-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-652"><a href="#cb33-652" aria-hidden="true" tabindex="-1"></a>We can consider a $p$-th order polynomial model. This model will have a state-space vector of dimension $p$ and a polynomial of order $p-1$ forecast function on $h$. The model can be written as </span>
<span id="cb33-653"><a href="#cb33-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-654"><a href="#cb33-654" aria-hidden="true" tabindex="-1"></a>$$<span class="sc">\{</span>E_p, J_p(1), v_t, W_t<span class="sc">\}</span>  \qquad \text{(short notation)}</span>
<span id="cb33-655"><a href="#cb33-655" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-656"><a href="#cb33-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-657"><a href="#cb33-657" aria-hidden="true" tabindex="-1"></a>with $\mathbf{F}_t = \mathbf{E}_p = (1, 0, \dots, 0)'$ and $\mathbf{G}_t = \mathbf{J}_p(1)$, with</span>
<span id="cb33-658"><a href="#cb33-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-659"><a href="#cb33-659" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-660"><a href="#cb33-660" aria-hidden="true" tabindex="-1"></a>\mathbf{J}_p(1) =</span>
<span id="cb33-661"><a href="#cb33-661" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-662"><a href="#cb33-662" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb33-663"><a href="#cb33-663" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span></span>
<span id="cb33-664"><a href="#cb33-664" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; &amp; \vdots <span class="sc">\\</span></span>
<span id="cb33-665"><a href="#cb33-665" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-666"><a href="#cb33-666" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; 1</span>
<span id="cb33-667"><a href="#cb33-667" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb33-668"><a href="#cb33-668" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-669"><a href="#cb33-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-670"><a href="#cb33-670" aria-hidden="true" tabindex="-1"></a>The forecast function is given by</span>
<span id="cb33-671"><a href="#cb33-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-672"><a href="#cb33-672" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-673"><a href="#cb33-673" aria-hidden="true" tabindex="-1"></a>f_t(h) = k_{t,0} + k_{t,1} h + \dots + k_{t,p-1} h^{p-1}.</span>
<span id="cb33-674"><a href="#cb33-674" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-675"><a href="#cb33-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-676"><a href="#cb33-676" aria-hidden="true" tabindex="-1"></a>There is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function, given by $<span class="sc">\{</span>E_p, L_p, v_t, W_t<span class="sc">\}</span>$, with</span>
<span id="cb33-677"><a href="#cb33-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-678"><a href="#cb33-678" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-679"><a href="#cb33-679" aria-hidden="true" tabindex="-1"></a>L_p =</span>
<span id="cb33-680"><a href="#cb33-680" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-681"><a href="#cb33-681" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-682"><a href="#cb33-682" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-683"><a href="#cb33-683" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb33-684"><a href="#cb33-684" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1</span>
<span id="cb33-685"><a href="#cb33-685" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb33-686"><a href="#cb33-686" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-687"><a href="#cb33-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-688"><a href="#cb33-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-689"><a href="#cb33-689" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regression models (Video)</span></span>
<span id="cb33-690"><a href="#cb33-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-691"><a href="#cb33-691" aria-hidden="true" tabindex="-1"></a><span class="al">![Regression models](images/m3_0021.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-692"><a href="#cb33-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-693"><a href="#cb33-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-694"><a href="#cb33-694" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simple dynamic regression</span></span>
<span id="cb33-695"><a href="#cb33-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-696"><a href="#cb33-696" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-697"><a href="#cb33-697" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-698"><a href="#cb33-698" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \beta_{t,0} + \beta_{t,1}x_t + ν_t <span class="sc">\\</span></span>
<span id="cb33-699"><a href="#cb33-699" aria-hidden="true" tabindex="-1"></a>\beta_{t,0} &amp;= \beta_{t−1,0} + \omega_{t,0} <span class="sc">\\</span></span>
<span id="cb33-700"><a href="#cb33-700" aria-hidden="true" tabindex="-1"></a>\beta_{t,1} &amp;= \beta_{t−1,1} + \omega_{t,1}</span>
<span id="cb33-701"><a href="#cb33-701" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-702"><a href="#cb33-702" aria-hidden="true" tabindex="-1"></a>$$ {#eq-simple-dynamic-regression}</span>
<span id="cb33-703"><a href="#cb33-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-704"><a href="#cb33-704" aria-hidden="true" tabindex="-1"></a>and so $\theta_t = (\beta_t,0, \beta_{t,1})′$, $F_t = (1, x_t)′$ and $G = I_2$. </span>
<span id="cb33-705"><a href="#cb33-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-706"><a href="#cb33-706" aria-hidden="true" tabindex="-1"></a>This results in a forecast function of the form </span>
<span id="cb33-707"><a href="#cb33-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-708"><a href="#cb33-708" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-709"><a href="#cb33-709" aria-hidden="true" tabindex="-1"></a>f_t(h) = k_{t,0} + k_{t,1}x_{t+h}</span>
<span id="cb33-710"><a href="#cb33-710" aria-hidden="true" tabindex="-1"></a>$$ {#eq-simple-dynamic-regression-forecast-function}</span>
<span id="cb33-711"><a href="#cb33-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-712"><a href="#cb33-712" aria-hidden="true" tabindex="-1"></a>where $k_{t,0} = \mathbb{E}<span class="co">[</span><span class="ot">\beta_{t,0} \mid \mathcal{D}_t</span><span class="co">]</span>$ and $k_{t,1} = \mathbb{E}<span class="co">[</span><span class="ot">\beta_{t,1} \mid \mathcal{D}_t</span><span class="co">]</span>$.</span>
<span id="cb33-713"><a href="#cb33-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-714"><a href="#cb33-714" aria-hidden="true" tabindex="-1"></a><span class="fu">### General dynamic regression</span></span>
<span id="cb33-715"><a href="#cb33-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-716"><a href="#cb33-716" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-717"><a href="#cb33-717" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-718"><a href="#cb33-718" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \beta_{t,0} + \beta_{t,1}x_{t,1} + \ldots \beta_{t,M} x_{t,M} + ν_t <span class="sc">\\</span></span>
<span id="cb33-719"><a href="#cb33-719" aria-hidden="true" tabindex="-1"></a>\beta_{t,m} &amp;= \beta_{t−1,m} + \omega_{t,m,} &amp; m = 0 : M.</span>
<span id="cb33-720"><a href="#cb33-720" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-721"><a href="#cb33-721" aria-hidden="true" tabindex="-1"></a>$$ {#eq-regression-in-short-form}</span>
<span id="cb33-722"><a href="#cb33-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-723"><a href="#cb33-723" aria-hidden="true" tabindex="-1"></a>Then, $\theta = (\beta_t,0, \ldots , \beta_{t,M} )′$, $F_t = (1, x_{t,1}, \ldots , x_{t,M} )′$ and $G = I_M$ . The forecast</span>
<span id="cb33-724"><a href="#cb33-724" aria-hidden="true" tabindex="-1"></a>function is given by</span>
<span id="cb33-725"><a href="#cb33-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-726"><a href="#cb33-726" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-727"><a href="#cb33-727" aria-hidden="true" tabindex="-1"></a>f_t(h) = k_{t,0} + k_{t,1}x_{t+h,1} + \ldots + k_{t+h,M}x_{t+h,M}</span>
<span id="cb33-728"><a href="#cb33-728" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gen-regression-forecast-function}</span>
<span id="cb33-729"><a href="#cb33-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-730"><a href="#cb33-730" aria-hidden="true" tabindex="-1"></a>A particular case is of dynamic regressions is the case of time-varying auto-regressions (TVAR) with</span>
<span id="cb33-731"><a href="#cb33-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-732"><a href="#cb33-732" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-733"><a href="#cb33-733" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-734"><a href="#cb33-734" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \varphi_{t,1}y_{t−1} + \varphi_{t,2}y_{t−2} + \ldots + \varphi_{t,p} y_{t−p} + ν_t,<span class="sc">\\</span></span>
<span id="cb33-735"><a href="#cb33-735" aria-hidden="true" tabindex="-1"></a>\varphi_{t,m} &amp;= \varphi_{t−1,m} + \omega_{t,m,} &amp; m = 1 : p</span>
<span id="cb33-736"><a href="#cb33-736" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-737"><a href="#cb33-737" aria-hidden="true" tabindex="-1"></a>$$ {#eq-tvar}</span>
<span id="cb33-738"><a href="#cb33-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-739"><a href="#cb33-739" aria-hidden="true" tabindex="-1"></a>There is a paper <span class="co">[</span><span class="ot">@prado2000bayesian</span><span class="co">]</span> on TVAR models that is a good reference for this model.</span>
<span id="cb33-740"><a href="#cb33-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-741"><a href="#cb33-741" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-742"><a href="#cb33-742" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript {.unnumbered}</span></span>
<span id="cb33-743"><a href="#cb33-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-744"><a href="#cb33-744" aria-hidden="true" tabindex="-1"></a>{{&lt; include _C4-L03-T03.qmd &gt;}}</span>
<span id="cb33-745"><a href="#cb33-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-746"><a href="#cb33-746" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-747"><a href="#cb33-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-748"><a href="#cb33-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-749"><a href="#cb33-749" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary of Regression Models (Reading)</span></span>
<span id="cb33-750"><a href="#cb33-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-751"><a href="#cb33-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dynamic Regression Models</span></span>
<span id="cb33-752"><a href="#cb33-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-753"><a href="#cb33-753" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Simple Dynamic Regression</span></span>
<span id="cb33-754"><a href="#cb33-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-755"><a href="#cb33-755" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-756"><a href="#cb33-756" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-757"><a href="#cb33-757" aria-hidden="true" tabindex="-1"></a>  y_t &amp;= \beta_{t,0} + \beta_{t,1} x_t + \nu_t <span class="sc">\\</span></span>
<span id="cb33-758"><a href="#cb33-758" aria-hidden="true" tabindex="-1"></a>  \beta_{t,0} &amp;= \beta_{t-1,0} + \omega_{t,0} <span class="sc">\\</span></span>
<span id="cb33-759"><a href="#cb33-759" aria-hidden="true" tabindex="-1"></a>  \beta_{t,1} &amp;= \beta_{t-1,1} + \omega_{t,1} </span>
<span id="cb33-760"><a href="#cb33-760" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-761"><a href="#cb33-761" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-762"><a href="#cb33-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-763"><a href="#cb33-763" aria-hidden="true" tabindex="-1"></a>Thus:</span>
<span id="cb33-764"><a href="#cb33-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-765"><a href="#cb33-765" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-766"><a href="#cb33-766" aria-hidden="true" tabindex="-1"></a>\theta_t = (\beta_{t,0}, \beta_{t,1})'</span>
<span id="cb33-767"><a href="#cb33-767" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-768"><a href="#cb33-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-769"><a href="#cb33-769" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-770"><a href="#cb33-770" aria-hidden="true" tabindex="-1"></a>F_t = (1, x_t)'</span>
<span id="cb33-771"><a href="#cb33-771" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb33-772"><a href="#cb33-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-773"><a href="#cb33-773" aria-hidden="true" tabindex="-1"></a>and </span>
<span id="cb33-774"><a href="#cb33-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-775"><a href="#cb33-775" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-776"><a href="#cb33-776" aria-hidden="true" tabindex="-1"></a>G = I_2</span>
<span id="cb33-777"><a href="#cb33-777" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-778"><a href="#cb33-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-779"><a href="#cb33-779" aria-hidden="true" tabindex="-1"></a>This results in a forecast function of the form</span>
<span id="cb33-780"><a href="#cb33-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-781"><a href="#cb33-781" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-782"><a href="#cb33-782" aria-hidden="true" tabindex="-1"></a>f_t(h) = k_{t,0} + k_{t,1} x_{t+h}.</span>
<span id="cb33-783"><a href="#cb33-783" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-784"><a href="#cb33-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-785"><a href="#cb33-785" aria-hidden="true" tabindex="-1"></a><span class="fu">#### General Dynamic Regression</span></span>
<span id="cb33-786"><a href="#cb33-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-787"><a href="#cb33-787" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-788"><a href="#cb33-788" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-789"><a href="#cb33-789" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \beta_{t,0} + \beta_{t,1} x_{t,1} + \dots + \beta_{t,M} x_{t,M} + \nu_t <span class="sc">\\</span></span>
<span id="cb33-790"><a href="#cb33-790" aria-hidden="true" tabindex="-1"></a>\beta_{t,m} &amp;= \beta_{t-1,m} + \omega_{t,m}, \quad &amp;m = 0:M.</span>
<span id="cb33-791"><a href="#cb33-791" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-792"><a href="#cb33-792" aria-hidden="true" tabindex="-1"></a>$$ {#eq-general-dynamic-regression}</span>
<span id="cb33-793"><a href="#cb33-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-794"><a href="#cb33-794" aria-hidden="true" tabindex="-1"></a>Then, </span>
<span id="cb33-795"><a href="#cb33-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-796"><a href="#cb33-796" aria-hidden="true" tabindex="-1"></a>$\theta_t = (\beta_{t,0}, \dots, \beta_{t,M})'$, </span>
<span id="cb33-797"><a href="#cb33-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-798"><a href="#cb33-798" aria-hidden="true" tabindex="-1"></a>$\mathbf{F}_t = (1, x_{t,1}, \dots, x_{t,M})'$ and </span>
<span id="cb33-799"><a href="#cb33-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-800"><a href="#cb33-800" aria-hidden="true" tabindex="-1"></a>$\mathbf{G} = \mathbf{I}_M$. </span>
<span id="cb33-801"><a href="#cb33-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-802"><a href="#cb33-802" aria-hidden="true" tabindex="-1"></a>The forecast function is given by:</span>
<span id="cb33-803"><a href="#cb33-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-804"><a href="#cb33-804" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-805"><a href="#cb33-805" aria-hidden="true" tabindex="-1"></a>f_t(h) = k_{t,0} + k_{t,1} x_{t+h,1} + \dots + k_{t,M} x_{t+h,M}.</span>
<span id="cb33-806"><a href="#cb33-806" aria-hidden="true" tabindex="-1"></a>$$ {#eq-forecast-function-general-dynamic-regression}</span>
<span id="cb33-807"><a href="#cb33-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-808"><a href="#cb33-808" aria-hidden="true" tabindex="-1"></a>A particular case of dynamic regressions is the case of **time-varying autoregressive (TVAR)** with</span>
<span id="cb33-809"><a href="#cb33-809" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">time-varying autoregressive (TVAR)</span><span class="co">]</span>{.column-margin width="200px" group="slides"}</span>
<span id="cb33-810"><a href="#cb33-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-811"><a href="#cb33-811" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-812"><a href="#cb33-812" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-813"><a href="#cb33-813" aria-hidden="true" tabindex="-1"></a>  y_t &amp;= \phi_{t,1} y_{t-1} + \phi_{t,2} y_{t-2} + \dots + \phi_{t,p} y_{t-p} + \nu_t <span class="sc">\\</span></span>
<span id="cb33-814"><a href="#cb33-814" aria-hidden="true" tabindex="-1"></a>  \phi_{t,m} &amp;= \phi_{t-1,m} + \omega_{t,m}, \quad m = 1:p.</span>
<span id="cb33-815"><a href="#cb33-815" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-816"><a href="#cb33-816" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-817"><a href="#cb33-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-818"><a href="#cb33-818" aria-hidden="true" tabindex="-1"></a><span class="fu">## The superposition principle (Video)</span></span>
<span id="cb33-819"><a href="#cb33-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-820"><a href="#cb33-820" aria-hidden="true" tabindex="-1"></a><span class="al">![The superposition principle](images/m3_0031.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-821"><a href="#cb33-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-822"><a href="#cb33-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-823"><a href="#cb33-823" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">We can use the superposition principle to build models that have different kinds of components. The main idea is to think about what is the general structure we want for the forecast function and then isolate the different components of the forecast function and think about the classes of dynamic linear models that are represented in each of those components.</span><span class="co">]</span>{.mark} Each of those components has a class and then we can build the general dynamic linear model with all those pieces together using this principle.</span>
<span id="cb33-824"><a href="#cb33-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-825"><a href="#cb33-825" aria-hidden="true" tabindex="-1"></a>Two references for the Superposition principle are</span>
<span id="cb33-826"><a href="#cb33-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-827"><a href="#cb33-827" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">@west2013bayesian §3.1 p. 98</span><span class="co">]</span></span>
<span id="cb33-828"><a href="#cb33-828" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">@prado2023time §4.2.1 p. 136</span><span class="co">]</span></span>
<span id="cb33-829"><a href="#cb33-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-830"><a href="#cb33-830" aria-hidden="true" tabindex="-1"></a>::: {#imp-superposition .callout-important}</span>
<span id="cb33-831"><a href="#cb33-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-832"><a href="#cb33-832" aria-hidden="true" tabindex="-1"></a><span class="fu">## Superposition Principle</span></span>
<span id="cb33-833"><a href="#cb33-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-834"><a href="#cb33-834" aria-hidden="true" tabindex="-1"></a>In the first the author state:</span>
<span id="cb33-835"><a href="#cb33-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-836"><a href="#cb33-836" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Conditional independence also features strongly in initial model building and in choosing an appropriate parametrization. For example, the linear superposition principle states that any linear combination of deterministic linear models is a linear model. This extends to a normal linear superposition principle:</span></span>
<span id="cb33-837"><a href="#cb33-837" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb33-838"><a href="#cb33-838" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">Any linear combination of independent normal DLMs is a normal DLM.</span><span class="co">]</span><span class="at">{.mark} - &gt; -- </span><span class="co">[</span><span class="ot">@west2013bayesian §3.1 p. 98</span><span class="co">]</span></span>
<span id="cb33-839"><a href="#cb33-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-840"><a href="#cb33-840" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-841"><a href="#cb33-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-842"><a href="#cb33-842" aria-hidden="true" tabindex="-1"></a>We will illustrate how to do that with an example:</span>
<span id="cb33-843"><a href="#cb33-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-844"><a href="#cb33-844" aria-hidden="true" tabindex="-1"></a>Let's say that we want to create a model here with a forecast function that has a linear trend component. Let's say we have a linear function as a function of the number of steps ahead that you want to consider. Then suppose you also have a covariate here that you want to include in your model as a regression component.</span>
<span id="cb33-845"><a href="#cb33-845" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-846"><a href="#cb33-846" aria-hidden="true" tabindex="-1"></a>f_t(h) = \underbrace{(k_{t,0} + k_{t,1}\; h)}_{\text{linear trend component}} + \underbrace{(k_{t,2}\; x_{t+h})}_{\text{regression component}}</span>
<span id="cb33-847"><a href="#cb33-847" aria-hidden="true" tabindex="-1"></a>$$ {#eq-superposition-principle-example}</span>
<span id="cb33-848"><a href="#cb33-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-849"><a href="#cb33-849" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb33-850"><a href="#cb33-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-851"><a href="#cb33-851" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f_t(h)$ is our forecast function.</span>
<span id="cb33-852"><a href="#cb33-852" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k_{t,0}$, $k_{t,1}$ and $k_{t,2}$ are just constants (that we index using time $t$ and a second subscript).</span>
<span id="cb33-853"><a href="#cb33-853" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$x_{t+h}$ is a time dependent regression covariate.</span>
<span id="cb33-854"><a href="#cb33-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-855"><a href="#cb33-855" aria-hidden="true" tabindex="-1"></a>When we look at the forecast function, we can isolate a linear trend and a regression components as indicated.</span>
<span id="cb33-856"><a href="#cb33-856" aria-hidden="true" tabindex="-1"></a>Each of these can be set in terms of two forecast functions]{.mark}. I'm going to call the forecast function $f_{1,t}(h)$, this is just the first piece.</span>
<span id="cb33-857"><a href="#cb33-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-858"><a href="#cb33-858" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-859"><a href="#cb33-859" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-860"><a href="#cb33-860" aria-hidden="true" tabindex="-1"></a>f_t(h) &amp;= f_{1,t}(h) + f_{2,t}(h) <span class="sc">\\</span></span>
<span id="cb33-861"><a href="#cb33-861" aria-hidden="true" tabindex="-1"></a>f_{1,t}(h) &amp;= k_{t,0} + k_{t,1} &amp; \text{(linear trend component)} <span class="sc">\\</span></span>
<span id="cb33-862"><a href="#cb33-862" aria-hidden="true" tabindex="-1"></a>f_{2,t}(h) &amp;= k_{t,2}x_{t+h} &amp; \text{(regression component)}</span>
<span id="cb33-863"><a href="#cb33-863" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-864"><a href="#cb33-864" aria-hidden="true" tabindex="-1"></a>$$ {#eq-superposition-breaking-it-down}</span>
<span id="cb33-865"><a href="#cb33-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-866"><a href="#cb33-866" aria-hidden="true" tabindex="-1"></a>We know how to represent forecast function $f_{1,t}$ and $f_{2,t}$ in terms of dynamic linear models.</span>
<span id="cb33-867"><a href="#cb33-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-868"><a href="#cb33-868" aria-hidden="true" tabindex="-1"></a>For the linear trend component, $f_{1,t}(h)$ , we have a 2-dimensional state vector, $\theta_t = (\theta_{t,1}, \theta_{t,2})'$, which yields the following DLM shortform:</span>
<span id="cb33-869"><a href="#cb33-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-870"><a href="#cb33-870" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-871"><a href="#cb33-871" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>F_1, G_1, \cdot, \cdot<span class="sc">\}</span>  \qquad \text{(short notation)}</span>
<span id="cb33-872"><a href="#cb33-872" aria-hidden="true" tabindex="-1"></a>$$ {#eq-superposition-linear-trend-short-form}</span>
<span id="cb33-873"><a href="#cb33-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-874"><a href="#cb33-874" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Where we don't explicitly specify the observational and system variances, $V$ and $W$ </span>
<span id="cb33-875"><a href="#cb33-875" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The important bit are $F$ and $G$. The forecast function is given by:</span>
<span id="cb33-876"><a href="#cb33-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-877"><a href="#cb33-877" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-878"><a href="#cb33-878" aria-hidden="true" tabindex="-1"></a>F_{1} = E_2 = (1, 0)'</span>
<span id="cb33-879"><a href="#cb33-879" aria-hidden="true" tabindex="-1"></a>$$ {#eq-superposition-F1}</span>
<span id="cb33-880"><a href="#cb33-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-881"><a href="#cb33-881" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-882"><a href="#cb33-882" aria-hidden="true" tabindex="-1"></a>G_{1} =</span>
<span id="cb33-883"><a href="#cb33-883" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-884"><a href="#cb33-884" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-885"><a href="#cb33-885" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb33-886"><a href="#cb33-886" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb33-887"><a href="#cb33-887" aria-hidden="true" tabindex="-1"></a>$$ {#eq-superposition-G1}</span>
<span id="cb33-888"><a href="#cb33-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-889"><a href="#cb33-889" aria-hidden="true" tabindex="-1"></a>for the regression component $f_{2,t}(h)$ we have the following DLM representation:</span>
<span id="cb33-890"><a href="#cb33-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-891"><a href="#cb33-891" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-892"><a href="#cb33-892" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>F_2,t, G_2, \cdot, \cdot<span class="sc">\}</span>  \qquad \text{(short notation)}</span>
<span id="cb33-893"><a href="#cb33-893" aria-hidden="true" tabindex="-1"></a>$$ {#eq-superposition-regression-short-form}</span>
<span id="cb33-894"><a href="#cb33-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-895"><a href="#cb33-895" aria-hidden="true" tabindex="-1"></a>where we have $F_{2t}$ is $X_t$ and my $G$ is simply going to be 1. This is a one-dimensional vector in terms of the state parameter vector. </span>
<span id="cb33-896"><a href="#cb33-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-897"><a href="#cb33-897" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-898"><a href="#cb33-898" aria-hidden="true" tabindex="-1"></a>F_{2,t} = x_{t+h}</span>
<span id="cb33-899"><a href="#cb33-899" aria-hidden="true" tabindex="-1"></a>$$ {#eq-superposition-F2}</span>
<span id="cb33-900"><a href="#cb33-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-901"><a href="#cb33-901" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-902"><a href="#cb33-902" aria-hidden="true" tabindex="-1"></a>G_{2} = 1</span>
<span id="cb33-903"><a href="#cb33-903" aria-hidden="true" tabindex="-1"></a>$$ {#eq-superposition-G2}</span>
<span id="cb33-904"><a href="#cb33-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-905"><a href="#cb33-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-906"><a href="#cb33-906" aria-hidden="true" tabindex="-1"></a>Once we have these, we can assemble them into our final model. $<span class="sc">\{</span>F_t, G, \cdot, \cdot<span class="sc">\}</span>$</span>
<span id="cb33-907"><a href="#cb33-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-908"><a href="#cb33-908" aria-hidden="true" tabindex="-1"></a>We care more about $F$, $G$, and less about the observational variance and some covariance also for the system where the</span>
<span id="cb33-909"><a href="#cb33-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-910"><a href="#cb33-910" aria-hidden="true" tabindex="-1"></a>F is going to be an F that has, you just concatenate the two Fs. You're going to get 1, 0 and then you're going to put the next component here.</span>
<span id="cb33-911"><a href="#cb33-911" aria-hidden="true" tabindex="-1"></a>Again, this one is dependent on time because this component is time dependent and </span>
<span id="cb33-912"><a href="#cb33-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-913"><a href="#cb33-913" aria-hidden="true" tabindex="-1"></a>The model with forecast function $f_t(h)$ above is a model with a 3-dimensional state vector with </span>
<span id="cb33-914"><a href="#cb33-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-915"><a href="#cb33-915" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-916"><a href="#cb33-916" aria-hidden="true" tabindex="-1"></a>F_t = (F_1', F_{2,t})' = (1, 0, x_t)'</span>
<span id="cb33-917"><a href="#cb33-917" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb33-918"><a href="#cb33-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-919"><a href="#cb33-919" aria-hidden="true" tabindex="-1"></a>Then the G, you can create it just taking a block diagonal structure by concatenating $G_1$ and $G_2$. though formally there must be a better term for this operation.</span>
<span id="cb33-920"><a href="#cb33-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-921"><a href="#cb33-921" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-922"><a href="#cb33-922" aria-hidden="true" tabindex="-1"></a>G = \text{blockdiag}<span class="co">[</span><span class="ot">G_1, G_2</span><span class="co">]</span> = </span>
<span id="cb33-923"><a href="#cb33-923" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-924"><a href="#cb33-924" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb33-925"><a href="#cb33-925" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb33-926"><a href="#cb33-926" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 1</span>
<span id="cb33-927"><a href="#cb33-927" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb33-928"><a href="#cb33-928" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-929"><a href="#cb33-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-930"><a href="#cb33-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-931"><a href="#cb33-931" aria-hidden="true" tabindex="-1"></a>This gives us the full $G$ dynamics matrix for the model. A model with this $F_t$ and this $G$ that is constant over time will give us this particular forecast function @eq-superposition-principle-example we started with.</span>
<span id="cb33-932"><a href="#cb33-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-933"><a href="#cb33-933" aria-hidden="true" tabindex="-1"></a>We used the superposition principle to build this model. If we need additional components, we will learn how to incorporate seasonal components, regression components, trend components. One  can build a fairly sophisticated model with different structures into this particular model using the superposition principle.</span>
<span id="cb33-934"><a href="#cb33-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-935"><a href="#cb33-935" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-936"><a href="#cb33-936" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript {.unnumbered}</span></span>
<span id="cb33-937"><a href="#cb33-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-938"><a href="#cb33-938" aria-hidden="true" tabindex="-1"></a>{{&lt; include _C4-L03-T04.qmd &gt;}}</span>
<span id="cb33-939"><a href="#cb33-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-940"><a href="#cb33-940" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-941"><a href="#cb33-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-942"><a href="#cb33-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-943"><a href="#cb33-943" aria-hidden="true" tabindex="-1"></a><span class="fu">## Superposition principle: General case (Reading)</span></span>
<span id="cb33-944"><a href="#cb33-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-945"><a href="#cb33-945" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">You can build dynamic models with different components, for example, a trend component plus a regression component, by using the principle of superposition. The idea is to think about the general form of the forecast function you want to have for prediction. You then write that forecast function as a sum of different components where each component corresponds to a class of DLM with its own state-space representation. The final DLM can then be written by combining the pieces of the different components.</span><span class="co">]</span>{.mark}</span>
<span id="cb33-946"><a href="#cb33-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-947"><a href="#cb33-947" aria-hidden="true" tabindex="-1"></a>For example, suppose you are interested in a model with a forecast function that includes a linear polynomial trend and a single covariate $x_t$, i.e.,</span>
<span id="cb33-948"><a href="#cb33-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-949"><a href="#cb33-949" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-950"><a href="#cb33-950" aria-hidden="true" tabindex="-1"></a>f_t(h) = k_{t,0} + k_{t,1}h + k_{t,3}x_{t+h}.</span>
<span id="cb33-951"><a href="#cb33-951" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-952"><a href="#cb33-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-953"><a href="#cb33-953" aria-hidden="true" tabindex="-1"></a>This forecast function can be written as $f_t(h) = f_{1,t}(h) + f_{2,t}(h)$, with</span>
<span id="cb33-954"><a href="#cb33-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-955"><a href="#cb33-955" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-956"><a href="#cb33-956" aria-hidden="true" tabindex="-1"></a>f_{1,t}(h) = (k_{t,0} + k_{t,1}h), \quad f_{2,t}(h) = k_{t,3}x_{t+h}.</span>
<span id="cb33-957"><a href="#cb33-957" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-958"><a href="#cb33-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-959"><a href="#cb33-959" aria-hidden="true" tabindex="-1"></a>The first component in the forecast function corresponds to a model with a 2-dimensional state vector, $F_{1,t} = F_1 = (1, 0)'$,</span>
<span id="cb33-960"><a href="#cb33-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-961"><a href="#cb33-961" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-962"><a href="#cb33-962" aria-hidden="true" tabindex="-1"></a>G_{1,t} = G_1 = </span>
<span id="cb33-963"><a href="#cb33-963" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-964"><a href="#cb33-964" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 <span class="sc">\\</span></span>
<span id="cb33-965"><a href="#cb33-965" aria-hidden="true" tabindex="-1"></a>0 &amp; 1</span>
<span id="cb33-966"><a href="#cb33-966" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb33-967"><a href="#cb33-967" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-968"><a href="#cb33-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-969"><a href="#cb33-969" aria-hidden="true" tabindex="-1"></a>The second component corresponds to a model with a 1-dimensional state vector, $F_{2,t} = x_t$, $G_{2,t} = G_2 = 1$.</span>
<span id="cb33-970"><a href="#cb33-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-971"><a href="#cb33-971" aria-hidden="true" tabindex="-1"></a>The model with forecast function $f_t(h)$ above is a model with a 3-dimensional state vector with $F_t = (F_1', F_{2,t})' = (1, 0, x_t)'$ and</span>
<span id="cb33-972"><a href="#cb33-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-973"><a href="#cb33-973" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-974"><a href="#cb33-974" aria-hidden="true" tabindex="-1"></a>G_t = \text{blockdiag}<span class="co">[</span><span class="ot">G_1, G_2</span><span class="co">]</span> = </span>
<span id="cb33-975"><a href="#cb33-975" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb33-976"><a href="#cb33-976" aria-hidden="true" tabindex="-1"></a>1 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb33-977"><a href="#cb33-977" aria-hidden="true" tabindex="-1"></a>0 &amp; 1 &amp; 0 <span class="sc">\\</span></span>
<span id="cb33-978"><a href="#cb33-978" aria-hidden="true" tabindex="-1"></a>0 &amp; 0 &amp; 1</span>
<span id="cb33-979"><a href="#cb33-979" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}.</span>
<span id="cb33-980"><a href="#cb33-980" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-981"><a href="#cb33-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-982"><a href="#cb33-982" aria-hidden="true" tabindex="-1"></a><span class="fu">### General Case</span></span>
<span id="cb33-983"><a href="#cb33-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-984"><a href="#cb33-984" aria-hidden="true" tabindex="-1"></a>The general case wasn't covered in the video and we didn't have a proper statement of the superposition principle. However, in <span class="co">[</span><span class="ot">@imp-superposition</span><span class="co">]</span> I extracted the statement of the principle above. This statement clarifies that the principle arises via conditional independence, a tool we also used extensively in the previous course on mixture models. </span>
<span id="cb33-985"><a href="#cb33-985" aria-hidden="true" tabindex="-1"></a>Now let us consider the general case from the handout.</span>
<span id="cb33-986"><a href="#cb33-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-987"><a href="#cb33-987" aria-hidden="true" tabindex="-1"></a>Assume that you have a time series process $y_t$ with a forecast function</span>
<span id="cb33-988"><a href="#cb33-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-989"><a href="#cb33-989" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-990"><a href="#cb33-990" aria-hidden="true" tabindex="-1"></a>f_t(h) = \sum_{i=1}^{m} f_{i,t}(h),</span>
<span id="cb33-991"><a href="#cb33-991" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-992"><a href="#cb33-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-993"><a href="#cb33-993" aria-hidden="true" tabindex="-1"></a>where each $f_{i,t}(h)$ is the forecast function of a DLM with representation $<span class="sc">\{</span>F_{i,t}, G_{i,t}, v_{i,t}, W_{i,t}<span class="sc">\}</span>$.</span>
<span id="cb33-994"><a href="#cb33-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-995"><a href="#cb33-995" aria-hidden="true" tabindex="-1"></a>Then, $f_t(h)$ has a DLM representation $<span class="sc">\{</span>F_t, G_t, v_t, W_t<span class="sc">\}</span>$ with</span>
<span id="cb33-996"><a href="#cb33-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-997"><a href="#cb33-997" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-998"><a href="#cb33-998" aria-hidden="true" tabindex="-1"></a>F_t = (F_{1,t}', F_{2,t}', \dots, F_{m,t}')',</span>
<span id="cb33-999"><a href="#cb33-999" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1000"><a href="#cb33-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1001"><a href="#cb33-1001" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1002"><a href="#cb33-1002" aria-hidden="true" tabindex="-1"></a>G_t = \text{blockdiag}<span class="co">[</span><span class="ot">G_{1,t}, \dots, G_{m,t}</span><span class="co">]</span>,</span>
<span id="cb33-1003"><a href="#cb33-1003" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1004"><a href="#cb33-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1005"><a href="#cb33-1005" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1006"><a href="#cb33-1006" aria-hidden="true" tabindex="-1"></a>v_t = \sum_{i=1}^{m} v_{i,t},</span>
<span id="cb33-1007"><a href="#cb33-1007" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1008"><a href="#cb33-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1009"><a href="#cb33-1009" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb33-1010"><a href="#cb33-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1011"><a href="#cb33-1011" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1012"><a href="#cb33-1012" aria-hidden="true" tabindex="-1"></a>W_t = \text{blockdiag}<span class="co">[</span><span class="ot">W_{1,t}, \dots, W_{m,t}</span><span class="co">]</span>.</span>
<span id="cb33-1013"><a href="#cb33-1013" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1014"><a href="#cb33-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1015"><a href="#cb33-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1016"><a href="#cb33-1016" aria-hidden="true" tabindex="-1"></a><span class="fu">## Quiz: The Normal Dynamic Linear Model</span></span>
<span id="cb33-1017"><a href="#cb33-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1018"><a href="#cb33-1018" aria-hidden="true" tabindex="-1"></a>Omitted due to Coursera honor code</span>
<span id="cb33-1019"><a href="#cb33-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1020"><a href="#cb33-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1021"><a href="#cb33-1021" aria-hidden="true" tabindex="-1"></a><span class="fu"># Bayesian Inference in the NDLM: Part 1</span></span>
<span id="cb33-1022"><a href="#cb33-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1023"><a href="#cb33-1023" aria-hidden="true" tabindex="-1"></a>::: {#vid-sean-law .column-margin .content-visible when-format="html"}</span>
<span id="cb33-1024"><a href="#cb33-1024" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/watch?v=0O6dlq6a4rA&amp;ab_channel=SciPy &gt;}}</span>
<span id="cb33-1025"><a href="#cb33-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1026"><a href="#cb33-1026" aria-hidden="true" tabindex="-1"></a>Sean Law - STUMPY: Modern Time Series Analysis with Matrix Profiles </span>
<span id="cb33-1027"><a href="#cb33-1027" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-1028"><a href="#cb33-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1029"><a href="#cb33-1029" aria-hidden="true" tabindex="-1"></a>We will now delve into Bayesian inference in the case of the normal dynamic linear model where both the observational variance and the system variance are known. We will talk about filtering equations, smoothing equations and also forecasting in this setting using Bayesian approach</span>
<span id="cb33-1030"><a href="#cb33-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1031"><a href="#cb33-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1032"><a href="#cb33-1032" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse='true'}</span>
<span id="cb33-1033"><a href="#cb33-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1034"><a href="#cb33-1034" aria-hidden="true" tabindex="-1"></a><span class="fu">### Reality check {.unnumbered}</span></span>
<span id="cb33-1035"><a href="#cb33-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1036"><a href="#cb33-1036" aria-hidden="true" tabindex="-1"></a>A large portion of the next video is dedicated to laying the ground for use bayesian inference in the NDLM. I was thinking we will infer $F, G,V, W$ from the data, once we supply a normal prior $\mathcal{N}(M,C)$. But this is not the case. In fact we don't seem to be particularly Bayesian when going about constructing the NDLM model. </span>
<span id="cb33-1037"><a href="#cb33-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1038"><a href="#cb33-1038" aria-hidden="true" tabindex="-1"></a> (i.e. filtering smoothing, of the parameters $\theta_t$ given the data and forecasting $y_t$ based on that)</span>
<span id="cb33-1039"><a href="#cb33-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1040"><a href="#cb33-1040" aria-hidden="true" tabindex="-1"></a>I was initially disappointed that we assume that we know everything I want to infer. This is an open question for the proverbial <span class="co">[</span><span class="ot">Feynman Notebook</span><span class="co">](./C4-L06.qmd)</span>. In reality that is just another problem.</span>
<span id="cb33-1041"><a href="#cb33-1041" aria-hidden="true" tabindex="-1"></a> But that not the process with time series analysis. </span>
<span id="cb33-1042"><a href="#cb33-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1043"><a href="#cb33-1043" aria-hidden="true" tabindex="-1"></a>However, at some point I saw a talk about <span class="co">[</span><span class="ot">STUMPY</span><span class="co">](https://stumpy.readthedocs.io/en/latest/Tutorial_STUMPY_Basics.html)</span> which does many cool time series stuff in python. And the speaker Sean Law talks about all the different Data Science &amp; DSP Mojos <span class="in">`#magicspells`</span> one can use, the nascent issues when comparing lots of times series or even just time series with lots of data. He makes another point that there is <span class="in">`#NoFreeLunch`</span> - each Mojo comes with with its own assumptions and limitations before making the case for using <span class="co">[</span><span class="ot">Matrix Profiles</span><span class="co">](https://stumpy.readthedocs.io/en/latest/Tutorial_STUMPY_Basics.html)</span>.</span>
<span id="cb33-1044"><a href="#cb33-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1045"><a href="#cb33-1045" aria-hidden="true" tabindex="-1"></a>Prado lays the ground for working with a time series with around 300 data points - i.e. something we can still plot and view on the screen then inspect it. This might unlock for us, the NDLM framework which while flexible requires us to have a detailed form of the the model and its priors. This is easy enough if we deal with a synthetic data set, less so if we have need to analyze an novel time series for which we lack much intuition. Here we will need todo a preliminary exploratory data analysis before we can step in and construct our NDLM.</span>
<span id="cb33-1046"><a href="#cb33-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1047"><a href="#cb33-1047" aria-hidden="true" tabindex="-1"></a>I suppose they expect that you will use some other non-bayesian tools to do this as we haven't really covered this in her course.</span>
<span id="cb33-1048"><a href="#cb33-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1049"><a href="#cb33-1049" aria-hidden="true" tabindex="-1"></a>Splitting a TS into trend periods and a stationary residual isn't too tricky in R. Getting the inverse periods is then possible. Doing regression on the residuals is also possible. So if we have done all that we should be able to write down an NDLM based on all that we learned. And this will allow us to do filtering, smoothing and forecasting with error estimates.</span>
<span id="cb33-1050"><a href="#cb33-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1051"><a href="#cb33-1051" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-1052"><a href="#cb33-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1053"><a href="#cb33-1053" aria-hidden="true" tabindex="-1"></a><span class="fu">## Filtering (Video)</span></span>
<span id="cb33-1054"><a href="#cb33-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1055"><a href="#cb33-1055" aria-hidden="true" tabindex="-1"></a><span class="al">![Derivation for the Prior and Forecast at Time t](images/m3_0041.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-1056"><a href="#cb33-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1057"><a href="#cb33-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1058"><a href="#cb33-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1059"><a href="#cb33-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1060"><a href="#cb33-1060" aria-hidden="true" tabindex="-1"></a><span class="al">![Derivation of the Posterior at Time t](images/m3_0042.png)</span>{.column-margin  group="slides" width="200px"}</span>
<span id="cb33-1061"><a href="#cb33-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1062"><a href="#cb33-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1063"><a href="#cb33-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1064"><a href="#cb33-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1065"><a href="#cb33-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1066"><a href="#cb33-1066" aria-hidden="true" tabindex="-1"></a>Recall we are working in a Bayesian setting where a NDLM model with a normal prior would like this:</span>
<span id="cb33-1067"><a href="#cb33-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1068"><a href="#cb33-1068" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1069"><a href="#cb33-1069" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-1070"><a href="#cb33-1070" aria-hidden="true" tabindex="-1"></a>  y_t &amp;= F_t' \theta_t + \nu_t &amp; \nu_t &amp;\sim \mathcal{N}(0, v_t) &amp; \text{(observation)}<span class="sc">\\</span></span>
<span id="cb33-1071"><a href="#cb33-1071" aria-hidden="true" tabindex="-1"></a>  \theta_t &amp;= G_t \theta_{t-1} + \omega_t &amp; \omega_t &amp;\sim  \mathcal{N}(0, W_t) &amp; \text{(evolution)} <span class="sc">\\</span></span>
<span id="cb33-1072"><a href="#cb33-1072" aria-hidden="true" tabindex="-1"></a>  &amp; &amp;(\theta_0 \mid \mathcal{D}_0) &amp; \sim  \mathcal{N}(m_0, C_0) &amp; \text{(prior)}</span>
<span id="cb33-1073"><a href="#cb33-1073" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-1074"><a href="#cb33-1074" aria-hidden="true" tabindex="-1"></a>$$ {#eq-generic-NDLM}</span>
<span id="cb33-1075"><a href="#cb33-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1076"><a href="#cb33-1076" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the prior  $\mathcal{D}_0$ stands for the information that we have before collecting any data and </span>
<span id="cb33-1077"><a href="#cb33-1077" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We are assuming $\theta_0$ follows a normal distribution with </span>
<span id="cb33-1078"><a href="#cb33-1078" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$m_0$ mean</span>
<span id="cb33-1079"><a href="#cb33-1079" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$C_0$ variance covariance matrix.</span>
<span id="cb33-1080"><a href="#cb33-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1081"><a href="#cb33-1081" aria-hidden="true" tabindex="-1"></a>Since we are doing filtering which is a retrospective analysis, of past states we assume that we know $m_0, C_0, \nu_t, \omega_t, F_t, G_t \qquad \forall t$.</span>
<span id="cb33-1082"><a href="#cb33-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1083"><a href="#cb33-1083" aria-hidden="true" tabindex="-1"></a>However, there is often great interest in looking back in time in order to get a clearer picture of what happened.</span>
<span id="cb33-1084"><a href="#cb33-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1085"><a href="#cb33-1085" aria-hidden="true" tabindex="-1"></a>We are interested in performing Bayesian inference in this setting and we talked about different kinds of distributions. </span>
<span id="cb33-1086"><a href="#cb33-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1087"><a href="#cb33-1087" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>One is the **filtering distribution** that allows us to update the distribution of $\theta_t$ as we receive observations and information over time. </span>
<span id="cb33-1088"><a href="#cb33-1088" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The other one is **smoothing equations** that allows us to just revisit the past once we have observed a chunk of data. </span>
<span id="cb33-1089"><a href="#cb33-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1090"><a href="#cb33-1090" aria-hidden="true" tabindex="-1"></a>In a Bayesian setting, you have to set *a prior distribution*. <span class="co">[</span><span class="ot">We will work with the prior distribution that is conjugate.</span><span class="co">]</span>{.mark} </span>
<span id="cb33-1091"><a href="#cb33-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1092"><a href="#cb33-1092" aria-hidden="true" tabindex="-1"></a>In this case we have to begin with a distribution at time zero for $\theta_0$. So before we have seen any data at all, I have this prior distribution. </span>
<span id="cb33-1093"><a href="#cb33-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1094"><a href="#cb33-1094" aria-hidden="true" tabindex="-1"></a>We also assume a prior distribution of the form:</span>
<span id="cb33-1095"><a href="#cb33-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1096"><a href="#cb33-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1097"><a href="#cb33-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1098"><a href="#cb33-1098" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1099"><a href="#cb33-1099" aria-hidden="true" tabindex="-1"></a>(\theta_{t} \mid \mathcal{D}_{t-1}) \sim \mathcal{N}(m_{t-1}, C_{t-1}).</span>
<span id="cb33-1100"><a href="#cb33-1100" aria-hidden="true" tabindex="-1"></a>$$ {#eq-filtering-assumption-1}</span>
<span id="cb33-1101"><a href="#cb33-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1102"><a href="#cb33-1102" aria-hidden="true" tabindex="-1"></a>We assume that this the filtering distribution follows this normal distribution based on </span>
<span id="cb33-1103"><a href="#cb33-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1104"><a href="#cb33-1104" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>the prior in <span class="co">[</span><span class="ot">@eq-generic-NDLM</span><span class="co">]</span> being conjugate of the normal and  </span>
<span id="cb33-1105"><a href="#cb33-1105" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>the linearity of the model in <span class="co">[</span><span class="ot">@eq-generic-NDLM</span><span class="co">]</span>. </span>
<span id="cb33-1106"><a href="#cb33-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1107"><a href="#cb33-1107" aria-hidden="true" tabindex="-1"></a>These result in updates to the model parameters and uncertainty, at each time step, preserving the normal structure from the prior.</span>
<span id="cb33-1108"><a href="#cb33-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1109"><a href="#cb33-1109" aria-hidden="true" tabindex="-1"></a>Then, we can obtain the following distributions:</span>
<span id="cb33-1110"><a href="#cb33-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1111"><a href="#cb33-1111" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Prior at Time $t$</span>
<span id="cb33-1112"><a href="#cb33-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1113"><a href="#cb33-1113" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb33-1114"><a href="#cb33-1114" aria-hidden="true" tabindex="-1"></a>  (\theta_t \mid \mathcal{D}_{t-1}) \sim  \mathcal{N}(a_t, R_t) \qquad \text{(prior at time t)} \qquad</span>
<span id="cb33-1115"><a href="#cb33-1115" aria-hidden="true" tabindex="-1"></a>  $$ {#eq-prior-at-time-t}</span>
<span id="cb33-1116"><a href="#cb33-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1117"><a href="#cb33-1117" aria-hidden="true" tabindex="-1"></a>  with </span>
<span id="cb33-1118"><a href="#cb33-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1119"><a href="#cb33-1119" aria-hidden="true" tabindex="-1"></a>  $$ \begin{aligned}</span>
<span id="cb33-1120"><a href="#cb33-1120" aria-hidden="true" tabindex="-1"></a>  a_t \doteq&amp; \mathbb{E}<span class="co">[</span><span class="ot">\theta_t \mid \mathcal{D}_{t-1}</span><span class="co">]</span> =&amp; G_t  \mathbb{E}<span class="co">[</span><span class="ot">G_t \theta_{t-1} \mid \mathcal{D}_{t-1} </span><span class="co">]</span> =&amp; G_t m_{t-1} <span class="sc">\\</span></span>
<span id="cb33-1121"><a href="#cb33-1121" aria-hidden="true" tabindex="-1"></a>  R_t \doteq&amp; \mathbb{V}ar<span class="co">[</span><span class="ot">\theta_t \mid \mathcal{D}_{t-1}</span><span class="co">]</span> =&amp; G_t \mathbb{V}ar<span class="co">[</span><span class="ot">\theta_t \mid \mathcal{D}_{t-1}</span><span class="co">]</span> =&amp; G_t C_{t-1} G_t' + W_t.</span>
<span id="cb33-1122"><a href="#cb33-1122" aria-hidden="true" tabindex="-1"></a>  \end{aligned}</span>
<span id="cb33-1123"><a href="#cb33-1123" aria-hidden="true" tabindex="-1"></a>  $$ {#eq-derivation-for-a-t-and-R-t}</span>
<span id="cb33-1124"><a href="#cb33-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1125"><a href="#cb33-1125" aria-hidden="true" tabindex="-1"></a>  Where we simply took the first and second moments of the system equation from <span class="co">[</span><span class="ot">@eq-generic-NDLM</span><span class="co">]</span> conditioned on our information set $\mathcal{D}_{t-1}$ </span>
<span id="cb33-1126"><a href="#cb33-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1127"><a href="#cb33-1127" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>One-Step Forecast</span>
<span id="cb33-1128"><a href="#cb33-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1129"><a href="#cb33-1129" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb33-1130"><a href="#cb33-1130" aria-hidden="true" tabindex="-1"></a>  (y_t \mid \mathcal{D}_{t-1}) \sim  \mathcal{N}(f_t, q_t) \qquad \text{(one step forecast fn)} \qquad</span>
<span id="cb33-1131"><a href="#cb33-1131" aria-hidden="true" tabindex="-1"></a>  $$ {#eq-one-step-forecast-fn}</span>
<span id="cb33-1132"><a href="#cb33-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1133"><a href="#cb33-1133" aria-hidden="true" tabindex="-1"></a>  with</span>
<span id="cb33-1134"><a href="#cb33-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1135"><a href="#cb33-1135" aria-hidden="true" tabindex="-1"></a>  $$\begin{aligned}</span>
<span id="cb33-1136"><a href="#cb33-1136" aria-hidden="true" tabindex="-1"></a>  f_t </span>
<span id="cb33-1137"><a href="#cb33-1137" aria-hidden="true" tabindex="-1"></a>     &amp; \doteq \mathbb{E}<span class="co">[</span><span class="ot"> y_t \mid \mathcal{D}_{t-1} </span><span class="co">]</span></span>
<span id="cb33-1138"><a href="#cb33-1138" aria-hidden="true" tabindex="-1"></a>     &amp; = F_t' \mathbb{E}<span class="co">[</span><span class="ot"> y_t \mid \mathcal{D}_{t-1} </span><span class="co">]</span></span>
<span id="cb33-1139"><a href="#cb33-1139" aria-hidden="true" tabindex="-1"></a>     &amp; = F_t' a_t <span class="sc">\\</span></span>
<span id="cb33-1140"><a href="#cb33-1140" aria-hidden="true" tabindex="-1"></a>  q_t </span>
<span id="cb33-1141"><a href="#cb33-1141" aria-hidden="true" tabindex="-1"></a>     &amp; \doteq \mathbb{V}ar<span class="co">[</span><span class="ot">y_t \mid \mathcal{D}_{t-1}</span><span class="co">]</span> </span>
<span id="cb33-1142"><a href="#cb33-1142" aria-hidden="true" tabindex="-1"></a>     &amp; = F_t' \mathbb{V}ar<span class="co">[</span><span class="ot">y_t \mid \mathcal{D}_{t-1}</span><span class="co">]</span>  </span>
<span id="cb33-1143"><a href="#cb33-1143" aria-hidden="true" tabindex="-1"></a>     &amp; = F_t' R_t F_t + v_t</span>
<span id="cb33-1144"><a href="#cb33-1144" aria-hidden="true" tabindex="-1"></a>  \end{aligned}</span>
<span id="cb33-1145"><a href="#cb33-1145" aria-hidden="true" tabindex="-1"></a>  $$ {#eq-derivation-for-f-t-and-q-t}</span>
<span id="cb33-1146"><a href="#cb33-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1147"><a href="#cb33-1147" aria-hidden="true" tabindex="-1"></a>  Where we took the first moments on the observation equation conditioned on the information set $\mathcal{D}_t$ and substituted @eq-one-step-forecast-fn </span>
<span id="cb33-1148"><a href="#cb33-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1149"><a href="#cb33-1149" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Posterior at Time $t$ </span>
<span id="cb33-1150"><a href="#cb33-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1151"><a href="#cb33-1151" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb33-1152"><a href="#cb33-1152" aria-hidden="true" tabindex="-1"></a>  (\theta_t \mid \mathcal{D}_t) \sim  \mathcal{N}(m_t, C_t)</span>
<span id="cb33-1153"><a href="#cb33-1153" aria-hidden="true" tabindex="-1"></a>  $$ </span>
<span id="cb33-1154"><a href="#cb33-1154" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1155"><a href="#cb33-1155" aria-hidden="true" tabindex="-1"></a>  with</span>
<span id="cb33-1156"><a href="#cb33-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1157"><a href="#cb33-1157" aria-hidden="true" tabindex="-1"></a>  $$\begin{aligned}</span>
<span id="cb33-1158"><a href="#cb33-1158" aria-hidden="true" tabindex="-1"></a>  m_t &amp;= a_t + R_t F_t q_t^{-1} (y_t - f_t), <span class="sc">\\</span></span>
<span id="cb33-1159"><a href="#cb33-1159" aria-hidden="true" tabindex="-1"></a>  C_t &amp;= R_t - R_t F_t q_t^{-1} F_t' R_t.</span>
<span id="cb33-1160"><a href="#cb33-1160" aria-hidden="true" tabindex="-1"></a>  \end{aligned}</span>
<span id="cb33-1161"><a href="#cb33-1161" aria-hidden="true" tabindex="-1"></a>  $$ {#eq-posterior-at-time-t}</span>
<span id="cb33-1162"><a href="#cb33-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1163"><a href="#cb33-1163" aria-hidden="true" tabindex="-1"></a>  These can be derived via Normal theory or via the Multivariate Bayes' theorem. The background for both seems to be provided in <span class="co">[</span><span class="ot">@west2013bayesian §17.2.3 p.639</span><span class="co">]</span></span>
<span id="cb33-1164"><a href="#cb33-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1165"><a href="#cb33-1165" aria-hidden="true" tabindex="-1"></a>Now, denoting $e_t = (y_t - f_t)$ and $A_t = R_t F_t q_t^{-1}$, we can rewrite the equations above as:</span>
<span id="cb33-1166"><a href="#cb33-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1167"><a href="#cb33-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1168"><a href="#cb33-1168" aria-hidden="true" tabindex="-1"></a>It follows that </span>
<span id="cb33-1169"><a href="#cb33-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1170"><a href="#cb33-1170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1171"><a href="#cb33-1171" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}Y <span class="sc">\\</span> \theta\end{pmatrix} \sim \mathcal{N}</span>
<span id="cb33-1172"><a href="#cb33-1172" aria-hidden="true" tabindex="-1"></a>\left(</span>
<span id="cb33-1173"><a href="#cb33-1173" aria-hidden="true" tabindex="-1"></a>  \begin{pmatrix}F'a <span class="sc">\\</span> a \end{pmatrix},</span>
<span id="cb33-1174"><a href="#cb33-1174" aria-hidden="true" tabindex="-1"></a>  \begin{pmatrix} F'RF + V &amp; F'R <span class="sc">\\</span> RF &amp; R \end{pmatrix}</span>
<span id="cb33-1175"><a href="#cb33-1175" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb33-1176"><a href="#cb33-1176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1177"><a href="#cb33-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1178"><a href="#cb33-1178" aria-hidden="true" tabindex="-1"></a>Therefore, identifying $Y$ with $X_1$ and $\theta$ with $X_2$ in the partition of $X$ in 17.2.2, we have</span>
<span id="cb33-1179"><a href="#cb33-1179" aria-hidden="true" tabindex="-1"></a>RF R</span>
<span id="cb33-1180"><a href="#cb33-1180" aria-hidden="true" tabindex="-1"></a>)]</span>
<span id="cb33-1181"><a href="#cb33-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1182"><a href="#cb33-1182" aria-hidden="true" tabindex="-1"></a>Therefore, identifying Y with X1 and θ with X2 in the partition of X in 17.2.2, we have</span>
<span id="cb33-1183"><a href="#cb33-1183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1184"><a href="#cb33-1184" aria-hidden="true" tabindex="-1"></a>Y \sim \mathcal{N}<span class="co">[</span><span class="ot">F'a, F'RF + V</span><span class="co">]</span></span>
<span id="cb33-1185"><a href="#cb33-1185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1186"><a href="#cb33-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1187"><a href="#cb33-1187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1188"><a href="#cb33-1188" aria-hidden="true" tabindex="-1"></a>(\theta \mid Y) \sim \mathcal{N}<span class="co">[</span><span class="ot">m, C</span><span class="co">]</span>,</span>
<span id="cb33-1189"><a href="#cb33-1189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1190"><a href="#cb33-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1191"><a href="#cb33-1191" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb33-1192"><a href="#cb33-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1193"><a href="#cb33-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1194"><a href="#cb33-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1195"><a href="#cb33-1195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1196"><a href="#cb33-1196" aria-hidden="true" tabindex="-1"></a>m = a + RF<span class="co">[</span><span class="ot">F′RF + V</span><span class="co">]</span>−1<span class="co">[</span><span class="ot">Y − F′a</span><span class="co">]</span></span>
<span id="cb33-1197"><a href="#cb33-1197" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1198"><a href="#cb33-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1199"><a href="#cb33-1199" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb33-1200"><a href="#cb33-1200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1201"><a href="#cb33-1201" aria-hidden="true" tabindex="-1"></a>C = R − RF<span class="co">[</span><span class="ot">F′RF + V</span><span class="co">]</span>−1F′R.</span>
<span id="cb33-1202"><a href="#cb33-1202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1203"><a href="#cb33-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1204"><a href="#cb33-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1205"><a href="#cb33-1205" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1206"><a href="#cb33-1206" aria-hidden="true" tabindex="-1"></a>  \begin{aligned}</span>
<span id="cb33-1207"><a href="#cb33-1207" aria-hidden="true" tabindex="-1"></a>  \theta \mid \mathcal{D}_t &amp;\sim \mathcal{N}(m_t,C_t)<span class="sc">\\</span></span>
<span id="cb33-1208"><a href="#cb33-1208" aria-hidden="true" tabindex="-1"></a>  m_t &amp;\doteq a_t + A_t e_t, <span class="sc">\\</span></span>
<span id="cb33-1209"><a href="#cb33-1209" aria-hidden="true" tabindex="-1"></a>  C_t &amp;\doteq R_t - A_t q_t A_t'</span>
<span id="cb33-1210"><a href="#cb33-1210" aria-hidden="true" tabindex="-1"></a>  \end{aligned}</span>
<span id="cb33-1211"><a href="#cb33-1211" aria-hidden="true" tabindex="-1"></a>$$ {#eq-posterior-distribution}</span>
<span id="cb33-1212"><a href="#cb33-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1213"><a href="#cb33-1213" aria-hidden="true" tabindex="-1"></a>@eq-derivation-for-a-t-and-R-t , @eq-derivation-for-f-t-and-q-t and @eq-posterior-distribution are often referred to as the <span class="co">[</span><span class="ot">Kalman filtering</span><span class="co">](https://en.wikipedia.org/wiki/Kalman_filter)</span> equations. </span>
<span id="cb33-1214"><a href="#cb33-1214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1215"><a href="#cb33-1215" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-1216"><a href="#cb33-1216" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb33-1217"><a href="#cb33-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1218"><a href="#cb33-1218" aria-hidden="true" tabindex="-1"></a>{{&lt; include _C4-L03-T05.qmd &gt;}}</span>
<span id="cb33-1219"><a href="#cb33-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1220"><a href="#cb33-1220" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-1221"><a href="#cb33-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1222"><a href="#cb33-1222" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary of filtering distributions (Reading)</span></span>
<span id="cb33-1223"><a href="#cb33-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1224"><a href="#cb33-1224" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Inference in NDLM: Known Variances</span></span>
<span id="cb33-1225"><a href="#cb33-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1226"><a href="#cb33-1226" aria-hidden="true" tabindex="-1"></a>Consider an NDLM given by:</span>
<span id="cb33-1227"><a href="#cb33-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1228"><a href="#cb33-1228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1229"><a href="#cb33-1229" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb33-1230"><a href="#cb33-1230" aria-hidden="true" tabindex="-1"></a>y_t &amp;= F_t' \theta_t + \nu_t, \quad \nu_t \sim  \mathcal{N}(0, v_t), <span class="sc">\\</span></span>
<span id="cb33-1231"><a href="#cb33-1231" aria-hidden="true" tabindex="-1"></a>\theta_t &amp;= G_t \theta_{t-1} + \omega_t, \quad \omega_t \sim  \mathcal{N}(0, W_t), </span>
<span id="cb33-1232"><a href="#cb33-1232" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-1233"><a href="#cb33-1233" aria-hidden="true" tabindex="-1"></a>$$ {#eq-generic-NDLM}</span>
<span id="cb33-1234"><a href="#cb33-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1235"><a href="#cb33-1235" aria-hidden="true" tabindex="-1"></a>with $F_t$, $G_t$, $v_t$, and $W_t$ known. We also assume a prior distribution of the form $(\theta_0 \mid \mathcal{D}_0) \sim  \mathcal{N}(m_0, C_0)$, with $m_0$, $C_0$ known.</span>
<span id="cb33-1236"><a href="#cb33-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1237"><a href="#cb33-1237" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Filtering</span></span>
<span id="cb33-1238"><a href="#cb33-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1239"><a href="#cb33-1239" aria-hidden="true" tabindex="-1"></a>We are interested in finding $\mathbb{P}r(\theta_t \mid \mathcal{D}_t)$ for all $t$. Assume that the posterior at $t-1$ is such that:</span>
<span id="cb33-1240"><a href="#cb33-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1241"><a href="#cb33-1241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1242"><a href="#cb33-1242" aria-hidden="true" tabindex="-1"></a>(\theta_{t-1} \mid \mathcal{D}_{t-1}) \sim  \mathcal{N}(m_{t-1}, C_{t-1}).</span>
<span id="cb33-1243"><a href="#cb33-1243" aria-hidden="true" tabindex="-1"></a>$$ {#eq-filtering}</span>
<span id="cb33-1244"><a href="#cb33-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1245"><a href="#cb33-1245" aria-hidden="true" tabindex="-1"></a>Then, we can obtain the following:</span>
<span id="cb33-1246"><a href="#cb33-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1247"><a href="#cb33-1247" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Prior at Time $t$</span>
<span id="cb33-1248"><a href="#cb33-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1249"><a href="#cb33-1249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1250"><a href="#cb33-1250" aria-hidden="true" tabindex="-1"></a>(\theta_t \mid \mathcal{D}_{t-1}) \sim  \mathcal{N}(a_t, R_t),</span>
<span id="cb33-1251"><a href="#cb33-1251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1252"><a href="#cb33-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1253"><a href="#cb33-1253" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb33-1254"><a href="#cb33-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1255"><a href="#cb33-1255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1256"><a href="#cb33-1256" aria-hidden="true" tabindex="-1"></a>a_t = G_t m_{t-1} \qquad R_t = G_t C_{t-1} G_t' + W_t.</span>
<span id="cb33-1257"><a href="#cb33-1257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1258"><a href="#cb33-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1259"><a href="#cb33-1259" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>One-Step Forecast</span>
<span id="cb33-1260"><a href="#cb33-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1261"><a href="#cb33-1261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1262"><a href="#cb33-1262" aria-hidden="true" tabindex="-1"></a>(y_t \mid D_{t-1}) \sim  \mathcal{N}(f_t, q_t),</span>
<span id="cb33-1263"><a href="#cb33-1263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1264"><a href="#cb33-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1265"><a href="#cb33-1265" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb33-1266"><a href="#cb33-1266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1267"><a href="#cb33-1267" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1268"><a href="#cb33-1268" aria-hidden="true" tabindex="-1"></a>f_t = F_t' a_t, \quad q_t = F_t' R_t F_t + v_t.</span>
<span id="cb33-1269"><a href="#cb33-1269" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1270"><a href="#cb33-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1271"><a href="#cb33-1271" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Posterior at Time $t: (\theta_t \mid \mathcal{D}_t) \sim  \mathcal{N}(m_t, C_t)$ with</span>
<span id="cb33-1272"><a href="#cb33-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1273"><a href="#cb33-1273" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb33-1274"><a href="#cb33-1274" aria-hidden="true" tabindex="-1"></a>m_t &amp;= a_t + R_t F_t q_t^{-1} (y_t - f_t), <span class="sc">\\</span></span>
<span id="cb33-1275"><a href="#cb33-1275" aria-hidden="true" tabindex="-1"></a>C_t &amp;= R_t - R_t F_t q_t^{-1} F_t' R_t.</span>
<span id="cb33-1276"><a href="#cb33-1276" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-1277"><a href="#cb33-1277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1278"><a href="#cb33-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1279"><a href="#cb33-1279" aria-hidden="true" tabindex="-1"></a>Now, denoting $e_t = (y_t - f_t)$ and $A_t = R_t F_t q_t^{-1}$, we can rewrite the equations above as:</span>
<span id="cb33-1280"><a href="#cb33-1280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1281"><a href="#cb33-1281" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb33-1282"><a href="#cb33-1282" aria-hidden="true" tabindex="-1"></a>m_t &amp;= a_t + A_t e_t, <span class="sc">\\</span></span>
<span id="cb33-1283"><a href="#cb33-1283" aria-hidden="true" tabindex="-1"></a>C_t &amp;= R_t - A_t q_t A_t'</span>
<span id="cb33-1284"><a href="#cb33-1284" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-1285"><a href="#cb33-1285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1286"><a href="#cb33-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1287"><a href="#cb33-1287" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rcode Filtering in the NDLM: Example (Reading)</span></span>
<span id="cb33-1288"><a href="#cb33-1288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1291"><a href="#cb33-1291" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb33-1292"><a href="#cb33-1292" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lst-filtering-in-the-NDLM</span></span>
<span id="cb33-1293"><a href="#cb33-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1294"><a href="#cb33-1294" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb33-1295"><a href="#cb33-1295" aria-hidden="true" tabindex="-1"></a><span class="do">##### Univariate DLM: Known, constant variances</span></span>
<span id="cb33-1296"><a href="#cb33-1296" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb33-1297"><a href="#cb33-1297" aria-hidden="true" tabindex="-1"></a>set_up_dlm_matrices <span class="ot">&lt;-</span> <span class="cf">function</span>(FF, GG, VV, WW){</span>
<span id="cb33-1298"><a href="#cb33-1298" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">FF=</span>FF, <span class="at">GG=</span>GG, <span class="at">VV=</span>VV, <span class="at">WW=</span>WW))</span>
<span id="cb33-1299"><a href="#cb33-1299" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1300"><a href="#cb33-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1301"><a href="#cb33-1301" aria-hidden="true" tabindex="-1"></a>set_up_initial_states <span class="ot">&lt;-</span> <span class="cf">function</span>(m0, C0){</span>
<span id="cb33-1302"><a href="#cb33-1302" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">m0=</span>m0, <span class="at">C0=</span>C0))</span>
<span id="cb33-1303"><a href="#cb33-1303" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1304"><a href="#cb33-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1305"><a href="#cb33-1305" aria-hidden="true" tabindex="-1"></a><span class="do">### forward update equations </span><span class="al">###</span></span>
<span id="cb33-1306"><a href="#cb33-1306" aria-hidden="true" tabindex="-1"></a>forward_filter <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, initial_states){</span>
<span id="cb33-1307"><a href="#cb33-1307" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve dataset</span></span>
<span id="cb33-1308"><a href="#cb33-1308" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb33-1309"><a href="#cb33-1309" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t)</span>
<span id="cb33-1310"><a href="#cb33-1310" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1311"><a href="#cb33-1311" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve a set of quadruples </span></span>
<span id="cb33-1312"><a href="#cb33-1312" aria-hidden="true" tabindex="-1"></a>  <span class="co"># FF, GG, VV, WW are scalar</span></span>
<span id="cb33-1313"><a href="#cb33-1313" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF  </span>
<span id="cb33-1314"><a href="#cb33-1314" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb33-1315"><a href="#cb33-1315" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb33-1316"><a href="#cb33-1316" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb33-1317"><a href="#cb33-1317" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1318"><a href="#cb33-1318" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve initial states</span></span>
<span id="cb33-1319"><a href="#cb33-1319" aria-hidden="true" tabindex="-1"></a>  m0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>m0</span>
<span id="cb33-1320"><a href="#cb33-1320" aria-hidden="true" tabindex="-1"></a>  C0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>C0</span>
<span id="cb33-1321"><a href="#cb33-1321" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1322"><a href="#cb33-1322" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for results</span></span>
<span id="cb33-1323"><a href="#cb33-1323" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(GG)[<span class="dv">1</span>]</span>
<span id="cb33-1324"><a href="#cb33-1324" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb33-1325"><a href="#cb33-1325" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb33-1326"><a href="#cb33-1326" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1327"><a href="#cb33-1327" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1328"><a href="#cb33-1328" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb33-1329"><a href="#cb33-1329" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb33-1330"><a href="#cb33-1330" aria-hidden="true" tabindex="-1"></a>  et <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1331"><a href="#cb33-1331" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1332"><a href="#cb33-1332" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T){</span>
<span id="cb33-1333"><a href="#cb33-1333" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of priors at t</span></span>
<span id="cb33-1334"><a href="#cb33-1334" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb33-1335"><a href="#cb33-1335" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(m0)</span>
<span id="cb33-1336"><a href="#cb33-1336" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> C0 <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-1337"><a href="#cb33-1337" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[, , i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[, , i])</span>
<span id="cb33-1338"><a href="#cb33-1338" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb33-1339"><a href="#cb33-1339" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1340"><a href="#cb33-1340" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-1341"><a href="#cb33-1341" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[, , i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[, , i])</span>
<span id="cb33-1342"><a href="#cb33-1342" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-1343"><a href="#cb33-1343" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-1344"><a href="#cb33-1344" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of one-step forecast:</span></span>
<span id="cb33-1345"><a href="#cb33-1345" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> (at[i, ]) </span>
<span id="cb33-1346"><a href="#cb33-1346" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb33-1347"><a href="#cb33-1347" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-1348"><a href="#cb33-1348" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of posterior at t:</span></span>
<span id="cb33-1349"><a href="#cb33-1349" aria-hidden="true" tabindex="-1"></a>    At <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">/</span> Qt[i]</span>
<span id="cb33-1350"><a href="#cb33-1350" aria-hidden="true" tabindex="-1"></a>    et[i] <span class="ot">&lt;-</span> y_t[i] <span class="sc">-</span> ft[i]</span>
<span id="cb33-1351"><a href="#cb33-1351" aria-hidden="true" tabindex="-1"></a>    mt[i, ] <span class="ot">&lt;-</span> at[i, ] <span class="sc">+</span> <span class="fu">t</span>(At) <span class="sc">*</span> et[i]</span>
<span id="cb33-1352"><a href="#cb33-1352" aria-hidden="true" tabindex="-1"></a>    Ct[, , i] <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">-</span> Qt[i] <span class="sc">*</span> At <span class="sc">%*%</span> <span class="fu">t</span>(At)</span>
<span id="cb33-1353"><a href="#cb33-1353" aria-hidden="true" tabindex="-1"></a>    Ct[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Ct[, , i]<span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Ct[, , i])</span>
<span id="cb33-1354"><a href="#cb33-1354" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-1355"><a href="#cb33-1355" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forward filtering is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb33-1356"><a href="#cb33-1356" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mt =</span> mt, <span class="at">Ct =</span> Ct, <span class="at">at =</span> at, <span class="at">Rt =</span> </span>
<span id="cb33-1357"><a href="#cb33-1357" aria-hidden="true" tabindex="-1"></a>                Rt, <span class="at">ft =</span> ft, <span class="at">Qt =</span> Qt))</span>
<span id="cb33-1358"><a href="#cb33-1358" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1359"><a href="#cb33-1359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1360"><a href="#cb33-1360" aria-hidden="true" tabindex="-1"></a>forecast_function <span class="ot">&lt;-</span> <span class="cf">function</span>(posterior_states, k, matrices){</span>
<span id="cb33-1361"><a href="#cb33-1361" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1362"><a href="#cb33-1362" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb33-1363"><a href="#cb33-1363" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb33-1364"><a href="#cb33-1364" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb33-1365"><a href="#cb33-1365" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb33-1366"><a href="#cb33-1366" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb33-1367"><a href="#cb33-1367" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb33-1368"><a href="#cb33-1368" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb33-1369"><a href="#cb33-1369" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1370"><a href="#cb33-1370" aria-hidden="true" tabindex="-1"></a>  <span class="do">## set up matrices</span></span>
<span id="cb33-1371"><a href="#cb33-1371" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>] <span class="co"># time points</span></span>
<span id="cb33-1372"><a href="#cb33-1372" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>] <span class="co"># dimension of state-space parameter vector</span></span>
<span id="cb33-1373"><a href="#cb33-1373" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1374"><a href="#cb33-1374" aria-hidden="true" tabindex="-1"></a>  <span class="do">## placeholder for results</span></span>
<span id="cb33-1375"><a href="#cb33-1375" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> k, <span class="at">ncol =</span> d)</span>
<span id="cb33-1376"><a href="#cb33-1376" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, k))</span>
<span id="cb33-1377"><a href="#cb33-1377" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb33-1378"><a href="#cb33-1378" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb33-1379"><a href="#cb33-1379" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1380"><a href="#cb33-1380" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1381"><a href="#cb33-1381" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb33-1382"><a href="#cb33-1382" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of state distribution</span></span>
<span id="cb33-1383"><a href="#cb33-1383" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb33-1384"><a href="#cb33-1384" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[T, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1385"><a href="#cb33-1385" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , T] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-1386"><a href="#cb33-1386" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[, , i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[, , i])</span>
<span id="cb33-1387"><a href="#cb33-1387" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb33-1388"><a href="#cb33-1388" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(at[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1389"><a href="#cb33-1389" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Rt[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-1390"><a href="#cb33-1390" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[, , i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[, , i])</span>
<span id="cb33-1391"><a href="#cb33-1391" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-1392"><a href="#cb33-1392" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-1393"><a href="#cb33-1393" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of forecast distribution</span></span>
<span id="cb33-1394"><a href="#cb33-1394" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(at[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1395"><a href="#cb33-1395" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb33-1396"><a href="#cb33-1396" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-1397"><a href="#cb33-1397" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forecasting is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb33-1398"><a href="#cb33-1398" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">at=</span>at, <span class="at">Rt=</span>Rt, <span class="at">ft=</span>ft, <span class="at">Qt=</span>Qt))</span>
<span id="cb33-1399"><a href="#cb33-1399" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1400"><a href="#cb33-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1401"><a href="#cb33-1401" aria-hidden="true" tabindex="-1"></a><span class="do">## obtain 95% credible interval</span></span>
<span id="cb33-1402"><a href="#cb33-1402" aria-hidden="true" tabindex="-1"></a>get_credible_interval <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma2, </span>
<span id="cb33-1403"><a href="#cb33-1403" aria-hidden="true" tabindex="-1"></a>                          <span class="at">quantile =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)){</span>
<span id="cb33-1404"><a href="#cb33-1404" aria-hidden="true" tabindex="-1"></a>  z_quantile <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(quantile)</span>
<span id="cb33-1405"><a href="#cb33-1405" aria-hidden="true" tabindex="-1"></a>  bound <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">length</span>(mu), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb33-1406"><a href="#cb33-1406" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">1</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> </span>
<span id="cb33-1407"><a href="#cb33-1407" aria-hidden="true" tabindex="-1"></a>    z_quantile[<span class="dv">1</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># lower bound</span></span>
<span id="cb33-1408"><a href="#cb33-1408" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">2</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> </span>
<span id="cb33-1409"><a href="#cb33-1409" aria-hidden="true" tabindex="-1"></a>    z_quantile[<span class="dv">2</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># upper bound</span></span>
<span id="cb33-1410"><a href="#cb33-1410" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(bound)</span>
<span id="cb33-1411"><a href="#cb33-1411" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1412"><a href="#cb33-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1413"><a href="#cb33-1413" aria-hidden="true" tabindex="-1"></a><span class="do">####################### Example: Lake Huron Data ######################</span></span>
<span id="cb33-1414"><a href="#cb33-1414" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(LakeHuron,<span class="at">main=</span><span class="st">"Lake Huron Data"</span>,</span>
<span id="cb33-1415"><a href="#cb33-1415" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">"level in feet"</span>) <span class="co"># Total of 98 observations </span></span>
<span id="cb33-1416"><a href="#cb33-1416" aria-hidden="true" tabindex="-1"></a>k<span class="ot">=</span><span class="dv">4</span></span>
<span id="cb33-1417"><a href="#cb33-1417" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(LakeHuron)<span class="sc">-</span>k <span class="co"># We take the first 94 observations </span></span>
<span id="cb33-1418"><a href="#cb33-1418" aria-hidden="true" tabindex="-1"></a>                      <span class="co"># only as our data</span></span>
<span id="cb33-1419"><a href="#cb33-1419" aria-hidden="true" tabindex="-1"></a>ts_data<span class="ot">=</span>LakeHuron[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb33-1420"><a href="#cb33-1420" aria-hidden="true" tabindex="-1"></a>ts_validation_data <span class="ot">&lt;-</span> LakeHuron[(T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">98</span>]</span>
<span id="cb33-1421"><a href="#cb33-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1422"><a href="#cb33-1422" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y_t =</span> ts_data)</span>
<span id="cb33-1423"><a href="#cb33-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1424"><a href="#cb33-1424" aria-hidden="true" tabindex="-1"></a><span class="co"># First order polynomial model </span></span>
<span id="cb33-1425"><a href="#cb33-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1426"><a href="#cb33-1426" aria-hidden="true" tabindex="-1"></a><span class="do">## set up the DLM matrices </span></span>
<span id="cb33-1427"><a href="#cb33-1427" aria-hidden="true" tabindex="-1"></a>FF <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1428"><a href="#cb33-1428" aria-hidden="true" tabindex="-1"></a>GG <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1429"><a href="#cb33-1429" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1430"><a href="#cb33-1430" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1431"><a href="#cb33-1431" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">570</span>)</span>
<span id="cb33-1432"><a href="#cb33-1432" aria-hidden="true" tabindex="-1"></a>C0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fl">1e4</span>)</span>
<span id="cb33-1433"><a href="#cb33-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1434"><a href="#cb33-1434" aria-hidden="true" tabindex="-1"></a><span class="do">## wrap up all matrices and initial values</span></span>
<span id="cb33-1435"><a href="#cb33-1435" aria-hidden="true" tabindex="-1"></a>matrices <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF, GG, VV, WW)</span>
<span id="cb33-1436"><a href="#cb33-1436" aria-hidden="true" tabindex="-1"></a>initial_states <span class="ot">&lt;-</span> <span class="fu">set_up_initial_states</span>(m0, C0)</span>
<span id="cb33-1437"><a href="#cb33-1437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1438"><a href="#cb33-1438" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering</span></span>
<span id="cb33-1439"><a href="#cb33-1439" aria-hidden="true" tabindex="-1"></a>results_filtered <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices, </span>
<span id="cb33-1440"><a href="#cb33-1440" aria-hidden="true" tabindex="-1"></a>                                   initial_states)</span>
<span id="cb33-1441"><a href="#cb33-1441" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(results_filtered)</span>
<span id="cb33-1442"><a href="#cb33-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1443"><a href="#cb33-1443" aria-hidden="true" tabindex="-1"></a>ci_filtered <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_filtered<span class="sc">$</span>mt, </span>
<span id="cb33-1444"><a href="#cb33-1444" aria-hidden="true" tabindex="-1"></a>                                     results_filtered<span class="sc">$</span>Ct)</span>
<span id="cb33-1445"><a href="#cb33-1445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1446"><a href="#cb33-1446" aria-hidden="true" tabindex="-1"></a><span class="do">## forecasting </span></span>
<span id="cb33-1447"><a href="#cb33-1447" aria-hidden="true" tabindex="-1"></a>results_forecast <span class="ot">&lt;-</span> <span class="fu">forecast_function</span>(results_filtered,k, </span>
<span id="cb33-1448"><a href="#cb33-1448" aria-hidden="true" tabindex="-1"></a>                                      matrices)</span>
<span id="cb33-1449"><a href="#cb33-1449" aria-hidden="true" tabindex="-1"></a>ci_forecast <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_forecast<span class="sc">$</span>ft, </span>
<span id="cb33-1450"><a href="#cb33-1450" aria-hidden="true" tabindex="-1"></a>                                     results_forecast<span class="sc">$</span>Qt)</span>
<span id="cb33-1451"><a href="#cb33-1451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1452"><a href="#cb33-1452" aria-hidden="true" tabindex="-1"></a>index<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1875</span>, <span class="dv">1972</span>, <span class="at">length.out =</span> <span class="fu">length</span>(LakeHuron))</span>
<span id="cb33-1453"><a href="#cb33-1453" aria-hidden="true" tabindex="-1"></a>index_filt<span class="ot">=</span>index[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb33-1454"><a href="#cb33-1454" aria-hidden="true" tabindex="-1"></a>index_forecast<span class="ot">=</span>index[(T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">98</span>]</span>
<span id="cb33-1455"><a href="#cb33-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1456"><a href="#cb33-1456" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index, LakeHuron, <span class="at">ylab =</span> <span class="st">"level"</span>, </span>
<span id="cb33-1457"><a href="#cb33-1457" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Lake Huron Level"</span>,<span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb33-1458"><a href="#cb33-1458" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">574</span>,<span class="dv">584</span>))</span>
<span id="cb33-1459"><a href="#cb33-1459" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb33-1460"><a href="#cb33-1460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1461"><a href="#cb33-1461" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_filtered<span class="sc">$</span>mt, <span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb33-1462"><a href="#cb33-1462" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-1463"><a href="#cb33-1463" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered[, <span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1464"><a href="#cb33-1464" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1465"><a href="#cb33-1465" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered[, <span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'red'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1466"><a href="#cb33-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1467"><a href="#cb33-1467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1468"><a href="#cb33-1468" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, results_forecast<span class="sc">$</span>ft, <span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb33-1469"><a href="#cb33-1469" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-1470"><a href="#cb33-1470" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, ci_forecast[, <span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb33-1471"><a href="#cb33-1471" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1472"><a href="#cb33-1472" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, ci_forecast[, <span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb33-1473"><a href="#cb33-1473" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1474"><a href="#cb33-1474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1475"><a href="#cb33-1475" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">'bottomleft'</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"filtered"</span>,<span class="st">"forecast"</span>),</span>
<span id="cb33-1476"><a href="#cb33-1476" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>), <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb33-1477"><a href="#cb33-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1478"><a href="#cb33-1478" aria-hidden="true" tabindex="-1"></a><span class="co">#Now consider a 100 times smaller signal to noise ratio </span></span>
<span id="cb33-1479"><a href="#cb33-1479" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1480"><a href="#cb33-1480" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fl">0.01</span>)</span>
<span id="cb33-1481"><a href="#cb33-1481" aria-hidden="true" tabindex="-1"></a>matrices_2 <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF,GG, VV, WW)</span>
<span id="cb33-1482"><a href="#cb33-1482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1483"><a href="#cb33-1483" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering</span></span>
<span id="cb33-1484"><a href="#cb33-1484" aria-hidden="true" tabindex="-1"></a>results_filtered_2 <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices_2, </span>
<span id="cb33-1485"><a href="#cb33-1485" aria-hidden="true" tabindex="-1"></a>                                     initial_states)</span>
<span id="cb33-1486"><a href="#cb33-1486" aria-hidden="true" tabindex="-1"></a>ci_filtered_2 <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_filtered_2<span class="sc">$</span>mt, </span>
<span id="cb33-1487"><a href="#cb33-1487" aria-hidden="true" tabindex="-1"></a>                                       results_filtered_2<span class="sc">$</span>Ct)</span>
<span id="cb33-1488"><a href="#cb33-1488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1489"><a href="#cb33-1489" aria-hidden="true" tabindex="-1"></a>results_forecast_2 <span class="ot">&lt;-</span> <span class="fu">forecast_function</span>(results_filtered_2, </span>
<span id="cb33-1490"><a href="#cb33-1490" aria-hidden="true" tabindex="-1"></a>                             <span class="fu">length</span>(ts_validation_data), </span>
<span id="cb33-1491"><a href="#cb33-1491" aria-hidden="true" tabindex="-1"></a>                             matrices_2)</span>
<span id="cb33-1492"><a href="#cb33-1492" aria-hidden="true" tabindex="-1"></a>ci_forecast_2 <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_forecast_2<span class="sc">$</span>ft, </span>
<span id="cb33-1493"><a href="#cb33-1493" aria-hidden="true" tabindex="-1"></a>                                       results_forecast_2<span class="sc">$</span>Qt)</span>
<span id="cb33-1494"><a href="#cb33-1494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1495"><a href="#cb33-1495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1496"><a href="#cb33-1496" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index, LakeHuron, <span class="at">ylab =</span> <span class="st">"level"</span>, </span>
<span id="cb33-1497"><a href="#cb33-1497" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Lake Huron Level"</span>,<span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb33-1498"><a href="#cb33-1498" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">574</span>,<span class="dv">584</span>))</span>
<span id="cb33-1499"><a href="#cb33-1499" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb33-1500"><a href="#cb33-1500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1501"><a href="#cb33-1501" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_filtered_2<span class="sc">$</span>mt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1502"><a href="#cb33-1502" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'magenta'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-1503"><a href="#cb33-1503" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered_2[, <span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1504"><a href="#cb33-1504" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'magenta'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1505"><a href="#cb33-1505" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered_2[, <span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1506"><a href="#cb33-1506" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'magenta'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1507"><a href="#cb33-1507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1508"><a href="#cb33-1508" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, results_forecast_2<span class="sc">$</span>ft, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1509"><a href="#cb33-1509" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-1510"><a href="#cb33-1510" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, ci_forecast_2[, <span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1511"><a href="#cb33-1511" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1512"><a href="#cb33-1512" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_forecast, ci_forecast_2[, <span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1513"><a href="#cb33-1513" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'green'</span>, <span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1514"><a href="#cb33-1514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1515"><a href="#cb33-1515" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">'bottomleft'</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"filtered"</span>,<span class="st">"forecast"</span>),</span>
<span id="cb33-1516"><a href="#cb33-1516" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"magenta"</span>, <span class="st">"green"</span>), <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb33-1517"><a href="#cb33-1517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1518"><a href="#cb33-1518" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index_filt,results_filtered<span class="sc">$</span>mt,<span class="at">type=</span><span class="st">'l'</span>,<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb33-1519"><a href="#cb33-1519" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">574</span>,<span class="dv">584</span>),<span class="at">ylab=</span><span class="st">"level"</span>)</span>
<span id="cb33-1520"><a href="#cb33-1520" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt,results_filtered_2<span class="sc">$</span>mt,<span class="at">col=</span><span class="st">'magenta'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-1521"><a href="#cb33-1521" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb33-1522"><a href="#cb33-1522" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index,LakeHuron,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1523"><a href="#cb33-1523" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-1524"><a href="#cb33-1524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1525"><a href="#cb33-1525" aria-hidden="true" tabindex="-1"></a><span class="fu">## Smoothing and forecasting (Video)</span></span>
<span id="cb33-1526"><a href="#cb33-1526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1527"><a href="#cb33-1527" aria-hidden="true" tabindex="-1"></a><span class="al">![Smoothing](images/m3_0051.png)</span>{.column-margin group="slides" width="200px"}</span>
<span id="cb33-1528"><a href="#cb33-1528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1529"><a href="#cb33-1529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1530"><a href="#cb33-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1531"><a href="#cb33-1531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1532"><a href="#cb33-1532" aria-hidden="true" tabindex="-1"></a><span class="al">![Forecasting](images/m3_0052.png)</span>{.column-margin group="slides" width="200px"}</span>
<span id="cb33-1533"><a href="#cb33-1533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1534"><a href="#cb33-1534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1535"><a href="#cb33-1535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1536"><a href="#cb33-1536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1537"><a href="#cb33-1537" aria-hidden="true" tabindex="-1"></a>We now discuss the **smoothing equations** for the case of the NDLM, where we are assuming that the variance at the *observation level* $\nu_t$ and the covariance matrix at the *system level* $\mathbf{W}_t$ are both known.</span>
<span id="cb33-1538"><a href="#cb33-1538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1539"><a href="#cb33-1539" aria-hidden="true" tabindex="-1"></a>$$ \begin{aligned}</span>
<span id="cb33-1540"><a href="#cb33-1540" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \mathbf{F}_t' \mathbf{\theta}_t + \nu_t, &amp;\nu_t &amp;\sim \mathcal{N} (0, v_t), &amp; \text{(observation)} <span class="sc">\\</span></span>
<span id="cb33-1541"><a href="#cb33-1541" aria-hidden="true" tabindex="-1"></a>\mathbf{\theta}_t &amp; = \mathbf{G}_t \mathbf{\theta}_{t-1} + \mathbf{\omega}_t, &amp;\mathbf{\omega}_t &amp; \sim \mathcal{N} (0, \mathbf{W}_t), &amp; \text{(evolution)} <span class="sc">\\</span></span>
<span id="cb33-1542"><a href="#cb33-1542" aria-hidden="true" tabindex="-1"></a>&amp;<span class="sc">\{</span> \mathbf{F}_t, \mathbf{G}_t, v_t, \mathbf{W}_t <span class="sc">\}</span>  &amp;(\mathbf{\omega}_0 \mid \mathcal{D}_0) &amp; \sim \mathcal{N}(\mathbf{m}_0, \mathbf{C}_0) &amp; \text{(prior)}</span>
<span id="cb33-1543"><a href="#cb33-1543" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-1544"><a href="#cb33-1544" aria-hidden="true" tabindex="-1"></a>$$ {#eq-inference-NDLM}</span>
<span id="cb33-1545"><a href="#cb33-1545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1546"><a href="#cb33-1546" aria-hidden="true" tabindex="-1"></a>with $F_t$, $G_t$, $v_t$, $W_t$, $m_0$ and $C_0$ known.</span>
<span id="cb33-1547"><a href="#cb33-1547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1548"><a href="#cb33-1548" aria-hidden="true" tabindex="-1"></a>We have discussed the filtering equations, i.e. the process for obtaining the distributions of $\theta_t \mid \mathcal{D}_t$, as we collect observations over time, called filtering. </span>
<span id="cb33-1549"><a href="#cb33-1549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1550"><a href="#cb33-1550" aria-hidden="true" tabindex="-1"></a>We do this by updating the distribution of $\theta_t$ given the data we have collected step by step, as we move forward in time - updating the from the prior distribution.</span>
<span id="cb33-1551"><a href="#cb33-1551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1552"><a href="#cb33-1552" aria-hidden="true" tabindex="-1"></a>Now we will discuss what happens when we do smoothing, meaning when we revisit the distributions of $\theta_t$, given now that we have received a set of observations.</span>
<span id="cb33-1553"><a href="#cb33-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1554"><a href="#cb33-1554" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Smoothing</span></span>
<span id="cb33-1555"><a href="#cb33-1555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1556"><a href="#cb33-1556" aria-hidden="true" tabindex="-1"></a>For $t &lt; T$, we have that:</span>
<span id="cb33-1557"><a href="#cb33-1557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1558"><a href="#cb33-1558" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1559"><a href="#cb33-1559" aria-hidden="true" tabindex="-1"></a>(\theta_t \mid D_T) \sim  \mathcal{N}(a_T(t - T), R_T(t - T)),</span>
<span id="cb33-1560"><a href="#cb33-1560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1561"><a href="#cb33-1561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1562"><a href="#cb33-1562" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb33-1563"><a href="#cb33-1563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1564"><a href="#cb33-1564" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1565"><a href="#cb33-1565" aria-hidden="true" tabindex="-1"></a>a_T(t - T) = m_t - B_t <span class="co">[</span><span class="ot">a_{t+1} - a_T(t - T + 1)</span><span class="co">]</span>,</span>
<span id="cb33-1566"><a href="#cb33-1566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1567"><a href="#cb33-1567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1568"><a href="#cb33-1568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1569"><a href="#cb33-1569" aria-hidden="true" tabindex="-1"></a>R_T(t - T) = C_t - B_t <span class="co">[</span><span class="ot">R_{t+1} - R_T(t - T + 1)</span><span class="co">]</span> B_t',</span>
<span id="cb33-1570"><a href="#cb33-1570" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1571"><a href="#cb33-1571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1572"><a href="#cb33-1572" aria-hidden="true" tabindex="-1"></a>for $t = (T - 1), (T - 2), \dots, 0$, with $B_t = C_t G_t' R_{t+1}^{-1}$, and $a_T(0) = m_T$, $R_T(0) = C_T$. Here $a_t$, $m_t$, $R_t$, and $C_t$ are obtained using the filtering equations as explained before.</span>
<span id="cb33-1573"><a href="#cb33-1573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1574"><a href="#cb33-1574" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Forecasting</span></span>
<span id="cb33-1575"><a href="#cb33-1575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1576"><a href="#cb33-1576" aria-hidden="true" tabindex="-1"></a>For $h \geq 0$, it is possible to show that:</span>
<span id="cb33-1577"><a href="#cb33-1577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1578"><a href="#cb33-1578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1579"><a href="#cb33-1579" aria-hidden="true" tabindex="-1"></a>(\theta_{t+h} \mid D_t) \sim  \mathcal{N}(a_t(h), R_t(h)),</span>
<span id="cb33-1580"><a href="#cb33-1580" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1581"><a href="#cb33-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1582"><a href="#cb33-1582" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1583"><a href="#cb33-1583" aria-hidden="true" tabindex="-1"></a>(y_{t+h} \mid D_t) \sim  \mathcal{N}(f_t(h), q_t(h)),</span>
<span id="cb33-1584"><a href="#cb33-1584" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1585"><a href="#cb33-1585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1586"><a href="#cb33-1586" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb33-1587"><a href="#cb33-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1588"><a href="#cb33-1588" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1589"><a href="#cb33-1589" aria-hidden="true" tabindex="-1"></a>a_t(h) = G_{t+h} a_t(h - 1),</span>
<span id="cb33-1590"><a href="#cb33-1590" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1591"><a href="#cb33-1591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1592"><a href="#cb33-1592" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1593"><a href="#cb33-1593" aria-hidden="true" tabindex="-1"></a>R_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},</span>
<span id="cb33-1594"><a href="#cb33-1594" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1595"><a href="#cb33-1595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1596"><a href="#cb33-1596" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1597"><a href="#cb33-1597" aria-hidden="true" tabindex="-1"></a>f_t(h) = F_{t+h}' a_t(h),</span>
<span id="cb33-1598"><a href="#cb33-1598" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1599"><a href="#cb33-1599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1600"><a href="#cb33-1600" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1601"><a href="#cb33-1601" aria-hidden="true" tabindex="-1"></a>q_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},</span>
<span id="cb33-1602"><a href="#cb33-1602" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1603"><a href="#cb33-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1604"><a href="#cb33-1604" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb33-1605"><a href="#cb33-1605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1606"><a href="#cb33-1606" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1607"><a href="#cb33-1607" aria-hidden="true" tabindex="-1"></a>a_t(0) = m_t, \quad R_t(0) = C_t.</span>
<span id="cb33-1608"><a href="#cb33-1608" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1609"><a href="#cb33-1609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1610"><a href="#cb33-1610" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-1611"><a href="#cb33-1611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1612"><a href="#cb33-1612" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb33-1613"><a href="#cb33-1613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1614"><a href="#cb33-1614" aria-hidden="true" tabindex="-1"></a>{{&lt; include _C4-L03-T06.qmd &gt;}}</span>
<span id="cb33-1615"><a href="#cb33-1615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1616"><a href="#cb33-1616" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-1617"><a href="#cb33-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1618"><a href="#cb33-1618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1619"><a href="#cb33-1619" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary of the smoothing and forecasting distributions (reading)</span></span>
<span id="cb33-1620"><a href="#cb33-1620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1621"><a href="#cb33-1621" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- start --&gt;</span></span>
<span id="cb33-1622"><a href="#cb33-1622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1623"><a href="#cb33-1623" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Inference in NDLM: Known Variances</span></span>
<span id="cb33-1624"><a href="#cb33-1624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1625"><a href="#cb33-1625" aria-hidden="true" tabindex="-1"></a>Consider the NDLM given by:</span>
<span id="cb33-1626"><a href="#cb33-1626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1627"><a href="#cb33-1627" aria-hidden="true" tabindex="-1"></a>$$ \begin{aligned}</span>
<span id="cb33-1628"><a href="#cb33-1628" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \mathbf{F}_t' \mathbf{\theta}_t + \nu_t, &amp;\nu_t &amp;\sim \mathcal{N} (0, v_t), <span class="sc">\\</span></span>
<span id="cb33-1629"><a href="#cb33-1629" aria-hidden="true" tabindex="-1"></a>\mathbf{\theta}_t &amp;= \mathbf{G}_t \mathbf{\theta}_{t-1} + \mathbf{\omega}_t, &amp;\mathbf{\omega}_t &amp;\sim \mathcal{N} (0, \mathbf{W}_t), <span class="sc">\\</span></span>
<span id="cb33-1630"><a href="#cb33-1630" aria-hidden="true" tabindex="-1"></a>&amp;<span class="sc">\{</span> \mathbf{F}_t, \mathbf{G}_t, v_t, \mathbf{W}_t <span class="sc">\}</span>  &amp;(\mathbf{\omega}_0 \mid \mathcal{D}_0) &amp;\sim  \mathcal{N}(\mathbf{m}_0, \mathbf{C}_0)</span>
<span id="cb33-1631"><a href="#cb33-1631" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb33-1632"><a href="#cb33-1632" aria-hidden="true" tabindex="-1"></a>$$ {#eq-inference-NDLM}</span>
<span id="cb33-1633"><a href="#cb33-1633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1634"><a href="#cb33-1634" aria-hidden="true" tabindex="-1"></a>with $F_t$, $G_t$, $v_t$, and $W_t$ known. </span>
<span id="cb33-1635"><a href="#cb33-1635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1636"><a href="#cb33-1636" aria-hidden="true" tabindex="-1"></a>We also assume a prior distribution of the form </span>
<span id="cb33-1637"><a href="#cb33-1637" aria-hidden="true" tabindex="-1"></a>$(\theta_0 \mid D_0) \sim  \mathcal{N}(m_0, C_0)$, with $m_0$ and $C_0$ known.</span>
<span id="cb33-1638"><a href="#cb33-1638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1639"><a href="#cb33-1639" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Smoothing</span></span>
<span id="cb33-1640"><a href="#cb33-1640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1641"><a href="#cb33-1641" aria-hidden="true" tabindex="-1"></a>For $t &lt; T$, we have that:</span>
<span id="cb33-1642"><a href="#cb33-1642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1643"><a href="#cb33-1643" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1644"><a href="#cb33-1644" aria-hidden="true" tabindex="-1"></a>(\theta_t \mid D_T) \sim  \mathcal{N}(a_T(t - T), R_T(t - T)),</span>
<span id="cb33-1645"><a href="#cb33-1645" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1646"><a href="#cb33-1646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1647"><a href="#cb33-1647" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb33-1648"><a href="#cb33-1648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1649"><a href="#cb33-1649" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1650"><a href="#cb33-1650" aria-hidden="true" tabindex="-1"></a>a_T(t - T) = m_t - B_t <span class="co">[</span><span class="ot">a_{t+1} - a_T(t - T + 1)</span><span class="co">]</span>,</span>
<span id="cb33-1651"><a href="#cb33-1651" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1652"><a href="#cb33-1652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1653"><a href="#cb33-1653" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1654"><a href="#cb33-1654" aria-hidden="true" tabindex="-1"></a>R_T(t - T) = C_t - B_t <span class="co">[</span><span class="ot">R_{t+1} - R_T(t - T + 1)</span><span class="co">]</span> B_t',</span>
<span id="cb33-1655"><a href="#cb33-1655" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1656"><a href="#cb33-1656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1657"><a href="#cb33-1657" aria-hidden="true" tabindex="-1"></a>for $t = (T - 1), (T - 2), \dots, 0$, with $B_t = C_t G_t' R_{t+1}^{-1}$, and $a_T(0) = m_T$, $R_T(0) = C_T$. Here $a_t$, $m_t$, $R_t$, and $C_t$ are obtained using the filtering equations as explained before.</span>
<span id="cb33-1658"><a href="#cb33-1658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1659"><a href="#cb33-1659" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Forecasting</span></span>
<span id="cb33-1660"><a href="#cb33-1660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1661"><a href="#cb33-1661" aria-hidden="true" tabindex="-1"></a>For $h \geq 0$, it is possible to show that:</span>
<span id="cb33-1662"><a href="#cb33-1662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1663"><a href="#cb33-1663" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1664"><a href="#cb33-1664" aria-hidden="true" tabindex="-1"></a>(\theta_{t+h} \mid D_t) \sim  \mathcal{N}(a_t(h), R_t(h)),</span>
<span id="cb33-1665"><a href="#cb33-1665" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1666"><a href="#cb33-1666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1667"><a href="#cb33-1667" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1668"><a href="#cb33-1668" aria-hidden="true" tabindex="-1"></a>(y_{t+h} \mid D_t) \sim  \mathcal{N}(f_t(h), q_t(h)),</span>
<span id="cb33-1669"><a href="#cb33-1669" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1670"><a href="#cb33-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1671"><a href="#cb33-1671" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb33-1672"><a href="#cb33-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1673"><a href="#cb33-1673" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1674"><a href="#cb33-1674" aria-hidden="true" tabindex="-1"></a>a_t(h) = G_{t+h} a_t(h - 1),</span>
<span id="cb33-1675"><a href="#cb33-1675" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1676"><a href="#cb33-1676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1677"><a href="#cb33-1677" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1678"><a href="#cb33-1678" aria-hidden="true" tabindex="-1"></a>R_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},</span>
<span id="cb33-1679"><a href="#cb33-1679" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1680"><a href="#cb33-1680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1681"><a href="#cb33-1681" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1682"><a href="#cb33-1682" aria-hidden="true" tabindex="-1"></a>f_t(h) = F_{t+h}' a_t(h),</span>
<span id="cb33-1683"><a href="#cb33-1683" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1684"><a href="#cb33-1684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1685"><a href="#cb33-1685" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1686"><a href="#cb33-1686" aria-hidden="true" tabindex="-1"></a>q_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},</span>
<span id="cb33-1687"><a href="#cb33-1687" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1688"><a href="#cb33-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1689"><a href="#cb33-1689" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb33-1690"><a href="#cb33-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1691"><a href="#cb33-1691" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1692"><a href="#cb33-1692" aria-hidden="true" tabindex="-1"></a>a_t(0) = m_t, \quad R_t(0) = C_t.</span>
<span id="cb33-1693"><a href="#cb33-1693" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb33-1694"><a href="#cb33-1694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1695"><a href="#cb33-1695" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- end --&gt;</span></span>
<span id="cb33-1696"><a href="#cb33-1696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1697"><a href="#cb33-1697" aria-hidden="true" tabindex="-1"></a><span class="fu">## Smoothing in the NDLM, Example (Video)</span></span>
<span id="cb33-1698"><a href="#cb33-1698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1699"><a href="#cb33-1699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1700"><a href="#cb33-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1701"><a href="#cb33-1701" aria-hidden="true" tabindex="-1"></a><span class="fu">## R-code: Smoothing in the NDLM, Example (Reading)</span></span>
<span id="cb33-1702"><a href="#cb33-1702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1705"><a href="#cb33-1705" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb33-1706"><a href="#cb33-1706" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lst-smoothing-in-the-NDLM</span></span>
<span id="cb33-1707"><a href="#cb33-1707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1708"><a href="#cb33-1708" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb33-1709"><a href="#cb33-1709" aria-hidden="true" tabindex="-1"></a><span class="do">##### Univariate DLM: Known, constant variances</span></span>
<span id="cb33-1710"><a href="#cb33-1710" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb33-1711"><a href="#cb33-1711" aria-hidden="true" tabindex="-1"></a>set_up_dlm_matrices <span class="ot">&lt;-</span> <span class="cf">function</span>(FF, GG, VV, WW){</span>
<span id="cb33-1712"><a href="#cb33-1712" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">FF=</span>FF, <span class="at">GG=</span>GG, <span class="at">VV=</span>VV, <span class="at">WW=</span>WW))</span>
<span id="cb33-1713"><a href="#cb33-1713" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1714"><a href="#cb33-1714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1715"><a href="#cb33-1715" aria-hidden="true" tabindex="-1"></a>set_up_initial_states <span class="ot">&lt;-</span> <span class="cf">function</span>(m0, C0){</span>
<span id="cb33-1716"><a href="#cb33-1716" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">m0=</span>m0, <span class="at">C0=</span>C0))</span>
<span id="cb33-1717"><a href="#cb33-1717" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1718"><a href="#cb33-1718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1719"><a href="#cb33-1719" aria-hidden="true" tabindex="-1"></a><span class="do">### forward update equations </span><span class="al">###</span></span>
<span id="cb33-1720"><a href="#cb33-1720" aria-hidden="true" tabindex="-1"></a>forward_filter <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, initial_states){</span>
<span id="cb33-1721"><a href="#cb33-1721" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve dataset</span></span>
<span id="cb33-1722"><a href="#cb33-1722" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb33-1723"><a href="#cb33-1723" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t)</span>
<span id="cb33-1724"><a href="#cb33-1724" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1725"><a href="#cb33-1725" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve a set of quadruples </span></span>
<span id="cb33-1726"><a href="#cb33-1726" aria-hidden="true" tabindex="-1"></a>  <span class="co"># FF, GG, VV, WW are scalar</span></span>
<span id="cb33-1727"><a href="#cb33-1727" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF  </span>
<span id="cb33-1728"><a href="#cb33-1728" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb33-1729"><a href="#cb33-1729" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb33-1730"><a href="#cb33-1730" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb33-1731"><a href="#cb33-1731" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1732"><a href="#cb33-1732" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve initial states</span></span>
<span id="cb33-1733"><a href="#cb33-1733" aria-hidden="true" tabindex="-1"></a>  m0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>m0</span>
<span id="cb33-1734"><a href="#cb33-1734" aria-hidden="true" tabindex="-1"></a>  C0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>C0</span>
<span id="cb33-1735"><a href="#cb33-1735" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1736"><a href="#cb33-1736" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for results</span></span>
<span id="cb33-1737"><a href="#cb33-1737" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(GG)[<span class="dv">1</span>]</span>
<span id="cb33-1738"><a href="#cb33-1738" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb33-1739"><a href="#cb33-1739" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb33-1740"><a href="#cb33-1740" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1741"><a href="#cb33-1741" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1742"><a href="#cb33-1742" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb33-1743"><a href="#cb33-1743" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb33-1744"><a href="#cb33-1744" aria-hidden="true" tabindex="-1"></a>  et <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1745"><a href="#cb33-1745" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1746"><a href="#cb33-1746" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1747"><a href="#cb33-1747" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T){</span>
<span id="cb33-1748"><a href="#cb33-1748" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of priors at t</span></span>
<span id="cb33-1749"><a href="#cb33-1749" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb33-1750"><a href="#cb33-1750" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(m0)</span>
<span id="cb33-1751"><a href="#cb33-1751" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> C0 <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-1752"><a href="#cb33-1752" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb33-1753"><a href="#cb33-1753" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb33-1754"><a href="#cb33-1754" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1755"><a href="#cb33-1755" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-1756"><a href="#cb33-1756" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb33-1757"><a href="#cb33-1757" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-1758"><a href="#cb33-1758" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-1759"><a href="#cb33-1759" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of one-step forecast:</span></span>
<span id="cb33-1760"><a href="#cb33-1760" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> (at[i, ]) </span>
<span id="cb33-1761"><a href="#cb33-1761" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb33-1762"><a href="#cb33-1762" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-1763"><a href="#cb33-1763" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of posterior at t:</span></span>
<span id="cb33-1764"><a href="#cb33-1764" aria-hidden="true" tabindex="-1"></a>    At <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">/</span> Qt[i]</span>
<span id="cb33-1765"><a href="#cb33-1765" aria-hidden="true" tabindex="-1"></a>    et[i] <span class="ot">&lt;-</span> y_t[i] <span class="sc">-</span> ft[i]</span>
<span id="cb33-1766"><a href="#cb33-1766" aria-hidden="true" tabindex="-1"></a>    mt[i, ] <span class="ot">&lt;-</span> at[i, ] <span class="sc">+</span> <span class="fu">t</span>(At) <span class="sc">*</span> et[i]</span>
<span id="cb33-1767"><a href="#cb33-1767" aria-hidden="true" tabindex="-1"></a>    Ct[, , i] <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">-</span> Qt[i] <span class="sc">*</span> At <span class="sc">%*%</span> <span class="fu">t</span>(At)</span>
<span id="cb33-1768"><a href="#cb33-1768" aria-hidden="true" tabindex="-1"></a>    Ct[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Ct[,,i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Ct[,,i]) </span>
<span id="cb33-1769"><a href="#cb33-1769" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-1770"><a href="#cb33-1770" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forward filtering is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb33-1771"><a href="#cb33-1771" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mt =</span> mt, <span class="at">Ct =</span> Ct, <span class="at">at =</span> at, <span class="at">Rt =</span> Rt, </span>
<span id="cb33-1772"><a href="#cb33-1772" aria-hidden="true" tabindex="-1"></a>              <span class="at">ft =</span> ft, <span class="at">Qt =</span> Qt))</span>
<span id="cb33-1773"><a href="#cb33-1773" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1774"><a href="#cb33-1774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1775"><a href="#cb33-1775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1776"><a href="#cb33-1776" aria-hidden="true" tabindex="-1"></a>forecast_function <span class="ot">&lt;-</span> <span class="cf">function</span>(posterior_states, k, matrices){</span>
<span id="cb33-1777"><a href="#cb33-1777" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1778"><a href="#cb33-1778" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb33-1779"><a href="#cb33-1779" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb33-1780"><a href="#cb33-1780" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb33-1781"><a href="#cb33-1781" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb33-1782"><a href="#cb33-1782" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb33-1783"><a href="#cb33-1783" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb33-1784"><a href="#cb33-1784" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb33-1785"><a href="#cb33-1785" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1786"><a href="#cb33-1786" aria-hidden="true" tabindex="-1"></a>  <span class="do">## set up matrices</span></span>
<span id="cb33-1787"><a href="#cb33-1787" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>] <span class="co"># time points</span></span>
<span id="cb33-1788"><a href="#cb33-1788" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>] <span class="co"># dimension of state parameter vector</span></span>
<span id="cb33-1789"><a href="#cb33-1789" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1790"><a href="#cb33-1790" aria-hidden="true" tabindex="-1"></a>  <span class="do">## placeholder for results</span></span>
<span id="cb33-1791"><a href="#cb33-1791" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> k, <span class="at">ncol =</span> d)</span>
<span id="cb33-1792"><a href="#cb33-1792" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, k))</span>
<span id="cb33-1793"><a href="#cb33-1793" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb33-1794"><a href="#cb33-1794" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb33-1795"><a href="#cb33-1795" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1796"><a href="#cb33-1796" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1797"><a href="#cb33-1797" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb33-1798"><a href="#cb33-1798" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of state distribution</span></span>
<span id="cb33-1799"><a href="#cb33-1799" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb33-1800"><a href="#cb33-1800" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[T, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1801"><a href="#cb33-1801" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , T] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-1802"><a href="#cb33-1802" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb33-1803"><a href="#cb33-1803" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb33-1804"><a href="#cb33-1804" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(at[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1805"><a href="#cb33-1805" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Rt[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-1806"><a href="#cb33-1806" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb33-1807"><a href="#cb33-1807" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-1808"><a href="#cb33-1808" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-1809"><a href="#cb33-1809" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of forecast distribution</span></span>
<span id="cb33-1810"><a href="#cb33-1810" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(at[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1811"><a href="#cb33-1811" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb33-1812"><a href="#cb33-1812" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-1813"><a href="#cb33-1813" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forecasting is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb33-1814"><a href="#cb33-1814" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">at=</span>at, <span class="at">Rt=</span>Rt, <span class="at">ft=</span>ft, <span class="at">Qt=</span>Qt))</span>
<span id="cb33-1815"><a href="#cb33-1815" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1816"><a href="#cb33-1816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1817"><a href="#cb33-1817" aria-hidden="true" tabindex="-1"></a><span class="do">## obtain 95% credible interval</span></span>
<span id="cb33-1818"><a href="#cb33-1818" aria-hidden="true" tabindex="-1"></a>get_credible_interval <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma2, </span>
<span id="cb33-1819"><a href="#cb33-1819" aria-hidden="true" tabindex="-1"></a>                          <span class="at">quantile =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)){</span>
<span id="cb33-1820"><a href="#cb33-1820" aria-hidden="true" tabindex="-1"></a>  z_quantile <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(quantile)</span>
<span id="cb33-1821"><a href="#cb33-1821" aria-hidden="true" tabindex="-1"></a>  bound <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">length</span>(mu), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb33-1822"><a href="#cb33-1822" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">1</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> z_quantile[<span class="dv">1</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># lower bound</span></span>
<span id="cb33-1823"><a href="#cb33-1823" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">2</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> z_quantile[<span class="dv">2</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># upper bound</span></span>
<span id="cb33-1824"><a href="#cb33-1824" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(bound)</span>
<span id="cb33-1825"><a href="#cb33-1825" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1826"><a href="#cb33-1826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1827"><a href="#cb33-1827" aria-hidden="true" tabindex="-1"></a><span class="do">### smoothing equations </span><span class="al">###</span></span>
<span id="cb33-1828"><a href="#cb33-1828" aria-hidden="true" tabindex="-1"></a>backward_smoothing <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, </span>
<span id="cb33-1829"><a href="#cb33-1829" aria-hidden="true" tabindex="-1"></a>                               posterior_states){</span>
<span id="cb33-1830"><a href="#cb33-1830" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve data </span></span>
<span id="cb33-1831"><a href="#cb33-1831" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb33-1832"><a href="#cb33-1832" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t) </span>
<span id="cb33-1833"><a href="#cb33-1833" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1834"><a href="#cb33-1834" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb33-1835"><a href="#cb33-1835" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb33-1836"><a href="#cb33-1836" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb33-1837"><a href="#cb33-1837" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1838"><a href="#cb33-1838" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb33-1839"><a href="#cb33-1839" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb33-1840"><a href="#cb33-1840" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb33-1841"><a href="#cb33-1841" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>at</span>
<span id="cb33-1842"><a href="#cb33-1842" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Rt</span>
<span id="cb33-1843"><a href="#cb33-1843" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1844"><a href="#cb33-1844" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for posterior moments </span></span>
<span id="cb33-1845"><a href="#cb33-1845" aria-hidden="true" tabindex="-1"></a>  mnt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>], <span class="at">ncol =</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>])</span>
<span id="cb33-1846"><a href="#cb33-1846" aria-hidden="true" tabindex="-1"></a>  Cnt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim =</span> <span class="fu">dim</span>(Ct))</span>
<span id="cb33-1847"><a href="#cb33-1847" aria-hidden="true" tabindex="-1"></a>  fnt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1848"><a href="#cb33-1848" aria-hidden="true" tabindex="-1"></a>  Qnt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1849"><a href="#cb33-1849" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> T<span class="sc">:</span><span class="dv">1</span>){</span>
<span id="cb33-1850"><a href="#cb33-1850" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for the distributions of the state vector given D_T</span></span>
<span id="cb33-1851"><a href="#cb33-1851" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> T){</span>
<span id="cb33-1852"><a href="#cb33-1852" aria-hidden="true" tabindex="-1"></a>      mnt[i, ] <span class="ot">&lt;-</span> mt[i, ]</span>
<span id="cb33-1853"><a href="#cb33-1853" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> Ct[, , i]</span>
<span id="cb33-1854"><a href="#cb33-1854" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Cnt[, , i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Cnt[, , i]) </span>
<span id="cb33-1855"><a href="#cb33-1855" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb33-1856"><a href="#cb33-1856" aria-hidden="true" tabindex="-1"></a>      inv_Rtp1<span class="ot">&lt;-</span><span class="fu">solve</span>(Rt[,,i<span class="sc">+</span><span class="dv">1</span>])</span>
<span id="cb33-1857"><a href="#cb33-1857" aria-hidden="true" tabindex="-1"></a>      Bt <span class="ot">&lt;-</span> Ct[, , i] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">%*%</span> inv_Rtp1</span>
<span id="cb33-1858"><a href="#cb33-1858" aria-hidden="true" tabindex="-1"></a>      mnt[i, ] <span class="ot">&lt;-</span> mt[i, ] <span class="sc">+</span> Bt <span class="sc">%*%</span> (mnt[i<span class="sc">+</span><span class="dv">1</span>, ] <span class="sc">-</span> at[i<span class="sc">+</span><span class="dv">1</span>, ])</span>
<span id="cb33-1859"><a href="#cb33-1859" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> Ct[, , i] <span class="sc">+</span> Bt <span class="sc">%*%</span> (Cnt[, , i <span class="sc">+</span> <span class="dv">1</span>] <span class="sc">-</span> Rt[, , i<span class="sc">+</span><span class="dv">1</span>]) <span class="sc">%*%</span> <span class="fu">t</span>(Bt)</span>
<span id="cb33-1860"><a href="#cb33-1860" aria-hidden="true" tabindex="-1"></a>      Cnt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Cnt[,,i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Cnt[,,i]) </span>
<span id="cb33-1861"><a href="#cb33-1861" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-1862"><a href="#cb33-1862" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for the smoothed distribution of the mean response of the series</span></span>
<span id="cb33-1863"><a href="#cb33-1863" aria-hidden="true" tabindex="-1"></a>    fnt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(mnt[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-1864"><a href="#cb33-1864" aria-hidden="true" tabindex="-1"></a>    Qnt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(Cnt[, , i]) <span class="sc">%*%</span> FF</span>
<span id="cb33-1865"><a href="#cb33-1865" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-1866"><a href="#cb33-1866" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Backward smoothing is completed!"</span>)</span>
<span id="cb33-1867"><a href="#cb33-1867" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mnt =</span> mnt, <span class="at">Cnt =</span> Cnt, <span class="at">fnt=</span>fnt, <span class="at">Qnt=</span>Qnt))</span>
<span id="cb33-1868"><a href="#cb33-1868" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1869"><a href="#cb33-1869" aria-hidden="true" tabindex="-1"></a><span class="do">####################### Example: Lake Huron Data ######################</span></span>
<span id="cb33-1870"><a href="#cb33-1870" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(LakeHuron,<span class="at">main=</span><span class="st">"Lake Huron Data"</span>,<span class="at">ylab=</span><span class="st">"level in feet"</span>) </span>
<span id="cb33-1871"><a href="#cb33-1871" aria-hidden="true" tabindex="-1"></a><span class="co"># 98 observations total </span></span>
<span id="cb33-1872"><a href="#cb33-1872" aria-hidden="true" tabindex="-1"></a>k<span class="ot">=</span><span class="dv">4</span></span>
<span id="cb33-1873"><a href="#cb33-1873" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(LakeHuron)<span class="sc">-</span>k <span class="co"># We take the first 94 observations </span></span>
<span id="cb33-1874"><a href="#cb33-1874" aria-hidden="true" tabindex="-1"></a>                     <span class="co">#  as our data</span></span>
<span id="cb33-1875"><a href="#cb33-1875" aria-hidden="true" tabindex="-1"></a>ts_data<span class="ot">=</span>LakeHuron[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb33-1876"><a href="#cb33-1876" aria-hidden="true" tabindex="-1"></a>ts_validation_data <span class="ot">&lt;-</span> LakeHuron[(T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">98</span>]</span>
<span id="cb33-1877"><a href="#cb33-1877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1878"><a href="#cb33-1878" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y_t =</span> ts_data)</span>
<span id="cb33-1879"><a href="#cb33-1879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1880"><a href="#cb33-1880" aria-hidden="true" tabindex="-1"></a><span class="do">## set up matrices</span></span>
<span id="cb33-1881"><a href="#cb33-1881" aria-hidden="true" tabindex="-1"></a>FF <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1882"><a href="#cb33-1882" aria-hidden="true" tabindex="-1"></a>GG <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1883"><a href="#cb33-1883" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1884"><a href="#cb33-1884" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-1885"><a href="#cb33-1885" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">570</span>)</span>
<span id="cb33-1886"><a href="#cb33-1886" aria-hidden="true" tabindex="-1"></a>C0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fl">1e4</span>)</span>
<span id="cb33-1887"><a href="#cb33-1887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1888"><a href="#cb33-1888" aria-hidden="true" tabindex="-1"></a><span class="do">## wrap up all matrices and initial values</span></span>
<span id="cb33-1889"><a href="#cb33-1889" aria-hidden="true" tabindex="-1"></a>matrices <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF,GG,VV,WW)</span>
<span id="cb33-1890"><a href="#cb33-1890" aria-hidden="true" tabindex="-1"></a>initial_states <span class="ot">&lt;-</span> <span class="fu">set_up_initial_states</span>(m0, C0)</span>
<span id="cb33-1891"><a href="#cb33-1891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1892"><a href="#cb33-1892" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering</span></span>
<span id="cb33-1893"><a href="#cb33-1893" aria-hidden="true" tabindex="-1"></a>results_filtered <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices, </span>
<span id="cb33-1894"><a href="#cb33-1894" aria-hidden="true" tabindex="-1"></a>                                   initial_states)</span>
<span id="cb33-1895"><a href="#cb33-1895" aria-hidden="true" tabindex="-1"></a>ci_filtered<span class="ot">&lt;-</span><span class="fu">get_credible_interval</span>(results_filtered<span class="sc">$</span>mt,</span>
<span id="cb33-1896"><a href="#cb33-1896" aria-hidden="true" tabindex="-1"></a>                                   results_filtered<span class="sc">$</span>Ct)</span>
<span id="cb33-1897"><a href="#cb33-1897" aria-hidden="true" tabindex="-1"></a><span class="do">## smoothing</span></span>
<span id="cb33-1898"><a href="#cb33-1898" aria-hidden="true" tabindex="-1"></a>results_smoothed <span class="ot">&lt;-</span> <span class="fu">backward_smoothing</span>(data, matrices, </span>
<span id="cb33-1899"><a href="#cb33-1899" aria-hidden="true" tabindex="-1"></a>                                       results_filtered)</span>
<span id="cb33-1900"><a href="#cb33-1900" aria-hidden="true" tabindex="-1"></a>ci_smoothed <span class="ot">&lt;-</span> <span class="fu">get_credible_interval</span>(results_smoothed<span class="sc">$</span>mnt, </span>
<span id="cb33-1901"><a href="#cb33-1901" aria-hidden="true" tabindex="-1"></a>                                     results_smoothed<span class="sc">$</span>Cnt)</span>
<span id="cb33-1902"><a href="#cb33-1902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1903"><a href="#cb33-1903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1904"><a href="#cb33-1904" aria-hidden="true" tabindex="-1"></a>index<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1875</span>, <span class="dv">1972</span>, <span class="at">length.out =</span> <span class="fu">length</span>(LakeHuron))</span>
<span id="cb33-1905"><a href="#cb33-1905" aria-hidden="true" tabindex="-1"></a>index_filt<span class="ot">=</span>index[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb33-1906"><a href="#cb33-1906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1907"><a href="#cb33-1907" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index, LakeHuron, <span class="at">main =</span> <span class="st">"Lake Huron Level "</span>,<span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb33-1908"><a href="#cb33-1908" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">ylab=</span><span class="st">"level in feet"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">575</span>,<span class="dv">583</span>))</span>
<span id="cb33-1909"><a href="#cb33-1909" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb33-1910"><a href="#cb33-1910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1911"><a href="#cb33-1911" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_filtered<span class="sc">$</span>mt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1912"><a href="#cb33-1912" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-1913"><a href="#cb33-1913" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered[,<span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1914"><a href="#cb33-1914" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_filtered[,<span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1915"><a href="#cb33-1915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1916"><a href="#cb33-1916" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_smoothed<span class="sc">$</span>mnt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-1917"><a href="#cb33-1917" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-1918"><a href="#cb33-1918" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_smoothed[,<span class="dv">1</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1919"><a href="#cb33-1919" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, ci_smoothed[,<span class="dv">2</span>], <span class="at">type=</span><span class="st">'l'</span>, <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb33-1920"><a href="#cb33-1920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1921"><a href="#cb33-1921" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">'bottomleft'</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"filtered"</span>,<span class="st">"smoothed"</span>),</span>
<span id="cb33-1922"><a href="#cb33-1922" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lty=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb33-1923"><a href="#cb33-1923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1924"><a href="#cb33-1924" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-1925"><a href="#cb33-1925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1926"><a href="#cb33-1926" aria-hidden="true" tabindex="-1"></a><span class="fu">## Second order polynomial: Filtering and smoothing example (Video)</span></span>
<span id="cb33-1927"><a href="#cb33-1927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1928"><a href="#cb33-1928" aria-hidden="true" tabindex="-1"></a>In this video walk through the code provided in the section below the comment</span>
<span id="cb33-1929"><a href="#cb33-1929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1930"><a href="#cb33-1930" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-1931"><a href="#cb33-1931" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb33-1932"><a href="#cb33-1932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1933"><a href="#cb33-1933" aria-hidden="true" tabindex="-1"></a>{{&lt; include _C4-L03-T07.qmd &gt;}}</span>
<span id="cb33-1934"><a href="#cb33-1934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1935"><a href="#cb33-1935" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-1936"><a href="#cb33-1936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1937"><a href="#cb33-1937" aria-hidden="true" tabindex="-1"></a><span class="fu">## Using the dlm package in R (Video)</span></span>
<span id="cb33-1938"><a href="#cb33-1938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1939"><a href="#cb33-1939" aria-hidden="true" tabindex="-1"></a>The <span class="in">`dlm`</span> package in R is a powerful tool for working with dynamic linear models. The package provides a wide range of functions for filtering, smoothing, forecasting, and parameter estimation in DLMs. In this video, we walk through the code provided in @lst-dlm-package.</span>
<span id="cb33-1940"><a href="#cb33-1940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1941"><a href="#cb33-1941" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb33-1942"><a href="#cb33-1942" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Transcript</span></span>
<span id="cb33-1943"><a href="#cb33-1943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1944"><a href="#cb33-1944" aria-hidden="true" tabindex="-1"></a>{{&lt; include _C4-L03-T08.qmd &gt;}}</span>
<span id="cb33-1945"><a href="#cb33-1945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1946"><a href="#cb33-1946" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-1947"><a href="#cb33-1947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1948"><a href="#cb33-1948" aria-hidden="true" tabindex="-1"></a><span class="fu">## R-code: Using the `dlm` package in R (Reading)</span></span>
<span id="cb33-1949"><a href="#cb33-1949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1952"><a href="#cb33-1952" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb33-1953"><a href="#cb33-1953" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lst-dlm-package</span></span>
<span id="cb33-1954"><a href="#cb33-1954" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-label: lst-dlm-package</span></span>
<span id="cb33-1955"><a href="#cb33-1955" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-cap: Using the `dlm` package for dynamic linear models</span></span>
<span id="cb33-1956"><a href="#cb33-1956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1957"><a href="#cb33-1957" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb33-1958"><a href="#cb33-1958" aria-hidden="true" tabindex="-1"></a><span class="do">##### Univariate DLM: Known, constant variances</span></span>
<span id="cb33-1959"><a href="#cb33-1959" aria-hidden="true" tabindex="-1"></a><span class="do">#################################################</span></span>
<span id="cb33-1960"><a href="#cb33-1960" aria-hidden="true" tabindex="-1"></a>set_up_dlm_matrices <span class="ot">&lt;-</span> <span class="cf">function</span>(FF, GG, VV, WW){</span>
<span id="cb33-1961"><a href="#cb33-1961" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">FF=</span>FF, <span class="at">GG=</span>GG, <span class="at">VV=</span>VV, <span class="at">WW=</span>WW))</span>
<span id="cb33-1962"><a href="#cb33-1962" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1963"><a href="#cb33-1963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1964"><a href="#cb33-1964" aria-hidden="true" tabindex="-1"></a>set_up_initial_states <span class="ot">&lt;-</span> <span class="cf">function</span>(m0, C0){</span>
<span id="cb33-1965"><a href="#cb33-1965" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">m0=</span>m0, <span class="at">C0=</span>C0))</span>
<span id="cb33-1966"><a href="#cb33-1966" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-1967"><a href="#cb33-1967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-1968"><a href="#cb33-1968" aria-hidden="true" tabindex="-1"></a><span class="do">### forward update equations </span><span class="al">###</span></span>
<span id="cb33-1969"><a href="#cb33-1969" aria-hidden="true" tabindex="-1"></a>forward_filter <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, initial_states){</span>
<span id="cb33-1970"><a href="#cb33-1970" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve dataset</span></span>
<span id="cb33-1971"><a href="#cb33-1971" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb33-1972"><a href="#cb33-1972" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t)</span>
<span id="cb33-1973"><a href="#cb33-1973" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1974"><a href="#cb33-1974" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve a set of quadruples </span></span>
<span id="cb33-1975"><a href="#cb33-1975" aria-hidden="true" tabindex="-1"></a>  <span class="co"># FF, GG, VV, WW are scalar</span></span>
<span id="cb33-1976"><a href="#cb33-1976" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF  </span>
<span id="cb33-1977"><a href="#cb33-1977" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb33-1978"><a href="#cb33-1978" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb33-1979"><a href="#cb33-1979" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb33-1980"><a href="#cb33-1980" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1981"><a href="#cb33-1981" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve initial states</span></span>
<span id="cb33-1982"><a href="#cb33-1982" aria-hidden="true" tabindex="-1"></a>  m0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>m0</span>
<span id="cb33-1983"><a href="#cb33-1983" aria-hidden="true" tabindex="-1"></a>  C0 <span class="ot">&lt;-</span> initial_states<span class="sc">$</span>C0</span>
<span id="cb33-1984"><a href="#cb33-1984" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1985"><a href="#cb33-1985" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for results</span></span>
<span id="cb33-1986"><a href="#cb33-1986" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(GG)[<span class="dv">1</span>]</span>
<span id="cb33-1987"><a href="#cb33-1987" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb33-1988"><a href="#cb33-1988" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb33-1989"><a href="#cb33-1989" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1990"><a href="#cb33-1990" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1991"><a href="#cb33-1991" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>T, <span class="at">ncol=</span>d)</span>
<span id="cb33-1992"><a href="#cb33-1992" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, T))</span>
<span id="cb33-1993"><a href="#cb33-1993" aria-hidden="true" tabindex="-1"></a>  et <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-1994"><a href="#cb33-1994" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1995"><a href="#cb33-1995" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-1996"><a href="#cb33-1996" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T){</span>
<span id="cb33-1997"><a href="#cb33-1997" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of priors at t</span></span>
<span id="cb33-1998"><a href="#cb33-1998" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb33-1999"><a href="#cb33-1999" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(m0)</span>
<span id="cb33-2000"><a href="#cb33-2000" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> C0 <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-2001"><a href="#cb33-2001" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb33-2002"><a href="#cb33-2002" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb33-2003"><a href="#cb33-2003" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-2004"><a href="#cb33-2004" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-2005"><a href="#cb33-2005" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb33-2006"><a href="#cb33-2006" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-2007"><a href="#cb33-2007" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-2008"><a href="#cb33-2008" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of one-step forecast:</span></span>
<span id="cb33-2009"><a href="#cb33-2009" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> (at[i, ]) </span>
<span id="cb33-2010"><a href="#cb33-2010" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb33-2011"><a href="#cb33-2011" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-2012"><a href="#cb33-2012" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments of posterior at t:</span></span>
<span id="cb33-2013"><a href="#cb33-2013" aria-hidden="true" tabindex="-1"></a>    At <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">/</span> Qt[i]</span>
<span id="cb33-2014"><a href="#cb33-2014" aria-hidden="true" tabindex="-1"></a>    et[i] <span class="ot">&lt;-</span> y_t[i] <span class="sc">-</span> ft[i]</span>
<span id="cb33-2015"><a href="#cb33-2015" aria-hidden="true" tabindex="-1"></a>    mt[i, ] <span class="ot">&lt;-</span> at[i, ] <span class="sc">+</span> <span class="fu">t</span>(At) <span class="sc">*</span> et[i]</span>
<span id="cb33-2016"><a href="#cb33-2016" aria-hidden="true" tabindex="-1"></a>    Ct[, , i] <span class="ot">&lt;-</span> Rt[, , i] <span class="sc">-</span> Qt[i] <span class="sc">*</span> At <span class="sc">%*%</span> <span class="fu">t</span>(At)</span>
<span id="cb33-2017"><a href="#cb33-2017" aria-hidden="true" tabindex="-1"></a>    Ct[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Ct[,,i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Ct[,,i]) </span>
<span id="cb33-2018"><a href="#cb33-2018" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-2019"><a href="#cb33-2019" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forward filtering is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb33-2020"><a href="#cb33-2020" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mt =</span> mt, <span class="at">Ct =</span> Ct, <span class="at">at =</span> at, <span class="at">Rt =</span> Rt, </span>
<span id="cb33-2021"><a href="#cb33-2021" aria-hidden="true" tabindex="-1"></a>              <span class="at">ft =</span> ft, <span class="at">Qt =</span> Qt))</span>
<span id="cb33-2022"><a href="#cb33-2022" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-2023"><a href="#cb33-2023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2024"><a href="#cb33-2024" aria-hidden="true" tabindex="-1"></a>forecast_function <span class="ot">&lt;-</span> <span class="cf">function</span>(posterior_states, k, matrices){</span>
<span id="cb33-2025"><a href="#cb33-2025" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-2026"><a href="#cb33-2026" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb33-2027"><a href="#cb33-2027" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb33-2028"><a href="#cb33-2028" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb33-2029"><a href="#cb33-2029" aria-hidden="true" tabindex="-1"></a>  WW <span class="ot">&lt;-</span> matrices<span class="sc">$</span>WW</span>
<span id="cb33-2030"><a href="#cb33-2030" aria-hidden="true" tabindex="-1"></a>  VV <span class="ot">&lt;-</span> matrices<span class="sc">$</span>VV</span>
<span id="cb33-2031"><a href="#cb33-2031" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb33-2032"><a href="#cb33-2032" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb33-2033"><a href="#cb33-2033" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-2034"><a href="#cb33-2034" aria-hidden="true" tabindex="-1"></a>  <span class="do">## set up matrices</span></span>
<span id="cb33-2035"><a href="#cb33-2035" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>] <span class="co"># time points</span></span>
<span id="cb33-2036"><a href="#cb33-2036" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>] <span class="co"># dimension of state parameter vector</span></span>
<span id="cb33-2037"><a href="#cb33-2037" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-2038"><a href="#cb33-2038" aria-hidden="true" tabindex="-1"></a>  <span class="do">## placeholder for results</span></span>
<span id="cb33-2039"><a href="#cb33-2039" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> k, <span class="at">ncol =</span> d)</span>
<span id="cb33-2040"><a href="#cb33-2040" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim=</span><span class="fu">c</span>(d, d, k))</span>
<span id="cb33-2041"><a href="#cb33-2041" aria-hidden="true" tabindex="-1"></a>  ft <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb33-2042"><a href="#cb33-2042" aria-hidden="true" tabindex="-1"></a>  Qt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(k)</span>
<span id="cb33-2043"><a href="#cb33-2043" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-2044"><a href="#cb33-2044" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-2045"><a href="#cb33-2045" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k){</span>
<span id="cb33-2046"><a href="#cb33-2046" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of state distribution</span></span>
<span id="cb33-2047"><a href="#cb33-2047" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> <span class="dv">1</span>){</span>
<span id="cb33-2048"><a href="#cb33-2048" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(mt[T, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-2049"><a href="#cb33-2049" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Ct[, , T] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-2050"><a href="#cb33-2050" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb33-2051"><a href="#cb33-2051" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb33-2052"><a href="#cb33-2052" aria-hidden="true" tabindex="-1"></a>      at[i, ] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> <span class="fu">t</span>(at[i<span class="dv">-1</span>, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-2053"><a href="#cb33-2053" aria-hidden="true" tabindex="-1"></a>      Rt[, , i] <span class="ot">&lt;-</span> GG <span class="sc">%*%</span> Rt[, , i<span class="dv">-1</span>] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">+</span> WW</span>
<span id="cb33-2054"><a href="#cb33-2054" aria-hidden="true" tabindex="-1"></a>      Rt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Rt[,,i]<span class="sc">+</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Rt[,,i]) </span>
<span id="cb33-2055"><a href="#cb33-2055" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-2056"><a href="#cb33-2056" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-2057"><a href="#cb33-2057" aria-hidden="true" tabindex="-1"></a>    <span class="do">## moments of forecast distribution</span></span>
<span id="cb33-2058"><a href="#cb33-2058" aria-hidden="true" tabindex="-1"></a>    ft[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(at[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-2059"><a href="#cb33-2059" aria-hidden="true" tabindex="-1"></a>    Qt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> Rt[, , i] <span class="sc">%*%</span> FF <span class="sc">+</span> VV</span>
<span id="cb33-2060"><a href="#cb33-2060" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-2061"><a href="#cb33-2061" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Forecasting is completed!"</span>) <span class="co"># indicator of completion</span></span>
<span id="cb33-2062"><a href="#cb33-2062" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">at=</span>at, <span class="at">Rt=</span>Rt, <span class="at">ft=</span>ft, <span class="at">Qt=</span>Qt))</span>
<span id="cb33-2063"><a href="#cb33-2063" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-2064"><a href="#cb33-2064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2065"><a href="#cb33-2065" aria-hidden="true" tabindex="-1"></a><span class="do">## obtain 95% credible interval</span></span>
<span id="cb33-2066"><a href="#cb33-2066" aria-hidden="true" tabindex="-1"></a>get_credible_interval <span class="ot">&lt;-</span> <span class="cf">function</span>(mu, sigma2, </span>
<span id="cb33-2067"><a href="#cb33-2067" aria-hidden="true" tabindex="-1"></a>                          <span class="at">quantile =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)){</span>
<span id="cb33-2068"><a href="#cb33-2068" aria-hidden="true" tabindex="-1"></a>  z_quantile <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(quantile)</span>
<span id="cb33-2069"><a href="#cb33-2069" aria-hidden="true" tabindex="-1"></a>  bound <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span><span class="fu">length</span>(mu), <span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb33-2070"><a href="#cb33-2070" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">1</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> z_quantile[<span class="dv">1</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># lower bound</span></span>
<span id="cb33-2071"><a href="#cb33-2071" aria-hidden="true" tabindex="-1"></a>  bound[, <span class="dv">2</span>] <span class="ot">&lt;-</span> mu <span class="sc">+</span> z_quantile[<span class="dv">2</span>]<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">as.numeric</span>(sigma2)) <span class="co"># upper bound</span></span>
<span id="cb33-2072"><a href="#cb33-2072" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(bound)</span>
<span id="cb33-2073"><a href="#cb33-2073" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-2074"><a href="#cb33-2074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2075"><a href="#cb33-2075" aria-hidden="true" tabindex="-1"></a><span class="do">### smoothing equations </span><span class="al">###</span></span>
<span id="cb33-2076"><a href="#cb33-2076" aria-hidden="true" tabindex="-1"></a>backward_smoothing <span class="ot">&lt;-</span> <span class="cf">function</span>(data, matrices, </span>
<span id="cb33-2077"><a href="#cb33-2077" aria-hidden="true" tabindex="-1"></a>                               posterior_states){</span>
<span id="cb33-2078"><a href="#cb33-2078" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve data </span></span>
<span id="cb33-2079"><a href="#cb33-2079" aria-hidden="true" tabindex="-1"></a>  y_t <span class="ot">&lt;-</span> data<span class="sc">$</span>y_t</span>
<span id="cb33-2080"><a href="#cb33-2080" aria-hidden="true" tabindex="-1"></a>  T <span class="ot">&lt;-</span> <span class="fu">length</span>(y_t) </span>
<span id="cb33-2081"><a href="#cb33-2081" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-2082"><a href="#cb33-2082" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb33-2083"><a href="#cb33-2083" aria-hidden="true" tabindex="-1"></a>  FF <span class="ot">&lt;-</span> matrices<span class="sc">$</span>FF</span>
<span id="cb33-2084"><a href="#cb33-2084" aria-hidden="true" tabindex="-1"></a>  GG <span class="ot">&lt;-</span> matrices<span class="sc">$</span>GG</span>
<span id="cb33-2085"><a href="#cb33-2085" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-2086"><a href="#cb33-2086" aria-hidden="true" tabindex="-1"></a>  <span class="do">## retrieve matrices</span></span>
<span id="cb33-2087"><a href="#cb33-2087" aria-hidden="true" tabindex="-1"></a>  mt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>mt</span>
<span id="cb33-2088"><a href="#cb33-2088" aria-hidden="true" tabindex="-1"></a>  Ct <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Ct</span>
<span id="cb33-2089"><a href="#cb33-2089" aria-hidden="true" tabindex="-1"></a>  at <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>at</span>
<span id="cb33-2090"><a href="#cb33-2090" aria-hidden="true" tabindex="-1"></a>  Rt <span class="ot">&lt;-</span> posterior_states<span class="sc">$</span>Rt</span>
<span id="cb33-2091"><a href="#cb33-2091" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb33-2092"><a href="#cb33-2092" aria-hidden="true" tabindex="-1"></a>  <span class="do">## create placeholder for posterior moments </span></span>
<span id="cb33-2093"><a href="#cb33-2093" aria-hidden="true" tabindex="-1"></a>  mnt <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">dim</span>(mt)[<span class="dv">1</span>], <span class="at">ncol =</span> <span class="fu">dim</span>(mt)[<span class="dv">2</span>])</span>
<span id="cb33-2094"><a href="#cb33-2094" aria-hidden="true" tabindex="-1"></a>  Cnt <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim =</span> <span class="fu">dim</span>(Ct))</span>
<span id="cb33-2095"><a href="#cb33-2095" aria-hidden="true" tabindex="-1"></a>  fnt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-2096"><a href="#cb33-2096" aria-hidden="true" tabindex="-1"></a>  Qnt <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb33-2097"><a href="#cb33-2097" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> T<span class="sc">:</span><span class="dv">1</span>){</span>
<span id="cb33-2098"><a href="#cb33-2098" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for the distributions of the state vector given D_T</span></span>
<span id="cb33-2099"><a href="#cb33-2099" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">==</span> T){</span>
<span id="cb33-2100"><a href="#cb33-2100" aria-hidden="true" tabindex="-1"></a>      mnt[i, ] <span class="ot">&lt;-</span> mt[i, ]</span>
<span id="cb33-2101"><a href="#cb33-2101" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> Ct[, , i]</span>
<span id="cb33-2102"><a href="#cb33-2102" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Cnt[, , i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Cnt[, , i]) </span>
<span id="cb33-2103"><a href="#cb33-2103" aria-hidden="true" tabindex="-1"></a>    }<span class="cf">else</span>{</span>
<span id="cb33-2104"><a href="#cb33-2104" aria-hidden="true" tabindex="-1"></a>      inv_Rtp1<span class="ot">&lt;-</span><span class="fu">solve</span>(Rt[,,i<span class="sc">+</span><span class="dv">1</span>])</span>
<span id="cb33-2105"><a href="#cb33-2105" aria-hidden="true" tabindex="-1"></a>      Bt <span class="ot">&lt;-</span> Ct[, , i] <span class="sc">%*%</span> <span class="fu">t</span>(GG) <span class="sc">%*%</span> inv_Rtp1</span>
<span id="cb33-2106"><a href="#cb33-2106" aria-hidden="true" tabindex="-1"></a>      mnt[i, ] <span class="ot">&lt;-</span> mt[i, ] <span class="sc">+</span> Bt <span class="sc">%*%</span> (mnt[i<span class="sc">+</span><span class="dv">1</span>, ] <span class="sc">-</span> at[i<span class="sc">+</span><span class="dv">1</span>, ])</span>
<span id="cb33-2107"><a href="#cb33-2107" aria-hidden="true" tabindex="-1"></a>      Cnt[, , i] <span class="ot">&lt;-</span> Ct[, , i] <span class="sc">+</span> Bt <span class="sc">%*%</span> (Cnt[, , i <span class="sc">+</span> <span class="dv">1</span>] <span class="sc">-</span> Rt[, , i<span class="sc">+</span><span class="dv">1</span>]) <span class="sc">%*%</span> <span class="fu">t</span>(Bt)</span>
<span id="cb33-2108"><a href="#cb33-2108" aria-hidden="true" tabindex="-1"></a>      Cnt[,,i] <span class="ot">&lt;-</span> <span class="fl">0.5</span><span class="sc">*</span>Cnt[,,i] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(Cnt[,,i]) </span>
<span id="cb33-2109"><a href="#cb33-2109" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-2110"><a href="#cb33-2110" aria-hidden="true" tabindex="-1"></a>    <span class="co"># moments for the smoothed distribution of the mean response of the series</span></span>
<span id="cb33-2111"><a href="#cb33-2111" aria-hidden="true" tabindex="-1"></a>    fnt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(mnt[i, , <span class="at">drop=</span><span class="cn">FALSE</span>])</span>
<span id="cb33-2112"><a href="#cb33-2112" aria-hidden="true" tabindex="-1"></a>    Qnt[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(FF) <span class="sc">%*%</span> <span class="fu">t</span>(Cnt[, , i]) <span class="sc">%*%</span> FF</span>
<span id="cb33-2113"><a href="#cb33-2113" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-2114"><a href="#cb33-2114" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Backward smoothing is completed!"</span>)</span>
<span id="cb33-2115"><a href="#cb33-2115" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">mnt =</span> mnt, <span class="at">Cnt =</span> Cnt, <span class="at">fnt=</span>fnt, <span class="at">Qnt=</span>Qnt))</span>
<span id="cb33-2116"><a href="#cb33-2116" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-2117"><a href="#cb33-2117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2118"><a href="#cb33-2118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2119"><a href="#cb33-2119" aria-hidden="true" tabindex="-1"></a><span class="do">####################### Example: Lake Huron Data ######################</span></span>
<span id="cb33-2120"><a href="#cb33-2120" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(LakeHuron) <span class="co"># 98 observations total </span></span>
<span id="cb33-2121"><a href="#cb33-2121" aria-hidden="true" tabindex="-1"></a>k<span class="ot">=</span><span class="dv">4</span></span>
<span id="cb33-2122"><a href="#cb33-2122" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(LakeHuron)<span class="sc">-</span>k <span class="co"># We take the first </span></span>
<span id="cb33-2123"><a href="#cb33-2123" aria-hidden="true" tabindex="-1"></a>                      <span class="co"># 94 observations only as our data</span></span>
<span id="cb33-2124"><a href="#cb33-2124" aria-hidden="true" tabindex="-1"></a>ts_data<span class="ot">=</span>LakeHuron[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb33-2125"><a href="#cb33-2125" aria-hidden="true" tabindex="-1"></a>ts_validation_data <span class="ot">&lt;-</span> LakeHuron[(T<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">98</span>]</span>
<span id="cb33-2126"><a href="#cb33-2126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2127"><a href="#cb33-2127" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y_t =</span> ts_data)</span>
<span id="cb33-2128"><a href="#cb33-2128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2129"><a href="#cb33-2129" aria-hidden="true" tabindex="-1"></a><span class="do">## set up dlm matrices</span></span>
<span id="cb33-2130"><a href="#cb33-2130" aria-hidden="true" tabindex="-1"></a>GG <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-2131"><a href="#cb33-2131" aria-hidden="true" tabindex="-1"></a>FF <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-2132"><a href="#cb33-2132" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-2133"><a href="#cb33-2133" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">1</span>)</span>
<span id="cb33-2134"><a href="#cb33-2134" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">570</span>)</span>
<span id="cb33-2135"><a href="#cb33-2135" aria-hidden="true" tabindex="-1"></a>C0 <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fl">1e4</span>)</span>
<span id="cb33-2136"><a href="#cb33-2136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2137"><a href="#cb33-2137" aria-hidden="true" tabindex="-1"></a><span class="do">## wrap up all matrices and initial values</span></span>
<span id="cb33-2138"><a href="#cb33-2138" aria-hidden="true" tabindex="-1"></a>matrices <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF, GG, VV, WW)</span>
<span id="cb33-2139"><a href="#cb33-2139" aria-hidden="true" tabindex="-1"></a>initial_states <span class="ot">&lt;-</span> <span class="fu">set_up_initial_states</span>(m0, C0)</span>
<span id="cb33-2140"><a href="#cb33-2140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2141"><a href="#cb33-2141" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering and smoothing </span></span>
<span id="cb33-2142"><a href="#cb33-2142" aria-hidden="true" tabindex="-1"></a>results_filtered <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices, </span>
<span id="cb33-2143"><a href="#cb33-2143" aria-hidden="true" tabindex="-1"></a>                                   initial_states)</span>
<span id="cb33-2144"><a href="#cb33-2144" aria-hidden="true" tabindex="-1"></a>results_smoothed <span class="ot">&lt;-</span> <span class="fu">backward_smoothing</span>(data, matrices, </span>
<span id="cb33-2145"><a href="#cb33-2145" aria-hidden="true" tabindex="-1"></a>                                       results_filtered)</span>
<span id="cb33-2146"><a href="#cb33-2146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2147"><a href="#cb33-2147" aria-hidden="true" tabindex="-1"></a>index<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1875</span>, <span class="dv">1972</span>, <span class="at">length.out =</span> <span class="fu">length</span>(LakeHuron))</span>
<span id="cb33-2148"><a href="#cb33-2148" aria-hidden="true" tabindex="-1"></a>index_filt<span class="ot">=</span>index[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb33-2149"><a href="#cb33-2149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2150"><a href="#cb33-2150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2151"><a href="#cb33-2151" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb33-2152"><a href="#cb33-2152" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index, LakeHuron, <span class="at">main =</span> <span class="st">"Lake Huron Level "</span>,<span class="at">type=</span><span class="st">'l'</span>,</span>
<span id="cb33-2153"><a href="#cb33-2153" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">ylab=</span><span class="st">"feet"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">575</span>,<span class="dv">583</span>))</span>
<span id="cb33-2154"><a href="#cb33-2154" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index,LakeHuron,<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb33-2155"><a href="#cb33-2155" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_filtered<span class="sc">$</span>mt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-2156"><a href="#cb33-2156" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2157"><a href="#cb33-2157" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt, results_smoothed<span class="sc">$</span>mnt, <span class="at">type=</span><span class="st">'l'</span>, </span>
<span id="cb33-2158"><a href="#cb33-2158" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2159"><a href="#cb33-2159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2160"><a href="#cb33-2160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2161"><a href="#cb33-2161" aria-hidden="true" tabindex="-1"></a><span class="co"># Now let's look at the DLM package </span></span>
<span id="cb33-2162"><a href="#cb33-2162" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dlm)</span>
<span id="cb33-2163"><a href="#cb33-2163" aria-hidden="true" tabindex="-1"></a>model<span class="ot">=</span><span class="fu">dlmModPoly</span>(<span class="at">order=</span><span class="dv">1</span>,<span class="at">dV=</span><span class="dv">1</span>,<span class="at">dW=</span><span class="dv">1</span>,<span class="at">m0=</span><span class="dv">570</span>,<span class="at">C0=</span><span class="fl">1e4</span>)</span>
<span id="cb33-2164"><a href="#cb33-2164" aria-hidden="true" tabindex="-1"></a>results_filtered_dlm<span class="ot">=</span><span class="fu">dlmFilter</span>(LakeHuron[<span class="dv">1</span><span class="sc">:</span>T],model)</span>
<span id="cb33-2165"><a href="#cb33-2165" aria-hidden="true" tabindex="-1"></a>results_smoothed_dlm<span class="ot">=</span><span class="fu">dlmSmooth</span>(results_filtered_dlm)</span>
<span id="cb33-2166"><a href="#cb33-2166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2167"><a href="#cb33-2167" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index_filt, LakeHuron[<span class="dv">1</span><span class="sc">:</span>T], <span class="at">ylab =</span> <span class="st">"level"</span>, </span>
<span id="cb33-2168"><a href="#cb33-2168" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Lake Huron Level"</span>,</span>
<span id="cb33-2169"><a href="#cb33-2169" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">'l'</span>, <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">575</span>,<span class="dv">583</span>))</span>
<span id="cb33-2170"><a href="#cb33-2170" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index_filt,LakeHuron[<span class="dv">1</span><span class="sc">:</span>T],<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb33-2171"><a href="#cb33-2171" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt,results_filtered_dlm<span class="sc">$</span>m[<span class="sc">-</span><span class="dv">1</span>],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2172"><a href="#cb33-2172" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt,results_smoothed_dlm<span class="sc">$</span>s[<span class="sc">-</span><span class="dv">1</span>],<span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2173"><a href="#cb33-2173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2174"><a href="#cb33-2174" aria-hidden="true" tabindex="-1"></a><span class="co"># Similarly, for the second order polynomial and the co2 data:</span></span>
<span id="cb33-2175"><a href="#cb33-2175" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(co2)</span>
<span id="cb33-2176"><a href="#cb33-2176" aria-hidden="true" tabindex="-1"></a>data<span class="ot">=</span><span class="fu">list</span>(<span class="at">y_t =</span> co2)</span>
<span id="cb33-2177"><a href="#cb33-2177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2178"><a href="#cb33-2178" aria-hidden="true" tabindex="-1"></a>FF <span class="ot">&lt;-</span> (<span class="fu">as.matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>)))</span>
<span id="cb33-2179"><a href="#cb33-2179" aria-hidden="true" tabindex="-1"></a>GG <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span>T)</span>
<span id="cb33-2180"><a href="#cb33-2180" aria-hidden="true" tabindex="-1"></a>VV <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="dv">200</span>)</span>
<span id="cb33-2181"><a href="#cb33-2181" aria-hidden="true" tabindex="-1"></a>WW <span class="ot">&lt;-</span> <span class="fl">0.01</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>)</span>
<span id="cb33-2182"><a href="#cb33-2182" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">c</span>(<span class="dv">320</span>,<span class="dv">0</span>)))</span>
<span id="cb33-2183"><a href="#cb33-2183" aria-hidden="true" tabindex="-1"></a>C0 <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>)</span>
<span id="cb33-2184"><a href="#cb33-2184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2185"><a href="#cb33-2185" aria-hidden="true" tabindex="-1"></a><span class="do">## wrap up all matrices and initial values</span></span>
<span id="cb33-2186"><a href="#cb33-2186" aria-hidden="true" tabindex="-1"></a>matrices <span class="ot">&lt;-</span> <span class="fu">set_up_dlm_matrices</span>(FF,GG, VV, WW)</span>
<span id="cb33-2187"><a href="#cb33-2187" aria-hidden="true" tabindex="-1"></a>initial_states <span class="ot">&lt;-</span> <span class="fu">set_up_initial_states</span>(m0, C0)</span>
<span id="cb33-2188"><a href="#cb33-2188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2189"><a href="#cb33-2189" aria-hidden="true" tabindex="-1"></a><span class="do">## filtering and smoothing </span></span>
<span id="cb33-2190"><a href="#cb33-2190" aria-hidden="true" tabindex="-1"></a>results_filtered <span class="ot">&lt;-</span> <span class="fu">forward_filter</span>(data, matrices, </span>
<span id="cb33-2191"><a href="#cb33-2191" aria-hidden="true" tabindex="-1"></a>                                   initial_states)</span>
<span id="cb33-2192"><a href="#cb33-2192" aria-hidden="true" tabindex="-1"></a>results_smoothed <span class="ot">&lt;-</span> <span class="fu">backward_smoothing</span>(data, matrices, </span>
<span id="cb33-2193"><a href="#cb33-2193" aria-hidden="true" tabindex="-1"></a>                                       results_filtered)</span>
<span id="cb33-2194"><a href="#cb33-2194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2195"><a href="#cb33-2195" aria-hidden="true" tabindex="-1"></a><span class="do">#### Now, using the DLM package: </span></span>
<span id="cb33-2196"><a href="#cb33-2196" aria-hidden="true" tabindex="-1"></a>model<span class="ot">=</span><span class="fu">dlmModPoly</span>(<span class="at">order=</span><span class="dv">2</span>,<span class="at">dV=</span><span class="dv">200</span>,<span class="at">dW=</span><span class="fl">0.01</span><span class="sc">*</span><span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">2</span>),</span>
<span id="cb33-2197"><a href="#cb33-2197" aria-hidden="true" tabindex="-1"></a>                 <span class="at">m0=</span><span class="fu">c</span>(<span class="dv">320</span>,<span class="dv">0</span>),<span class="at">C0=</span><span class="dv">10</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>))</span>
<span id="cb33-2198"><a href="#cb33-2198" aria-hidden="true" tabindex="-1"></a><span class="co"># filtering and smoothing </span></span>
<span id="cb33-2199"><a href="#cb33-2199" aria-hidden="true" tabindex="-1"></a>results_filtered_dlm<span class="ot">=</span><span class="fu">dlmFilter</span>(data<span class="sc">$</span>y_t,model)</span>
<span id="cb33-2200"><a href="#cb33-2200" aria-hidden="true" tabindex="-1"></a>results_smoothed_dlm<span class="ot">=</span><span class="fu">dlmSmooth</span>(results_filtered_dlm)</span>
<span id="cb33-2201"><a href="#cb33-2201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2202"><a href="#cb33-2202" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb33-2203"><a href="#cb33-2203" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),co2,<span class="at">type=</span><span class="st">'l'</span>,<span class="at">xlab=</span><span class="st">"time"</span>,</span>
<span id="cb33-2204"><a href="#cb33-2204" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">300</span>,<span class="dv">380</span>))</span>
<span id="cb33-2205"><a href="#cb33-2205" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),results_filtered<span class="sc">$</span>mt[,<span class="dv">1</span>],</span>
<span id="cb33-2206"><a href="#cb33-2206" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2207"><a href="#cb33-2207" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),results_smoothed<span class="sc">$</span>mnt[,<span class="dv">1</span>],</span>
<span id="cb33-2208"><a href="#cb33-2208" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2209"><a href="#cb33-2209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2210"><a href="#cb33-2210" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),co2,<span class="at">type=</span><span class="st">'l'</span>,<span class="at">xlab=</span><span class="st">"time"</span>,</span>
<span id="cb33-2211"><a href="#cb33-2211" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">300</span>,<span class="dv">380</span>))</span>
<span id="cb33-2212"><a href="#cb33-2212" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),results_filtered_dlm<span class="sc">$</span>m[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb33-2213"><a href="#cb33-2213" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2214"><a href="#cb33-2214" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">as.vector</span>(<span class="fu">time</span>(co2)),results_smoothed_dlm<span class="sc">$</span>s[<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb33-2215"><a href="#cb33-2215" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2216"><a href="#cb33-2216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-2217"><a href="#cb33-2217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2218"><a href="#cb33-2218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2219"><a href="#cb33-2219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2220"><a href="#cb33-2220" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practice Graded Assignment: NDLM -- sensitivity to the model parameters</span></span>
<span id="cb33-2221"><a href="#cb33-2221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2222"><a href="#cb33-2222" aria-hidden="true" tabindex="-1"></a>:::: {.content-visible when-profile="advanced"}</span>
<span id="cb33-2223"><a href="#cb33-2223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2224"><a href="#cb33-2224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2225"><a href="#cb33-2225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2226"><a href="#cb33-2226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2227"><a href="#cb33-2227" aria-hidden="true" tabindex="-1"></a>This is a peer reviewed assignment. I may drop in the instructions but the solution will not be provided here due to the Coursera honor code.</span>
<span id="cb33-2228"><a href="#cb33-2228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2229"><a href="#cb33-2229" aria-hidden="true" tabindex="-1"></a>This peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you've learned in R and prepare you for your data analysis project in week 5. </span>
<span id="cb33-2230"><a href="#cb33-2230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2231"><a href="#cb33-2231" aria-hidden="true" tabindex="-1"></a>Consider the following R code: </span>
<span id="cb33-2232"><a href="#cb33-2232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2235"><a href="#cb33-2235" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb33-2236"><a href="#cb33-2236" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: lst-sensitivity-to-model-params</span></span>
<span id="cb33-2237"><a href="#cb33-2237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2238"><a href="#cb33-2238" aria-hidden="true" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb33-2239"><a href="#cb33-2239" aria-hidden="true" tabindex="-1"></a><span class="do">##### DLM package #####</span></span>
<span id="cb33-2240"><a href="#cb33-2240" aria-hidden="true" tabindex="-1"></a><span class="do">#######################</span></span>
<span id="cb33-2241"><a href="#cb33-2241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2242"><a href="#cb33-2242" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dlm)</span>
<span id="cb33-2243"><a href="#cb33-2243" aria-hidden="true" tabindex="-1"></a>k<span class="ot">=</span><span class="dv">4</span></span>
<span id="cb33-2244"><a href="#cb33-2244" aria-hidden="true" tabindex="-1"></a>T<span class="ot">=</span><span class="fu">length</span>(LakeHuron)<span class="sc">-</span>k <span class="co"># We take the first </span></span>
<span id="cb33-2245"><a href="#cb33-2245" aria-hidden="true" tabindex="-1"></a>                      <span class="co"># 94 observations only as our data</span></span>
<span id="cb33-2246"><a href="#cb33-2246" aria-hidden="true" tabindex="-1"></a>index<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1875</span>, <span class="dv">1972</span>, <span class="at">length.out =</span> <span class="fu">length</span>(LakeHuron))</span>
<span id="cb33-2247"><a href="#cb33-2247" aria-hidden="true" tabindex="-1"></a>index_filt<span class="ot">=</span>index[<span class="dv">1</span><span class="sc">:</span>T]</span>
<span id="cb33-2248"><a href="#cb33-2248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2249"><a href="#cb33-2249" aria-hidden="true" tabindex="-1"></a>model<span class="ot">=</span><span class="fu">dlmModPoly</span>(<span class="at">order=</span><span class="dv">1</span>,<span class="at">dV=</span><span class="dv">1</span>,<span class="at">dW=</span><span class="dv">1</span>,<span class="at">m0=</span><span class="dv">570</span>,<span class="at">C0=</span><span class="fl">1e4</span>)</span>
<span id="cb33-2250"><a href="#cb33-2250" aria-hidden="true" tabindex="-1"></a>results_filtered_dlm<span class="ot">=</span><span class="fu">dlmFilter</span>(LakeHuron[<span class="dv">1</span><span class="sc">:</span>T],model)</span>
<span id="cb33-2251"><a href="#cb33-2251" aria-hidden="true" tabindex="-1"></a>results_smoothed_dlm<span class="ot">=</span><span class="fu">dlmSmooth</span>(results_filtered_dlm)</span>
<span id="cb33-2252"><a href="#cb33-2252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2253"><a href="#cb33-2253" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(index_filt, LakeHuron[<span class="dv">1</span><span class="sc">:</span>T], <span class="at">ylab =</span> <span class="st">"level"</span>, </span>
<span id="cb33-2254"><a href="#cb33-2254" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Lake Huron Level"</span>,</span>
<span id="cb33-2255"><a href="#cb33-2255" aria-hidden="true" tabindex="-1"></a>     <span class="at">type=</span><span class="st">'l'</span>, <span class="at">xlab=</span><span class="st">"time"</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">575</span>,<span class="dv">583</span>))</span>
<span id="cb33-2256"><a href="#cb33-2256" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(index_filt,LakeHuron[<span class="dv">1</span><span class="sc">:</span>T],<span class="at">pch=</span><span class="dv">20</span>)</span>
<span id="cb33-2257"><a href="#cb33-2257" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt,results_filtered_dlm<span class="sc">$</span>m[<span class="sc">-</span><span class="dv">1</span>],<span class="at">col=</span><span class="st">'red'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2258"><a href="#cb33-2258" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(index_filt,results_smoothed_dlm<span class="sc">$</span>s[<span class="sc">-</span><span class="dv">1</span>],<span class="at">col=</span><span class="st">'blue'</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb33-2259"><a href="#cb33-2259" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="dv">1880</span>,<span class="dv">577</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"filtered"</span>, <span class="st">"smoothed"</span>),</span>
<span id="cb33-2260"><a href="#cb33-2260" aria-hidden="true" tabindex="-1"></a>       <span class="at">col=</span><span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lty=</span><span class="dv">1</span>, <span class="at">cex=</span><span class="fl">0.8</span>)</span>
<span id="cb33-2261"><a href="#cb33-2261" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-2262"><a href="#cb33-2262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2263"><a href="#cb33-2263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2264"><a href="#cb33-2264" aria-hidden="true" tabindex="-1"></a>Note that you will need to install the <span class="in">`dlm`</span> package in R if you don't have it  installed in order to run the code above. After installing the package and running the code above you will be asked to change some of the model specifications, upload some graphs and and answer some questions. In particular, you will be asked to: </span>
<span id="cb33-2265"><a href="#cb33-2265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2266"><a href="#cb33-2266" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Modify the above code to change the variance of the prior distribution from $C_0=10^4$ to $C_0=10$ and plot and upload the traces of $\mathbb{E}(\theta_t \mid \mathcal D_T)$ (mean of the filtered distribution) and $\mathbb{E}(\theta_t \mid \mathcal D_T)$ for $T\geq t$ and all $t=1:T$ (mean of the smoothed distribution). Are these new results different from the results with the model with $C_0=10^4$?</span>
<span id="cb33-2267"><a href="#cb33-2267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2268"><a href="#cb33-2268" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Keep the variance of the prior distribution at $C_0=10^4$. Now change the evolution variance from </span>
<span id="cb33-2269"><a href="#cb33-2269" aria-hidden="true" tabindex="-1"></a>$W=1$ to $W=0.01$ . Plot and upload the new means of the filtered and smoothed results. Are they different from the results when evolution variance is $W=1$ ?</span>
<span id="cb33-2270"><a href="#cb33-2270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2271"><a href="#cb33-2271" aria-hidden="true" tabindex="-1"></a>::: {.callout-info}</span>
<span id="cb33-2272"><a href="#cb33-2272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2273"><a href="#cb33-2273" aria-hidden="true" tabindex="-1"></a><span class="fu">### Grading Criteria</span></span>
<span id="cb33-2274"><a href="#cb33-2274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2275"><a href="#cb33-2275" aria-hidden="true" tabindex="-1"></a>Peer reviewers will be asked to check whether </span>
<span id="cb33-2276"><a href="#cb33-2276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2277"><a href="#cb33-2277" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>the plots are correct, especially the shape of red and blue lines. </span>
<span id="cb33-2278"><a href="#cb33-2278" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>the responses provided to the questions are correct. </span>
<span id="cb33-2279"><a href="#cb33-2279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2280"><a href="#cb33-2280" aria-hidden="true" tabindex="-1"></a>To receive full credit for this assignment you will have to grade the assignments of 2 students taking the course.</span>
<span id="cb33-2281"><a href="#cb33-2281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2282"><a href="#cb33-2282" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-2283"><a href="#cb33-2283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2284"><a href="#cb33-2284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2285"><a href="#cb33-2285" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb33-2286"><a href="#cb33-2286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2287"><a href="#cb33-2287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2288"><a href="#cb33-2288" aria-hidden="true" tabindex="-1"></a>:::: {.content-hidden when-profile="advanced"}</span>
<span id="cb33-2289"><a href="#cb33-2289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2290"><a href="#cb33-2290" aria-hidden="true" tabindex="-1"></a>Omitted due to the Coursera honor code.</span>
<span id="cb33-2291"><a href="#cb33-2291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2292"><a href="#cb33-2292" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb33-2293"><a href="#cb33-2293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2294"><a href="#cb33-2294" aria-hidden="true" tabindex="-1"></a><span class="fu">## Quiz - NDLM, Part I: Review</span></span>
<span id="cb33-2295"><a href="#cb33-2295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2296"><a href="#cb33-2296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2297"><a href="#cb33-2297" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-profile="advanced"}</span>
<span id="cb33-2298"><a href="#cb33-2298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2299"><a href="#cb33-2299" aria-hidden="true" tabindex="-1"></a>This content will only appear in the advanced version.</span>
<span id="cb33-2300"><a href="#cb33-2300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2301"><a href="#cb33-2301" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb33-2302"><a href="#cb33-2302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2303"><a href="#cb33-2303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2304"><a href="#cb33-2304" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden when-profile="advanced"}</span>
<span id="cb33-2305"><a href="#cb33-2305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2306"><a href="#cb33-2306" aria-hidden="true" tabindex="-1"></a>Omitted due to the Coursera honor code.</span>
<span id="cb33-2307"><a href="#cb33-2307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2308"><a href="#cb33-2308" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>