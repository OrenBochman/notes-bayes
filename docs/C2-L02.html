<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="keywords" content="Statistical Modeling">

<title>31&nbsp; M1L2 - Bayesian Modeling – Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C2-L03.html" rel="next">
<link href="./C2-L01.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-cec409635dab7d42c7b562035797153a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-cec409635dab7d42c7b562035797153a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(images/banner_deep.jpg);
background-size: cover;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C2-L00.html">Techniques and Models</a></li><li class="breadcrumb-item"><a href="./C2-L02.html"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C2-L00.html">Techniques and Models</a></li><li class="breadcrumb-item"><a href="./C2-L02.html"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></a></li></ol></nav>
      <div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Bayesian Statistics: Techniques and Models</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Bayesian Statistics</div>
                <div class="quarto-category">Monte Carlo Estimation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Statistical Modeling</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C1-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">From Concept to Data Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Probability - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Paradigms of probability - M1L1HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayes’ Theorem - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Law - M1L2HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probability and Bayes’ Theorem - M1L2HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Distributions - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Random Variables - M1L3HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Homework on Distributions - M1L3HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Frequentist Inference - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Frequentist MLE - M2L3HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Bayesian Inference - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference - M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Priors - M3L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities - M3L6HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Homework on Priors - M2L7HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Poisson Data - M3L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Homework on Poisson Data - M3L8HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Beta Bernoulli - M3L8HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Homework on Exponential Data - M4L9HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Normally distributed Data - M4L10</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Homework on Normal Data - M4L10HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Non-Informative Priors - M4L11</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Homework Alternative Priors - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Brief Review of Regression - M4L12</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW2</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C2-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Techniques and Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm - M2L4HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Gibbs sampling - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Homework Gibbs-Sampling algorithm - M2L22HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Assessing Convergence - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Homework on M-H algorithm M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Linear regression - M3L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Homework on Linear Regression Model Part 1 - M2L5HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Homework on Deviance information criterion - M2L5HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">ANOVA - M3L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Homework on ANOVA - M3L8HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Homework on Multiple Factor ANOVA - M3L8HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Logistic regression - M3L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression - M3L9HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Poisson regression - M4L10</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Homework on Poisson regression - M4L10HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Hierarchical modeling - M4L11</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Capstone Project - M4L12</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models - M4L12HW1</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C3-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mixture Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Definitions of Mixture Models - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Likelihood functions for Mixture Models - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Homework The Likelihood function - M1L2HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Homework Identifiability - M1L2HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Homework The likelihood function M1L2HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Homework on simulating from a Poisson Mixture Model - M1L2HW4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model - M1L2HW5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">Homework Sim mixture of exponential distributions - M1L2HW6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture models - M2L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">MCMC for Mixture Models - M4L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Density Estimation - M4L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Computational Considerations - M5L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Determining the number of components - M5L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Homework on Bayesian Information Criteria (BIC) - M5L09HW1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Homework on BIC for zero-inflated mixtures - M5L09HW4</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Time Series Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Stationarity, The ACF and the PCF M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">The AR(1) process: definitions and properties - M1L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">The AR(1): MLE and Bayesian inference - M1L3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">The AR(p) process - M2L4</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">Bayesian Inference in the AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">Quiz: Spectral representation of the AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p) - M2L5</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1 - M3L6</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 1 M3L7</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Seasonal NDLMs M4L8</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 2 - M4L9</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C5-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Capstone Project</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Bayesian Conjugate Analysis for Autogressive Time Series Models - M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Homework - Practice Quiz for Week 1 – M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L01-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Homework - first-step-for-the-project – M1L1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Model Selection Criteria - M2L2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C5-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Bayesian location mixture of AR(p) models - M3L3</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./C6-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian Non-Parametric Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C6-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Gaussian Processes for Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C6-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Dirichlet Process</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Appendix: Bayes by backprop</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Appendix: Yule-Walker Equations &amp; Durbin-Levinson Recursion</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Moore-Penrose Inversion &amp; Cholesky Decomposition</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Appendix: Inequalities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A16.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Appendix: Wold’s theorem</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-c2l02-components" id="toc-sec-c2l02-components" class="nav-link active" data-scroll-target="#sec-c2l02-components"><span class="header-section-number">31.1</span> Components of a Bayesian Model 🎥</a></li>
  <li><a href="#sec-c2l02-model-specification" id="toc-sec-c2l02-model-specification" class="nav-link" data-scroll-target="#sec-c2l02-model-specification"><span class="header-section-number">31.2</span> Model Specification 🎥</a>
  <ul class="collapse">
  <li><a href="#hierarchical-representation" id="toc-hierarchical-representation" class="nav-link" data-scroll-target="#hierarchical-representation"><span class="header-section-number">31.2.1</span> Hierarchical representation</a></li>
  <li><a href="#graphical-representation" id="toc-graphical-representation" class="nav-link" data-scroll-target="#graphical-representation"><span class="header-section-number">31.2.2</span> Graphical representation</a></li>
  </ul></li>
  <li><a href="#sec-c2l02-posterior-derivation" id="toc-sec-c2l02-posterior-derivation" class="nav-link" data-scroll-target="#sec-c2l02-posterior-derivation"><span class="header-section-number">31.3</span> Posterior derivation 🎥</a></li>
  <li><a href="#sec-c2l02-non-conjugate-models" id="toc-sec-c2l02-non-conjugate-models" class="nav-link" data-scroll-target="#sec-c2l02-non-conjugate-models"><span class="header-section-number">31.4</span> Non-conjugate models 🎥</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<section id="sec-c2l02-components" class="level2 page-columns page-full" data-number="31.1">
<h2 data-number="31.1" class="anchored" data-anchor-id="sec-c2l02-components"><span class="header-section-number">31.1</span> Components of a Bayesian Model 🎥</h2>

<div class="no-row-height column-margin column-container"><div id="fig-c2l02-ss-01" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-c2l02-ss-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l02-ss-01.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;31.1: a Bayesian Model"><img src="images/c2l02-ss-01.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-c2l02-ss-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31.1: a Bayesian Model
</figcaption>
</figure>
</div></div><p><span id="eq-c2l02-hierarcial-model-definition"><span class="math display">
\begin{aligned}
y_i \mid \mu,\sigma &amp;\stackrel{iid}{\sim} \mathcal{N}(\mu,\sigma^2) \\
\mu &amp;\sim \mathcal{N}(\mu_0,\sigma_0^2) \\
\sigma^2 &amp;\sim \mathcal{IG}(\alpha_0,\beta_0)
\end{aligned}
\tag{31.1}</span></span></p>
<p>In lesson one, we defined <mark><strong>a statistical model</strong> as <em>a mathematical structure used to imitate or approximate the data generating process</em>.</mark> It incorporates uncertainty and variability using the theory of probability. A model could be very simple, involving only one variable.</p>
<div id="exm-heights-of-men" class="theorem example page-columns page-full">
<div class="page-columns page-full"><p><span class="theorem-title"><strong>Example 31.1 (heights of men)</strong></span>  Suppose our data consists of the heights of <span class="math inline">N=15</span> adult men. Clearly it would be very expensive or even impossible to collect the genetic information that fully explains the variability in these men’s heights. We only have the height measurements available to us. To account for the variability, we might assume that the men’s heights follow a normal distribution.</p><div class="no-row-height column-margin column-container"><span class="">heights of men</span></div></div>
<p>So we could write the model like this: <span class="math display">
y_i= \mu + \varepsilon_i
</span></p>
<ul>
<li>where:
<ul>
<li><span class="math inline">y_i</span> will represent the height for person <span class="math inline">i</span>.</li>
<li><span class="math inline">i</span> will be our index.</li>
<li><span class="math inline">\mu</span> is a constant that represents the mean for all men.</li>
<li><span class="math inline">\varepsilon_i</span>, the individual error term for individual <span class="math inline">i</span>.</li>
</ul></li>
</ul>
<p>We are also going to assume that these uncertainties <span class="math inline">\varepsilon_i</span> are drawn independently from an the same normal distribution. We can make this more precise if we specify that the <span class="math inline">\varepsilon_i</span> a drawn from a normal distribution with mean zero and variance <span class="math inline">\sigma^2</span>.</p>
<p><span class="math display">
\varepsilon_i \stackrel{iid}\sim \mathcal{N}(0,\sigma^2) \quad  i\in 1 \dots N
</span></p>
<ul>
<li>where:
<ul>
<li><span class="math inline">i</span> equal to 1 up to <span class="math inline">N</span> which will be 15 in our case.</li>
</ul></li>
</ul>
<p>Equivalently<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> we could rewrite this model expressing the variability for the <span class="math inline">y_i</span> directly as:</p>
<p><span class="math display">
y_i \stackrel{iid}\sim \mathcal{N}(\mu,\sigma^2) \quad i \in 1 \dots N
</span></p>
<p>What we did is substitute Normal RV of <span class="math inline">\varepsilon</span> then push in the mean <span class="math inline">\mu</span> into the RV. Which is shown in the example below. So each <span class="math inline">y_i</span> is drawn independently from identically distributed RV with a Normal distribution parameterized with mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma^2</span>. This specifies a probability distribution and a model for the data.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>heights of men
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">
\begin{aligned}
y_i&amp;= \mu+\varepsilon_i,
\\ \varepsilon_i &amp;\stackrel{iid}\sim \mathcal{N}(0,\sigma^2)
\end{aligned}
</span> another way to write this if we know the values of <span class="math inline">\mu</span> and <span class="math inline">\sigma</span>:</p>
<p><span class="math display">
\begin{aligned}
y_i &amp;\stackrel{iid}\sim \mathcal{N}(\mu,\sigma^2)
\end{aligned}
</span></p>
</div>
</div>
<p>It also suggests how we might generate more fake data that behaves similarly to our original data set.</p>
</div>
<p>A model can be as simple as the one right here or as complicated and sophisticated as we need to capture the behavior of the data. So far, this model is the same for Frequentists and Bayesians.</p>
<p>As you may recall from the previous course, the <em>frequentist</em> approach to fitting this model would be to consider <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> to be fixed but unknown constants, and then we would estimate them. To calculate our uncertainty in those estimates a frequentist approach would consider how much the estimates of <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> might change if we were to repeat the sampling process and obtain another sample of 15 men, over, and over.</p>

<div class="no-row-height column-margin column-container"><div id="fig-heights-of-men" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-heights-of-men-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l02-ss-02.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;31.2: Components of a Bayesian Model"><img src="images/c2l02-ss-02.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-heights-of-men-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31.2: Components of a Bayesian Model
</figcaption>
</figure>
</div></div><div class="page-columns page-full"><p>The Bayesian approach, which we will follow in this class, treats our uncertainty in <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span> using probabilities directly. We will model them as  <em>random variables</em> with their own probability distributions. These are often called <strong>priors</strong>, and they complete a Bayesian model.</p><div class="no-row-height column-margin column-container"><span class="">uncertainty <span class="math inline">\to</span> random variables</span></div></div>
<p>In the rest of this segment, we’re going to review the three key components of Bayesian models, that were used extensively in the previous course. These three primary components of Bayesian models which we often work with are:</p>
<ul>
<li><span class="math inline">\mathbb{P}r(y\mid \theta)</span> <strong>the likelihood</strong> of the data,</li>
<li><span class="math inline">\mathbb{P}r(\theta)</span> <strong>the prior</strong> distribution for the parameters,</li>
<li><span class="math inline">\mathbb{P}r(\theta \mid y)</span> <strong>the posterior</strong> distribution for the parameters given the data</li>
</ul>
<p><span class="math display">
\mathbb{P}r(y\mid \theta)\ \text{(likelihood)}
</span></p>
<div class="page-columns page-full"><p> <strong>The likelihood</strong> is the probabilistic model for the <em>data</em>. It describes how, given the unknown parameters, the data might be generated. We’re going to call unknown parameter theta right here. Also, in this expression, you might recognize this from the previous class, as describing a probability distribution.</p><div class="no-row-height column-margin column-container"><span class="">The likelihood</span></div></div>
<p><span class="math display">
\mathbb{P}r(\theta)\ \text{(prior)}
</span></p>
<div class="page-columns page-full"><p> <strong>The prior</strong>, the next step, is the probability distribution that characterizes our uncertainty with the parameter theta. We’re going to write it as <span class="math inline">\mathbb{P}r(\theta)</span>. It’s not the same distribution as this one. We’re just using this notation <span class="math inline">\mathbb{P}r</span> to represent the probability distribution of <span class="math inline">\theta</span>.</p><div class="no-row-height column-margin column-container"><span class="">The prior</span></div></div>
<div class="page-columns page-full"><p> By specifying a likelihood and a prior. We get have a <strong>joint probability model</strong> for both the knowns, i.e.&nbsp;the data, and the unknowns, i.e.&nbsp;the parameters <span class="math inline">\theta</span>.</p><div class="no-row-height column-margin column-container"><span class="">joint probability distribution</span></div></div>
<p><span class="math display">
\mathbb{P}r(y,\theta) = \underbrace{\mathbb{P}r(\theta)}_{prior} \cdot
                        \underbrace{\mathbb{P}r(y \mid \theta)}_{likelihood}
                        \qquad \text{(joint probability)}
</span></p>
<div class="page-columns page-full"><p>We can see this by using the chain rule of probability. If we wanted the joint distribution of both the data and the parameters theta. Using the chain rule of probability, we could start with the distribution of <span class="math inline">\theta</span> and multiply that by the probability of <span class="math inline">y \mid \theta</span>. That gives us an expression for the joint distribution. <strong>However if we’re going to make inferences about data and we already know the values of</strong> <span class="math inline">y</span>, we don’t need the <em>joint distribution</em>, what we need is the <em>posterior distribution</em>. </p><div class="no-row-height column-margin column-container"><span class="">posterior distribution</span></div></div>
<p><span class="math display">
\mathbb{P}r(\theta \mid y)\ \text{(posterior)}
</span></p>
<p>The <strong>posterior distribution</strong> is the distribution of <span class="math inline">\mathbb{P}r(\theta \mid y)</span>, i.e.&nbsp;<span class="math inline">\theta</span> given <span class="math inline">y</span>. We can obtain this expression using the laws of conditional probability and specifically using Bayes’ theorem.</p>
<p><span class="math display">
\begin{aligned}
\mathbb{P}r(\theta \mid y) &amp;= \frac{\mathbb{P}r(\theta,y)}{\mathbb{P}r(y)}
\\ &amp;= \frac{\mathbb{P}r(\theta,y)}{\int \mathbb{P}r(\theta,y)}
\\ &amp;= \frac{\mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)}{\int \mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)\ d\theta}
\end{aligned}
</span></p>
<p>We start with the definition of conditional probability (1). The conditional distribution, <span class="math inline">\mathbb{P}r(\theta \mid y)</span> is the ratio of the <em>joint distribution</em> of <span class="math inline">\theta</span> and <span class="math inline">y</span>, i.e.&nbsp;<span class="math inline">\mathbb{P}r(\theta,y)</span>; with the <em>marginal distribution</em> of <span class="math inline">y</span>, <span class="math inline">\mathbb{P}r(y)</span>.</p>
<div class="page-columns page-full"><p> We start with the <strong>joint distribution</strong> like we have on top, and we <em>integrate out</em> or <em>marginalize</em> over the values of theta (2)</p><div class="no-row-height column-margin column-container"><span class="">How do we get the marginal distribution of y?</span></div></div>
<p>To make this look like the Bayes theorem that we’re familiar with the joint distribution can be rewritten as the product of the prior and the likelihood. We start with the likelihood, because that’s how we usually write Bayes’ theorem. We have the same thing in the denominator here. But we’re going to integrate over the values of theta. These integrals are replaced by summations if we know that <span class="math inline">\theta</span> is a discrete random variable. The <em>marginal distribution</em> is another important piece which we may use when we more advanced Bayesian modeling.</p>
<p>The <strong>posterior distribution</strong> is our primary tool for achieving the statistical modeling objectives from lesson one.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Anatomy of a posterior probability
</div>
</div>
<div class="callout-body-container callout-body">
<p><span id="eq-posterior-anatomy"><span class="math display">
  \begin{aligned}
  &amp;\mathbb{P}r(y\mid \theta) &amp;&amp; (likelihood) \\
&amp;  \mathbb{P}r(\theta) &amp;&amp; (prior) \\
   \mathbb{P}r(y,\theta) &amp;= \mathbb{P}r(\theta)\mathbb{P}r(y|\theta) &amp;&amp;(joint\ distribution) \\
   \mathbb{P}r(\theta \mid y) &amp;= \frac{\mathbb{P}r(\theta,y)}{\mathbb{P}r(y)} &amp;&amp; (conditional\ probability) \\
&amp;= \frac{\mathbb{P}r(\theta,y)}{\int \mathbb{P}r(\theta,y)} \\
&amp;= \frac{\mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)}{\int \mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)\ d\theta} \\
\end{aligned}
\tag{31.2}</span></span></p>
</div>
</div>
<div id="qst-question">
<p>Whereas non-Bayesian approaches consider a probability model for the data only, the hallmark characteristic of Bayesian models is that they specify a joint probability distribution for both data <em>and</em> parameters. How does the Bayesian paradigm leverage this additional assumption?</p>
</div>
<div id="solution">
<ul class="task-list">
<li><label><input type="checkbox"><strong>This allows us to make probabilistic assessments about how likely our particular data outcome is under any parameter setting.</strong></label></li>
<li><label><input type="checkbox"><strong>This allows us to select the most accurate prior distribution.</strong></label></li>
<li><label><input type="checkbox"><strong>This allows us to make probabilistic assessments about hypothetical data outcomes given particular parameter values.</strong></label></li>
<li><label><input type="checkbox" checked=""><strong>This allows us to use the laws of conditional probability to describe our updated information about parameters given the data.</strong></label></li>
</ul>
</div>
</section>
<section id="sec-c2l02-model-specification" class="level2 page-columns page-full" data-number="31.2">
<h2 data-number="31.2" class="anchored" data-anchor-id="sec-c2l02-model-specification"><span class="header-section-number">31.2</span> Model Specification 🎥</h2>

<div class="no-row-height column-margin column-container"><div id="fig-c2l02-ss-05" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-c2l02-ss-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l02-ss-05.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;31.3: Model specification"><img src="images/c2l02-ss-05.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-c2l02-ss-05-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31.3: Model specification
</figcaption>
</figure>
</div></div><p>Before fitting any model we first need to specify all of its components.</p>
<div id="cell-fig-graphical-model" class="cell" data-fig-dpi="300" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-graphical-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graphical-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C2-L02_files/figure-html/fig-graphical-model-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;31.4: The graphical model specification for the height model"><img src="C2-L02_files/figure-html/fig-graphical-model-output-1.png" width="697" height="520" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graphical-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31.4: The graphical model specification for the height model
</figcaption>
</figure>
</div>
</div>
</div>
<section id="hierarchical-representation" class="level3" data-number="31.2.1">
<h3 data-number="31.2.1" class="anchored" data-anchor-id="hierarchical-representation"><span class="header-section-number">31.2.1</span> Hierarchical representation</h3>
<p>One convenient way to do this is to write down the <strong>hierarchical form of the model</strong>. By hierarchy, we mean that the model is specified in steps or in layers. We usually start with the model for the data directly, or the likelihood. Let’s write, again, the model from the previous lesson.</p>
<p>We had the height for person i, given our parameters <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, so conditional on those parameters, <span class="math inline">y_i</span> came from a normal distribution that was independent and identically distributed, where the normal distribution has mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma^2</span>, and we’re doing this for individuals 1 up to N, which was 15 in this example.</p>
<p><span class="math display">y_i \mid \mu,\sigma^2 \stackrel{iid}\sim N(\mu,\sigma^2) \qquad \forall \ i \in 1,\dots,15
</span></p>
<p>The next level that we need is the prior distribution from <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>. For now we’re going to say that they’re independent priors. So that our prior from <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span> is going to be able to factor Into the product of two independent priors.</p>
<p><span class="math display">
\mathbb{P}r(\mu,\sigma^2)~=~\mathbb{P}r(\mu)\mathbb{P}r(\sigma^2)\qquad (independence)
</span></p>
<p>We can assume independents in the prior and still get dependents in the posterior distribution.</p>
<p>In the previous course we learned that <em>the conjugate prior</em> for <span class="math inline">\mu</span>, if we know the value of <span class="math inline">\sigma^2</span>, is a <em>normal distribution</em>, and that the conjugate prior for <span class="math inline">\sigma^2</span> when <span class="math inline">\mu</span> is known is the <em>Inverse Gamma distribution</em>.</p>
<p>Let’s suppose that our prior distribution for <span class="math inline">\mu</span> is a normal distribution where mean will be <span class="math inline">\mu_0</span>.</p>
<p><span class="math display">
\mu \sim \mathcal{N}(\mu_0,\sigma^2_0)
</span></p>
<p>This is just some number that you’re going to fill in here when you decide what the prior should be. Mean <span class="math inline">\mu_0</span>, and less say <span class="math inline">\sigma^2_0</span> would be the variance of that prior.</p>
<p>The prior for <span class="math inline">\sigma^2</span> will be Inverse Gamma</p>
<p><span class="math display">
\sigma^2 \sim \mathcal{IG}(\nu_0,\beta_0)
</span></p>
<ul>
<li>which has two parameters:
<ul>
<li>a <em>shape parameter</em>, <span class="math inline">\nu_0</span>, and</li>
<li>a <em>scale parameter</em>, <span class="math inline">\beta_0</span>.</li>
</ul></li>
</ul>
<p>We need to choose values for these hyper-parameters here. But we do now have a complete Bayesian model.</p>
<p>We now introduce some new ideas that were not presented in the previous course.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Hierarchical representation
</div>
</div>
<div class="callout-body-container callout-body">
<p>By hierarchy, we mean that the model is specified in steps or in layers.</p>
<ul>
<li>start with the model for the data, or the likelihood.</li>
<li>write the priors</li>
<li>add hyper-priors for the parameters of the priors.</li>
</ul>
<p>More details can be seen on this <a href="https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling">wikipedia article</a> and on this <a href="https://en.wikipedia.org/wiki/Multilevel_model">one</a></p>
</div>
</div>
</section>
<section id="graphical-representation" class="level3" data-number="31.2.2">
<h3 data-number="31.2.2" class="anchored" data-anchor-id="graphical-representation"><span class="header-section-number">31.2.2</span> Graphical representation</h3>
<p>Another useful way to write out this model Is using what’s called a <strong>graphical representation</strong>. To write a graphical representation, we’re going to do the reverse order, we’ll start with the priors and finish with the likelihood.</p>
<p>In the graphical representation we draw what are called nodes so this would be a node for mu. The <strong>circle</strong> means that the this is a random variable that has its own distribution. So <span class="math inline">\mu</span> with its prior will be represented with that. And then we also have <span class="math inline">\sigma^2</span>. The next part of a graphical model is showing the dependence on other variables. Once we have the parameters, we can generate the data.</p>
<p>For example we have <span class="math inline">y_1, \dots y_n</span>. These are also random variables, so we’ll create these as nodes. And I’m going to double up the circle here to indicate that these nodes are observed, you see them in the data. So we’ll do this for all of the <span class="math inline">y_i</span> here. And to indicate the dependence of the distributions of the <span class="math inline">y_i</span> on <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, we’re going to draw arrows. So <span class="math inline">\mu</span> influences the distribution of <span class="math inline">y</span> for each one of these <span class="math inline">y_i</span>. The same is true for sigma squared, the distribution of each <span class="math inline">y</span> depends on the distribution of <span class="math inline">\sigma^2</span>. Again, these nodes right here, that are double-circled, mean that they’ve been observed. If they’re shaded, which is the usual case, that also means that they’re observed. The arrows indicate the dependence between the random variables and their distributions.</p>
<p>Notice that in this hierarchical representation, I wrote the dependence of the distributions also. We can simplify the graphical model by writing exchangeable random variables and I’ll define exchangeable later.</p>
<p>We’re going to write this using a representative of the <span class="math inline">y_i</span> here on what’s called the <strong>plate</strong>. So I’m going to re draw this hierarchical structure, we have <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>. And we don’t want to have to write all of these notes again. So I’m going to indicate that there are n of them, and I’m just going to draw one representative, <span class="math inline">y_i</span>. And they depend on <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>. To write a model like this, we must assume that the <span class="math inline">y_i</span> are <em>exchangeable</em>. That means that the distribution for the <span class="math inline">y_i</span> does not change if we were to switch the index label like the <span class="math inline">i</span> on the <span class="math inline">y</span> there. So, if for some reason, we knew that one of the <span class="math inline">y_i</span> was different from the other <span class="math inline">y_i</span> in its distribution, and if we also know which one it is, then we would need to write a separate node for it and not use a plate like we have here.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Graphical representation
</div>
</div>
<div class="callout-body-container callout-body">
<div id="cell-fig-graphical-model-posterior" class="cell" data-fig-dpi="300" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-graphical-model-posterior" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graphical-model-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C2-L02_files/figure-html/fig-graphical-model-posterior-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;31.5: pgm-posterior"><img src="C2-L02_files/figure-html/fig-graphical-model-posterior-output-1.png" width="697" height="520" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graphical-model-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31.5: pgm-posterior
</figcaption>
</figure>
</div>
</div>
</div>
<p>In the graphical representation we start at the top by drawing:</p>
<ul>
<li>circle nodes for the hyperparameters.</li>
<li>arrows indicating that they determine the</li>
<li>nodes for the priors.</li>
<li>nodes for the RVs (doubled circles)</li>
<li>plates (rectangles) indicating RVs that are exchangeable. We add an index to the corner of the plate to indicate the amount of replicated RVs</li>
</ul>
<p>More details can be seen on this <a href="https://en.wikipedia.org/wiki/Plate_notation">wikipedia article</a></p>
</div>
</div>
<p>Both the hierarchical and graphical representations show how you could hypothetically <strong>simulate data</strong> from this model. You start with the variables that don’t have any dependence on any other variables. You would simulate those, and then given those draws, you would simulate from the distributions for these other variables further down the chain.</p>
<p>This is also how you might simulate from a prior predictive distribution.</p>
</section>
</section>
<section id="sec-c2l02-posterior-derivation" class="level2 page-columns page-full" data-number="31.3">
<h2 data-number="31.3" class="anchored" data-anchor-id="sec-c2l02-posterior-derivation"><span class="header-section-number">31.3</span> Posterior derivation 🎥</h2>

<div class="no-row-height column-margin column-container"><div id="fig-c2l02-ss-04" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-c2l02-ss-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l02-ss-04.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;31.6: Posterior derivation"><img src="images/c2l02-ss-04.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-c2l02-ss-04-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31.6: Posterior derivation
</figcaption>
</figure>
</div></div><p> So far, we’ve only drawn the model with two levels. But in reality, there’s nothing that will stop us from diving deeper into the rabbit hole and adding more layers.</p>
<p>For example, instead of fixing the values for the hyperparameters in the previous segment recall those hyperparameters were the <span class="math inline">\mu_0</span>, the <span class="math inline">\sigma_0</span>, the <span class="math inline">\nu_0</span> and the <span class="math inline">\beta_0</span>, we could either specify fixed numeric values for those, or we could try to infer them from the data and model them using additional prior distributions for those variables to make this a hierarchical model.</p>
<p>A good reason to construct the model hierarchically is if the data generating process is organized in levels so that groups of observations generated at a certain level are more naturally grouped e.g there is greater similarity within groups than between groups together for each subsequent levels. We will examine these types of hierarchical models in depth later in the course. Another simple example of a hierarchical model is one you saw already in the previous course c.f. <a href="C1-L10.html#sec-normal-likelihood-with-expectation-and-variance-unknown" class="quarto-xref"><span>Section 23.2</span></a></p>
<p>Back to our model:</p>
<ol type="1">
<li>At the top level are the observation: <span class="math inline">y_i \mid \mu,\sigma^2</span>. This is just like the model from the previous lesson, where the observations were from independent and identically distributed normal RV with a mean <span class="math inline">\mu</span> and a variance, <span class="math inline">\sigma^2</span>.</li>
<li>At the next level we diverge, instead of having independent priors for <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, we’re going to have the prior for <span class="math inline">\mu</span> depend on the value of <span class="math inline">\sigma^2</span>. That is given <span class="math inline">\sigma^2</span>, <span class="math inline">\mu</span> follows a normal distribution with mean <span class="math inline">\mu_0</span>, just some hyperparameter that you’re going to chose. And the variance of this prior will be <span class="math inline">\sigma^2</span>, this parameter, divided by <span class="math inline">\omega_0</span>. Another hyperparameter that will scale it.</li>
</ol>
<p><span id="eq-c2l02-hierarchical-model-definition-2"><span class="math display">
\begin{aligned}
y_i \mid \mu,\sigma^2 &amp;\stackrel{iid}{\sim} \mathcal{N}(\mu,\sigma^2) \\
\mu \mid \sigma^2 &amp;\sim \mathcal{N}(\mu_0,\frac{\sigma^2}{\omega_0}) \\
\sigma^2 \mid  &amp;\sim \mathcal{IG}(\nu_0,\beta_0)
\end{aligned}
\tag{31.3}</span></span></p>
<ul>
<li>where:
<ul>
<li><span class="math inline">\mu_0</span> is the mean of the prior for <span class="math inline">\mu</span>,</li>
<li><span class="math inline">\omega_0</span> is a scaling factor for the variance of the prior for <span class="math inline">\mu</span>,</li>
<li><span class="math inline">\nu_0</span> and <span class="math inline">\beta_0</span> are the shape and scale parameters of the inverse gamma prior for <span class="math inline">\sigma^2</span>.</li>
</ul></li>
</ul>
<p>We now have a joint distribution of <span class="math inline">y \mid \mu</span> and <span class="math inline">\mu \mid \sigma^2</span> To complete this model we need to provide a prior for <span class="math inline">\sigma^2</span>.</p>
<ol start="3" type="1">
<li>We’ll just use the standard Inverse-Gamma with the same hyperparameters as last time.</li>
</ol>
<p>The graphical representation for this model looks like this:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Graphical representation
</div>
</div>
<div class="callout-body-container callout-body">
<div id="cell-fig-graphical-model-posterior-2" class="cell" data-fig-dpi="300" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-graphical-model-posterior-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graphical-model-posterior-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C2-L02_files/figure-html/fig-graphical-model-posterior-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;31.7: pgm-posterior-2"><img src="C2-L02_files/figure-html/fig-graphical-model-posterior-2-output-1.png" width="449" height="756" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graphical-model-posterior-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31.7: pgm-posterior-2
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<ul>
<li>We start with the variables that don’t depend on anything else.
<ol type="1">
<li>That would be <span class="math inline">\sigma^2</span> and move down the chain.</li>
<li>The next variable is <span class="math inline">\mu</span>, which depends on <span class="math inline">\sigma^2</span>.</li>
<li>Then we have the <span class="math inline">y_i</span> dependent on both, We use a double or filled circle because the <span class="math inline">y_i</span>’s are observed, their data, and we’re going to assume that they’re exchangeable.</li>
<li>We place the <span class="math inline">y_i</span>’s on a plate for <span class="math inline">i \in 1 \dots N</span>.</li>
<li>The distribution of <span class="math inline">y_i</span> depends on both <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, so we’ll draw curves connecting those pieces there.</li>
</ol></li>
</ul>
<p><span class="math display">
\begin{aligned}
p(\mu \mid y_i \dots y_n) &amp;\propto \prod_{i=1}^{n} \left[\frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y_i - \mu)^2}{2 \sigma^2}\right)\right] \cdot \frac{1}{\pi(1 + \mu^2)} \\
&amp;\propto   \exp \left[\left(-\frac{(y_i - \mu)^2}{2 \sigma^2}\right) \right ] \cdot \frac{1}{(1 + \mu^2)} \\
&amp;\propto \frac{ \exp \left[ n (\bar{y} - \frac{\mu^2}2{}) \right] }{1 + \mu^2}
\end{aligned}
</span></p>
<p>To simulate hypothetical data from this model, we would have to first draw from the distribution of the prior for <span class="math inline">\sigma^2</span>. Then the distribution for mu which depends on <span class="math inline">\sigma^2</span>. And once we’ve drawn both of these, then we can draw random draws from the <span class="math inline">y_i</span>, which of course depends on both of those. With multiple levels, this is an example of a hierarchical model. Once we have a model specification, we can write out what the full posterior distribution for all the parameters given the data looks like. Remember that the numerator in Bayes’ theorem is the joint distribution of all random quantities, all the nodes in this graphical representation over here from all of the layers. So for this model that we have right here, we have a joint distribution that’ll look like this. We’re going to write the joint distribution of everything <span class="math inline">y_1:n</span>, <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, using the <em>chain rule of probability</em>, we’re going to multiply all of the distributions in the hierarchy together. So let’s start with the likelihood piece. And we’ll multiply that by the next layer, the distribution of mu, given <span class="math inline">\sigma^2</span>. And finally, with the prior for sigma squared. So what do these expressions right here look like?</p>
<p>The likelihood in this level because they’re all independent will be a product of normal densities. So we’re going to multiply the normal density for each <span class="math inline">y_i</span>, given those parameters. This, again, is shorthand right here for the density of a normal distribution. So that represents this piece right here. The conditional prior of <span class="math inline">\mu</span> given sigma squared is also a normal. So we’re going to multiply this by a normal distribution of <span class="math inline">\mu</span>, where its parameters are <span class="math inline">\mu</span> naught and sigma squared over omega naught. And finally, we have the prior for sigma squared. We’ll multiply by the density of an inverse gamma for <span class="math inline">\sigma^2</span> given the hyper parameters <span class="math inline">\mu</span> naught, sorry, that is given, the hyper parameters <span class="math inline">\mu</span> naught and and beta naught. What we have right here is the joint distribution of everything. It is the numerator in Bayes theorem. Let’s remind ourselves really fast what Bayes theorem looks like again. We have that the posterior distribution of the parameter given the data is equal to the likelihood, Times the prior. Over the same thing again. So this gives us in the numerator the joint distribution of everything which is what we’ve written right here.</p>
<p>In Bayes theorem, the numerator and the denominator are the exact same expression accept that we integrate or marginalize over all of the parameters.</p>
<p>Because the denominator is a function of the y’s only, which are known values, the denominator is just a constant number. So we can actually write the posterior distribution as being proportional to, this symbol right here represents proportional to. The joint distribution of the data and parameters, or the likelihood times the prior. The poster distribution is proportional to the joint distribution, or everything we have right here. In other words, what we have already written for this particular model is proportional to the posterior distribution of <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, given all of the data. The only thing missing in this expression right here is just some constant number that causes the expression to integrate to 1. If we can recognize this expression as being proportional to a common distribution, then our work is done, and we know what our posterior distribution is. This was the case for all models in the previous course. However, if we do not use conjugate priors or if the models are more complicated, then the posterior distribution will not have a standard form that we can recognize. We’re going to explore a couple of examples of this issue in the next segment.</p>
</section>
<section id="sec-c2l02-non-conjugate-models" class="level2 page-columns page-full" data-number="31.4">
<h2 data-number="31.4" class="anchored" data-anchor-id="sec-c2l02-non-conjugate-models"><span class="header-section-number">31.4</span> Non-conjugate models 🎥</h2>

<div class="no-row-height column-margin column-container"><div id="fig-c2l02-ss-03" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="slides">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-c2l02-ss-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l02-ss-03.png" class="lightbox" data-gallery="slides" title="Figure&nbsp;31.8: Non-conjugate models"><img src="images/c2l02-ss-03.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-c2l02-ss-03-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31.8: Non-conjugate models
</figcaption>
</figure>
</div></div><p>We’ll first look at an example of a one parameter model that is not conjugate.</p>
<div id="exp-company-personnel">
<section id="company-personnel" class="level4" data-number="31.4.0.1">
<h4 data-number="31.4.0.1" class="anchored" data-anchor-id="company-personnel"><span class="header-section-number">31.4.0.1</span> Company Personnel</h4>
<p>Suppose we have values that represent the percentage change in total personnel from last year to this year for, we’ll say, ten companies. These companies come from a particular industry. We’re going to assume for now, that these are independent measurements from a normal distribution with a known variance equal to one, but an unknown mean.</p>
<p>So we’ll say the percentage change in the total personnel for company I, given the unknown mean <span class="math inline">\mu</span> will be distributed normally with mean <span class="math inline">\mu</span>, and we’re just going to use variance 1.</p>
<p>In this case, the unknown mean could represent growth for this particular industry.</p>
<p>It’s the average of the growth of all the different companies. The small variance between the companies and percentage growth might be appropriate if the industry is stable.</p>
<p>We know that the <strong>conjugate</strong> <strong>prior</strong> for <span class="math inline">\mu</span> in this location would be a <strong>normal distribution</strong>.</p>
<p>But suppose we decide that our prior believes about <span class="math inline">\mu</span> are better reflected using a <strong>standard t distribution</strong> with <strong>one degree of freedom</strong>. So we could write that as the prior for <span class="math inline">\mu</span> is a t distribution with a location parameter 0. That’s where the center of the distribution is. A scale parameter of 1 to make it the <strong>standard t-distribution</strong> similar to a standard normal, and 1 degree of freedom.</p>
<p>This particular prior distribution has heavier tails than the conjugate and normal distribution, which can more easily accommodate the possibility of extreme values for <span class="math inline">\mu</span>. It is centered on zero so, that apriori, there is a 50% chance that the growth is positive and a 50% chance that the growth is negative.</p>
</section>
</div>
<p>Recall that the posterior distribution of <span class="math inline">\mu</span> is proportional to the likelihood times the prior. Let’s write the expression for that in this model. That is the posterior distribution for <span class="math inline">\mu</span> given the data <span class="math inline">y_1 \dots y_n</span> is going to be proportional to the likelihood.</p>
<p>It is a product from i equals 1 to n, in this case that’s 10.</p>
<p>Densities from a normal distribution.</p>
<p>Let’s write the density from this particular normal distribution.</p>
<p>Is <span class="math inline">1 \over \sqrt{2 \pi}</span>.</p>
<p>E to the negative one-half.</p>
<p><span class="math inline">y_i - \mu^2</span>, this is the normal density for each individual <span class="math inline">y_i</span> and we multiplied it for likelihood.</p>
<p>The density for this t prior looks like this.</p>
<p>It’s 1 over pi times 1 plus <span class="math inline">\mu</span> squared.</p>
<p>This is the likelihood times the prior.</p>
<p>If we do a little algebra here, first of all, we’re doing this up to proportionality.</p>
<p>So, constants being multiplied by this expression are not important.</p>
<p>The <span class="math inline">\sqrt{2 \pi}^n</span>, is just a constant number, and <span class="math inline">\pi</span> creates a constant number. So we will drop them in our next step.</p>
<p>So this is now proportional too, we’re removing this piece and now we’re going to use properties of exponents.</p>
<p>The product of exponents is the sum of the exponentiated pieces.</p>
<p>So we have the exponent of negative one-half times the sum from i equals 1 to n, of Yi minus <span class="math inline">\mu</span> squared.</p>
<p>And then we’re dropping the pie over here, so times 1 plus <span class="math inline">\mu</span> squared.</p>
<p>We’re going to do a few more steps of algebra here to get a nicer expression for this piece.</p>
<p>But we’re going to skip ahead to that.</p>
<p>We’ve now added these last two expressions.</p>
<p>To arrive at this expression here for the posterior, or what’s proportional to the posterior distribution.</p>
<p>This expression right here is almost proportional to a normal distribution except we have this 1 plus <span class="math inline">\mu</span> squared term in the denominator.</p>
<p>We know the posterior distribution up to a constant but we don’t recognize its form as a standard distribution.</p>
<p>That we can integrate or simulate from, so we’ll have to do something else.</p>
<p>Let’s move on to our second example. For a two parameter example, we’re going to return to the case where we have a normal likelihood.</p>
<p>And we’re now going to estimate <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, because they’re both unknown.</p>
<p>Recall that if <span class="math inline">\sigma^2</span> were known, the conjugate prior from <span class="math inline">\mu</span> would be a normal distribution.</p>
<p>And if <span class="math inline">\mu</span> were known, the conjugate prior we could choose for <span class="math inline">\sigma^2</span> would be an inverse gamma.</p>
<p>We saw earlier that if you include <span class="math inline">\sigma^2</span> in the prior for <span class="math inline">\mu</span>, and use the hierarchical model that we presented earlier, that model would be conjugate and have a closed form solution. However, in the more general case that we have right here, the posterior distribution does not appear as a distribution that we can simulate or integrate.</p>
<p>Challenging posterior distributions like these ones and most others that we’ll encounter in this course kept Bayesian methods from entering the main stream of statistics for many years. Since only the simplest problems were tractable. However, computational methods invented by physicists in the 1950’s, and implemented by statisticians decades later, revolutionized the field. We now do have the ability to simulate from the posterior distributions from this lesson, as well as for many other more complicated models.</p>


<!-- -->

</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>via the linearity of the normal distribution in the mean<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/⌘/g, '⌃');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C2-L01.html" class="pagination-link" aria-label="Statistical Modeling - M1L1">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C2-L03.html" class="pagination-link" aria-label="Monte Carlo estimation - M1L3">
        <span class="nav-page-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "M1L2 - Bayesian Modeling"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Bayesian Statistics: Techniques and Models"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">  - Bayesian Statistics</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - Monte Carlo Estimation</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - Statistical Modeling</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Components of a Bayesian Model 🎥 {#sec-c2l02-components}</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="al">![a Bayesian Model](images/c2l02-ss-01.png)</span>{#fig-c2l02-ss-01 .column-margin group="slides" width="53mm"}</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>y_i \mid \mu,\sigma &amp;\stackrel{iid}{\sim} \mathcal{N}(\mu,\sigma^2) <span class="sc">\\</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>\mu &amp;\sim \mathcal{N}(\mu_0,\sigma_0^2) <span class="sc">\\</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>\sigma^2 &amp;\sim \mathcal{IG}(\alpha_0,\beta_0)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>$$ {#eq-c2l02-hierarcial-model-definition}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>In lesson one, we defined <span class="co">[</span><span class="ot">**a statistical model** as *a mathematical structure used to imitate or approximate the data generating process*.</span><span class="co">]</span>{.mark} </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>It incorporates uncertainty and variability using the theory of probability. A model could be very simple, involving only one variable.</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>::: {#exm-heights-of-men}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## heights of men</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">heights of men</span><span class="co">]</span>{.column-margin} Suppose our data consists of the heights of $N=15$ adult men.</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>Clearly it would be very expensive or even impossible to collect the genetic information that fully explains the variability in these men's heights. </span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>We only have the height measurements available to us. </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>To account for the variability, we might assume that the men's heights follow a normal distribution.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>So we could write the model like this: </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>y_i= \mu + \varepsilon_i</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>where:</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$y_i$ will represent the height for person $i$. </span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$i$ will be our index. </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\mu$ is a constant that represents the mean for all men. </span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\varepsilon_i$, the individual error term for individual $i$.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>We are also going to assume that these uncertainties $\varepsilon_i$ are drawn independently from an the same normal distribution. </span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>We can make this more precise if we specify that the $\varepsilon_i$ a drawn from a normal distribution with mean zero and variance $\sigma^2$. </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>\varepsilon_i \stackrel{iid}\sim \mathcal{N}(0,\sigma^2) \quad  i\in 1 \dots N</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>where:</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$i$ equal to 1 up to $N$ which will be 15 in our case. </span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>Equivalently^<span class="co">[</span><span class="ot">via the linearity of the normal distribution in the mean</span><span class="co">]</span> we could rewrite this model expressing the variability for the $y_i$ directly as:</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>y_i \stackrel{iid}\sim \mathcal{N}(\mu,\sigma^2) \quad i \in 1 \dots N</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>What we did is substitute Normal RV of $\varepsilon$ then push in the mean $\mu$ into the RV. </span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>Which is shown in the example below.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>So each $y_i$ is drawn independently from identically distributed RV with a Normal distribution parameterized with mean $\mu$ and variance $\sigma^2$.</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>This specifies a probability distribution and a model for the data.</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>:::: {.callout-note}</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="fu">### heights of men {.unnumbered}</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>y_i&amp;= \mu+\varepsilon_i,</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \varepsilon_i &amp;\stackrel{iid}\sim \mathcal{N}(0,\sigma^2) </span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>another way to write this if we know the values of $\mu$ and $\sigma$:</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>y_i &amp;\stackrel{iid}\sim \mathcal{N}(\mu,\sigma^2)</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>It also suggests how we might generate more fake data that behaves similarly to our original data set.</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>A model can be as simple as the one right here or as complicated and sophisticated as we need to capture the behavior of the data. </span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>So far, this model is the same for Frequentists and Bayesians.</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>As you may recall from the previous course, the *frequentist* approach to fitting this model would be to consider $\mu$ and $\sigma$ to be fixed but unknown constants, and then we would estimate them. </span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>To calculate our uncertainty in those estimates a frequentist approach would consider how much the estimates of $\mu$ and $\sigma$ might change if we were to repeat the sampling process and obtain another sample of 15 men, over, and over.</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="al">![Components of a Bayesian Model](images/c2l02-ss-02.png)</span>{#fig-heights-of-men .column-margin group="slides" width="53mm"}</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>The Bayesian approach, which we will follow in this class, treats our uncertainty in $\mu$ and $\sigma^2$ using probabilities directly. We will model them as <span class="co">[</span><span class="ot">uncertainty $\to$ random variables</span><span class="co">]</span>{.column-margin} *random variables* with their own probability distributions. These are often called **priors**, and they complete a Bayesian model.</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>In the rest of this segment, we're going to review the three key components of Bayesian models, that were used extensively in the previous course. These three primary components of Bayesian models which we often work with are:</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}r(y\mid \theta)$ **the likelihood** of the data,</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}r(\theta)$ **the prior** distribution for the parameters,</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{P}r(\theta \mid y)$ **the posterior** distribution for the parameters given the data</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(y\mid \theta)\ \text{(likelihood)}</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">The likelihood</span><span class="co">]</span>{.column-margin} **The likelihood** is the probabilistic model for the *data*. It describes how, given the unknown parameters, the data might be generated. We're going to call unknown parameter theta right here. Also, in this expression, you might recognize this from the previous class, as describing a probability distribution.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta)\ \text{(prior)}</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">The prior</span><span class="co">]</span>{.column-margin} </span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>**The prior**, the next step, is the probability distribution that characterizes our uncertainty with the parameter theta. We're going to write it as $\mathbb{P}r(\theta)$. It's not the same distribution as this one. We're just using this notation $\mathbb{P}r$ to represent the probability distribution of $\theta$. </span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">joint probability distribution</span><span class="co">]</span>{.column-margin} </span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>By specifying a likelihood and a prior. We get have a  **joint probability model** for both the knowns, i.e. the data, and the unknowns, i.e. the parameters $\theta$. </span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(y,\theta) = \underbrace{\mathbb{P}r(\theta)}_{prior} \cdot</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>                        \underbrace{\mathbb{P}r(y \mid \theta)}_{likelihood}</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>                        \qquad \text{(joint probability)}</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>We can see this by using the chain rule of probability. If we wanted the joint distribution of both the data and the parameters theta. Using the chain rule of probability, we could start with the distribution of $\theta$ and multiply that by the probability of $y \mid \theta$. That gives us an expression for the joint distribution. **However if we're going to make inferences about data and we already know the values of** $y$, we don't need the *joint distribution*, what we need is the *posterior distribution*. <span class="co">[</span><span class="ot">posterior distribution</span><span class="co">]</span>{.column-margin}</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta \mid y)\ \text{(posterior)}</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>The **posterior distribution** is the distribution of $\mathbb{P}r(\theta \mid y)$, i.e. $\theta$ given $y$. We can obtain this expression using the laws of conditional probability and specifically using Bayes' theorem.</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta \mid y) &amp;= \frac{\mathbb{P}r(\theta,y)}{\mathbb{P}r(y)} </span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{\mathbb{P}r(\theta,y)}{\int \mathbb{P}r(\theta,y)}</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{\mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)}{\int \mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)\ d\theta}</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>We start with the definition of conditional probability (1). The conditional distribution, $\mathbb{P}r(\theta \mid y)$ is the ratio of the *joint distribution* of $\theta$ and $y$, i.e. $\mathbb{P}r(\theta,y)$; with the *marginal distribution* of $y$, $\mathbb{P}r(y)$.</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">How do we get the marginal distribution of y?</span><span class="co">]</span>{.column-margin} We start with the **joint distribution** like we have on top, and we *integrate out* or *marginalize* over the values of theta (2)</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>To make this look like the Bayes theorem that we're familiar with the joint distribution can be rewritten as the product of the prior and the likelihood. We start with the likelihood, because that's how we usually write Bayes' theorem. We have the same thing in the denominator here. But we're going to integrate over the values of theta. These integrals are replaced by summations if we know that $\theta$ is a discrete random variable. The *marginal distribution* is another important piece which we may use when we more advanced Bayesian modeling.</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>The **posterior distribution** is our primary tool for achieving the statistical modeling objectives from lesson one.</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="fu">### Anatomy of a posterior probability</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>  \begin{aligned}</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>  &amp;\mathbb{P}r(y\mid \theta) &amp;&amp; (likelihood) <span class="sc">\\</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a> &amp;  \mathbb{P}r(\theta) &amp;&amp; (prior) <span class="sc">\\</span></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>   \mathbb{P}r(y,\theta) &amp;= \mathbb{P}r(\theta)\mathbb{P}r(y|\theta) &amp;&amp;(joint\ distribution) <span class="sc">\\</span></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>   \mathbb{P}r(\theta \mid y) &amp;= \frac{\mathbb{P}r(\theta,y)}{\mathbb{P}r(y)} &amp;&amp; (conditional\ probability) <span class="sc">\\</span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a> &amp;= \frac{\mathbb{P}r(\theta,y)}{\int \mathbb{P}r(\theta,y)} <span class="sc">\\</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a> &amp;= \frac{\mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)}{\int \mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)\ d\theta} <span class="sc">\\</span></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>$$ {#eq-posterior-anatomy}</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>::: {#qst-question}</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>Whereas non-Bayesian approaches consider a probability model for the data only, the hallmark characteristic of Bayesian models is that they specify a joint probability distribution for both data *and* parameters. How does the Bayesian paradigm leverage this additional assumption?</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>::: {#solution}</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[ ]</span> **This allows us to make probabilistic assessments about how likely our particular data outcome is under any parameter setting.**</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[ ]</span> **This allows us to select the most accurate prior distribution.**</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[ ]</span> **This allows us to make probabilistic assessments about hypothetical data outcomes given particular parameter values.**</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[x]</span> **This allows us to use the laws of conditional probability to describe our updated information about parameters given the data.**</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Specification  🎥 {#sec-c2l02-model-specification}</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a><span class="al">![Model specification](images/c2l02-ss-05.png)</span>{#fig-c2l02-ss-05 .column-margin group="slides" width="53mm"}</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>Before fitting any model we first need to specify all of its components.</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-graphical-model</span></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: The graphical model specification for the height model</span></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-dpi: 300</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: true</span></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> daft <span class="im">as</span> daft</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>pgm <span class="op">=</span> daft.PGM(</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>  <span class="co">#[6, 3.2],</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>  node_unit<span class="op">=</span><span class="fl">1.0</span>,alternate_style<span class="op">=</span><span class="st">'outer'</span>,dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"mu"</span>, <span class="vs">r"</span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">0.0</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"sigma"</span>, <span class="vs">r"</span><span class="dv">$\s</span><span class="vs">igma</span><span class="dv">^</span><span class="vs">2</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">1.0</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"y"</span>, <span class="vs">r"</span><span class="dv">$</span><span class="vs">y_i</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, observed<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"mu"</span>, <span class="st">"y"</span>)<span class="op">;</span></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"sigma"</span>, <span class="st">"y"</span>)<span class="op">;</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>pgm.add_plate([<span class="op">-</span><span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">2.5</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">n = 1</span><span class="dv">...</span><span class="vs">N</span><span class="dv">$</span><span class="vs">"</span>, position<span class="op">=</span><span class="st">"bottom right"</span>)</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>pgm.render()</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hierarchical representation</span></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>One convenient way to do this is to write down the **hierarchical form of the model**. By hierarchy, we mean that the model is specified in steps or in layers. We usually start with the model for the data directly, or the likelihood. Let's write, again, the model from the previous lesson.</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>We had the height for person i, given our parameters $\mu$ and $\sigma^2$, so conditional on those parameters, $y_i$ came from a normal distribution that was independent and identically distributed, where the normal distribution has mean $\mu$ and variance $\sigma^2$, and we're doing this for individuals 1 up to N, which was 15 in this example. </span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>$$y_i \mid \mu,\sigma^2 \stackrel{iid}\sim N(\mu,\sigma^2) \qquad \forall \ i \in 1,\dots,15</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>The next level that we need is the prior distribution from $\mu$ and $\sigma^2$. For now we're going to say that they're independent priors. So that our prior from $\mu$ and $\sigma^2$ is going to be able to factor Into the product of two independent priors. </span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\mu,\sigma^2)~=~\mathbb{P}r(\mu)\mathbb{P}r(\sigma^2)\qquad (independence)</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>We can assume independents in the prior and still get dependents in the posterior distribution.</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>In the previous course we learned that *the conjugate prior* for $\mu$, if we know the value of $\sigma^2$, is a *normal distribution*, and that the conjugate prior for $\sigma^2$ when $\mu$ is known is the *Inverse Gamma distribution*.</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>Let's suppose that our prior distribution for $\mu$ is a normal distribution where mean will be $\mu_0$.</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>\mu \sim \mathcal{N}(\mu_0,\sigma^2_0)</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>This is just some number that you're going to fill in here when you decide what the prior should be. </span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>Mean $\mu_0$, and less say $\sigma^2_0$ would be the variance of that prior.</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>The prior for $\sigma^2$ will be Inverse Gamma </span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>\sigma^2 \sim \mathcal{IG}(\nu_0,\beta_0)</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>which has two parameters:</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a><span class="ss">  -   </span>a *shape parameter*, $\nu_0$, and</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a><span class="ss">  -   </span>a *scale parameter*, $\beta_0$.</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>We need to choose values for these hyper-parameters here. But we do now have a complete Bayesian model.</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>We now introduce some new ideas that were not presented in the previous course.</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Hierarchical representation</span></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>By hierarchy, we mean that the model is specified in steps or in layers.</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>start with the model for the data, or the likelihood.</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>write the priors</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>add hyper-priors for the parameters of the priors.</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>More details can be seen on this <span class="co">[</span><span class="ot">wikipedia article</span><span class="co">](https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling)</span> and on this <span class="co">[</span><span class="ot">one</span><span class="co">](https://en.wikipedia.org/wiki/Multilevel_model)</span></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### Graphical representation</span></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>Another useful way to write out this model Is using what's called a **graphical representation**. </span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>To write a graphical representation, we're going to do the reverse order, we'll start with the priors and finish with the likelihood.</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>In the graphical representation we draw what are called nodes so this would be a node for mu. </span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>The **circle** means that the this is a random variable that has its own distribution. </span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>So $\mu$ with its prior will be represented with that. </span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>And then we also have $\sigma^2$. </span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>The next part of a graphical model is showing the dependence on other variables. </span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>Once we have the parameters, we can generate the data.</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>For example we have $y_1, \dots y_n$. These are also random variables, so we'll create these as nodes. </span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>And I'm going to double up the circle here to indicate that these nodes are observed, you see them in the data. </span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>So we'll do this for all of the $y_i$ here. </span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>And to indicate the dependence of the distributions of the $y_i$ on $\mu$ and $\sigma^2$, we're going to draw arrows. </span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>So $\mu$ influences the distribution of $y$ for each one of these $y_i$. </span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>The same is true for sigma squared, the distribution of each $y$ depends on the distribution of $\sigma^2$. </span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>Again, these nodes right here, that are double-circled, mean that they've been observed. </span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>If they're shaded, which is the usual case, that also means that they're observed. </span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>The arrows indicate the dependence between the random variables and their distributions.</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>Notice that in this hierarchical representation, I wrote the dependence of the distributions also. </span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>We can simplify the graphical model by writing exchangeable random variables and I'll define exchangeable later.</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>We're going to write this using a representative of the $y_i$ here on what's called the **plate**. </span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>So I'm going to re draw this hierarchical structure, we have $\mu$ and $\sigma^2$. </span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>And we don't want to have to write all of these notes again. </span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>So I'm going to indicate that there are n of them, and I'm just going to draw one representative, $y_i$. </span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>And they depend on $\mu$ and $\sigma^2$. </span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>To write a model like this, we must assume that the $y_i$ are *exchangeable*. </span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>That means that the distribution for the $y_i$ does not change if we were to switch the index label like the $i$ on the $y$ there. </span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>So, if for some reason, we knew that one of the $y_i$ was different from the other $y_i$ in its distribution, and if we also know which one it is, then we would need to write a separate node for it and not use a plate like we have here.</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Graphical representation</span></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-graphical-model-posterior</span></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: pgm-posterior</span></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-dpi: 300</span></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: true</span></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> daft <span class="im">as</span> daft</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>pgm <span class="op">=</span> daft.PGM(</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>  <span class="co">#[6, 3.2],</span></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>  node_unit<span class="op">=</span><span class="fl">1.0</span>,alternate_style<span class="op">=</span><span class="st">'outer'</span>,dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"sigma"</span>, <span class="vs">r"</span><span class="dv">$\s</span><span class="vs">igma</span><span class="dv">^</span><span class="vs">2</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">0.</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"mu"</span>, <span class="vs">r"</span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">1.</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"y"</span>, <span class="vs">r"</span><span class="dv">$</span><span class="vs">y_i</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, observed<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"sigma"</span>, <span class="st">"mu"</span>)<span class="op">;</span></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"sigma"</span>, <span class="st">"y"</span>, )<span class="op">;</span></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"mu"</span>, <span class="st">"y"</span>)<span class="op">;</span></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>pgm.add_plate([<span class="op">-</span><span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">2.5</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">n = 1</span><span class="dv">...</span><span class="vs">N</span><span class="dv">$</span><span class="vs">"</span>, position<span class="op">=</span><span class="st">"bottom right"</span>)</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>pgm.render()</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>In the graphical representation we start at the top by drawing:</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>circle nodes for the hyperparameters.</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>arrows indicating that they determine the</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>nodes for the priors.</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>nodes for the RVs (doubled circles)</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>plates (rectangles) indicating RVs that are exchangeable. We add an index to the corner of the plate to indicate the amount of replicated RVs</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>More details can be seen on this <span class="co">[</span><span class="ot">wikipedia article</span><span class="co">](https://en.wikipedia.org/wiki/Plate_notation)</span></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>Both the hierarchical and graphical representations show how you could hypothetically **simulate data** from this model. You start with the variables that don't have any dependence on any other variables. You would simulate those, and then given those draws, you would simulate from the distributions for these other variables further down the chain.</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>This is also how you might simulate from a prior predictive distribution.</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a><span class="fu">## Posterior derivation  🎥 {#sec-c2l02-posterior-derivation}</span></span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a><span class="al">![Posterior derivation](images/c2l02-ss-04.png)</span>{#fig-c2l02-ss-04 .column-margin group="slides" width="53mm"}</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>\index{model!hierarchical}</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>So far, we've only drawn the model with two levels.</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>But in reality, there's nothing that will stop us from diving deeper into the rabbit hole and adding more layers.</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>For example, instead of fixing the values for the hyperparameters in the previous segment recall those hyperparameters were the $\mu_0$, the $\sigma_0$, the $\nu_0$ and the $\beta_0$, we could  either specify fixed numeric values for those, or we could try to infer them from the data and model them using additional prior distributions for those variables to make this a hierarchical model.</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>A good reason to construct the model hierarchically is if the data generating process is organized in levels so that groups of observations generated at a certain level are more naturally grouped e.g there is greater similarity within groups than between groups together for each subsequent levels.</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a>We will examine these types of hierarchical models in depth later in the course.</span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a>Another simple example of a hierarchical model is one you saw already in the previous course c.f. @sec-normal-likelihood-with-expectation-and-variance-unknown</span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>Back to our model:</span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>At the top level are the observation: $y_i \mid \mu,\sigma^2$. This is just like the model from the previous lesson, where the observations were from independent and identically distributed normal RV with a mean $\mu$ and a variance, $\sigma^2$. </span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>At the next level we diverge, instead of having independent priors for $\mu$ and $\sigma^2$, we're going to have the prior for $\mu$ depend on the value of $\sigma^2$. That is given $\sigma^2$, $\mu$ follows a normal distribution with mean $\mu_0$, just some hyperparameter that you're going to chose. And the variance of this prior will be $\sigma^2$, this parameter, divided by $\omega_0$. Another hyperparameter that will scale it.</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>y_i \mid \mu,\sigma^2 &amp;\stackrel{iid}{\sim} \mathcal{N}(\mu,\sigma^2) <span class="sc">\\</span></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>\mu \mid \sigma^2 &amp;\sim \mathcal{N}(\mu_0,\frac{\sigma^2}{\omega_0}) <span class="sc">\\</span></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>\sigma^2 \mid  &amp;\sim \mathcal{IG}(\nu_0,\beta_0)</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>$$ {#eq-c2l02-hierarchical-model-definition-2}</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>where:</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\mu_0$ is the mean of the prior for $\mu$,</span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\omega_0$ is a scaling factor for the variance of the prior for $\mu$,</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\nu_0$ and $\beta_0$ are the shape and scale parameters of the inverse gamma prior for $\sigma^2$.</span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>We now have a joint distribution of $y \mid \mu$ and $\mu \mid \sigma^2$ </span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>To complete this model we need to provide a prior for $\sigma^2$. </span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We'll just use the standard Inverse-Gamma with the same hyperparameters as last time. </span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>The graphical representation for this model looks like this:</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Graphical representation</span></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-graphical-model-posterior-2</span></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: pgm-posterior-2</span></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-dpi: 300</span></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: true</span></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> daft <span class="im">as</span> daft</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>pgm <span class="op">=</span> daft.PGM(</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a>  <span class="co">#[6, 3.2],</span></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>  node_unit<span class="op">=</span><span class="fl">1.0</span>,alternate_style<span class="op">=</span><span class="st">'outer'</span>,dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"sigma"</span>, <span class="vs">r"</span><span class="dv">$\s</span><span class="vs">igma</span><span class="dv">^</span><span class="vs">2</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">1.2</span>, <span class="dv">3</span>)<span class="op">;</span></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"mu"</span>, <span class="vs">r"</span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">.5</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"y"</span>, <span class="vs">r"</span><span class="dv">$</span><span class="vs">y_i</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">.5</span>, <span class="dv">1</span>, observed<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"sigma"</span>, <span class="st">"mu"</span>)<span class="op">;</span></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"sigma"</span>, <span class="st">"y"</span>)<span class="op">;</span></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"mu"</span>, <span class="st">"y"</span>)<span class="op">;</span></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>pgm.add_plate([<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">n = 1</span><span class="dv">...</span><span class="vs">N</span><span class="dv">$</span><span class="vs">"</span>, position<span class="op">=</span><span class="st">"bottom right"</span>)</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>pgm.render()</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We start with the variables that don't depend on anything else. </span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>That would be $\sigma^2$ and move down the chain.</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>The next variable is $\mu$, which depends on $\sigma^2$.</span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a><span class="ss">  3. </span>Then we have the $y_i$  dependent on both, We use a double or filled circle because the $y_i$'s are observed, their data, and we're going to assume that they're exchangeable.</span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a><span class="ss">  4. </span>We place the $y_i$'s on a plate for $i \in 1 \dots N$. </span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a><span class="ss">  5. </span>The distribution of $y_i$ depends on both $\mu$ and $\sigma^2$, so we'll draw curves connecting those pieces there.</span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>p(\mu \mid y_i \dots y_n) &amp;\propto \prod_{i=1}^{n} \left<span class="co">[</span><span class="ot">\frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y_i - \mu)^2}{2 \sigma^2}\right)\right</span><span class="co">]</span> \cdot \frac{1}{\pi(1 + \mu^2)} <span class="sc">\\</span></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>&amp;\propto   \exp \left<span class="co">[</span><span class="ot">\left(-\frac{(y_i - \mu)^2}{2 \sigma^2}\right) \right </span><span class="co">]</span> \cdot \frac{1}{(1 + \mu^2)} <span class="sc">\\</span></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{ \exp \left<span class="co">[</span><span class="ot"> n (\bar{y} - \frac{\mu^2}2{}) \right</span><span class="co">]</span> }{1 + \mu^2} </span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>To simulate hypothetical data from this model, we would have to first draw from the distribution of the prior for $\sigma^2$. </span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>Then the distribution for mu which depends on $\sigma^2$. </span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a>And once we've drawn both of these, then we can draw random draws from the $y_i$, which of course depends on both of those. </span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>With multiple levels, this is an example of a hierarchical model. </span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a>Once we have a model specification, we can write out what the full posterior distribution for all the parameters given the data looks like. Remember that the numerator in Bayes' theorem is the joint distribution of all random quantities, all the nodes in this graphical representation over here from all of the layers. </span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>So for this model that we have right here, we have a joint distribution that'll look like this. </span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a>We're going to write the joint distribution of everything $y_1:n$, $\mu$ and $\sigma^2$, using the *chain rule of probability*, we're going to multiply all of the distributions in the hierarchy together. </span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>So let's start with the likelihood piece. </span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a>And we'll multiply that by the next layer, the distribution of mu, given $\sigma^2$. And finally, with the prior for sigma squared. </span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>So what do these expressions right here look like? </span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>The likelihood in this level because they're all independent will be a product of normal densities. </span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a>So we're going to multiply the normal density for each $y_i$, given those parameters. </span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>This, again, is shorthand right here for the density of a normal distribution. So that represents this piece right here. </span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a>The conditional prior of $\mu$ given sigma squared is also a normal. </span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>So we're going to multiply this by a normal distribution of $\mu$, where its parameters are $\mu$ naught and sigma squared over omega naught. </span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a>And finally, we have the prior for sigma squared. </span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a>We'll multiply by the density of an inverse gamma for $\sigma^2$ given the hyper parameters $\mu$ naught, sorry, that is given, the hyper parameters $\mu$ naught and and beta naught. </span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a>What we have right here is the joint distribution of everything. </span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a>It is the numerator in Bayes theorem. </span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a>Let's remind ourselves really fast what Bayes theorem looks like again. We have that the posterior distribution of the parameter given the data is equal to the likelihood, Times the prior. </span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a>Over the same thing again. </span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a>So this gives us in the numerator the joint distribution of everything which is what we've written right here.</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a>In Bayes theorem, the numerator and the denominator are the exact same expression accept that we integrate or marginalize over all of the parameters.</span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>Because the denominator is a function of the y's only, which are known values, the denominator is just a constant number. </span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a>So we can actually write the posterior distribution as being proportional to, this symbol right here represents proportional to. </span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a>The joint distribution of the data and parameters, or the likelihood times the prior. </span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>The poster distribution is proportional to the joint distribution, or everything we have right here. </span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a>In other words, what we have already written for this particular model is proportional to the posterior distribution of $\mu$ and $\sigma^2$, given all of the data. </span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>The only thing missing in this expression right here is just some constant number that causes the expression to integrate to 1. </span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a>If we can recognize this expression as being proportional to a common distribution, then our work is done, and we know what our posterior distribution is. </span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a>This was the case for all models in the previous course. </span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a>However, if we do not use conjugate priors or if the models are more complicated, then the posterior distribution will not have a standard form that we can recognize. </span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a>We're going to explore a couple of examples of this issue in the next segment.</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a><span class="fu">## Non-conjugate models  🎥 {#sec-c2l02-non-conjugate-models}</span></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a><span class="al">![Non-conjugate models](images/c2l02-ss-03.png)</span>{#fig-c2l02-ss-03 .column-margin group="slides" width="53mm"}</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a>We'll first look at an example of a one parameter model that is not conjugate.</span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>::: {#exp-company-personnel}</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Company Personnel</span></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a>Suppose we have values that represent the percentage change in total personnel from last year to this year for, we'll say, ten companies. These companies come from a particular industry. </span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a>We're going to assume for now, that these are independent measurements from a normal distribution with a known variance equal to one, but an unknown mean.</span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a>So we'll say the percentage change in the total personnel for company I, given the unknown mean $\mu$ will be distributed normally with mean $\mu$, and we're just going to use variance 1.</span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a>In this case, the unknown mean could represent growth for this particular industry.</span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>It's the average of the growth of all the different companies. </span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a>The small variance between the companies and percentage growth might be appropriate if the industry is stable.</span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a>We know that the **conjugate** **prior** for $\mu$ in this location would be a **normal distribution**.</span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a>But suppose we decide that our prior believes about $\mu$ are better reflected using a **standard t distribution** with **one degree of freedom**. </span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a>So we could write that as the prior for $\mu$ is a t distribution with a location parameter 0. </span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>That's where the center of the distribution is. </span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a>A scale parameter of 1 to make it the **standard t-distribution** similar to a standard normal, and 1 degree of freedom.</span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a>This particular prior distribution has heavier tails than the conjugate and normal distribution, which can more easily accommodate the possibility of extreme values for $\mu$. </span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a>It is centered on zero so, that apriori, there is a 50% chance that the growth is positive and a 50% chance that the growth is negative.</span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a>Recall that the posterior distribution of $\mu$ is proportional to the likelihood times the prior. </span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a>Let's write the expression for that in this model. </span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a>That is the posterior distribution for $\mu$ given the data $y_1 \dots y_n$ is going to be proportional to the likelihood.</span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a>It is a product from i equals 1 to n, in this case that's 10.</span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a>Densities from a normal distribution.</span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a>Let's write the density from this particular normal distribution.</span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a>Is $1 \over \sqrt{2 \pi}$.</span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a>E to the negative one-half.</span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a>$y_i - \mu^2$, this is the normal density for each individual $y_i$ and we multiplied it for likelihood.</span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a>The density for this t prior looks like this.</span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a>It's 1 over pi times 1 plus $\mu$ squared.</span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a>This is the likelihood times the prior.</span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a>If we do a little algebra here, first of all, we're doing this up to proportionality.</span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a>So, constants being multiplied by this expression are not important.</span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a>The $\sqrt{2 \pi}^n$, is just a constant number, and $\pi$ creates a constant number.</span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a>So we will drop them in our next step.</span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a>So this is now proportional too, we're removing this piece and now we're going to use properties of exponents.</span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a>The product of exponents is the sum of the exponentiated pieces.</span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a>So we have the exponent of negative one-half times the sum from i equals 1 to n, of Yi minus $\mu$ squared.</span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a>And then we're dropping the pie over here, so times 1 plus $\mu$ squared.</span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a>We're going to do a few more steps of algebra here to get a nicer expression for this piece.</span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a>But we're going to skip ahead to that.</span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a>We've now added these last two expressions.</span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a>To arrive at this expression here for the posterior, or what's proportional to the posterior distribution.</span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a>This expression right here is almost proportional to a normal distribution except we have this 1 plus $\mu$ squared term in the denominator.</span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a>We know the posterior distribution up to a constant but we don't recognize its form as a standard distribution.</span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a>That we can integrate or simulate from, so we'll have to do something else.</span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a>Let's move on to our second example. </span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a>For a two parameter example, we're going to return to the case where we have a normal likelihood.</span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a>And we're now going to estimate $\mu$ and $\sigma^2$, because they're both unknown.</span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a>Recall that if $\sigma^2$ were known, the conjugate prior from $\mu$ would be a normal distribution.</span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a>And if $\mu$ were known, the conjugate prior we could choose for $\sigma^2$ would be an inverse gamma.</span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a>We saw earlier that if you include $\sigma^2$ in the prior for $\mu$, and use the hierarchical model that we presented earlier, that model would be conjugate and have a closed form solution. </span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a>However, in the more general case that we have right here, the posterior distribution does not appear as a distribution that we can simulate or integrate.</span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a>Challenging posterior distributions like these ones and most others that we'll encounter in this course kept Bayesian methods from entering the main stream of statistics for many years. </span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a>Since only the simplest problems were tractable. </span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a>However, computational methods invented by physicists in the 1950's, and implemented by statisticians decades later, revolutionized the field. </span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a>We now do have the ability to simulate from the posterior distributions from this lesson, as well as for many other more complicated models.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Oren Bochman</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>