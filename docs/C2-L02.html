<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.11">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="keywords" content="Statistical Modeling">

<title>32&nbsp; M1L2 - Bayesian Modeling â€“ Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./C2-L03.html" rel="next">
<link href="./C2-L01.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-923206e44a13b4518db4dede9f4ebdc9.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-279bd408c6dfe7bd56e8b154fe269440.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-987387ce273b62b48f1747d606bce1d5.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-84d0c58e965b114532ef2814536305ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./C2-L02.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Statistics</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Bayesian Statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Probability - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L01-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Paradigms of probability - M1L1HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesâ€™ Theorem - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayesâ€™ Law - M1L2HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Probability and Bayesâ€™ Theorem - M1L2HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Distributions - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Random Variables - M1L3HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Homework on Distributions - M1L3HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Frequentist Inference - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Frequentist MLE - M2L3HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Bayesian Inference - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Homework on Likelihoods and MLEs - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L05-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Homework on Bayesian Inference - M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Priors - M3L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Homework Posterior Probabilities - M3L6HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">M3L7 - Binomial Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Homework on Priors - M2L7HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Poisson Data - M3L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Homework on Poisson Data - M3L8HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Beta Bernoulli - M3L8HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">M4L9 - Exponential Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Homework on Exponential Data - M4L9HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Normally distributed Data - M4L10</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Homework on Normal Data - M4L10HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Non-Informative Priors - M4L11</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Homework Alternative Priors - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Brief Review of Regression - M4L12</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C1-L12-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Homework Regression - M4L12HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L02.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Metropolis-Hastings - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Homework on the Metropolis-Hastings algorithm - M2L4HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Gibbs sampling - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L05-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Homework Gibbs-Sampling algorithm - M2L22HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Assessing Convergence - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Homework on the Gibbs-Sampling algorithm - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L06-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Homework on M-H algorithm M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Linear regression - M3L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Homework on Linear Regression Model Part 1 - M2L5HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Homework on Deviance information criterion - M2L5HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">ANOVA - M3L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Homework on ANOVA - M3L8HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L08-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Homework on Multiple Factor ANOVA - M3L8HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Logistic regression - M3L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Homework on Logistic Regression - M3L9HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Poisson regression - M4L10</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L10-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Homework on Poisson regression - M4L10HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Hierarchical modeling - M4L11</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Homework on Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L11-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Homework on Non-Normal Hierarchical Models - M4L11HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Capstone Project - M4L12</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C2-L12-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Homework on Predictive distributions and mixture models - M4L12HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Definitions of Mixture Models - M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex1-Basic-Definitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">57</span>&nbsp; <span class="chapter-title">Basic Concepts of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex2-Gaussian-mixtures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">58</span>&nbsp; <span class="chapter-title">Mixtures of Gaussians</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex3-Zero-Inflated-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">59</span>&nbsp; <span class="chapter-title">Zero inflated distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L01-Ex4-Def-mixture-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">60</span>&nbsp; <span class="chapter-title">Definition of Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">61</span>&nbsp; <span class="chapter-title">Likelihood functions for Mixture Models - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">62</span>&nbsp; <span class="chapter-title">Homework The Likelihood function - M1L2HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">63</span>&nbsp; <span class="chapter-title">Homework Identifiability - M1L2HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">64</span>&nbsp; <span class="chapter-title">Homework The likelihood function M1L2HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">65</span>&nbsp; <span class="chapter-title">Homework on simulating from a Poisson Mixture Model - M1L2HW4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">66</span>&nbsp; <span class="chapter-title">HW - Simulation of Poisson mixture model - M1L2HW5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L02-Ex6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">67</span>&nbsp; <span class="chapter-title">Homework Sim mixture of exponential distributions - M1L2HW6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">68</span>&nbsp; <span class="chapter-title">M2L3 - The EM algorithm for Mixture models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">69</span>&nbsp; <span class="chapter-title">The EM algorithm for Zero-Inflated Mixtures</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L03-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">70</span>&nbsp; <span class="chapter-title">The EM algorithm for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">71</span>&nbsp; <span class="chapter-title">MCMC for Mixture Models - M4L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">72</span>&nbsp; <span class="chapter-title">The MCMC algorithm for Zero-Inflated Mixtures - M4L1HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L04-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">73</span>&nbsp; <span class="chapter-title">Markov chain Monte Carlo algorithms for Mixture Models - M4L1HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">74</span>&nbsp; <span class="chapter-title">Density Estimation - M4L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">75</span>&nbsp; <span class="chapter-title">Clustering - M4L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">76</span>&nbsp; <span class="chapter-title">Classification - M4L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">77</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the EM algorithm - M4L7HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">78</span>&nbsp; <span class="chapter-title">Old Faithful eruptions density estimation with the MCMC algorithms - M4L7HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L07-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">79</span>&nbsp; <span class="chapter-title">Homework on Bayesian Mixture Models for Classification of Banknotes - M4L7HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">80</span>&nbsp; <span class="chapter-title">Computational Considerations - M5L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L08-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">81</span>&nbsp; <span class="chapter-title">Computational considerations for Mixture Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">82</span>&nbsp; <span class="chapter-title">Determining the number of components - M5L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">83</span>&nbsp; <span class="chapter-title">Homework on Bayesian Information Criteria (BIC) - M5L09HW1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">84</span>&nbsp; <span class="chapter-title">Homework on Estimating the number of components in Bayesian settings - M5L09HW2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">85</span>&nbsp; <span class="chapter-title">Homework on Estimating the partition structure in Bayesian models - M5L09HW3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C3-L09-Ex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">86</span>&nbsp; <span class="chapter-title">Homework on BIC for zero-inflated mixtures - M5L09HW4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L00.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">87</span>&nbsp; <span class="chapter-title">Week 0: Introductions to time series analysis and the AR(1) process</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">88</span>&nbsp; <span class="chapter-title">Stationarity, The ACF and the PCF M1L1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">89</span>&nbsp; <span class="chapter-title">The AR(1) process: definitions and properties - M1L2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">90</span>&nbsp; <span class="chapter-title">The AR(1) process:Maximum likelihood estimation and Bayesian inference M1L3</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">91</span>&nbsp; <span class="chapter-title">The AR(p) process - M2L4</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">92</span>&nbsp; <span class="chapter-title">Bayesian Inference in the AR(p) - M2L5</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">93</span>&nbsp; <span class="chapter-title">Normal Dynamic Linear Models, Part 1 - M3L6</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">94</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 1 M3L7</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">95</span>&nbsp; <span class="chapter-title">Seasonal NDLMs M4L8</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">96</span>&nbsp; <span class="chapter-title">Bayesian Inference in the NDLM: Part 2 - M4L9</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">97</span>&nbsp; <span class="chapter-title">Final Project</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C4-L11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">98</span>&nbsp; <span class="chapter-title">Week 0: Feynman Notebook on Bayesian Time Series Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">99</span>&nbsp; <span class="chapter-title">Appendix: Notation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">100</span>&nbsp; <span class="chapter-title">Appendix: Discrete Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">101</span>&nbsp; <span class="chapter-title">Appendix: Continuous Distributions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">102</span>&nbsp; <span class="chapter-title">Appendix: Exponents &amp; Logarithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">103</span>&nbsp; <span class="chapter-title">Appendix: The Law of Large Numbers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">104</span>&nbsp; <span class="chapter-title">Appendix: The Central Limit Theorem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">105</span>&nbsp; <span class="chapter-title">Appendix: Conjugate Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">106</span>&nbsp; <span class="chapter-title">Appendix: Link Function</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">107</span>&nbsp; <span class="chapter-title">Bayes by backprop</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">108</span>&nbsp; <span class="chapter-title">Bayesian Books in R &amp; Python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">109</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-c2l02-components" id="toc-sec-c2l02-components" class="nav-link active" data-scroll-target="#sec-c2l02-components"><span class="header-section-number">32.1</span> Components of a Bayesian Model (Video)</a></li>
  <li><a href="#sec-c2l02-model-specification" id="toc-sec-c2l02-model-specification" class="nav-link" data-scroll-target="#sec-c2l02-model-specification"><span class="header-section-number">32.2</span> Model Specification (Video)</a>
  <ul class="collapse">
  <li><a href="#hierarchical-representation" id="toc-hierarchical-representation" class="nav-link" data-scroll-target="#hierarchical-representation"><span class="header-section-number">32.2.1</span> Hierarchical representation</a></li>
  <li><a href="#graphical-representation" id="toc-graphical-representation" class="nav-link" data-scroll-target="#graphical-representation"><span class="header-section-number">32.2.2</span> Graphical representation</a></li>
  </ul></li>
  <li><a href="#sec-c2l02-posterior-derivation" id="toc-sec-c2l02-posterior-derivation" class="nav-link" data-scroll-target="#sec-c2l02-posterior-derivation"><span class="header-section-number">32.3</span> Posterior derivation (Video)</a></li>
  <li><a href="#sec-c2l02-non-conjugate-models" id="toc-sec-c2l02-non-conjugate-models" class="nav-link" data-scroll-target="#sec-c2l02-non-conjugate-models"><span class="header-section-number">32.4</span> Non-conjugate models (Video)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">M1L2 - Bayesian Modeling</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Bayesian Statistics: Techniques and Models</p>
  <div class="quarto-categories">
    <div class="quarto-category">Bayesian Statistics</div>
    <div class="quarto-category">Monte Carlo Estimation</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Oren Bochman </p>
          </div>
  </div>
    
  
    
  </div>
  

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>Statistical Modeling</p>
  </div>
</div>

</header>


<section id="sec-c2l02-components" class="level2 page-columns page-full" data-number="32.1">
<h2 data-number="32.1" class="anchored" data-anchor-id="sec-c2l02-components"><span class="header-section-number">32.1</span> Components of a Bayesian Model (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c2l02-ss-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="a Bayesian Model"><img src="images/c2l02-ss-01.png" class="img-fluid figure-img" style="width:53mm" alt="a Bayesian Model"></a></p>
<figcaption>a Bayesian Model</figcaption>
</figure>
</div></div><p>In lesson one, we defined <strong>a statistical model</strong> as <em>a mathematical structure used to imitate or approximate the data generating process</em>. It incorporates uncertainty and variability using the theory of probability. A model could be very simple, involving only one variable.</p>
<div id="exm-heights-of-men" class="theorem example page-columns page-full">
<div class="page-columns page-full"><p><span class="theorem-title"><strong>Example 32.1 (heights of men)</strong></span> Suppose our data consists of the heights of <span class="math inline">N=15</span> adult men. . Clearly it would be very expensive or even impossible to collect the genetic information that fully explains the variability in these menâ€™s heights. We only have the height measurements available to us. To account for the variability, we might assume that the menâ€™s heights follow a normal distribution.</p><div class="no-row-height column-margin column-container"><span class="">heights of <span class="math inline">N=15</span> men</span></div></div>
<div class="page-columns page-full"><p>So we could write the model like this:  where <span class="math inline">y_i</span> will represent the height for person i, i will be our index. This will be equal to a constant, a number <span class="math inline">\mu</span> which will represents the mean for all men plus <span class="math inline">\epsilon_i</span>. the individual error term for individual i.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">y_i= \mu + \epsilon_i</span></span></div></div>
<div class="page-columns page-full"><p> Weâ€™re going to assume that <span class="math inline">\epsilon_i</span> comes from a normal distribution with mean zero and variance <span class="math inline">\sigma^2</span>. We are also going to assume that these epsilons are independent and identically distributed from this normal distribution. This is also for i equal to 1 up to N which will be 15 in our case. Equivalently we could write this model directly for the <span class="math inline">y_i</span> themselves.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\epsilon_i \stackrel{iid}\sim N(0,\sigma^2) \  i\in 1 \dots N</span></span></div></div>
<div class="page-columns page-full"><p> So each <span class="math inline">y_i</span> comes from a normal distribution independent and identically distributed with the normal distribution. With mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma^2</span>. This specifies a probability distribution and a model for the data.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">y_i \stackrel{iid}\sim N(\mu,\sigma^2) \ i \in 1 \dots N</span></span></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>heights of men
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">
\begin{aligned}
y_i&amp;= \mu+\epsilon_i,
\\ \epsilon_i &amp;\stackrel{iid}\sim N(0,\sigma^2)
\end{aligned}
</span> another way to write this:</p>
<p><span class="math display">
\begin{aligned}
y_i &amp;\stackrel{iid}\sim N(\mu,\sigma^2)
\end{aligned}
</span></p>
</div>
</div>
<p>If we know the values of <span class="math inline">\mu</span> and <span class="math inline">\sigma</span>. It also suggests how we might generate more fake data that behaves similarly to our original data set.</p>
</div>
<p>A model can be as simple as the one right here or as complicated and sophisticated as we need to capture the behavior of the data. So far, this model is the same for Frequentists and Bayesians.</p>
<p>As you may recall from the previous course. The frequentist approach to fitting this model right here would be to consider <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> to be fixed but unknown constants, and then we would estimate them. To calculate our uncertainty in those estimates a frequentist approach would consider how much the estimates of <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> might change if we were to repeat the sampling process and obtain another sample of 15 men, over, and over.</p>

<div class="no-row-height column-margin column-container"><div id="fig-heights-of-men" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-heights-of-men-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/c2l02-ss-02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;32.1: Components of a Bayesian Model"><img src="images/c2l02-ss-02.png" class="img-fluid figure-img" style="width:53mm"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-heights-of-men-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32.1: Components of a Bayesian Model
</figcaption>
</figure>
</div></div><p>The Bayesian approach, the one weâ€™re going to take in this class. Tackles our uncertainty in <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span> with probability directly. By treating them as random variables with their own probability distributions. These are often called <strong>priors</strong>, and they complete a Bayesian model.</p>
<p>In the rest of this segment, weâ€™re going to review three key components of Bayesian models. That were used extensively in the previous course The three primary components of Bayesian models that we often work with are the <strong>likelihood</strong>, <strong>the prior</strong> and <strong>the posterior</strong>.</p>
<div class="page-columns page-full"><p> <strong>The likelihood</strong> is the probabilistic model for the data. It describes how, given the unknown parameters, the data might be generated. Weâ€™re going to call unknown parameter theta right here. Also, in this expression, you might recognize this from the previous class, as describing a probability distribution.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\mathbb{P}r(y\mid \theta)\ \text{(likelihood)}</span></span></div></div>
<div class="page-columns page-full"><p> <strong>The prior</strong>, the next step, is the probability distribution that characterizes our uncertainty with the parameter theta. Weâ€™re going to write it as <span class="math inline">\mathbb{P}r(\theta)</span>. Itâ€™s not the same distribution as this one. Weâ€™re just using this notation p to represent the probability distribution of theta. By specifying a likelihood and a prior.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\mathbb{P}r(\theta)\ \text{(prior)}</span></span></div></div>
<div class="page-columns page-full"><p> We now have a <strong>joint probability model</strong> for both the knowns, the data, and the unknowns, the parameters. We can see this by using the chain rule of probability. If we wanted the joint distribution of both the data and the parameters theta. Using the chain rule of probability, we could start with the distribution of <span class="math inline">\theta</span>. And multiply that by the probability or the distribution of <span class="math inline">y</span> given theta. That gives us an expression for the joint distribution. <strong>However if weâ€™re going to make inferences about data and we already know the values of</strong> <span class="math inline">y</span>, we donâ€™t need the <em>joint distribution</em>, what we need is the <em>posterior distribution</em>.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\mathbb{P}r(y,\theta) = \mathbb{P}r(\theta)\mathbb{P}r(y\mid\theta) \ \text{(joint probability)}</span></span></div></div>
<div class="page-columns page-full"><p> The <strong>posterior distribution</strong> is the distribution of <span class="math inline">\mathbb{P}r(\theta \mid y)</span>, i.e.&nbsp;<span class="math inline">\theta</span> given <span class="math inline">y</span>. We can obtain this expression using the laws of conditional probability and specifically using Bayesâ€™ theorem.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\mathbb{P}r(\theta \mid y)\ \text{(posterior)}</span></span></div></div>
<p><span class="math display">
\begin{aligned}
\mathbb{P}r(\theta \mid y) &amp;= \frac{\mathbb{P}r(\theta,y)}{\mathbb{P}r(y)}
\\ &amp;= \frac{\mathbb{P}r(\theta,y)}{\int \mathbb{P}r(\theta,y)}
\\ &amp;= \frac{\mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)}{\int \mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)\ d\theta}
\end{aligned}
</span></p>
<p>We start with the definition of conditional probability (1). The conditional distribution, <span class="math inline">\mathbb{P}r(\theta \mid y)</span> is the ratio of the <em>joint distribution</em> of <span class="math inline">\theta</span> and <span class="math inline">y</span>, i.e.&nbsp;<span class="math inline">\mathbb{P}r(\theta,y)</span>; with the <em>marginal distribution</em> of <span class="math inline">y</span>, <span class="math inline">\mathbb{P}r(y)</span>.</p>
<div class="page-columns page-full"><p> We start with the <strong>joint distribution</strong> like we have on top, and we <em>integrate out</em> or <em>marginalize</em> over the values of theta (2)</p><div class="no-row-height column-margin column-container"><span class="">How do we get the marginal distribution of y?</span></div></div>
<p>To make this look like the Bayes theorem that weâ€™re familiar with the joint distribution can be rewritten as the product of the prior and the likelihood. We start with the likelihood, because thatâ€™s how we usually write Bayesâ€™ theorem. We have the same thing in the denominator here. But weâ€™re going to integrate over the values of theta. These integrals are replaced by summations if we know that <span class="math inline">\theta</span> is a discrete random variable. The <em>marginal distribution</em> is another important piece which we may use when we more advanced Bayesian modeling.</p>
<p>The <strong>posterior distribution</strong> is our primary tool for achieving the statistical modeling objectives from lesson one.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Anatomy of a posterior probability
</div>
</div>
<div class="callout-body-container callout-body">
<p><span id="eq-posterior-anatomy"><span class="math display">
  \begin{aligned}
  &amp;\mathbb{P}r(y\mid \theta) &amp;&amp; (likelihood) \\
&amp;  \mathbb{P}r(\theta) &amp;&amp; (prior) \\
   \mathbb{P}r(y,\theta) &amp;= \mathbb{P}r(\theta)\mathbb{P}r(y|\theta) &amp;&amp;(joint\ distribution) \\
   \mathbb{P}r(\theta \mid y) &amp;= \frac{\mathbb{P}r(\theta,y)}{\mathbb{P}r(y)} &amp;&amp; (conditional\ probability) \\
&amp;= \frac{\mathbb{P}r(\theta,y)}{\int \mathbb{P}r(\theta,y)} \\
&amp;= \frac{\mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)}{\int \mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)\ d\theta} \\
\end{aligned}
\tag{32.1}</span></span></p>
</div>
</div>
<div id="qst-question">
<p>Whereas non-Bayesian approaches consider a probability model for the data only, the hallmark characteristic of Bayesian models is that they specify a joint probability distribution for both data <em>and</em> parameters. How does the Bayesian paradigm leverage this additional assumption?</p>
</div>
<div id="solution">
<ul class="task-list">
<li><label><input type="checkbox"><strong>This allows us to make probabilistic assessments about how likely our particular data outcome is under any parameter setting.</strong></label></li>
<li><label><input type="checkbox"><strong>This allows us to select the most accurate prior distribution.</strong></label></li>
<li><label><input type="checkbox"><strong>This allows us to make probabilistic assessments about hypothetical data outcomes given particular parameter values.</strong></label></li>
<li><label><input type="checkbox" checked=""><strong>This allows us to use the laws of conditional probability to describe our updated information about parameters given the data.</strong></label></li>
</ul>
</div>
</section>
<section id="sec-c2l02-model-specification" class="level2 page-columns page-full" data-number="32.2">
<h2 data-number="32.2" class="anchored" data-anchor-id="sec-c2l02-model-specification"><span class="header-section-number">32.2</span> Model Specification (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c2l02-ss-05.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Model specification"><img src="images/c2l02-ss-05.png" class="img-fluid figure-img" style="width:53mm" alt="Model specification"></a></p>
<figcaption>Model specification</figcaption>
</figure>
</div></div><p>Before fitting any model we first need to specify all of its components.</p>
<div id="cell-fig-graphical-model" class="cell" data-fig-dpi="300" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-graphical-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graphical-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C2-L02_files/figure-html/fig-graphical-model-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;32.2: The graphical model specification for the height model"><img src="C2-L02_files/figure-html/fig-graphical-model-output-1.png" width="697" height="520" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graphical-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32.2: The graphical model specification for the height model
</figcaption>
</figure>
</div>
</div>
</div>
<section id="hierarchical-representation" class="level3 page-columns page-full" data-number="32.2.1">
<h3 data-number="32.2.1" class="anchored" data-anchor-id="hierarchical-representation"><span class="header-section-number">32.2.1</span> Hierarchical representation</h3>
<p>One convenient way to do this is to write down the <strong>hierarchical form of the model</strong>. By hierarchy, we mean that the model is specified in steps or in layers. We usually start with the model for the data directly, or the likelihood. Letâ€™s write, again, the model from the previous lesson.</p>
<div class="page-columns page-full"><p>We had the height for person i, given our parameters <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, so conditional on those parameters, <span class="math inline">y_i</span> came from a normal distribution that was independent and identically distributed, where the normal distribution has mean <span class="math inline">\mu</span> and variance <span class="math inline">\sigma^2</span>, and weâ€™re doing this for individuals 1 up to N, which was 15 in this example. </p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">y_i | \mu,\sigma^2 \stackrel{iid}\sim N(\mu,\sigma^2) \ for\ i \in 1,\dots,15</span></span></div></div>
<div class="page-columns page-full"><p>The next level that we need is the prior distribution from <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>. For now weâ€™re going to say that theyâ€™re independent priors. So that our prior from <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span> is going to be able to factor Into the product of two independent priors.  We can assume independents in the prior and still get dependents in the posterior distribution.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\mathbb{P}r(\mu,\sigma^2)~=~\mathbb{P}r(\mu)\mathbb{P}r(\sigma^2)\ (independence)</span></span></div></div>
<p>In the previous course we learned that <em>the conjugate prior</em> for <span class="math inline">\mu</span>, if we know the value of <span class="math inline">\sigma^2</span>, is a <em>normal distribution</em>, and that the conjugate prior for <span class="math inline">\sigma^2</span> when <span class="math inline">\mu</span> is known is the <em>Inverse Gamma distribution</em>.</p>
<div class="page-columns page-full"><p>Letâ€™s suppose that our prior distribution for <span class="math inline">\mu</span> is a normal distribution where mean will be <span class="math inline">\mu_0</span>.  This is just some number that youâ€™re going to fill in here when you decide what the prior should be. Mean <span class="math inline">\mu_0</span>, and less say <span class="math inline">\sigma^2_0</span> would be the variance of that prior.</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\mu \sim N(\mu_0,\sigma^2_0)</span></span></div></div>
<div class="page-columns page-full"><p>The prior for <span class="math inline">\sigma^2</span> will be Inverse Gamma  which has two parameters:</p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\sigma^2 \sim \mathcal{IG}(\nu_0,\beta_0)</span></span></div></div>
<ul>
<li>It has a <em>shape parameter</em>, weâ€™re going to call that <span class="math inline">\nu_0</span>, and</li>
<li>It has a <em>scale parameter</em>, weâ€™ll call that <span class="math inline">\beta_0</span>.</li>
</ul>
<p>We need to choose values for these hyper-parameters here. But we do now have a complete Bayesian model.</p>
<p>We now introduce some new ideas that were not presented in the previous course.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Hierarchical representation
</div>
</div>
<div class="callout-body-container callout-body">
<p>By hierarchy, we mean that the model is specified in steps or in layers.</p>
<ul>
<li>start with the model for the data, or the likelihood.</li>
<li>write the priors</li>
<li>add hyper-priors for the parameters of the priors.</li>
</ul>
<p>More details can be seen on this <a href="https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling">wikipedia article</a> and on this <a href="https://en.wikipedia.org/wiki/Multilevel_model">one</a></p>
</div>
</div>
</section>
<section id="graphical-representation" class="level3" data-number="32.2.2">
<h3 data-number="32.2.2" class="anchored" data-anchor-id="graphical-representation"><span class="header-section-number">32.2.2</span> Graphical representation</h3>
<p>Another useful way to write out this model Is using whatâ€™s called a <strong>graphical representation</strong>. To write a graphical representation, weâ€™re going to do the reverse order, weâ€™ll start with the priors and finish with the likelihood.</p>
<p>In the graphical representation we draw what are called nodes so this would be a node for mu. The <strong>circle</strong> means that the this is a random variable that has its own distribution. So <span class="math inline">\mu</span> with its prior will be represented with that. And then we also have <span class="math inline">\sigma^2</span>. The next part of a graphical model is showing the dependence on other variables. Once we have the parameters, we can generate the data.</p>
<p>For example we have <span class="math inline">y_1, \dots y_n</span>. These are also random variables, so weâ€™ll create these as nodes. And Iâ€™m going to double up the circle here to indicate that these nodes are observed, you see them in the data. So weâ€™ll do this for all of the <span class="math inline">y</span>s here. And to indicate the dependence of the distributions of the <span class="math inline">y</span>s on <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, weâ€™re going to draw arrows. So <span class="math inline">\mu</span> influences the distribution of <span class="math inline">y</span> for each one of these <span class="math inline">y</span>s. The same is true for sigma squared, the distribution of each <span class="math inline">y</span> depends on the distribution of <span class="math inline">\sigma^2</span>. Again, these nodes right here, that are double-circled, mean that theyâ€™ve been observed. If theyâ€™re shaded, which is the usual case, that also means that theyâ€™re observed. The arrows indicate the dependence between the random variables and their distributions.</p>
<p>Notice that in this hierarchical representation, I wrote the dependence of the distributions also. We can simplify the graphical model by writing exchangeable random variables and Iâ€™ll define exchangeable later.</p>
<p>Weâ€™re going to write this using a representative of the <span class="math inline">y</span>s here on whatâ€™s called the <strong>plate</strong>. So Iâ€™m going to re draw this hierarchical structure, we have <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>. And we donâ€™t want to have to write all of these notes again. So Iâ€™m going to indicate that there are n of them, And Iâ€™m just going to draw one representative, <span class="math inline">y_i</span>. And they depend on <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>. To write a model like this, we must assume that the <span class="math inline">y</span>s are <em>exchangeable</em>. That means that the distribution for the <span class="math inline">y</span>s does not change if we were to switch the index label like the <span class="math inline">i</span> on the <span class="math inline">y</span> there. So, if for some reason, we knew that one of the <span class="math inline">y</span>s was different from the other <span class="math inline">y</span>s in its distribution, and if we also know which one it is, then we would need to write a separate node for it and not use a plate like we have here.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Graphical representation
</div>
</div>
<div class="callout-body-container callout-body">
<div id="cell-fig-graphical-model-posterior" class="cell" data-fig-dpi="300" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-graphical-model-posterior" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graphical-model-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="C2-L02_files/figure-html/fig-graphical-model-posterior-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;32.3: pgm-posterior"><img src="C2-L02_files/figure-html/fig-graphical-model-posterior-output-1.png" width="697" height="520" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graphical-model-posterior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32.3: pgm-posterior
</figcaption>
</figure>
</div>
</div>
</div>
<p>In the graphical representation we start at the top by drawing:</p>
<ul>
<li>circle nodes for the hyperparameters.</li>
<li>arrows indicating that they determine the</li>
<li>nodes for the priors.</li>
<li>nodes for the RVs (doubled circles)</li>
<li>plates (rectangles) indicating RVs that are exchangeable. We add an index to the corner of the plate to indicate the amount of replicated RVs</li>
</ul>
<p>More details can be seen on this <a href="https://en.wikipedia.org/wiki/Plate_notation">wikipedia article</a></p>
</div>
</div>
<p>Both the hierarchical and graphical representations show how you could hypothetically <strong>simulate data</strong> from this model. You start with the variables that donâ€™t have any dependence on any other variables. You would simulate those, and then given those draws, you would simulate from the distributions for these other variables further down the chain.</p>
<p>This is also how you might simulate from a prior predictive distribution.</p>
</section>
</section>
<section id="sec-c2l02-posterior-derivation" class="level2 page-columns page-full" data-number="32.3">
<h2 data-number="32.3" class="anchored" data-anchor-id="sec-c2l02-posterior-derivation"><span class="header-section-number">32.3</span> Posterior derivation (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c2l02-ss-04.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Posterior derivation"><img src="images/c2l02-ss-04.png" class="img-fluid figure-img" style="width:53mm" alt="Posterior derivation"></a></p>
<figcaption>Posterior derivation</figcaption>
</figure>
</div></div><p>So far, weâ€™ve only drawn the model with two levels. But in reality, thereâ€™s nothing that will stop us from adding more layers.</p>
<p>For example, instead of fixing the values for the hyper parameters in the previous segment, those hyper parameters were the <span class="math inline">\mu_0</span>, the <span class="math inline">\sigma_0</span>, the <span class="math inline">\nu_0</span> and the <span class="math inline">\beta_0</span>.</p>
<p>We could specify just fixed numeric values for those, or we could learn them from the data and model them using additional prior distributions for those variables to make this a hierarchical model.</p>
<p>One reason we might do this is if the data are hierarchically organized so that the observations are naturally grouped together. We will examine these types of hierarchical models in depth later in the course.</p>
<p>Another simple example of a hierarchical model is one you saw already in the previous course.</p>
<p>Letâ€™s write it as <span class="math inline">y_i \mid \mu,\sigma^2</span>, so this is just like the model from the previous lesson, will be independent and identically distributed normal with a mean <span class="math inline">\mu</span> and a variance, <span class="math inline">\sigma^2</span>. The next step, instead of doing independent priors for <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, weâ€™re going to have the prior for <span class="math inline">\mu</span> depend on the value of <span class="math inline">\sigma^2</span>. That is given <span class="math inline">\sigma^2</span>, <span class="math inline">\mu</span> follows a normal distribution with mean <span class="math inline">\mu</span> naught, just some hyper parameter that youâ€™re going to chose. And the variance of this prior will be <span class="math inline">\sigma^2</span>, this parameter, divided by omega naught. Another hyper parameter that will scale it.</p>
<p>We now have a joint distribution of y and <span class="math inline">\mu</span> given <span class="math inline">\sigma^2</span> So finally, we need to complete the model with the prior for <span class="math inline">\sigma^2</span>. Weâ€™ll use our standard inverse gamma with the same hyper parameters as last time. This model has three layers. And <span class="math inline">\mu</span> depends on sigma right here. The graphical representation for this model looks like this. We start with the variables that donâ€™t depend on anything else. So that would be <span class="math inline">\sigma^2</span> and move down the chain.</p>
<p>So here, the next variable is <span class="math inline">\mu</span> which depends on <span class="math inline">\sigma^2</span>. And then dependent on both, we have the yiâ€™s. We use a double circle because the yiâ€™s are observed, their data, and weâ€™re going to assume that theyâ€™re exchangeable. So letâ€™s put them on a plate here for i in 1 to n The distribution of yi depends on both <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, so weâ€™ll draw curves connecting those pieces there. To simulate hypothetical data from this model, we would have to first draw from the distribution of the prior for <span class="math inline">\sigma^2</span>. Then the distribution for mu which depends on <span class="math inline">\sigma^2</span>. And once weâ€™ve drawn both of these, then we can draw random draws from the yâ€™s, which of course depends on both of those. With multiple levels, this is an example of a hierarchical model. Once we have a model specification, we can write out what the full posterior distribution for all the parameters given the data looks like. Remember that the numerator in Bayesâ€™ theorem is the joint distribution of all random quantities, all the nodes in this graphical representation over here from all of the layers. So for this model that we have right here, we have a joint distribution thatâ€™ll look like this. Weâ€™re going to write the joint distribution of everything y1 up to yn, <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, Using the chain rule of probability, weâ€™re going to multiply all of the distributions in the hierarchy together. So letâ€™s start with the likelihood piece. And weâ€™ll multiply that by the next layer, the distribution of mu, given <span class="math inline">\sigma^2</span>. And finally, with the prior for sigma squared. So what do these expressions right here look like? The likelihood right here in this level because theyâ€™re all independent will be a product of normal densities. So weâ€™re going to multiply the normal density for each yi, Given those parameters. This, again, is shorthand right here for the density of a normal distribution. So that represents this piece right here. The conditional prior of <span class="math inline">\mu</span> given sigma squared is also a normal. So weâ€™re going to multiply this by a normal distribution of mu, where its parameters are <span class="math inline">\mu</span> naught and sigma squared over omega naught. And finally, we have the prior for sigma squared. Weâ€™ll multiply by the density of an inverse gamma for <span class="math inline">\sigma^2</span> given the hyper parameters <span class="math inline">\mu</span> naught, sorry, that is given, the hyper parameters <span class="math inline">\mu</span> naught and and beta naught. What we have right here is the joint distribution of everything. It is the numerator in Bayes theorem. Letâ€™s remind ourselves really fast what Bayes theorem looks like again. We have that the posterior distribution of the parameter given the data is equal to the likelihood, Times the prior. Over the same thing again. So this gives us in the numerator the joint distribution of everything which is what weâ€™ve written right here.</p>
<p>In Bayes theorem, the numerator and the denominator are the exact same expression accept that we integrate or marginalize over all of the parameters.</p>
<p>Because the denominator is a function of the yâ€™s only, which are known values, the denominator is just a constant number. So we can actually write the posterior distribution as being proportional to, this symbol right here represents proportional to. The joint distribution of the data and parameters, or the likelihood times the prior. The poster distribution is proportional to the joint distribution, or everything we have right here. In other words, what we have already written for this particular model is proportional to the posterior distribution of <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, given all of the data. The only thing missing in this expression right here is just some constant number that causes the expression to integrate to 1. If we can recognize this expression as being proportional to a common distribution, then our work is done, and we know what our posterior distribution is. This was the case for all models in the previous course. However, if we do not use conjugate priors or if the models are more complicated, then the posterior distribution will not have a standard form that we can recognize. Weâ€™re going to explore a couple of examples of this issue in the next segment.</p>
</section>
<section id="sec-c2l02-non-conjugate-models" class="level2 page-columns page-full" data-number="32.4">
<h2 data-number="32.4" class="anchored" data-anchor-id="sec-c2l02-non-conjugate-models"><span class="header-section-number">32.4</span> Non-conjugate models (Video)</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/c2l02-ss-03.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Non-conjugate models"><img src="images/c2l02-ss-03.png" class="img-fluid figure-img" style="width:53mm" alt="Non-conjugate models"></a></p>
<figcaption>Non-conjugate models</figcaption>
</figure>
</div></div><p>Weâ€™ll first look at an example of a one parameter model that is not conjugate.</p>
<div id="exp-company-personnel">
<section id="company-personnel" class="level4" data-number="32.4.0.1">
<h4 data-number="32.4.0.1" class="anchored" data-anchor-id="company-personnel"><span class="header-section-number">32.4.0.1</span> Company Personnel</h4>
<p>Suppose we have values that represent the percentage change in total personnel from last year to this year for, weâ€™ll say, ten companies. These companies come from a particular industry. Weâ€™re going to assume for now, that these are independent measurements from a normal distribution with a known variance equal to one, but an unknown mean.</p>
<p>So weâ€™ll say the percentage change in the total personnel for company I, given the unknown mean <span class="math inline">\mu</span> will be distributed normally with mean <span class="math inline">\mu</span>, and weâ€™re just going to use variance 1.</p>
<p>In this case, the unknown mean could represent growth for this particular industry.</p>
<p>Itâ€™s the average of the growth of all the different companies. The small variance between the companies and percentage growth might be appropriate if the industry is stable.</p>
<p>We know that the <strong>conjugate</strong> <strong>prior</strong> for <span class="math inline">\mu</span> in this location would be a <strong>normal distribution</strong>.</p>
<p>But suppose we decide that our prior believes about <span class="math inline">\mu</span> are better reflected using a <strong>standard t distribution</strong> with <strong>one degree of freedom</strong>. So we could write that as the prior for <span class="math inline">\mu</span> is a t distribution with a location parameter 0. Thatâ€™s where the center of the distribution is. A scale parameter of 1 to make it the <strong>standard t-distribution</strong> similar to a standard normal, and 1 degree of freedom.</p>
<p>This particular prior distribution has heavier tails than the conjugate and normal distribution, which can more easily accommodate the possibility of extreme values for mu. It is centered on zero so, that apriori, there is a 50% chance that the growth is positive and a 50% chance that the growth is negative.</p>
</section>
</div>
<p>Recall that the posterior distribution of <span class="math inline">\mu</span> is proportional to the likelihood times the prior. Letâ€™s write the expression for that in this model. That is the posterior distribution for <span class="math inline">\mu</span> given the data <span class="math inline">y_1 \dots y_n</span> is going to be proportional to the likelihood.</p>
<p>It is a product from i equals 1 to n, in this case thatâ€™s 10.</p>
<p>Densities from a normal distribution.</p>
<p>Letâ€™s write the density from this particular normal distribution.</p>
<p>Is 1 over the square root of 2 pi.</p>
<p>E to the negative one-half.</p>
<p>Yi minus the mean squared, this is the normal density for each individual Yi and we multiplied it for likelihood.</p>
<p>The density for this t prior looks like this.</p>
<p>Itâ€™s 1 over pi times 1 plus <span class="math inline">\mu</span> squared.</p>
<p>This is the likelihood times the prior.</p>
<p>If we do a little algebra here, first of all, weâ€™re doing this up to proportionality.</p>
<p>So, constants being multiplied by this expression are not important.</p>
<p>The square root of 2 pi being multiplied n times, is just a constant number, and <span class="math inline">\pi</span> creates a constant number. So we will drop them in our next step.</p>
<p>So this is now proportional too, weâ€™re removing this piece and now weâ€™re going to use properties of exponents.</p>
<p>The product of exponents is the sum of the exponentiated pieces.</p>
<p>So we have the exponent of negative one-half times the sum from i equals 1 to n, of Yi minus <span class="math inline">\mu</span> squared.</p>
<p>And then weâ€™re dropping the pie over here, so times 1 plus <span class="math inline">\mu</span> squared.</p>
<p>Weâ€™re going to do a few more steps of algebra here to get a nicer expression for this piece.</p>
<p>But weâ€™re going to skip ahead to that.</p>
<p>Weâ€™ve now added these last two expressions.</p>
<p>To arrive at this expression here for the posterior, or whatâ€™s proportional to the posterior distribution.</p>
<p>This expression right here is almost proportional to a normal distribution except we have this 1 plus <span class="math inline">\mu</span> squared term in the denominator.</p>
<p>We know the posterior distribution up to a constant but we donâ€™t recognize its form as a standard distribution.</p>
<p>That we can integrate or simulate from, so weâ€™ll have to do something else.</p>
<p>Letâ€™s move on to our second example. For a two parameter example, weâ€™re going to return to the case where we have a normal likelihood.</p>
<p>And weâ€™re now going to estimate <span class="math inline">\mu</span> and <span class="math inline">\sigma^2</span>, because theyâ€™re both unknown.</p>
<p>Recall that if <span class="math inline">\sigma^2</span> were known, the conjugate prior from <span class="math inline">\mu</span> would be a normal distribution.</p>
<p>And if <span class="math inline">\mu</span> were known, the conjugate prior we could choose for <span class="math inline">\sigma^2</span> would be an inverse gamma.</p>
<p>We saw earlier that if you include <span class="math inline">\sigma^2</span> in the prior for <span class="math inline">\mu</span>, and use the hierarchical model that we presented earlier, that model would be conjugate and have a closed form solution. However, in the more general case that we have right here, the posterior distribution does not appear as a distribution that we can simulate or integrate.</p>
<p>Challenging posterior distributions like these ones and most others that weâ€™ll encounter in this course kept Bayesian in methods from entering the main stream of statistics for many years. Since only the simplest problems were tractable. However, computational methods invented in the 1950â€™s, and implemented by statisticians decades later, revolutionized the field. We do have the ability to simulate from the posterior distributions in this lesson as well as for many other more complicated models.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script type="text/javascript">

// replace cmd keyboard shortcut w/ control on non-Mac platforms
const kPlatformMac = typeof navigator !== 'undefined' ? /Mac/.test(navigator.platform) : false;
if (!kPlatformMac) {
   var kbds = document.querySelectorAll("kbd")
   kbds.forEach(function(kbd) {
      kbd.innerHTML = kbd.innerHTML.replace(/âŒ˜/g, 'âŒƒ');
   });
}

// tweak headings in pymd
document.querySelectorAll(".pymd span.co").forEach(el => {
   if (!el.innerText.startsWith("#|")) {
      el.style.fontWeight = 1000;
   }
});

</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./C2-L01.html" class="pagination-link" aria-label="Statistical Modeling - M1L1">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Statistical Modeling - M1L1</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./C2-L03.html" class="pagination-link" aria-label="Monte Carlo estimation - M1L3">
        <span class="nav-page-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Monte Carlo estimation - M1L3</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "M1L2 - Bayesian Modeling"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Bayesian Statistics: Techniques and Models"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">  - Bayesian Statistics</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - Monte Carlo Estimation</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  - Statistical Modeling</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Components of a Bayesian Model (Video) {#sec-c2l02-components}</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="al">![a Bayesian Model](images/c2l02-ss-01.png)</span>{.column-margin width="53mm"}</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>In lesson one, we defined **a statistical model** as *a mathematical structure used to imitate or approximate the data generating process*. </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>It incorporates uncertainty and variability using the theory of probability. A model could be very simple, involving only one variable.</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>::: {#exm-heights-of-men}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## heights of men</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>Suppose our data consists of the heights of $N=15$ adult men. <span class="co">[</span><span class="ot">heights of $N=15$ men</span><span class="co">]</span>{.column-margin}. Clearly it would be very expensive or even impossible to collect the genetic information that fully explains the variability in these men's heights. We only have the height measurements available to us. To account for the variability, we might assume that the men's heights follow a normal distribution.</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>So we could write the model like this: <span class="co">[</span><span class="ot"> $y_i= \mu + \epsilon_i$ </span><span class="co">]</span>{.column-margin} where $y_i$ will represent the height for person i, i will be our index. This will be equal to a constant, a number $\mu$ which will represents the mean for all men plus $\epsilon_i$. the individual error term for individual i.</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">$\epsilon_i \stackrel{iid}\sim N(0,\sigma^2) \  i\in 1 \dots N$ </span><span class="co">]</span>{.column-margin} We're going to assume that $\epsilon_i$ comes from a normal distribution with mean zero and variance $\sigma^2$. We are also going to assume that these epsilons are independent and identically distributed from this normal distribution. This is also for i equal to 1 up to N which will be 15 in our case. Equivalently we could write this model directly for the $y_i$ themselves.</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">$y_i \stackrel{iid}\sim N(\mu,\sigma^2) \ i \in 1 \dots N$ </span><span class="co">]</span>{.column-margin} So each $y_i$ comes from a normal distribution independent and identically distributed with the normal distribution. With mean $\mu$ and variance $\sigma^2$. This specifies a probability distribution and a model for the data.</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### heights of men {.unnumbered}</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>y_i&amp;= \mu+\epsilon_i,</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \epsilon_i &amp;\stackrel{iid}\sim N(0,\sigma^2) </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>another way to write this:</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>y_i &amp;\stackrel{iid}\sim N(\mu,\sigma^2)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>If we know the values of $\mu$ and $\sigma$. It also suggests how we might generate more fake data that behaves similarly to our original data set.</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>A model can be as simple as the one right here or as complicated and sophisticated as we need to capture the behavior of the data. So far, this model is the same for Frequentists and Bayesians.</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>As you may recall from the previous course. The frequentist approach to fitting this model right here would be to consider $\mu$ and $\sigma$ to be fixed but unknown constants, and then we would estimate them. To calculate our uncertainty in those estimates a frequentist approach would consider how much the estimates of $\mu$ and $\sigma$ might change if we were to repeat the sampling process and obtain another sample of 15 men, over, and over.</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="al">![Components of a Bayesian Model](images/c2l02-ss-02.png)</span>{#fig-heights-of-men .column-margin width="53mm"}</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>The Bayesian approach, the one we're going to take in this class. Tackles our uncertainty in $\mu$ and $\sigma^2$ with probability directly. By treating them as random variables with their own probability distributions. These are often called **priors**, and they complete a Bayesian model.</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>In the rest of this segment, we're going to review three key components of Bayesian models. That were used extensively in the previous course The three primary components of Bayesian models that we often work with are the **likelihood**, **the prior** and **the posterior**.</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot"> $\mathbb{P}r(y\mid \theta)\ \text{(likelihood)}$ </span><span class="co">]</span>{.column-margin} **The likelihood** is the probabilistic model for the data. It describes how, given the unknown parameters, the data might be generated. We're going to call unknown parameter theta right here. Also, in this expression, you might recognize this from the previous class, as describing a probability distribution.</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot"> $\mathbb{P}r(\theta)\ \text{(prior)}$ </span><span class="co">]</span>{.column-margin} **The prior**, the next step, is the probability distribution that characterizes our uncertainty with the parameter theta. We're going to write it as $\mathbb{P}r(\theta)$. It's not the same distribution as this one. We're just using this notation p to represent the probability distribution of theta. By specifying a likelihood and a prior.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot"> $\mathbb{P}r(y,\theta) = \mathbb{P}r(\theta)\mathbb{P}r(y\mid\theta) \ \text{(joint probability)}$ </span><span class="co">]</span>{.column-margin} We now have a **joint probability model** for both the knowns, the data, and the unknowns, the parameters. We can see this by using the chain rule of probability. If we wanted the joint distribution of both the data and the parameters theta. Using the chain rule of probability, we could start with the distribution of $\theta$. And multiply that by the probability or the distribution of $y$ given theta. That gives us an expression for the joint distribution. **However if we're going to make inferences about data and we already know the values of** $y$, we don't need the *joint distribution*, what we need is the *posterior distribution*.</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">$\mathbb{P}r(\theta \mid y)\ \text{(posterior)}$</span><span class="co">]</span>{.column-margin} The **posterior distribution** is the distribution of $\mathbb{P}r(\theta \mid y)$, i.e. $\theta$ given $y$. We can obtain this expression using the laws of conditional probability and specifically using Bayes' theorem.</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>\mathbb{P}r(\theta \mid y) &amp;= \frac{\mathbb{P}r(\theta,y)}{\mathbb{P}r(y)} </span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{\mathbb{P}r(\theta,y)}{\int \mathbb{P}r(\theta,y)}</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{\mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)}{\int \mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)\ d\theta}</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>We start with the definition of conditional probability (1). The conditional distribution, $\mathbb{P}r(\theta \mid y)$ is the ratio of the *joint distribution* of $\theta$ and $y$, i.e. $\mathbb{P}r(\theta,y)$; with the *marginal distribution* of $y$, $\mathbb{P}r(y)$.</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">How do we get the marginal distribution of y?</span><span class="co">]</span>{.column-margin} We start with the **joint distribution** like we have on top, and we *integrate out* or *marginalize* over the values of theta (2)</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>To make this look like the Bayes theorem that we're familiar with the joint distribution can be rewritten as the product of the prior and the likelihood. We start with the likelihood, because that's how we usually write Bayes' theorem. We have the same thing in the denominator here. But we're going to integrate over the values of theta. These integrals are replaced by summations if we know that $\theta$ is a discrete random variable. The *marginal distribution* is another important piece which we may use when we more advanced Bayesian modeling.</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>The **posterior distribution** is our primary tool for achieving the statistical modeling objectives from lesson one.</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="fu">### Anatomy of a posterior probability</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>  \begin{aligned}</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>  &amp;\mathbb{P}r(y\mid \theta) &amp;&amp; (likelihood) <span class="sc">\\</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a> &amp;  \mathbb{P}r(\theta) &amp;&amp; (prior) <span class="sc">\\</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>   \mathbb{P}r(y,\theta) &amp;= \mathbb{P}r(\theta)\mathbb{P}r(y|\theta) &amp;&amp;(joint\ distribution) <span class="sc">\\</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>   \mathbb{P}r(\theta \mid y) &amp;= \frac{\mathbb{P}r(\theta,y)}{\mathbb{P}r(y)} &amp;&amp; (conditional\ probability) <span class="sc">\\</span></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a> &amp;= \frac{\mathbb{P}r(\theta,y)}{\int \mathbb{P}r(\theta,y)} <span class="sc">\\</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a> &amp;= \frac{\mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)}{\int \mathbb{P}r(y \mid \theta)\ \mathbb{P}r(\theta)\ d\theta} <span class="sc">\\</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>$$ {#eq-posterior-anatomy}</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>::: {#qst-question}</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>Whereas non-Bayesian approaches consider a probability model for the data only, the hallmark characteristic of Bayesian models is that they specify a joint probability distribution for both data *and* parameters. How does the Bayesian paradigm leverage this additional assumption?</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>::: {#solution}</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[ ]</span> **This allows us to make probabilistic assessments about how likely our particular data outcome is under any parameter setting.**</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[ ]</span> **This allows us to select the most accurate prior distribution.**</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[ ]</span> **This allows us to make probabilistic assessments about hypothetical data outcomes given particular parameter values.**</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="va">[x]</span> **This allows us to use the laws of conditional probability to describe our updated information about parameters given the data.**</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Specification  (Video) {#sec-c2l02-model-specification}</span></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="al">![Model specification](images/c2l02-ss-05.png)</span>{.column-margin width="53mm"} </span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>Before fitting any model we first need to specify all of its components.</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-graphical-model</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: The graphical model specification for the height model</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-dpi: 300</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: true</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> daft <span class="im">as</span> daft</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>pgm <span class="op">=</span> daft.PGM(</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>  <span class="co">#[6, 3.2],</span></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>  node_unit<span class="op">=</span><span class="fl">1.0</span>,alternate_style<span class="op">=</span><span class="st">'outer'</span>,dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"mu"</span>, <span class="vs">r"</span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">0.0</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"sigma"</span>, <span class="vs">r"</span><span class="dv">$\s</span><span class="vs">igma</span><span class="dv">^</span><span class="vs">2</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">1.0</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"y"</span>, <span class="vs">r"</span><span class="dv">$</span><span class="vs">y_i</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, observed<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"mu"</span>, <span class="st">"y"</span>)<span class="op">;</span></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"sigma"</span>, <span class="st">"y"</span>)<span class="op">;</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>pgm.add_plate([<span class="op">-</span><span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">2.5</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">n = 1</span><span class="dv">...</span><span class="vs">N</span><span class="dv">$</span><span class="vs">"</span>, position<span class="op">=</span><span class="st">"bottom right"</span>)</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>pgm.render()</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hierarchical representation</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>One convenient way to do this is to write down the **hierarchical form of the model**. By hierarchy, we mean that the model is specified in steps or in layers. We usually start with the model for the data directly, or the likelihood. Let's write, again, the model from the previous lesson.</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>We had the height for person i, given our parameters $\mu$ and $\sigma^2$, so conditional on those parameters, $y_i$ came from a normal distribution that was independent and identically distributed, where the normal distribution has mean $\mu$ and variance $\sigma^2$, and we're doing this for individuals 1 up to N, which was 15 in this example. <span class="co">[</span><span class="ot">$y_i | \mu,\sigma^2 \stackrel{iid}\sim N(\mu,\sigma^2) \ for\ i \in 1,\dots,15$</span><span class="co">]</span>{.column-margin}</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>The next level that we need is the prior distribution from $\mu$ and $\sigma^2$. For now we're going to say that they're independent priors. So that our prior from $\mu$ and $\sigma^2$ is going to be able to factor Into the product of two independent priors. <span class="co">[</span><span class="ot">$\mathbb{P}r(\mu,\sigma^2)~=~\mathbb{P}r(\mu)\mathbb{P}r(\sigma^2)\ (independence)$</span><span class="co">]</span>{.column-margin} We can assume independents in the prior and still get dependents in the posterior distribution.</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>In the previous course we learned that *the conjugate prior* for $\mu$, if we know the value of $\sigma^2$, is a *normal distribution*, and that the conjugate prior for $\sigma^2$ when $\mu$ is known is the *Inverse Gamma distribution*.</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>Let's suppose that our prior distribution for $\mu$ is a normal distribution where mean will be $\mu_0$. <span class="co">[</span><span class="ot">$\mu \sim N(\mu_0,\sigma^2_0)$</span><span class="co">]</span>{.column-margin} This is just some number that you're going to fill in here when you decide what the prior should be. Mean $\mu_0$, and less say $\sigma^2_0$ would be the variance of that prior.</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>The prior for $\sigma^2$ will be Inverse Gamma <span class="co">[</span><span class="ot">$\sigma^2 \sim \mathcal{IG}(\nu_0,\beta_0)$</span><span class="co">]</span>{.column-margin} which has two parameters:</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>It has a *shape parameter*, we're going to call that $\nu_0$, and</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>It has a *scale parameter*, we'll call that $\beta_0$.</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>We need to choose values for these hyper-parameters here. But we do now have a complete Bayesian model.</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>We now introduce some new ideas that were not presented in the previous course.</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Hierarchical representation</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>By hierarchy, we mean that the model is specified in steps or in layers.</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>start with the model for the data, or the likelihood.</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>write the priors</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>add hyper-priors for the parameters of the priors.</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>More details can be seen on this <span class="co">[</span><span class="ot">wikipedia article</span><span class="co">](https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling)</span> and on this <span class="co">[</span><span class="ot">one</span><span class="co">](https://en.wikipedia.org/wiki/Multilevel_model)</span></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="fu">### Graphical representation</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>Another useful way to write out this model Is using what's called a **graphical representation**. To write a graphical representation, we're going to do the reverse order, we'll start with the priors and finish with the likelihood.</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>In the graphical representation we draw what are called nodes so this would be a node for mu. The **circle** means that the this is a random variable that has its own distribution. So $\mu$ with its prior will be represented with that. And then we also have $\sigma^2$. The next part of a graphical model is showing the dependence on other variables. Once we have the parameters, we can generate the data.</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>For example we have $y_1, \dots y_n$. These are also random variables, so we'll create these as nodes. And I'm going to double up the circle here to indicate that these nodes are observed, you see them in the data. So we'll do this for all of the $y$s here. And to indicate the dependence of the distributions of the $y$s on $\mu$ and $\sigma^2$, we're going to draw arrows. So $\mu$ influences the distribution of $y$ for each one of these $y$s. The same is true for sigma squared, the distribution of each $y$ depends on the distribution of $\sigma^2$. Again, these nodes right here, that are double-circled, mean that they've been observed. If they're shaded, which is the usual case, that also means that they're observed. The arrows indicate the dependence between the random variables and their distributions.</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>Notice that in this hierarchical representation, I wrote the dependence of the distributions also. We can simplify the graphical model by writing exchangeable random variables and I'll define exchangeable later.</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>We're going to write this using a representative of the $y$s here on what's called the **plate**. So I'm going to re draw this hierarchical structure, we have $\mu$ and $\sigma^2$. And we don't want to have to write all of these notes again. So I'm going to indicate that there are n of them, And I'm just going to draw one representative, $y_i$. And they depend on $\mu$ and $\sigma^2$. To write a model like this, we must assume that the $y$s are *exchangeable*. That means that the distribution for the $y$s does not change if we were to switch the index label like the $i$ on the $y$ there. So, if for some reason, we knew that one of the $y$s was different from the other $y$s in its distribution, and if we also know which one it is, then we would need to write a separate node for it and not use a plate like we have here.</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Graphical representation</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-graphical-model-posterior</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: pgm-posterior</span></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-dpi: 300</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: true</span></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> daft <span class="im">as</span> daft</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>pgm <span class="op">=</span> daft.PGM(</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>  <span class="co">#[6, 3.2],</span></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>  node_unit<span class="op">=</span><span class="fl">1.0</span>,alternate_style<span class="op">=</span><span class="st">'outer'</span>,dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"sigma"</span>, <span class="vs">r"</span><span class="dv">$\s</span><span class="vs">igma</span><span class="dv">^</span><span class="vs">2</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">0.</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"mu"</span>, <span class="vs">r"</span><span class="dv">$\m</span><span class="vs">u</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">1.</span>, <span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>pgm.add_node(<span class="st">"y"</span>, <span class="vs">r"</span><span class="dv">$</span><span class="vs">y_i</span><span class="dv">$</span><span class="vs">"</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, observed<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"sigma"</span>, <span class="st">"mu"</span>)<span class="op">;</span></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"sigma"</span>, <span class="st">"y"</span>, )<span class="op">;</span></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>pgm.add_edge(<span class="st">"mu"</span>, <span class="st">"y"</span>)<span class="op">;</span></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>pgm.add_plate([<span class="op">-</span><span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">2.5</span>, <span class="dv">1</span>], label<span class="op">=</span><span class="vs">r"</span><span class="dv">$</span><span class="vs">n = 1</span><span class="dv">...</span><span class="vs">N</span><span class="dv">$</span><span class="vs">"</span>, position<span class="op">=</span><span class="st">"bottom right"</span>)</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>pgm.render()</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>In the graphical representation we start at the top by drawing:</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>circle nodes for the hyperparameters.</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>arrows indicating that they determine the</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>nodes for the priors.</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>nodes for the RVs (doubled circles)</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>plates (rectangles) indicating RVs that are exchangeable. We add an index to the corner of the plate to indicate the amount of replicated RVs</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>More details can be seen on this <span class="co">[</span><span class="ot">wikipedia article</span><span class="co">](https://en.wikipedia.org/wiki/Plate_notation)</span></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>Both the hierarchical and graphical representations show how you could hypothetically **simulate data** from this model. You start with the variables that don't have any dependence on any other variables. You would simulate those, and then given those draws, you would simulate from the distributions for these other variables further down the chain.</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>This is also how you might simulate from a prior predictive distribution.</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a><span class="fu">## Posterior derivation  (Video) {#sec-c2l02-posterior-derivation}</span></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a><span class="al">![Posterior derivation](images/c2l02-ss-04.png)</span>{.column-margin width="53mm"}</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>So far, we've only drawn the model with two levels. But in reality, there's nothing that will stop us from adding more layers.</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>For example, instead of fixing the values for the hyper parameters in the previous segment, those hyper parameters were the $\mu_0$, the $\sigma_0$, the $\nu_0$ and the $\beta_0$.</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>We could specify just fixed numeric values for those, or we could learn them from the data and model them using additional prior distributions for those variables to make this a hierarchical model.</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>One reason we might do this is if the data are hierarchically organized so that the observations are naturally grouped together. We will examine these types of hierarchical models in depth later in the course.</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>Another simple example of a hierarchical model is one you saw already in the previous course.</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>Let's write it as $y_i \mid \mu,\sigma^2$, so this is just like the model from the previous lesson, will be independent and identically distributed normal with a mean $\mu$ and a variance, $\sigma^2$. The next step, instead of doing independent priors for $\mu$ and $\sigma^2$, we're going to have the prior for $\mu$ depend on the value of $\sigma^2$. That is given $\sigma^2$, $\mu$ follows a normal distribution with mean $\mu$ naught, just some hyper parameter that you're going to chose. And the variance of this prior will be $\sigma^2$, this parameter, divided by omega naught. Another hyper parameter that will scale it.</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>We now have a joint distribution of y and $\mu$ given $\sigma^2$ So finally, we need to complete the model with the prior for $\sigma^2$. We'll use our standard inverse gamma with the same hyper parameters as last time. This model has three layers. And $\mu$ depends on sigma right here. The graphical representation for this model looks like this. We start with the variables that don't depend on anything else. So that would be $\sigma^2$ and move down the chain.</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>So here, the next variable is $\mu$ which depends on $\sigma^2$. And then dependent on both, we have the yi's. We use a double circle because the yi's are observed, their data, and we're going to assume that they're exchangeable. So let's put them on a plate here for i in 1 to n The distribution of yi depends on both $\mu$ and $\sigma^2$, so we'll draw curves connecting those pieces there. To simulate hypothetical data from this model, we would have to first draw from the distribution of the prior for $\sigma^2$. Then the distribution for mu which depends on $\sigma^2$. And once we've drawn both of these, then we can draw random draws from the y's, which of course depends on both of those. With multiple levels, this is an example of a hierarchical model. Once we have a model specification, we can write out what the full posterior distribution for all the parameters given the data looks like. Remember that the numerator in Bayes' theorem is the joint distribution of all random quantities, all the nodes in this graphical representation over here from all of the layers. So for this model that we have right here, we have a joint distribution that'll look like this. We're going to write the joint distribution of everything y1 up to yn, $\mu$ and $\sigma^2$, Using the chain rule of probability, we're going to multiply all of the distributions in the hierarchy together. So let's start with the likelihood piece. And we'll multiply that by the next layer, the distribution of mu, given $\sigma^2$. And finally, with the prior for sigma squared. So what do these expressions right here look like? The likelihood right here in this level because they're all independent will be a product of normal densities. So we're going to multiply the normal density for each yi, Given those parameters. This, again, is shorthand right here for the density of a normal distribution. So that represents this piece right here. The conditional prior of $\mu$ given sigma squared is also a normal. So we're going to multiply this by a normal distribution of mu, where its parameters are $\mu$ naught and sigma squared over omega naught. And finally, we have the prior for sigma squared. We'll multiply by the density of an inverse gamma for $\sigma^2$ given the hyper parameters $\mu$ naught, sorry, that is given, the hyper parameters $\mu$ naught and and beta naught. What we have right here is the joint distribution of everything. It is the numerator in Bayes theorem. Let's remind ourselves really fast what Bayes theorem looks like again. We have that the posterior distribution of the parameter given the data is equal to the likelihood, Times the prior. Over the same thing again. So this gives us in the numerator the joint distribution of everything which is what we've written right here.</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>In Bayes theorem, the numerator and the denominator are the exact same expression accept that we integrate or marginalize over all of the parameters.</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>Because the denominator is a function of the y's only, which are known values, the denominator is just a constant number. So we can actually write the posterior distribution as being proportional to, this symbol right here represents proportional to. The joint distribution of the data and parameters, or the likelihood times the prior. The poster distribution is proportional to the joint distribution, or everything we have right here. In other words, what we have already written for this particular model is proportional to the posterior distribution of $\mu$ and $\sigma^2$, given all of the data. The only thing missing in this expression right here is just some constant number that causes the expression to integrate to 1. If we can recognize this expression as being proportional to a common distribution, then our work is done, and we know what our posterior distribution is. This was the case for all models in the previous course. However, if we do not use conjugate priors or if the models are more complicated, then the posterior distribution will not have a standard form that we can recognize. We're going to explore a couple of examples of this issue in the next segment.</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a><span class="fu">## Non-conjugate models  (Video) {#sec-c2l02-non-conjugate-models}</span></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a><span class="al">![Non-conjugate models](images/c2l02-ss-03.png)</span>{.column-margin width="53mm"}</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>We'll first look at an example of a one parameter model that is not conjugate.</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>::: {#exp-company-personnel}</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Company Personnel</span></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>Suppose we have values that represent the percentage change in total personnel from last year to this year for, we'll say, ten companies. These companies come from a particular industry. We're going to assume for now, that these are independent measurements from a normal distribution with a known variance equal to one, but an unknown mean.</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>So we'll say the percentage change in the total personnel for company I, given the unknown mean $\mu$ will be distributed normally with mean $\mu$, and we're just going to use variance 1.</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>In this case, the unknown mean could represent growth for this particular industry.</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>It's the average of the growth of all the different companies. The small variance between the companies and percentage growth might be appropriate if the industry is stable.</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>We know that the **conjugate** **prior** for $\mu$ in this location would be a **normal distribution**.</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>But suppose we decide that our prior believes about $\mu$ are better reflected using a **standard t distribution** with **one degree of freedom**. So we could write that as the prior for $\mu$ is a t distribution with a location parameter 0. That's where the center of the distribution is. A scale parameter of 1 to make it the **standard t-distribution** similar to a standard normal, and 1 degree of freedom.</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>This particular prior distribution has heavier tails than the conjugate and normal distribution, which can more easily accommodate the possibility of extreme values for mu. It is centered on zero so, that apriori, there is a 50% chance that the growth is positive and a 50% chance that the growth is negative.</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>Recall that the posterior distribution of $\mu$ is proportional to the likelihood times the prior. Let's write the expression for that in this model. That is the posterior distribution for $\mu$ given the data $y_1 \dots y_n$ is going to be proportional to the likelihood.</span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>It is a product from i equals 1 to n, in this case that's 10.</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>Densities from a normal distribution.</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>Let's write the density from this particular normal distribution.</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>Is 1 over the square root of 2 pi.</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>E to the negative one-half.</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>Yi minus the mean squared, this is the normal density for each individual Yi and we multiplied it for likelihood.</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>The density for this t prior looks like this.</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>It's 1 over pi times 1 plus $\mu$ squared.</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>This is the likelihood times the prior.</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>If we do a little algebra here, first of all, we're doing this up to proportionality.</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>So, constants being multiplied by this expression are not important.</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>The square root of 2 pi being multiplied n times, is just a constant number, and $\pi$ creates a constant number.</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>So we will drop them in our next step.</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>So this is now proportional too, we're removing this piece and now we're going to use properties of exponents.</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>The product of exponents is the sum of the exponentiated pieces.</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>So we have the exponent of negative one-half times the sum from i equals 1 to n, of Yi minus $\mu$ squared.</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>And then we're dropping the pie over here, so times 1 plus $\mu$ squared.</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>We're going to do a few more steps of algebra here to get a nicer expression for this piece.</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>But we're going to skip ahead to that.</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>We've now added these last two expressions.</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>To arrive at this expression here for the posterior, or what's proportional to the posterior distribution.</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>This expression right here is almost proportional to a normal distribution except we have this 1 plus $\mu$ squared term in the denominator.</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>We know the posterior distribution up to a constant but we don't recognize its form as a standard distribution.</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>That we can integrate or simulate from, so we'll have to do something else.</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>Let's move on to our second example. For a two parameter example, we're going to return to the case where we have a normal likelihood.</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>And we're now going to estimate $\mu$ and $\sigma^2$, because they're both unknown.</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>Recall that if $\sigma^2$ were known, the conjugate prior from $\mu$ would be a normal distribution.</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>And if $\mu$ were known, the conjugate prior we could choose for $\sigma^2$ would be an inverse gamma.</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>We saw earlier that if you include $\sigma^2$ in the prior for $\mu$, and use the hierarchical model that we presented earlier, that model would be conjugate and have a closed form solution. However, in the more general case that we have right here, the posterior distribution does not appear as a distribution that we can simulate or integrate.</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>Challenging posterior distributions like these ones and most others that we'll encounter in this course kept Bayesian in methods from entering the main stream of statistics for many years. Since only the simplest problems were tractable. However, computational methods invented in the 1950's, and implemented by statisticians decades later, revolutionized the field. We do have the ability to simulate from the posterior distributions in this lesson as well as for many other more complicated models.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"fade","descPosition":"bottom","loop":true,"openEffect":"fade","selector":".lightbox","skin":"my-css-class"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>