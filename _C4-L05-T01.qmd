We can now think about Bayesian in
the general case of an AR(P) process. In the Bayesian in case, we have
two choices to make first what kind of likelihood we are going to be using and
then what kind of prior distribution we're going to be using for
the model parameters. I'm going to discuss the case in which
we use the conditional likelihood. We have to condition this time on
the first p values of the process. And then I'm also going to discuss
the case in which we use the reference prior for the model parameters. As in the AR(1) case,
there is a correspondence between that conditional likelihood setting and
the regression model. So recall we can write down the process, Like this. So, And the assumption is that
the epsilon t are iid normally distributed random variables
with zero mean and variance v. So that's the variance of the process. The parameters of the process
are going to be all the phi's, And the variance of the process. So we want to make inference
on those parameters. What we know given the conditional
dependency structure and past values of the series,
is that if I think about the distribution of yt,
given yt -1 all the way to y t-p. And then also given the phi's, And v, this is a normally distributed, Random variable with the mean is
going to be the sum, Of these. Past values
weighted by the phi's, And then I have variance v here. So now that I use this conditionally
independent structure, I can write a likelihood function
as the product of these terms. So if I think about my density here for y p+1 all the way to capital T. So let's assume that we have capital T,
we have observed capital T. Data points and
then we are conditioning on y1. And p, all the first p observations and all the phi's again and v. I can write this as
the product of all these terms. All these are are going to have a normal
density given by this expression. So as we said before,
there is a correspondence between this likelihood function,
the conditional likelihood and a regression model of
the form y=Xbeta +epsilon. Where y here is a vector,
X matrix, beta is a vector and epsilon is another vector with this
vector being multivariate normal. And then we're going to have
a v times the identity. So I can write down this expression and make a a connection with this
model by simply setting y. Again, because I'm conditioning
on the first p values. I'm going to start with yp+1, yp+2 and so on all the way to yT. Then here my beta is the vector
that goes with the coefficients. In this case, the linear
component has to do with the phi's. So it's going to be my
beta with all the phi's. And those are the AR parameters
that we want to estimate. And then I'm going to have an X matrix. The design matrix here for the linear
regression in the case of an AR(P), again, if I think about the first row
is going to be related to the yp+1. So we are regressing on the past p values. So it's going to go from yp, All the way, so
that's the first row, to y1. Then for y p+2,
I'm going to have something similar and then I go all the way down to yT. And so I'm going to have yT -1, -2 all the way to yT-p. So this is my X matrix. So as you know, I can find my maximum
likelihood estimator for this process. Which I'm just simply going to
call beta hat, its going to be assuming that
the design matrix is full rank. We can write it down like this. So we can simply just plug in this X and
this y vector to obtain the maximum likelihood estimator of the model
parameters for the AR coefficients. And then using these results, we can also write the posterior
distribution using a reference prior. So again, this gives me the likelihood. The reference prior assumes that we
are going to use a prior of this form. And in this case, the Bayesian inference can be done by writing down the density of the beta given all the y's and v. This is going to be normally distributed. The mean it's going to be beta hat, which is the maximum likelihood estimator. And then I have my v times
X transpose X inverse. And then I have my
marginal distribution for v given all the observations here,
it's going to be an inverse gamma. And then the parameters for the inverse gamma distribution
are going to be related to, again, you think about what is
the dimension of the vector here y. In the case of the AR(p) I have
something that is dimension T- p. So that's the first dimension
that we are going to consider. But then we also have to subtract the
dimension of the beta vector which is p. So I'm going to have T -p-p,
which gives me my 2p here. And then I have the other
component is my s square over 2. So once again, if I want to do
simulation based posterior inference, I can simply get a sample of v from
this inverse gamma distribution. And then I plug in that sample here and
I get a sample for the AR coefficients. So I can get full posterior 
inference in this way.

